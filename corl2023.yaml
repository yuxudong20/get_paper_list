
- title: 'Expansive Latent Planning for Sparse Reward Offline Reinforcement Learning'
  abstract: 'Sampling-based motion planning algorithms excel at searching global solution paths in geometrically complex settings. However, classical approaches, such as RRT, are difficult to scale beyond low-dimensional search spaces and rely on privileged knowledge e.g. about collision detection and underlying state distances. In this work, we take a step towards the integration of sampling-based planning into the reinforcement learning framework to solve sparse-reward control tasks from high-dimensional inputs. Our method, called VELAP, determines sequences of waypoints through sampling-based exploration in a learned state embedding. Unlike other sampling-based techniques, we iteratively expand a tree-based memory of visited latent areas, which is leveraged to explore a larger portion of the latent space for a given number of search iterations. We demonstrate state-of-the-art results in learning control from offline data in the context of vision-based manipulation under sparse reward feedback. Our method extends the set of available planning tools in model-based reinforcement learning by adding a latent planner that searches globally for feasible paths instead of being bound to a fixed prediction horizon.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/gieselmann23a.html
  PDF: https://proceedings.mlr.press/v229/gieselmann23a/gieselmann23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-gieselmann23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Robert
    family: Gieselmann
  - given: Florian T.
    family: Pokorny
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1-22
  id: gieselmann23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1
  lastpage: 22
  published: 2023-12-02 00:00:00 +0000
- title: 'Expansive Latent Planning for Sparse Reward Offline Reinforcement Learning'
  abstract: 'Sampling-based motion planning algorithms excel at searching global solution paths in geometrically complex settings. However, classical approaches, such as RRT, are difficult to scale beyond low-dimensional search spaces and rely on privileged knowledge e.g. about collision detection and underlying state distances. In this work, we take a step towards the integration of sampling-based planning into the reinforcement learning framework to solve sparse-reward control tasks from high-dimensional inputs. Our method, called VELAP, determines sequences of waypoints through sampling-based exploration in a learned state embedding. Unlike other sampling-based techniques, we iteratively expand a tree-based memory of visited latent areas, which is leveraged to explore a larger portion of the latent space for a given number of search iterations. We demonstrate state-of-the-art results in learning control from offline data in the context of vision-based manipulation under sparse reward feedback. Our method extends the set of available planning tools in model-based reinforcement learning by adding a latent planner that searches globally for feasible paths instead of being bound to a fixed prediction horizon.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/gieselmann23a.html
  PDF: https://proceedings.mlr.press/v229/gieselmann23a/gieselmann23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-11-13-gieselmann23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Robert
    family: Gieselmann
  - given: Florian T.
    family: Pokorny
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1-22
  id: gieselmann23a
  issued:
    date-parts: 
      - 2023
      - 11
      - 13
  firstpage: 1
  lastpage: 22
  published: 2023-11-13 00:00:00 +0000
- title: 'SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning'
  abstract: 'Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a "semantic search" for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an "iterative replanning" pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/rana23a.html
  PDF: https://proceedings.mlr.press/v229/rana23a/rana23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-rana23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Krishan
    family: Rana
  - given: Jesse
    family: Haviland
  - given: Sourav
    family: Garg
  - given: Jad
    family: Abou-Chakra
  - given: Ian
    family: Reid
  - given: Niko
    family: Suenderhauf
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 23-72
  id: rana23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 23
  lastpage: 72
  published: 2023-12-02 00:00:00 +0000
- title: 'SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning'
  abstract: 'Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a "semantic search" for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an "iterative replanning" pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/rana23a.html
  PDF: https://proceedings.mlr.press/v229/rana23a/rana23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-11-13-rana23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Krishan
    family: Rana
  - given: Jesse
    family: Haviland
  - given: Sourav
    family: Garg
  - given: Jad
    family: Abou-Chakra
  - given: Ian
    family: Reid
  - given: Niko
    family: Suenderhauf
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 23-72
  id: rana23a
  issued:
    date-parts: 
      - 2023
      - 11
      - 13
  firstpage: 23
  lastpage: 72
  published: 2023-11-13 00:00:00 +0000
- title: 'Robot Parkour Learning'
  abstract: 'Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower low-cost quadrupedal robots to autonomously select and execute appropriate parkour skills to traverse challenging environments in the real world. Project website: https://robot-parkour.github.io/'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhuang23a.html
  PDF: https://proceedings.mlr.press/v229/zhuang23a/zhuang23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhuang23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ziwen
    family: Zhuang
  - given: Zipeng
    family: Fu
  - given: Jianren
    family: Wang
  - given: Christopher G.
    family: Atkeson
  - given: Sören
    family: Schwertfeger
  - given: Chelsea
    family: Finn
  - given: Hang
    family: Zhao
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 73-92
  id: zhuang23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 73
  lastpage: 92
  published: 2023-12-02 00:00:00 +0000
- title: 'Task-Oriented Koopman-Based Control with Contrastive Encoder'
  abstract: 'We present task-oriented Koopman-based control that utilizes end-to-end reinforcement learning and contrastive encoder to simultaneously learn the Koopman latent embedding, operator, and associated linear controller within an iterative loop. By prioritizing the task cost as the main objective for controller learning, we reduce the reliance of controller design on a well-identified model, which, for the first time to the best of our knowledge, extends Koopman control from low to high-dimensional, complex nonlinear systems, including pixel-based tasks and a real robot with lidar observations. Code and videos are available: https://sites.google.com/view/kpmlilatsupp/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/lyu23a.html
  PDF: https://proceedings.mlr.press/v229/lyu23a/lyu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-lyu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xubo
    family: Lyu
  - given: Hanyang
    family: Hu
  - given: Seth
    family: Siriya
  - given: Ye
    family: Pu
  - given: Mo
    family: Chen
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 93-105
  id: lyu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 93
  lastpage: 105
  published: 2023-12-02 00:00:00 +0000
- title: 'On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills'
  abstract: 'Despite impressive dexterous manipulation capabilities enabled by learning-based approaches, we are yet to witness widespread adoption beyond well-resourced laboratories. This is likely due to practical limitations, such as significant computational burden, inscrutable learned behaviors, sensitivity to initialization, and the considerable technical expertise required for implementation. In this work, we investigate the utility of Koopman operator theory in alleviating these limitations. Koopman operators are simple yet powerful control-theoretic structures to represent complex nonlinear dynamics as linear systems in higher dimensions. Motivated by the fact that complex nonlinear dynamics underlie dexterous manipulation, we develop a Koopman operator-based imitation learning framework to learn the desired motions of both the robotic hand and the object simultaneously. We show that Koopman operators are surprisingly effective for dexterous manipulation and offer a number of unique benefits. Notably, policies can be learned analytically, drastically reducing computation burden and eliminating sensitivity to initialization and the need for painstaking hyperparameter optimization. Our experiments reveal that a Koopman operator-based approach can perform comparably to state-of-the-art imitation learning algorithms in terms of success rate and sample efficiency, while being an order of magnitude faster. Policy videos can be viewed at https://sites.google.com/view/kodex-corl.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/han23a.html
  PDF: https://proceedings.mlr.press/v229/han23a/han23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-han23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yunhai
    family: Han
  - given: Mandy
    family: Xie
  - given: Ye
    family: Zhao
  - given: Harish
    family: Ravichandar
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 106-126
  id: han23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 106
  lastpage: 126
  published: 2023-12-02 00:00:00 +0000
- title: 'Rearrangement Planning for General Part Assembly'
  abstract: 'Most successes in autonomous robotic assembly have been restricted to single target or category. We propose to investigate general part assembly, the task of creating novel target assemblies with unseen part shapes. As a fundamental step to a general part assembly system, we tackle the task of determining the precise poses of the parts in the target assembly, which we term “rearrangement planning". We present General Part Assembly Transformer (GPAT), a transformer-based model architecture that accurately predicts part poses by inferring how each part shape corresponds to the target shape. Our experiments on both 3D CAD models and real-world scans demonstrate GPAT’s generalization abilities to novel and diverse target and part shapes.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/li23a.html
  PDF: https://proceedings.mlr.press/v229/li23a/li23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-li23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yulong
    family: Li
  - given: Andy
    family: Zeng
  - given: Shuran
    family: Song
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 127-143
  id: li23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 127
  lastpage: 143
  published: 2023-12-02 00:00:00 +0000
- title: 'Language-Guided Traffic Simulation via Scene-Level Diffusion'
  abstract: 'Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user’s query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effectiveness of our proposed method in generating realistic, query-compliant traffic simulations.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhong23a.html
  PDF: https://proceedings.mlr.press/v229/zhong23a/zhong23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhong23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ziyuan
    family: Zhong
  - given: Davis
    family: Rempe
  - given: Yuxiao
    family: Chen
  - given: Boris
    family: Ivanovic
  - given: Yulong
    family: Cao
  - given: Danfei
    family: Xu
  - given: Marco
    family: Pavone
  - given: Baishakhi
    family: Ray
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 144-177
  id: zhong23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 144
  lastpage: 177
  published: 2023-12-02 00:00:00 +0000
- title: 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping'
  abstract: 'Grasping objects by a specific subpart is often crucial for safety and for executing downstream tasks. We propose LERF-TOGO, Language Embedded Radiance Fields for Task-Oriented Grasping of Objects, which uses vision-language models zero-shot to output a grasp distribution over an object given a natural language query. To accomplish this, we first construct a LERF of the scene, which distills CLIP embeddings into a multi-scale 3D language field queryable with text. However, LERF has no sense of object boundaries, so its relevancy outputs often return incomplete activations over an object which are insufficient for grasping. LERF-TOGO mitigates this lack of spatial grouping by extracting a 3D object mask via DINO features and then conditionally querying LERF on this mask to obtain a semantic distribution over the object to rank grasps from an off-the-shelf grasp planner. We evaluate LERF-TOGO’s ability to grasp task-oriented object parts on 31 physical objects, and find it selects grasps on the correct part in $81%$ of trials and grasps successfully in $69%$. Code, data, appendix, and details are available at: lerftogo.github.io'
  volume: 229
  URL: https://proceedings.mlr.press/v229/rashid23a.html
  PDF: https://proceedings.mlr.press/v229/rashid23a/rashid23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-rashid23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Adam
    family: Rashid
  - given: Satvik
    family: Sharma
  - given: Chung Min
    family: Kim
  - given: Justin
    family: Kerr
  - given: Lawrence Yunliang
    family: Chen
  - given: Angjoo
    family: Kanazawa
  - given: Ken
    family: Goldberg
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 178-200
  id: rashid23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 178
  lastpage: 200
  published: 2023-12-02 00:00:00 +0000
- title: 'MimicPlay: Long-Horizon Imitation Learning by Watching Human Play'
  abstract: 'Imitation learning from human demonstrations is a promising paradigm for teaching robots manipulation skills in the real world. However, learning complex long-horizon tasks often requires an unattainable amount of demonstrations. To reduce the high data requirement, we resort to human play data - video sequences of people freely interacting with the environment using their hands. Even with different morphologies, we hypothesize that human play data contain rich and salient information about physical interactions that can readily facilitate robot policy learning. Motivated by this, we introduce a hierarchical learning framework named MimicPlay that learns latent plans from human play data to guide low-level visuomotor control trained on a small number of teleoperated demonstrations. With systematic evaluations of 14 long-horizon manipulation tasks in the real world, we show that MimicPlay outperforms state-of-the-art imitation learning methods in task success rate, generalization ability, and robustness to disturbances. Code and videos are available at https://mimic-play.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wang23a.html
  PDF: https://proceedings.mlr.press/v229/wang23a/wang23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wang23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Chen
    family: Wang
  - given: Linxi
    family: Fan
  - given: Jiankai
    family: Sun
  - given: Ruohan
    family: Zhang
  - given: Li
    family: Fei-Fei
  - given: Danfei
    family: Xu
  - given: Yuke
    family: Zhu
  - given: Anima
    family: Anandkumar
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 201-221
  id: wang23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 201
  lastpage: 221
  published: 2023-12-02 00:00:00 +0000
- title: 'Continual Vision-based Reinforcement Learning with Group Symmetries'
  abstract: 'Continual reinforcement learning aims to sequentially learn a variety of tasks, retaining the ability to perform previously encountered tasks while simultaneously developing new policies for novel tasks. However, current continual RL approaches overlook the fact that certain tasks are identical under basic group operations like rotations or translations, especially with visual inputs. They may unnecessarily learn and maintain a new policy for each similar task, leading to poor sample efficiency and weak generalization capability. To address this, we introduce a unique Continual Vision-based Reinforcement Learning method that recognizes Group Symmetries, called COVERS, cultivating a policy for each group of equivalent tasks rather than an individual task. COVERS employs a proximal-policy-gradient-based (PPO-based) algorithm to train each policy, which contains an equivariant feature extractor and takes inputs with different modalities, including image observations and robot proprioceptive states. It also utilizes an unsupervised task grouping mechanism that relies on 1-Wasserstein distance on the extracted invariant features. We evaluate COVERS on a sequence of table-top manipulation tasks in simulation and on a real robot platform. Our results show that COVERS accurately assigns tasks to their respective groups and significantly outperforms baselines by generalizing to unseen but equivariant tasks in seen task groups. Demos are available on our project page: https://sites.google.com/view/rl-covers/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/liu23a.html
  PDF: https://proceedings.mlr.press/v229/liu23a/liu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-liu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Shiqi
    family: Liu
  - given: Mengdi
    family: Xu
  - given: Peide
    family: Huang
  - given: Xilun
    family: Zhang
  - given: Yongkang
    family: Liu
  - given: Kentaro
    family: Oguchi
  - given: Ding
    family: Zhao
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 222-240
  id: liu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 222
  lastpage: 240
  published: 2023-12-02 00:00:00 +0000
- title: 'HACMan: Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation'
  abstract: 'Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about gripper-object interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized initial poses, randomized 6D goals, and diverse object categories, our policy demonstrates strong generalization to unseen object categories without a performance drop, achieving an $89%$ success rate on unseen objects in simulation and $50%$ success rate with zero-shot transfer in the real world. Compared to alternative action representations, HACMan achieves a success rate more than three times higher than the best baseline. With zero-shot sim2real transfer, our policy can successfully manipulate unseen objects in the real world for challenging non-planar goals, using dynamic and contact-rich non-prehensile skills. Videos can be found on the project website: https://hacman-2023.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhou23a.html
  PDF: https://proceedings.mlr.press/v229/zhou23a/zhou23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhou23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Wenxuan
    family: Zhou
  - given: Bowen
    family: Jiang
  - given: Fan
    family: Yang
  - given: Chris
    family: Paxton
  - given: David
    family: Held
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 241-265
  id: zhou23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 241
  lastpage: 265
  published: 2023-12-02 00:00:00 +0000
- title: 'Hijacking Robot Teams Through Adversarial Communication'
  abstract: 'Communication is often necessary for robot teams to collaborate and complete a decentralized task. Multi-agent reinforcement learning (MARL) systems allow agents to learn how to collaborate and communicate to complete a task. These domains are ubiquitous and include safety-critical domains such as wildfire fighting, traffic control, or search and rescue missions. However, critical vulnerabilities may arise in communication systems as jamming the signals can interrupt the robot team. This work presents a framework for applying black-box adversarial attacks to learned MARL policies by manipulating only the communication signals between agents. Our system only requires observations of MARL policies after training is complete, as this is more realistic than attacking the training process. To this end, we imitate a learned policy of the targeted agents without direct interaction with the environment or ground truth rewards. Instead, we infer the rewards by only observing the behavior of the targeted agents. Our framework reduces reward by $201%$ compared to an equivalent baseline method and also shows favorable results when deployed in real swarm robots. Our novel attack methodology within MARL systems contributes to the field by enhancing our understanding on the reliability of multi-agent systems.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wu23a.html
  PDF: https://proceedings.mlr.press/v229/wu23a/wu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zixuan
    family: Wu
  - given: Sean Charles
    family: Ye
  - given: Byeolyi
    family: Han
  - given: Matthew
    family: Gombolay
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 266-283
  id: wu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 266
  lastpage: 283
  published: 2023-12-02 00:00:00 +0000
- title: 'GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields'
  abstract: 'It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot will need to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present GNFactor, a visual behavior cloning agent for multi-task robotic manipulation with Generalizable Neural feature Fields. GNFactor jointly optimizes a neural radiance field (NeRF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module incorporates a vision-language foundation model (e.g., Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real-robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of demonstrations. We observe a substantial improvement of GNFactor over current state-of-the-art methods in seen and unseen tasks, demonstrating the strong generalization ability of GNFactor. Project website: https://yanjieze.com/GNFactor/'
  volume: 229
  URL: https://proceedings.mlr.press/v229/ze23a.html
  PDF: https://proceedings.mlr.press/v229/ze23a/ze23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-ze23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yanjie
    family: Ze
  - given: Ge
    family: Yan
  - given: Yueh-Hua
    family: Wu
  - given: Annabella
    family: Macaluso
  - given: Yuying
    family: Ge
  - given: Jianglong
    family: Ye
  - given: Nicklas
    family: Hansen
  - given: Li Erran
    family: Li
  - given: Xiaolong
    family: Wang
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 284-301
  id: ze23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 284
  lastpage: 301
  published: 2023-12-02 00:00:00 +0000
- title: 'Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance'
  abstract: 'We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments. Website at clvrai.com/boss.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23a.html
  PDF: https://proceedings.mlr.press/v229/zhang23a/zhang23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jesse
    family: Zhang
  - given: Jiahui
    family: Zhang
  - given: Karl
    family: Pertsch
  - given: Ziyi
    family: Liu
  - given: Xiang
    family: Ren
  - given: Minsuk
    family: Chang
  - given: Shao-Hua
    family: Sun
  - given: Joseph J.
    family: Lim
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 302-325
  id: zhang23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 302
  lastpage: 325
  published: 2023-12-02 00:00:00 +0000
- title: 'DATT: Deep Adaptive Trajectory Tracking for Quadrotor Control'
  abstract: 'Precise arbitrary trajectory tracking for quadrotors is challenging due to unknown nonlinear dynamics, trajectory infeasibility, and actuation limits. To tackle these challenges, we present DATT, a learning-based approach that can precisely track arbitrary, potentially infeasible trajectories in the presence of large disturbances in the real world. DATT builds on a novel feedforward-feedback-adaptive control structure trained in simulation using reinforcement learning. When deployed on real hardware, DATT is augmented with a disturbance estimator using $\mathcal{L}_1$ adaptive control in closed-loop, without any fine-tuning. DATT significantly outperforms competitive adaptive nonlinear and model predictive controllers for both feasible smooth and infeasible trajectories in unsteady wind fields, including challenging scenarios where baselines completely fail. Moreover, DATT can efficiently run online with an inference time less than 3.2ms, less than 1/4 of the adaptive nonlinear model predictive control baseline.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/huang23a.html
  PDF: https://proceedings.mlr.press/v229/huang23a/huang23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-huang23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Kevin
    family: Huang
  - given: Rwik
    family: Rana
  - given: Alexander
    family: Spitzer
  - given: Guanya
    family: Shi
  - given: Byron
    family: Boots
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 326-340
  id: huang23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 326
  lastpage: 340
  published: 2023-12-02 00:00:00 +0000
- title: 'HANDLOOM: Learned Tracing of One-Dimensional Objects for Inspection and Manipulation'
  abstract: 'Tracing – estimating the spatial state of – long deformable linear objects such as cables, threads, hoses, or ropes, is useful for a broad range of tasks in homes, retail, factories, construction, transportation, and healthcare. For long deformable linear objects (DLOs or simply cables) with many (over 25) crossings, we present HANDLOOM (Heterogeneous Autoregressive Learned Deformable Linear Object Observation and Manipulation) a learning-based algorithm that fits a trace to a greyscale image of cables. We evaluate HANDLOOM on semi-planar DLO configurations where each crossing involves at most 2 segments. HANDLOOM makes use of neural networks trained with 30,000 simulated examples and 568 real examples to autoregressively estimate traces of cables and classify crossings. Experiments find that in settings with multiple identical cables, HANDLOOM can trace each cable with $80%$ accuracy. In single-cable images, HANDLOOM can trace and identify knots with $77%$ accuracy. When HANDLOOM is incorporated into a bimanual robot system, it enables state-based imitation of knot tying with $80%$ accuracy, and it successfully untangles $64%$ of cable configurations across 3 levels of difficulty. Additionally, HANDLOOM demonstrates generalization to knot types and materials (rubber, cloth rope) not present in the training dataset with $85%$ accuracy. Supplementary material, including all code and an annotated dataset of RGB-D images of cables along with ground-truth traces, is at https://sites.google.com/view/cable-tracing.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/viswanath23a.html
  PDF: https://proceedings.mlr.press/v229/viswanath23a/viswanath23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-viswanath23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Vainavi
    family: Viswanath
  - given: Kaushik
    family: Shivakumar
  - given: Mallika
    family: Parulekar
  - given: Jainil
    family: Ajmera
  - given: Justin
    family: Kerr
  - given: Jeffrey
    family: Ichnowski
  - given: Richard
    family: Cheng
  - given: Thomas
    family: Kollar
  - given: Ken
    family: Goldberg
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 341-357
  id: viswanath23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 341
  lastpage: 357
  published: 2023-12-02 00:00:00 +0000
- title: 'Predicting Object Interactions with Behavior Primitives: An Application in Stowing Tasks'
  abstract: 'Stowing, the task of placing objects in cluttered shelves or bins, is a common task in warehouse and manufacturing operations. However, this task is still predominantly carried out by human workers as stowing is challenging to automate due to the complex multi-object interactions and long-horizon nature of the task. Previous works typically involve extensive data collection and costly human labeling of semantic priors across diverse object categories. This paper presents a method to learn a generalizable robot stowing policy from predictive model of object interactions and a single demonstration with behavior primitives. We propose a novel framework that utilizes Graph Neural Networks (GNNs) to predict object interactions within the parameter space of behavioral primitives. We further employ primitive-augmented trajectory optimization to search the parameters of a predefined library of heterogeneous behavioral primitives to instantiate the control action. Our framework enables robots to proficiently execute long-horizon stowing tasks with a few keyframes (3-4) from a single demonstration. Despite being solely trained in a simulation, our framework demonstrates remarkable generalization capabilities. It efficiently adapts to a broad spectrum of real-world conditions, including various shelf widths, fluctuating quantities of objects, and objects with diverse attributes such as sizes and shapes.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/chen23a.html
  PDF: https://proceedings.mlr.press/v229/chen23a/chen23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-chen23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Haonan
    family: Chen
  - given: Yilong
    family: Niu
  - given: Kaiwen
    family: Hong
  - given: Shuijing
    family: Liu
  - given: Yixuan
    family: Wang
  - given: Yunzhu
    family: Li
  - given: Katherine Rose
    family: Driggs-Campbell
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 358-373
  id: chen23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 358
  lastpage: 373
  published: 2023-12-02 00:00:00 +0000
- title: 'Language to Rewards for Robotic Skill Synthesis'
  abstract: 'Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate interface generated by LLMs, we can effectively bridge the gap between high-level language instructions or corrections to low-level robot actions. Meanwhile, combining this with a real-time optimizer, MuJoCo MPC, empowers an interactive behavior creation experience where users can immediately observe the results and provide feedback to the system. To systematically evaluate the performance of our proposed method, we designed a total of 17 tasks for a simulated quadruped robot and a dexterous manipulator robot. We demonstrate that our proposed method reliably tackles $90%$ of the designed tasks, while a baseline using primitive skills as the interface with Code-as-policies achieves $50%$ of the tasks. We further validated our method on a real robot arm where complex manipulation skills such as non-prehensile pushing emerge through our interactive system.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yu23a.html
  PDF: https://proceedings.mlr.press/v229/yu23a/yu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Wenhao
    family: Yu
  - given: Nimrod
    family: Gileadi
  - given: Chuyuan
    family: Fu
  - given: Sean
    family: Kirmani
  - given: Kuang-Huei
    family: Lee
  - given: Montserrat Gonzalez
    family: Arenas
  - given: Hao-Tien Lewis
    family: Chiang
  - given: Tom
    family: Erez
  - given: Leonard
    family: Hasenclever
  - given: Jan
    family: Humplik
  - given: Brian
    family: Ichter
  - given: Ted
    family: Xiao
  - given: Peng
    family: Xu
  - given: Andy
    family: Zeng
  - given: Tingnan
    family: Zhang
  - given: Nicolas
    family: Heess
  - given: Dorsa
    family: Sadigh
  - given: Jie
    family: Tan
  - given: Yuval
    family: Tassa
  - given: Fei
    family: Xia
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 374-404
  id: yu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 374
  lastpage: 404
  published: 2023-12-02 00:00:00 +0000
- title: 'Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation'
  abstract: 'Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects. Project website: https://f3rm.csail.mit.edu'
  volume: 229
  URL: https://proceedings.mlr.press/v229/shen23a.html
  PDF: https://proceedings.mlr.press/v229/shen23a/shen23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-shen23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: William
    family: Shen
  - given: Ge
    family: Yang
  - given: Alan
    family: Yu
  - given: Jansen
    family: Wong
  - given: Leslie Pack
    family: Kaelbling
  - given: Phillip
    family: Isola
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 405-424
  id: shen23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 405
  lastpage: 424
  published: 2023-12-02 00:00:00 +0000
- title: 'Finetuning Offline World Models in the Real World'
  abstract: 'Reinforcement Learning (RL) is notoriously data-inefficient, which makes training on a real robot difficult. While model-based RL algorithms (world models) improve data-efficiency to some extent, they still require hours or days of interaction to learn skills. Recently, offline RL has been proposed as a framework for training RL policies on pre-existing datasets without any online interaction. However, constraining an algorithm to a fixed dataset induces a state-action distribution shift between training and inference, and limits its applicability to new tasks. In this work, we seek to get the best of both worlds: we consider the problem of pretraining a world model with offline data collected on a real robot, and then finetuning the model on online data collected by planning with the learned model. To mitigate extrapolation errors during online interaction, we propose to regularize the planner at test-time by balancing estimated returns and (epistemic) model uncertainty. We evaluate our method on a variety of visuo-motor control tasks in simulation and on a real robot, and find that our method enables few-shot finetuning to seen and unseen tasks even when offline data is limited. Videos are available at https://yunhaifeng.com/FOWM'
  volume: 229
  URL: https://proceedings.mlr.press/v229/feng23a.html
  PDF: https://proceedings.mlr.press/v229/feng23a/feng23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-feng23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yunhai
    family: Feng
  - given: Nicklas
    family: Hansen
  - given: Ziyan
    family: Xiong
  - given: Chandramouli
    family: Rajagopalan
  - given: Xiaolong
    family: Wang
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 425-445
  id: feng23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 425
  lastpage: 445
  published: 2023-12-02 00:00:00 +0000
- title: 'Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning'
  abstract: 'Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we introduce a distributed multi-agent reinforcement learning (MARL) algorithm for joint trajectory and intent prediction for autonomous vehicles in dense and heterogeneous environments. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers’ intents solely from their local observations. We model an explicit representation of agents’ private incentives: Behavioral Incentive for high-level decision-making strategy that sets planning sub-goals and Instant Incentive for low-level motion planning to execute sub-goals. Our approach enables agents to infer their opponents’ behavior incentives and integrate this inferred information into their decision-making and motion-planning processes. We perform experiments on two simulation environments, Non-Cooperative Navigation and Heterogeneous Highway. In Heterogeneous Highway, results show that, compared with centralized training decentralized execution (CTDE) MARL baselines such as QMIX and MAPPO, our method yields a $4.3%$ and $38.4%$ higher episodic reward in mild and chaotic traffic, with $48.1%$ higher success rate and $80.6%$ longer survival time in chaotic traffic. We also compare with a decentralized training decentralized execution (DTDE) baseline IPPO and demonstrate a higher episodic reward of $12.7%$ and $6.3%$ in mild traffic and chaotic traffic, $25.3%$ higher success rate, and $13.7%$ longer survival time.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wu23b.html
  PDF: https://proceedings.mlr.press/v229/wu23b/wu23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wu23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xiyang
    family: Wu
  - given: Rohan
    family: Chandra
  - given: Tianrui
    family: Guan
  - given: Amrit
    family: Bedi
  - given: Dinesh
    family: Manocha
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 446-477
  id: wu23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 446
  lastpage: 477
  published: 2023-12-02 00:00:00 +0000
- title: 'PreCo: Enhancing Generalization in Co-Design of Modular Soft Robots via Brain-Body Pre-Training'
  abstract: 'Brain-body co-design, which involves the collaborative design of control strategies and morphologies, has emerged as a promising approach to enhance a robot’s adaptability to its environment. However, the conventional co-design process often starts from scratch, lacking the utilization of prior knowledge. This can result in time-consuming and costly endeavors. In this paper, we present PreCo, a novel methodology that efficiently integrates brain-body pre-training into the co-design process of modular soft robots. PreCo is based on the insight of embedding co-design principles into models, achieved by pre-training a universal co-design policy on a diverse set of tasks. This pre-trained co-designer is utilized to generate initial designs and control policies, which are then fine-tuned for specific co-design tasks. Through experiments on a modular soft robot system, our method demonstrates zero-shot generalization to unseen co-design tasks, facilitating few-shot adaptation while significantly reducing the number of policy iterations required.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wang23b.html
  PDF: https://proceedings.mlr.press/v229/wang23b/wang23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wang23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuxing
    family: Wang
  - given: Shuang
    family: Wu
  - given: Tiantian
    family: Zhang
  - given: Yongzhe
    family: Chang
  - given: Haobo
    family: Fu
  - given: QIANG
    family: FU
  - given: Xueqian
    family: Wang
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 478-498
  id: wang23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 478
  lastpage: 498
  published: 2023-12-02 00:00:00 +0000
- title: 'Diff-LfD: Contact-aware Model-based Learning from Visual Demonstration for Robotic Manipulation via Differentiable Physics-based Simulation and Rendering'
  abstract: 'Learning from Demonstration (LfD) is an efficient technique for robots to acquire new skills through expert observation, significantly mitigating the need for laborious manual reward function design. This paper introduces a novel framework for model-based LfD in the context of robotic manipulation. Our proposed pipeline is underpinned by two primary components: self-supervised pose and shape estimation and contact sequence generation. The former utilizes differentiable rendering to estimate object poses and shapes from demonstration videos, while the latter iteratively optimizes contact points and forces using differentiable simulation, consequently effectuating object transformations. Empirical evidence demonstrates the efficacy of our LfD pipeline in acquiring manipulation actions from human demonstrations. Complementary to this, ablation studies focusing on object tracking and contact sequence inference underscore the robustness and efficiency of our approach in generating long-horizon manipulation actions, even amidst environmental noise. Validation of our results extends to real-world deployment of the proposed pipeline. Supplementary materials and videos are available on our webpage.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhu23a.html
  PDF: https://proceedings.mlr.press/v229/zhu23a/zhu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xinghao
    family: Zhu
  - given: JingHan
    family: Ke
  - given: Zhixuan
    family: Xu
  - given: Zhixin
    family: Sun
  - given: Bizhe
    family: Bai
  - given: Jun
    family: Lv
  - given: Qingtao
    family: Liu
  - given: Yuwei
    family: Zeng
  - given: Qi
    family: Ye
  - given: Cewu
    family: Lu
  - given: Masayoshi
    family: Tomizuka
  - given: Lin
    family: Shao
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 499-512
  id: zhu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 499
  lastpage: 512
  published: 2023-12-02 00:00:00 +0000
- title: 'Surrogate Assisted Generation of Human-Robot Interaction Scenarios'
  abstract: 'As human-robot interaction (HRI) systems advance, so does the difficulty of evaluating and understanding the strengths and limitations of these systems in different environments and with different users. To this end, previous methods have algorithmically generated diverse scenarios that reveal system failures in a shared control teleoperation task. However, these methods require directly evaluating generated scenarios by simulating robot policies and human actions. The computational cost of these evaluations limits their applicability in more complex domains. Thus, we propose augmenting scenario generation systems with surrogate models that predict both human and robot behaviors. In the shared control teleoperation domain and a more complex shared workspace collaboration task, we show that surrogate assisted scenario generation efficiently synthesizes diverse datasets of challenging scenarios. We demonstrate that these failures are reproducible in real-world interactions.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/bhatt23a.html
  PDF: https://proceedings.mlr.press/v229/bhatt23a/bhatt23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-bhatt23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Varun
    family: Bhatt
  - given: Heramb
    family: Nemlekar
  - given: Matthew Christopher
    family: Fontaine
  - given: Bryon
    family: Tjanaka
  - given: Hejia
    family: Zhang
  - given: Ya-Chuan
    family: Hsu
  - given: Stefanos
    family: Nikolaidis
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 513-539
  id: bhatt23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 513
  lastpage: 539
  published: 2023-12-02 00:00:00 +0000
- title: 'VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models'
  abstract: 'Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/huang23b.html
  PDF: https://proceedings.mlr.press/v229/huang23b/huang23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-huang23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Wenlong
    family: Huang
  - given: Chen
    family: Wang
  - given: Ruohan
    family: Zhang
  - given: Yunzhu
    family: Li
  - given: Jiajun
    family: Wu
  - given: Li
    family: Fei-Fei
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 540-562
  id: huang23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 540
  lastpage: 562
  published: 2023-12-02 00:00:00 +0000
- title: 'Stabilize to Act: Learning to Coordinate for Bimanual Manipulation'
  abstract: 'Key to rich, dexterous manipulation in the real world is the ability to coordinate control across two hands. However, while the promise afforded by bimanual robotic systems is immense, constructing control policies for dual arm autonomous systems brings inherent difficulties. One such difficulty is the high-dimensionality of the bimanual action space, which adds complexity to both model-based and data-driven methods. We counteract this challenge by drawing inspiration from humans to propose a novel role assignment framework: a stabilizing arm holds an object in place to simplify the environment while an acting arm executes the task. We instantiate this framework with BimanUal Dexterity from Stabilization (BUDS), which uses a learned restabilizing classifier to alternate between updating a learned stabilization position to keep the environment unchanged, and accomplishing the task with an acting policy learned from demonstrations. We evaluate BUDS on four bimanual tasks of varying complexities on real-world robots, such as zipping jackets and cutting vegetables. Given only 20 demonstrations, BUDS achieves $76.9%$ task success across our task suite, and generalizes to out-of-distribution objects within a class with a $52.7%$ success rate. BUDS is $56.0%$ more successful than an unstructured baseline that instead learns a BC stabilizing policy due to the precision required of these complex tasks. Supplementary material and videos can be found at https://tinyurl.com/stabilizetoact.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/grannen23a.html
  PDF: https://proceedings.mlr.press/v229/grannen23a/grannen23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-grannen23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jennifer
    family: Grannen
  - given: Yilin
    family: Wu
  - given: Brandon
    family: Vu
  - given: Dorsa
    family: Sadigh
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 563-576
  id: grannen23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 563
  lastpage: 576
  published: 2023-12-02 00:00:00 +0000
- title: 'How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations'
  abstract: 'We present a framework and algorithms to learn controlled dynamics models using neural stochastic differential equations (SDEs)—SDEs whose drift and diffusion terms are both parametrized by neural networks. We construct the drift term to leverage a priori physics knowledge as inductive bias, and we design the diffusion term to represent a distance-aware estimate of the uncertainty in the learned model’s predictions—it matches the system’s underlying stochasticity when evaluated on states near those from the training dataset, and it predicts highly stochastic dynamics when evaluated on states beyond the training regime. The proposed neural SDEs can be evaluated quickly enough for use in model predictive control algorithms, or they can be used as simulators for model-based reinforcement learning. Furthermore, they make accurate predictions over long time horizons, even when trained on small datasets that cover limited regions of the state space. We demonstrate these capabilities through experiments on simulated robotic systems, as well as by using them to model and control a hexacopter’s flight dynamics: A neural SDE trained using only three minutes of manually collected flight data results in a model-based control policy that accurately tracks aggressive trajectories that push the hexacopter’s velocity and Euler angles to nearly double the maximum values observed in the training dataset.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/djeumou23a.html
  PDF: https://proceedings.mlr.press/v229/djeumou23a/djeumou23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-djeumou23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Franck
    family: Djeumou
  - given: Cyrus
    family: Neary
  - given: Ufuk
    family: Topcu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 577-601
  id: djeumou23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 577
  lastpage: 601
  published: 2023-12-02 00:00:00 +0000
- title: 'Measuring Interpretability of Neural Policies of Robots with Disentangled Representation'
  abstract: 'The advancement of robots, particularly those functioning in complex human-centric environments, relies on control solutions that are driven by machine learning. Understanding how learning-based controllers make decisions is crucial since robots are mostly safety-critical systems. This urges a formal and quantitative understanding of the explanatory factors in the interpretability of robot learning. In this paper, we aim to study interpretability of compact neural policies through the lens of disentangled representation. We leverage decision trees to obtain factors of variation [1] for disentanglement in robot learning; these encapsulate skills, behaviors, or strategies toward solving tasks. To assess how well networks uncover the underlying task dynamics, we introduce interpretability metrics that measure disentanglement of learned neural dynamics from a concentration of decisions, mutual information and modularity perspective. We showcase the effectiveness of the connection between interpretability and disentanglement consistently across extensive experimental analysis.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wang23c.html
  PDF: https://proceedings.mlr.press/v229/wang23c/wang23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wang23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tsun-Hsuan
    family: Wang
  - given: Wei
    family: Xiao
  - given: Tim
    family: Seyde
  - given: Ramin
    family: Hasani
  - given: Daniela
    family: Rus
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 602-641
  id: wang23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 602
  lastpage: 641
  published: 2023-12-02 00:00:00 +0000
- title: 'RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools'
  abstract: 'Humans excel in complex long-horizon soft body manipulation tasks via flexible tool use: bread baking requires a knife to slice the dough and a rolling pin to flatten it. Often regarded as a hallmark of human cognition, tool use in autonomous robots remains limited due to challenges in understanding tool-object interactions. Here we develop an intelligent robotic system, RoboCook, which perceives, models, and manipulates elasto-plastic objects with various tools. RoboCook uses point cloud scene representations, models tool-object interactions with Graph Neural Networks (GNNs), and combines tool classification with self-supervised policy learning to devise manipulation plans. We demonstrate that from just 20 minutes of real-world interaction data per tool, a general-purpose robot arm can learn complex long-horizon soft object manipulation tasks, such as making dumplings and alphabet letter cookies. Extensive evaluations show that RoboCook substantially outperforms state-of-the-art approaches, exhibits robustness against severe external disturbances, and demonstrates adaptability to different materials.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/shi23a.html
  PDF: https://proceedings.mlr.press/v229/shi23a/shi23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-shi23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Haochen
    family: Shi
  - given: Huazhe
    family: Xu
  - given: Samuel
    family: Clarke
  - given: Yunzhu
    family: Li
  - given: Jiajun
    family: Wu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 642-660
  id: shi23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 642
  lastpage: 660
  published: 2023-12-02 00:00:00 +0000
- title: 'Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners'
  abstract: 'Large language models (LLMs) exhibit a wide range of promising capabilities — from step-by-step planning to commonsense reasoning — that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, a framework for measuring and aligning the uncertainty of LLM-based planners, such that they know when they don’t know, and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (for example, from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out-of-the-box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/ren23a.html
  PDF: https://proceedings.mlr.press/v229/ren23a/ren23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-ren23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Allen Z.
    family: Ren
  - given: Anushri
    family: Dixit
  - given: Alexandra
    family: Bodrova
  - given: Sumeet
    family: Singh
  - given: Stephen
    family: Tu
  - given: Noah
    family: Brown
  - given: Peng
    family: Xu
  - given: Leila
    family: Takayama
  - given: Fei
    family: Xia
  - given: Jake
    family: Varley
  - given: Zhenjia
    family: Xu
  - given: Dorsa
    family: Sadigh
  - given: Andy
    family: Zeng
  - given: Anirudha
    family: Majumdar
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 661-682
  id: ren23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 661
  lastpage: 682
  published: 2023-12-02 00:00:00 +0000
- title: 'Robot Learning with Sensorimotor Pre-training'
  abstract: 'We present a self-supervised sensorimotor pre-training approach for robotics. Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given a sequence of camera images, proprioceptive robot states, and actions, we encode the sequence into tokens, mask out a subset, and train a model to predict the missing content from the rest. We hypothesize that if a robot can predict the masked-out content it will have acquired a good model of the physical world that can enable it to act. RPT is designed to operate on latent visual representations which makes prediction tractable, enables scaling to larger models, and allows fast inference on a real robot. To evaluate our approach, we collected a dataset of 20,000 real-world trajectories over 9 months using a combination of motion planning and grasping algorithms. We find that sensorimotor pre-training consistently outperforms training from scratch, has favorable scaling properties, and enables transfer across different tasks, environments, and robots.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/radosavovic23a.html
  PDF: https://proceedings.mlr.press/v229/radosavovic23a/radosavovic23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-radosavovic23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ilija
    family: Radosavovic
  - given: Baifeng
    family: Shi
  - given: Letian
    family: Fu
  - given: Ken
    family: Goldberg
  - given: Trevor
    family: Darrell
  - given: Jitendra
    family: Malik
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 683-693
  id: radosavovic23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 683
  lastpage: 693
  published: 2023-12-02 00:00:00 +0000
- title: 'RVT: Robotic View Transformer for 3D Object Manipulation'
  abstract: 'For 3D object manipulation, methods that build an explicit 3D representation perform better than those relying only on camera images. But using explicit 3D representations like voxels comes at large computing cost, adversely affecting scalability. In this work, we propose RVT, a multi-view transformer for 3D manipulation that is both scalable and accurate. Some key features of RVT are an attention mechanism to aggregate information across views and re-rendering of the camera input from virtual views around the robot workspace. In simulations, we find that a single RVT model works well across 18 RLBench tasks with 249 task variations, achieving $26%$ higher relative success than the existing state-of-the-art method (PerAct). It also trains 36X faster than PerAct for achieving the same performance and achieves 2.3X the inference speed of PerAct. Further, RVT can perform a variety of manipulation tasks in the real world with just a few ($\sim$10) demonstrations per task. Visual results, code, and trained model are provided at: https://robotic-view-transformer.github.io/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/goyal23a.html
  PDF: https://proceedings.mlr.press/v229/goyal23a/goyal23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-goyal23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ankit
    family: Goyal
  - given: Jie
    family: Xu
  - given: Yijie
    family: Guo
  - given: Valts
    family: Blukis
  - given: Yu-Wei
    family: Chao
  - given: Dieter
    family: Fox
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 694-710
  id: goyal23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 694
  lastpage: 710
  published: 2023-12-02 00:00:00 +0000
- title: 'ViNT: A Foundation Model for Visual Navigation'
  abstract: 'General-purpose pre-trained models (“foundation models”) have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navigation from a variety of different robotic platforms, and exhibits positive transfer, outperforming specialist models trained on narrower datasets. ViNT can be augmented with diffusion-based goal proposals to explore novel environments, and can solve kilometer-scale navigation problems when equipped with long-range heuristics. ViNT can also be adapted to novel task specifications with a technique inspired by prompt-tuning, where the goal encoder is replaced by an encoding of another task modality (e.g., GPS waypoints or turn-by-turn directions) embedded into the same space of goal tokens. This flexibility and ability to accommodate a variety of downstream problem domains establish ViNT as an effective foundation model for mobile robotics.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/shah23a.html
  PDF: https://proceedings.mlr.press/v229/shah23a/shah23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-shah23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Dhruv
    family: Shah
  - given: Ajay
    family: Sridhar
  - given: Nitish
    family: Dashora
  - given: Kyle
    family: Stachowicz
  - given: Kevin
    family: Black
  - given: Noriaki
    family: Hirose
  - given: Sergey
    family: Levine
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 711-733
  id: shah23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 711
  lastpage: 733
  published: 2023-12-02 00:00:00 +0000
- title: 'What Went Wrong? Closing the Sim-to-Real Gap via Differentiable Causal Discovery'
  abstract: 'Training control policies in simulation is more appealing than on real robots directly, as it allows for exploring diverse states in an efficient manner. Yet, robot simulators inevitably exhibit disparities from the real-world \rebut{dynamics}, yielding inaccuracies that manifest as the dynamical simulation-to-reality (sim-to-real) gap. Existing literature has proposed to close this gap by actively modifying specific simulator parameters to align the simulated data with real-world observations. However, the set of tunable parameters is usually manually selected to reduce the search space in a case-by-case manner, which is hard to scale up for complex systems and requires extensive domain knowledge. To address the scalability issue and automate the parameter-tuning process, we introduce COMPASS, which aligns the simulator with the real world by discovering the causal relationship between the environment parameters and the sim-to-real gap. Concretely, our method learns a differentiable mapping from the environment parameters to the differences between simulated and real-world robot-object trajectories. This mapping is governed by a simultaneously learned causal graph to help prune the search space of parameters, provide better interpretability, and improve generalization on unseen parameters. We perform experiments to achieve both sim-to-sim and sim-to-real transfer, and show that our method has significant improvements in trajectory alignment and task success rate over strong baselines in several challenging manipulation tasks. Demos are available on our project website: https://sites.google.com/view/sim2real-compass.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/huang23c.html
  PDF: https://proceedings.mlr.press/v229/huang23c/huang23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-huang23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Peide
    family: Huang
  - given: Xilun
    family: Zhang
  - given: Ziang
    family: Cao
  - given: Shiqi
    family: Liu
  - given: Mengdi
    family: Xu
  - given: Wenhao
    family: Ding
  - given: Jonathan
    family: Francis
  - given: Bingqing
    family: Chen
  - given: Ding
    family: Zhao
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 734-760
  id: huang23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 734
  lastpage: 760
  published: 2023-12-02 00:00:00 +0000
- title: 'Scalable Deep Kernel Gaussian Process for Vehicle Dynamics in Autonomous Racing'
  abstract: 'Autonomous racing presents a challenging environment for testing the limits of autonomous vehicle technology. Accurately modeling the vehicle dynamics (with all forces and tires) is critical for high-speed racing, but it remains a difficult task and requires an intricate balance between run-time computational demands and modeling complexity. Researchers have proposed utilizing learning-based methods such as Gaussian Process (GP) for learning vehicle dynamics. However, current approaches often oversimplify the modeling process or apply strong assumptions, leading to unrealistic results that cannot translate to real-world settings. In this paper, we proposed DKL-SKIP method for vehicle dynamics modeling. Our approach outperforms standard GP methods and the N4SID system identification technique in terms of prediction accuracy. In addition to evaluating DKL-SKIP on real-world data, we also evaluate its performance using a high-fidelity autonomous racing AutoVerse simulator. The results highlight the potential of DKL-SKIP as a promising tool for modeling complex vehicle dynamics in both real-world and simulated environments.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/ning23a.html
  PDF: https://proceedings.mlr.press/v229/ning23a/ning23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-ning23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jingyun
    family: Ning
  - given: Madhur
    family: Behl
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 761-773
  id: ning23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 761
  lastpage: 773
  published: 2023-12-02 00:00:00 +0000
- title: 'Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback'
  abstract: 'Ideally, we would place a robot in a real-world environment and leave it there improving on its own by gathering more experience autonomously. However, algorithms for autonomous robotic learning have been challenging to realize in the real world. While this has often been attributed to the challenge of sample complexity, even sample-efficient techniques are hampered by two major challenges - the difficulty of providing well “shaped" rewards, and the difficulty of continual reset-free training. In this work, we describe a system for real-world reinforcement learning that enables agents to show continual improvement by training directly in the real world without requiring painstaking effort to hand-design reward functions or reset mechanisms. Our system leverages occasional non-expert human-in-the-loop feedback from remote users to learn informative distance functions to guide exploration while leveraging a simple self-supervised learning algorithm for goal-directed policy learning. We show that in the absence of resets, it is particularly important to account for the current “reachability" of the exploration policy when deciding which regions of the space to explore. Based on this insight, we instantiate a practical learning system - GEAR, which enables robots to simply be placed in real-world environments and left to train autonomously without interruption. The system streams robot experience to a web interface only requiring occasional asynchronous feedback from remote, crowdsourced, non-expert humans in the form of binary comparative feedback. We evaluate this system on a suite of robotic tasks in simulation and demonstrate its effectiveness at learning behaviors both in simulation and the real world. Project website https://guided-exploration-autonomous-rl.github.io/GEAR/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/balsells23a.html
  PDF: https://proceedings.mlr.press/v229/balsells23a/balsells23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-balsells23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Max
    family: Balsells
  - given: Marcel Torne
    family: Villasevil
  - given: Zihan
    family: Wang
  - given: Samedh
    family: Desai
  - given: Pulkit
    family: Agrawal
  - given: Abhishek
    family: Gupta
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 774-799
  id: balsells23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 774
  lastpage: 799
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning Realistic Traffic Agents in Closed-loop'
  abstract: 'Realistic traffic simulation is crucial for developing self-driving software in a safe and scalable manner prior to real-world deployment. Typically, imitation learning (IL) is used to learn human-like traffic agents directly from real-world observations collected offline, but without explicit specification of traffic rules, agents trained from IL alone frequently display unrealistic infractions like collisions and driving off the road. This problem is exacerbated in out-of-distribution and long-tail scenarios. On the other hand, reinforcement learning (RL) can train traffic agents to avoid infractions, but using RL alone results in unhuman-like driving behaviors. We propose Reinforcing Traffic Rules (RTR), a holistic closed-loop learning objective to match expert demonstrations under a traffic compliance constraint, which naturally gives rise to a joint IL + RL approach, obtaining the best of both worlds. Our method learns in closed-loop simulations of both nominal scenarios from real-world datasets as well as procedurally generated long-tail scenarios. Our experiments show that RTR learns more realistic and generalizable traffic simulation policies, achieving significantly better tradeoffs between human-like driving and traffic compliance in both nominal and long-tail scenarios. Moreover, when used as a data generation tool for training prediction models, our learned traffic policy leads to considerably improved downstream prediction metrics compared to baseline traffic agents.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23b.html
  PDF: https://proceedings.mlr.press/v229/zhang23b/zhang23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Chris
    family: Zhang
  - given: James
    family: Tu
  - given: Lunjun
    family: Zhang
  - given: Kelvin
    family: Wong
  - given: Simon
    family: Suo
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 800-821
  id: zhang23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 800
  lastpage: 821
  published: 2023-12-02 00:00:00 +0000
- title: 'Leveraging 3D Reconstruction for Mechanical Search on Cluttered Shelves'
  abstract: 'Finding and grasping a target object on a cluttered shelf, especially when the target is occluded by other unknown objects and initially invisible, remains a significant challenge in robotic manipulation. While there have been advances in finding the target object by rearranging surrounding objects using specialized tools, developing algorithms that work with standard robot grippers remains an unresolved issue. In this paper, we introduce a novel framework for finding and grasping the target object using a standard gripper, employing pushing and pick and-place actions. To achieve this, we introduce two indicator functions: (i) an existence function, determining the potential presence of the target, and (ii) a graspability function, assessing the feasibility of grasping the identified target. We then formulate a model-based optimal control problem. The core component of our approach involves leveraging a 3D recognition model, enabling efficient estimation of the proposed indicator functions and their associated dynamics models. Our method succeeds in finding and grasping the target object using a standard robot gripper in both simulations and real-world settings. In particular, we demonstrate the adaptability and robustness of our method in the presence of noise in real-world vision sensor data. The code for our framework is available at https://github.com/seungyeon-k/Search-for-Grasp-public.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kim23a.html
  PDF: https://proceedings.mlr.press/v229/kim23a/kim23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kim23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Seungyeon
    family: Kim
  - given: Young Hun
    family: Kim
  - given: Yonghyeon
    family: Lee
  - given: Frank C.
    family: Park
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 822-848
  id: kim23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 822
  lastpage: 848
  published: 2023-12-02 00:00:00 +0000
- title: 'SCONE: A Food Scooping Robot Learning Framework with Active Perception'
  abstract: 'Effectively scooping food items poses a substantial challenge for current robotic systems, due to the intricate states and diverse physical properties of food. To address this challenge, we believe in the importance of encoding food items into meaningful representations for effective food scooping. However, the distinctive properties of food items, including deformability, fragility, fluidity, or granularity, pose significant challenges for existing representations. In this paper, we investigate the potential of active perception for learning meaningful food representations in an implicit manner. To this end, we present SCONE, a food-scooping robot learning framework that leverages representations gained from active perception to facilitate food scooping policy learning. SCONE comprises two crucial encoding components: the interactive encoder and the state retrieval module. Through the encoding process, SCONE is capable of capturing properties of food items and vital state characteristics. In our real-world scooping experiments, SCONE excels with a $71%$ success rate when tasked with 6 previously unseen food items across three different difficulty levels, surpassing state-of-the art methods. This enhanced performance underscores SCONE’s stability, as all food items consistently achieve task success rates exceeding $50%$. Additionally, SCONE’s impressive capacity to accommodate diverse initial states enables it to precisely evaluate the present condition of the food, resulting in a compelling scooping success rate. For further information, please visit our website: https://sites.google.com/view/corlscone/home.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/tai23a.html
  PDF: https://proceedings.mlr.press/v229/tai23a/tai23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-tai23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yen-Ling
    family: Tai
  - given: Yu Chien
    family: Chiu
  - given: Yu-Wei
    family: Chao
  - given: Yi-Ting
    family: Chen
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 849-865
  id: tai23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 849
  lastpage: 865
  published: 2023-12-02 00:00:00 +0000
- title: 'Fine-Tuning Generative Models as an Inference Method for Robotic Tasks'
  abstract: 'Adaptable models could greatly benefit robotic agents operating in the real world, allowing them to deal with novel and varying conditions. While approaches such as Bayesian inference are well-studied frameworks for adapting models to evidence, we build on recent advances in deep generative models which have greatly affected many areas of robotics. Harnessing modern GPU acceleration, we investigate how to quickly adapt the sample generation of neural network models to observations in robotic tasks. We propose a simple and general method that is applicable to various deep generative models and robotic environments. The key idea is to quickly fine-tune the model by fitting it to generated samples matching the observed evidence, using the cross-entropy method. We show that our method can be applied to both autoregressive models and variational autoencoders, and demonstrate its usability in object shape inference from grasping, inverse kinematics calculation, and point cloud completion.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/krupnik23a.html
  PDF: https://proceedings.mlr.press/v229/krupnik23a/krupnik23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-krupnik23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Orr
    family: Krupnik
  - given: Elisei
    family: Shafer
  - given: Tom
    family: Jurgenson
  - given: Aviv
    family: Tamar
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 866-886
  id: krupnik23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 866
  lastpage: 886
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning to Design and Use Tools for Robotic Manipulation'
  abstract: 'When limited by their own morphologies, humans and some species of animals have the remarkable ability to use objects from the environment toward accomplishing otherwise impossible tasks. Robots might similarly unlock a range of additional capabilities through tool use. Recent techniques for jointly optimizing morphology and control via deep learning are effective at designing locomotion agents. But while outputting a single morphology makes sense for locomotion, manipulation involves a variety of strategies depending on the task goals at hand. A manipulation agent must be capable of rapidly prototyping specialized tools for different goals. Therefore, we propose learning a designer policy, rather than a single design. A designer policy is conditioned on task information and outputs a tool design that helps solve the task. A design-conditioned controller policy can then perform manipulation using these tools. In this work, we take a step towards this goal by introducing a reinforcement learning framework for jointly learning these policies. Through simulated manipulation tasks, we show that this framework is more sample efficient than prior methods in multi-goal or multi-variant settings, can perform zero-shot interpolation or fine-tuning to tackle previously unseen goals, and allows tradeoffs between the complexity of design and control policies under practical constraints. Finally, we deploy our learned policies onto a real robot. Please see our supplementary video and website at https://robotic-tool-design.github.io/ for visualizations.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/liu23b.html
  PDF: https://proceedings.mlr.press/v229/liu23b/liu23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-liu23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ziang
    family: Liu
  - given: Stephen
    family: Tian
  - given: Michelle
    family: Guo
  - given: Karen
    family: Liu
  - given: Jiajun
    family: Wu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 887-905
  id: liu23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 887
  lastpage: 905
  published: 2023-12-02 00:00:00 +0000
- title: 'CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning'
  abstract: 'Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected and labeled datasets, which eliminates the time-consuming data collection in online RL. However, offline RL still bears a large burden of specifying/handcrafting extrinsic rewards for each transition in the offline data. As a remedy for the labor-intensive labeling, we propose to endow offline RL tasks with a few expert data and utilize the limited expert data to drive intrinsic rewards, thus eliminating the need for extrinsic rewards. To achieve that, we introduce Calibrated Latent gUidancE (CLUE), which utilizes a conditional variational auto-encoder to learn a latent space such that intrinsic rewards can be directly qualified over the latent space. CLUE’s key idea is to align the intrinsic rewards consistent with the expert intention via enforcing the embeddings of expert data to a calibrated contextual representation. We instantiate the expert-driven intrinsic rewards in sparse-reward offline RL tasks, offline imitation learning (IL) tasks, and unsupervised offline RL tasks. Empirically, we find that CLUE can effectively improve the sparse-reward offline RL performance, outperform the state-of-the-art offline IL baselines, and discover diverse skills from static reward-free offline data.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/liu23c.html
  PDF: https://proceedings.mlr.press/v229/liu23c/liu23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-liu23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jinxin
    family: Liu
  - given: Lipeng
    family: Zu
  - given: Li
    family: He
  - given: Donglin
    family: Wang
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 906-927
  id: liu23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 906
  lastpage: 927
  published: 2023-12-02 00:00:00 +0000
- title: 'DEFT: Dexterous Fine-Tuning for Hand Policies'
  abstract: 'Dexterity is often seen as a cornerstone of complex manipulation. Humans are able to perform a host of skills with their hands, from making food to operating tools. In this paper, we investigate these challenges, especially in the case of soft, deformable objects as well as complex, relatively long-horizon tasks. Although, learning such behaviors from scratch can be data inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous Fine-Tuning for Hand Policies), that leverages human-driven priors, which are executed directly in the real world. In order to improve upon these priors, DEFT involves an efficient online optimization procedure. With the integration of human-based learning and online fine-tuning, coupled with a soft robotic hand, DEFT demonstrates success across various tasks, establishing a robust, data-efficient pathway toward general dexterous manipulation. Please see our website at https://dexterousfinetuning.github.io for video results.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kannan23a.html
  PDF: https://proceedings.mlr.press/v229/kannan23a/kannan23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kannan23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Aditya
    family: Kannan
  - given: Kenneth
    family: Shaw
  - given: Shikhar
    family: Bahl
  - given: Pragna
    family: Mannam
  - given: Deepak
    family: Pathak
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 928-942
  id: kannan23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 928
  lastpage: 942
  published: 2023-12-02 00:00:00 +0000
- title: 'One-Shot Imitation Learning: A Pose Estimation Perspective'
  abstract: 'In this paper, we study imitation learning under the challenging setting of: (1) only a single demonstration, (2) no further data collection, and (3) no prior task or object knowledge. We show how, with these constraints, imitation learning can be formulated as a combination of trajectory transfer and unseen object pose estimation. To explore this idea, we provide an in-depth study on how state-of-the-art unseen object pose estimators perform for one-shot imitation learning on ten real-world tasks, and we take a deep dive into the effects that camera calibration, pose estimation error, and spatial generalisation have on task success rates. For videos, please visit www.robot-learning.uk/pose-estimation-perspective.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/vitiello23a.html
  PDF: https://proceedings.mlr.press/v229/vitiello23a/vitiello23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-vitiello23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Pietro
    family: Vitiello
  - given: Kamil
    family: Dreczkowski
  - given: Edward
    family: Johns
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 943-970
  id: vitiello23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 943
  lastpage: 970
  published: 2023-12-02 00:00:00 +0000
- title: 'Semantic Mechanical Search with Large Vision and Language Models'
  abstract: 'Moving objects to find a fully-occluded target object, known as mechanical search, is a challenging problem in robotics. As objects are often organized semantically, we conjecture that semantic information about object relationships can facilitate mechanical search and reduce search time. Large pretrained vision and language models (VLMs and LLMs) have shown promise in generalizing to uncommon objects and previously unseen real-world environments. In this work, we propose a novel framework called Semantic Mechanical Search (SMS). SMS conducts scene understanding and generates a semantic occupancy distribution explicitly using LLMs. Compared to methods that rely on visual similarities offered by CLIP embeddings, SMS leverages the deep reasoning capabilities of LLMs. Unlike prior work that uses VLMs and LLMs as end-to-end planners, which may not integrate well with specialized geometric planners, SMS can serve as a plug-in semantic module for downstream manipulation or navigation policies. For mechanical search in closed-world settings such as shelves, we compare with a geometric-based planner and show that SMS improves mechanical search performance by $24%$ across the pharmacy, kitchen, and office domains in simulation and $47.1%$ in physical experiments. For open-world real environments, SMS can produce better semantic distributions compared to CLIP-based methods, with the potential to be integrated with downstream navigation policies to improve object navigation tasks. Code, data, videos, and Appendix are available here.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/sharma23a.html
  PDF: https://proceedings.mlr.press/v229/sharma23a/sharma23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-sharma23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Satvik
    family: Sharma
  - given: Huang
    family: Huang
  - given: Kaushik
    family: Shivakumar
  - given: Lawrence Yunliang
    family: Chen
  - given: Ryan
    family: Hoque
  - given: Brian
    family: Ichter
  - given: Ken
    family: Goldberg
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 971-1005
  id: sharma23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 971
  lastpage: 1005
  published: 2023-12-02 00:00:00 +0000
- title: 'KITE: Keypoint-Conditioned Policies for Semantic Manipulation'
  abstract: 'While natural language offers a convenient shared interface for humans and robots, enabling robots to interpret and follow language commands remains a longstanding challenge in manipulation. A crucial step to realizing a performant instruction-following robot is achieving semantic manipulation – where a robot interprets language at different specificities, from high-level instructions like "Pick up the stuffed animal" to more detailed inputs like "Grab the left ear of the elephant." To tackle this, we propose Keypoints + Instructions to Execution, a two-step framework for semantic manipulation which attends to both scene semantics (distinguishing between different objects in a visual scene) and object semantics (precisely localizing different parts within an object instance). KITE first grounds an input instruction in a visual scene through 2D image keypoints, providing a highly accurate object-centric bias for downstream action inference. Provided an RGB-D scene observation, KITE then executes a learned keypoint-conditioned skill to carry out the instruction. The combined precision of keypoints and parameterized skills enables fine-grained manipulation with generalization to scene and object variations. Empirically, we demonstrate KITE in 3 real-world environments: long-horizon 6-DoF tabletop manipulation, semantic grasping, and a high-precision coffee-making task. In these settings, KITE achieves a $75%$, $70%$, and $71%$ overall success rate for instruction-following, respectively. KITE outperforms frameworks that opt for pre-trained visual language models over keypoint-based grounding, or omit skills in favor of end-to-end visuomotor control, all while being trained from fewer or comparable amounts of demonstrations. Supplementary material, datasets, code, and videos can be found on our website: https://tinyurl.com/kite-site.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/sundaresan23a.html
  PDF: https://proceedings.mlr.press/v229/sundaresan23a/sundaresan23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-sundaresan23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Priya
    family: Sundaresan
  - given: Suneel
    family: Belkhale
  - given: Dorsa
    family: Sadigh
  - given: Jeannette
    family: Bohg
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1006-1021
  id: sundaresan23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1006
  lastpage: 1021
  published: 2023-12-02 00:00:00 +0000
- title: 'BM2CP: Efficient Collaborative Perception with LiDAR-Camera Modalities'
  abstract: 'Collaborative perception enables agents to share complementary perceptual information with nearby agents. This can significantly benefit the perception performance and alleviate the issues of single-view perception, such as occlusion and sparsity. Most proposed approaches mainly focus on single modality (especially LiDAR), and not fully exploit the superiority of multi-modal perception. We propose an collaborative perception paradigm, BM2CP, which employs LiDAR and camera to achieve efficient multi-modal perception. BM2CP utilizes LiDAR-guided modal fusion, cooperative depth generation and modality-guided intermediate fusion to acquire deep interactions between modalities and agents. Moreover, it is capable to cope with the special case that one of the sensors is unavailable. Extensive experiments validate that it outperforms the state-of-the-art methods with 50X lower communication volumes in real-world autonomous driving scenarios. Our code is available at supplementary materials.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhao23a.html
  PDF: https://proceedings.mlr.press/v229/zhao23a/zhao23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhao23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Binyu
    family: Zhao
  - given: Wei
    family: ZHANG
  - given: Zhaonian
    family: Zou
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1022-1035
  id: zhao23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1022
  lastpage: 1035
  published: 2023-12-02 00:00:00 +0000
- title: 'That Sounds Right: Auditory Self-Supervision for Dynamic Robot Manipulation'
  abstract: 'Learning to produce contact-rich, dynamic behaviors from raw sensory data has been a longstanding challenge in robotics. Prominent approaches primarily focus on using visual and tactile sensing. However, pure vision often fails to capture high-frequency interaction, while current tactile sensors can be too delicate for large-scale data collection. In this work, we propose a data-centric approach to dynamic manipulation that uses an often ignored source of information – sound. We first collect a dataset of 25k interaction-sound pairs across five dynamic tasks using contact microphones. Then, given this data, we leverage self-supervised learning to accelerate behavior prediction from sound. Our experiments indicate that this self-supervised ‘pretraining’ is crucial to achieving high performance, with a $34.5%$ lower MSE than plain supervised learning and a $54.3%$ lower MSE over visual training. Importantly, we find that when asked to generate desired sound profiles, online rollouts of our models on a UR10 robot can produce dynamic behavior that achieves an average of $11.5%$ improvement over supervised learning on audio similarity metrics. Videos and audio data are best seen on our project website: aurl-anon.github.io'
  volume: 229
  URL: https://proceedings.mlr.press/v229/thankaraj23a.html
  PDF: https://proceedings.mlr.press/v229/thankaraj23a/thankaraj23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-thankaraj23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Abitha
    family: Thankaraj
  - given: Lerrel
    family: Pinto
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1036-1049
  id: thankaraj23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1036
  lastpage: 1049
  published: 2023-12-02 00:00:00 +0000
- title: 'ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting'
  abstract: 'Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks, these models accrue high errors at critical transition points leading to degradation in downstream planning performance. Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that capture how future human motion would affect the cost of a robot’s plan. We present ManiCast, a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a range of learned and heuristic baselines while additionally contributing new datasets. We release our code and datasets at https://portal-cornell.github.io/manicast/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kedia23a.html
  PDF: https://proceedings.mlr.press/v229/kedia23a/kedia23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kedia23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Kushal
    family: Kedia
  - given: Prithwish
    family: Dan
  - given: Atiksh
    family: Bhardwaj
  - given: Sanjiban
    family: Choudhury
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1050-1067
  id: kedia23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1050
  lastpage: 1067
  published: 2023-12-02 00:00:00 +0000
- title: 'Predicting Routine Object Usage for Proactive Robot Assistance'
  abstract: 'Proactivity in robot assistance refers to the robot’s ability to anticipate user needs and perform assistive actions without explicit requests. This requires understanding user routines, predicting consistent activities, and actively seeking information to predict inconsistent behaviors. We propose SLaTe-PRO (Sequential Latent Temporal model for Predicting Routine Object usage), which improves upon prior state-of-the-art by combining object and user action information, and conditioning object usage predictions on past history. Additionally, we find some human behavior to be inherently stochastic and lacking in contextual cues that the robot can use for proactive assistance. To address such cases, we introduce an interactive query mechanism that can be used to ask queries about the user’s intended activities and object use to improve prediction. We evaluate our approach on longitudinal data from three households, spanning 24 activity classes. SLaTe-PRO performance raises the F1 score metric to 0.57 without queries, and 0.60 with user queries, over a score of 0.43 from prior work. We additionally present a case study with a fully autonomous household robot.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/patel23a.html
  PDF: https://proceedings.mlr.press/v229/patel23a/patel23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-patel23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Maithili
    family: Patel
  - given: Aswin Gururaj
    family: Prakash
  - given: Sonia
    family: Chernova
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1068-1083
  id: patel23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1068
  lastpage: 1083
  published: 2023-12-02 00:00:00 +0000
- title: 'Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments'
  abstract: 'Grounding navigational commands to linear temporal logic (LTL) leverages its unambiguous semantics for reasoning about long-horizon tasks and verifying the satisfaction of temporal constraints. Existing approaches require training data from the specific environment and landmarks that will be used in natural language to understand commands in those environments. We propose Lang2LTL, a modular system and a software package that leverages large language models (LLMs) to ground temporal navigational commands to LTL specifications in environments without prior language data. We comprehensively evaluate Lang2LTL for five well-defined generalization behaviors. Lang2LTL demonstrates the state-of-the-art ability of a single model to ground navigational commands to diverse temporal specifications in 21 city-scaled environments. Finally, we demonstrate a physical robot using Lang2LTL can follow 52 semantically diverse navigational commands in two indoor environments.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/liu23d.html
  PDF: https://proceedings.mlr.press/v229/liu23d/liu23d.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-liu23d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jason Xinyu
    family: Liu
  - given: Ziyi
    family: Yang
  - given: Ifrah
    family: Idrees
  - given: Sam
    family: Liang
  - given: Benjamin
    family: Schornstein
  - given: Stefanie
    family: Tellex
  - given: Ankit
    family: Shah
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1084-1110
  id: liu23d
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1084
  lastpage: 1110
  published: 2023-12-02 00:00:00 +0000
- title: 'HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs'
  abstract: 'Robots are becoming increasingly integrated into our lives, assisting us in various tasks. To ensure effective collaboration between humans and robots, it is essential that they understand our intentions and anticipate our actions. In this paper, we propose a Human-Object Interaction (HOI) anticipation framework for collaborative robots. We propose an efficient and robust transformer-based model to detect and anticipate HOIs from videos. This enhanced anticipation empowers robots to proactively assist humans, resulting in more efficient and intuitive collaborations. Our model outperforms state-of-the-art results in HOI detection and anticipation in VidHOI dataset with an increase of $1.76%$ and $1.04%$ in mAP respectively while being 15.4 times faster. We showcase the effectiveness of our approach through experimental results in a real robot, demonstrating that the robot’s ability to anticipate HOIs is key for better Human-Robot Interaction.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/mascaro23a.html
  PDF: https://proceedings.mlr.press/v229/mascaro23a/mascaro23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-mascaro23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Esteve Valls
    family: Mascaro
  - given: Daniel
    family: Sliwowski
  - given: Dongheui
    family: Lee
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1111-1130
  id: mascaro23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1111
  lastpage: 1130
  published: 2023-12-02 00:00:00 +0000
- title: 'Reinforcement Learning Enables Real-Time Planning and Control of Agile Maneuvers for Soft Robot Arms'
  abstract: 'Control policies for soft robot arms typically assume quasi-static motion or require a hand-designed motion plan. To achieve real-time planning and control for tasks requiring highly dynamic maneuvers, we apply deep reinforcement learning to train a policy entirely in simulation, and we identify strategies and insights that bridge the gap between simulation and reality. In particular, we strengthen the policy’s tolerance for inaccuracies with domain randomization and implement crucial simulator modifications that improve actuation and sensor modeling, enabling zero-shot sim-to-real transfer without requiring high-fidelity soft robot dynamics. We demonstrate the effectiveness of this approach with experiments on physical hardware and show that our soft robot can reach target positions that require dynamic swinging motions. This is the first work to achieve such agile maneuvers on a physical soft robot, advancing the field of soft robot arm planning and control. Our code and videos are publicly available at https://sites.google.com/view/rl-soft-robot.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/jitosho23a.html
  PDF: https://proceedings.mlr.press/v229/jitosho23a/jitosho23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-jitosho23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Rianna
    family: Jitosho
  - given: Tyler Ga Wei
    family: Lum
  - given: Allison
    family: Okamura
  - given: Karen
    family: Liu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1131-1153
  id: jitosho23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1131
  lastpage: 1153
  published: 2023-12-02 00:00:00 +0000
- title: 'A Policy Optimization Method Towards Optimal-time Stability'
  abstract: 'In current model-free reinforcement learning (RL) algorithms, stability criteria based on sampling methods are commonly utilized to guide policy optimization. However, these criteria only guarantee the infinite-time convergence of the system’s state to an equilibrium point, which leads to sub-optimality of the policy. In this paper, we propose a policy optimization technique incorporating sampling-based Lyapunov stability. Our approach enables the system’s state to reach an equilibrium point within an optimal time and maintain stability thereafter, referred to as "optimal-time stability". To achieve this, we integrate the optimization method into the Actor-Critic framework, resulting in the development of the Adaptive Lyapunov-based Actor-Critic (ALAC) algorithm. Through evaluations conducted on ten robotic tasks, our approach outperforms previous studies significantly, effectively guiding the system to generate stable patterns.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wang23d.html
  PDF: https://proceedings.mlr.press/v229/wang23d/wang23d.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wang23d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Shengjie
    family: Wang
  - given: Lan
    family: Fengb
  - given: Xiang
    family: Zheng
  - given: Yuxue
    family: Cao
  - given: Oluwatosin OluwaPelumi
    family: Oseni
  - given: Haotian
    family: Xu
  - given: Tao
    family: Zhang
  - given: Yang
    family: Gao
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1154-1182
  id: wang23d
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1154
  lastpage: 1182
  published: 2023-12-02 00:00:00 +0000
- title: 'An Unbiased Look at Datasets for Visuo-Motor Pre-Training'
  abstract: 'Visual representation learning hold great promise for robotics, but is severely hampered by the scarcity and homogeneity of robotics datasets. Recent works address this problem by pre-training visual representations on large-scale but out-of-domain data (e.g., videos of egocentric interactions) and then transferring them to target robotics tasks. While the field is heavily focused on developing better pre-training algorithms, we find that dataset choice is just as important to this paradigm’s success. After all, the representation can only learn the structures or priors present in the pre-training dataset. To this end, we flip the focus on algorithms, and instead conduct a dataset centric analysis of robotic pre-training. Our findings call into question some common wisdom in the field. We observe that traditional vision datasets (like ImageNet, Kinetics and 100 Days of Hands) are surprisingly competitive options for visuo-motor representation learning, and that the pre-training dataset’s image distribution matters more than its size. Finally, we show that common simulation benchmarks are not a reliable proxy for real world performance and that simple regularization strategies can dramatically improve real world policy learning.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/dasari23a.html
  PDF: https://proceedings.mlr.press/v229/dasari23a/dasari23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-dasari23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sudeep
    family: Dasari
  - given: Mohan Kumar
    family: Srirama
  - given: Unnat
    family: Jain
  - given: Abhinav
    family: Gupta
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1183-1198
  id: dasari23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1183
  lastpage: 1198
  published: 2023-12-02 00:00:00 +0000
- title: 'Equivariant Motion Manifold Primitives'
  abstract: 'Existing movement primitive models for the most part focus on representing and generating a single trajectory for a given task, limiting their adaptability to situations in which unforeseen obstacles or new constraints may arise. In this work we propose Motion Manifold Primitives (MMP), a movement primitive paradigm that encodes and generates, for a given task, a continuous manifold of trajectories each of which can achieve the given task. To address the challenge of learning each motion manifold from a limited amount of data, we exploit inherent symmetries in the robot task by constructing motion manifold primitives that are equivariant with respect to given symmetry groups. Under the assumption that each of the MMPs can be smoothly deformed into each other, an autoencoder framework is developed to encode the MMPs and also generate solution trajectories. Experiments involving synthetic and real-robot examples demonstrate that our method outperforms existing manifold primitive methods by significant margins. Code is available at https://github.com/dlsfldl/EMMP-public.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/lee23a.html
  PDF: https://proceedings.mlr.press/v229/lee23a/lee23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-lee23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Byeongho
    family: Lee
  - given: Yonghyeon
    family: Lee
  - given: Seungyeon
    family: Kim
  - given: MinJun
    family: Son
  - given: Frank C.
    family: Park
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1199-1221
  id: lee23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1199
  lastpage: 1221
  published: 2023-12-02 00:00:00 +0000
- title: 'FlowBot++: Learning Generalized Articulated Objects Manipulation via Articulation Projection'
  abstract: 'Understanding and manipulating articulated objects, such as doors and drawers, is crucial for robots operating in human environments. We wish to develop a system that can learn to articulate novel objects with no prior interaction, after training on other articulated objects. Previous approaches for articulated object manipulation rely on either modular methods which are brittle or end-to-end methods, which lack generalizability. This paper presents FlowBot++, a deep 3D vision-based robotic system that predicts dense per-point motion and dense articulation parameters of articulated objects to assist in downstream manipulation tasks. FlowBot++ introduces a novel per-point representation of the articulated motion and articulation parameters that are combined to produce a more accurate estimate than either method on their own. Simulated experiments on the PartNet-Mobility dataset validate the performance of our system in articulating a wide range of objects, while real-world experiments on real objects’ point clouds and a Sawyer robot demonstrate the generalizability and feasibility of our system in real-world scenarios. Videos are available on our anonymized website https://sites.google.com/view/flowbotpp/home'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23c.html
  PDF: https://proceedings.mlr.press/v229/zhang23c/zhang23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Harry
    family: Zhang
  - given: Ben
    family: Eisner
  - given: David
    family: Held
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1222-1241
  id: zhang23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1222
  lastpage: 1241
  published: 2023-12-02 00:00:00 +0000
- title: 'Geometry Matching for Multi-Embodiment Grasping'
  abstract: 'While significant progress has been made on the problem of generating grasps, many existing learning-based approaches still concentrate on a single embodiment, provide limited generalization to higher DoF end-effectors and cannot capture a diverse set of grasp modes. In this paper, we tackle the problem of grasping multi-embodiments through the viewpoint of learning rich geometric representations for both objects and end-effectors using Graph Neural Networks (GNN). Our novel method – GeoMatch – applies supervised learning on grasping data from multiple embodiments, learning end-to-end contact point likelihood maps as well as conditional autoregressive prediction of grasps keypoint-by-keypoint. We compare our method against 3 baselines that provide multi-embodiment support. Our approach performs better across 3 end-effectors, while also providing competitive diversity of grasps. Examples can be found at geomatch.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/attarian23a.html
  PDF: https://proceedings.mlr.press/v229/attarian23a/attarian23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-attarian23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Maria
    family: Attarian
  - given: Muhammad Adil
    family: Asif
  - given: Jingzhou
    family: Liu
  - given: Ruthrash
    family: Hari
  - given: Animesh
    family: Garg
  - given: Igor
    family: Gilitschenski
  - given: Jonathan
    family: Tompson
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1242-1256
  id: attarian23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1242
  lastpage: 1256
  published: 2023-12-02 00:00:00 +0000
- title: 'Contrastive Value Learning: Implicit Models for Simple Offline RL'
  abstract: 'Model-based reinforcement learning (RL) methods are appealing in the offline setting because they allow an agent to reason about the consequences of actions without interacting with the environment. While conventional model-based methods learn a 1-step model, predicting the immediate next state, these methods must be plugged into larger planning or RL systems to yield a policy. Can we model the environment dynamics in a different way, such that the learned model directly indicates the value of each action? In this paper, we propose Contrastive Value Learning (CVL), which learns an implicit, multi-step dynamics model. This model can be learned without access to reward functions, but nonetheless can be used to directly estimate the value of each action, without requiring any TD learning. Because this model represents the multi-step transitions implicitly, it avoids having to predict high-dimensional observations and thus scales to high-dimensional tasks. Our experiments demonstrate that CVL outperforms prior offline RL methods on complex robotics benchmarks.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/mazoure23a.html
  PDF: https://proceedings.mlr.press/v229/mazoure23a/mazoure23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-mazoure23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Bogdan
    family: Mazoure
  - given: Benjamin
    family: Eysenbach
  - given: Ofir
    family: Nachum
  - given: Jonathan
    family: Tompson
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1257-1267
  id: mazoure23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1257
  lastpage: 1267
  published: 2023-12-02 00:00:00 +0000
- title: 'Parting with Misconceptions about Learning-based Vehicle Motion Planning'
  abstract: 'The release of nuPlan marks a new era in vehicle motion planning research, offering the first large-scale real-world dataset and evaluation schemes requiring both precise short-term planning and long-horizon ego-forecasting. Existing systems struggle to simultaneously meet both requirements. Indeed, we find that these tasks are fundamentally misaligned and should be addressed independently. We further assess the current state of closed-loop planning in the field, revealing the limitations of learning-based methods in complex real-world scenarios and the value of simple rule-based priors such as centerline selection through lane graph search algorithms. More surprisingly, for the open-loop sub-task, we observe that the best results are achieved when using only this centerline as scene context (i.e., ignoring all information regarding the map and other agents). Combining these insights, we propose an extremely simple and efficient planner which outperforms an extensive set of competitors, winning the nuPlan planning challenge 2023.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/dauner23a.html
  PDF: https://proceedings.mlr.press/v229/dauner23a/dauner23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-dauner23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Daniel
    family: Dauner
  - given: Marcel
    family: Hallgarten
  - given: Andreas
    family: Geiger
  - given: Kashyap
    family: Chitta
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1268-1281
  id: dauner23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1268
  lastpage: 1281
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning Sequential Acquisition Policies for Robot-Assisted Feeding'
  abstract: 'A robot providing mealtime assistance must perform specialized maneuvers with various utensils in order to pick up and feed a range of food items. Beyond these dexterous low-level skills, an assistive robot must also plan these strategies in sequence over a long horizon to clear a plate and complete a meal. Previous methods in robot-assisted feeding introduce highly specialized primitives for food handling without a means to compose them together. Meanwhile, existing approaches to long-horizon manipulation lack the flexibility to embed highly specialized primitives into their frameworks. We propose Visual Action Planning OveR Sequences (VAPORS), a framework for long-horizon food acquisition. VAPORS learns a policy for high-level action selection by leveraging learned latent plate dynamics in simulation. To carry out sequential plans in the real world, VAPORS delegates action execution to visually parameterized primitives. We validate our approach on complex real-world acquisition trials involving noodle acquisition and bimanual scooping of jelly beans. Across 38 plates, VAPORS acquires much more efficiently than baselines, generalizes across realistic plate variations such as toppings and sauces, and qualitatively appeals to user feeding preferences in a survey conducted across 49 individuals. Code, datasets, videos, and supplementary materials can be found on our website: https://sites.google.com/view/vaporsbot.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/sundaresan23b.html
  PDF: https://proceedings.mlr.press/v229/sundaresan23b/sundaresan23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-sundaresan23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Priya
    family: Sundaresan
  - given: Jiajun
    family: Wu
  - given: Dorsa
    family: Sadigh
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1282-1299
  id: sundaresan23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1282
  lastpage: 1299
  published: 2023-12-02 00:00:00 +0000
- title: 'Composable Part-Based Manipulation'
  abstract: 'In this paper, we propose composable part-based manipulation (CPM), a novel approach that leverages object-part decomposition and part-part correspondences to improve learning and generalization of robotic manipulation skills. By considering the functional correspondences between object parts, we conceptualize functional actions, such as pouring and constrained placing, as combinations of different correspondence constraints. CPM comprises a collection of composable diffusion models, where each model captures a different inter-object correspondence. These diffusion models can generate parameters for manipulation skills based on the specific object parts. Leveraging part-based correspondences coupled with the task decomposition into distinct constraints enables strong generalization to novel objects and object categories. We validate our approach in both simulated and real-world scenarios, demonstrating its effectiveness in achieving robust and generalized manipulation capabilities.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/liu23e.html
  PDF: https://proceedings.mlr.press/v229/liu23e/liu23e.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-liu23e.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Weiyu
    family: Liu
  - given: Jiayuan
    family: Mao
  - given: Joy
    family: Hsu
  - given: Tucker
    family: Hermans
  - given: Animesh
    family: Garg
  - given: Jiajun
    family: Wu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1300-1315
  id: liu23e
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1300
  lastpage: 1315
  published: 2023-12-02 00:00:00 +0000
- title: 'Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation'
  abstract: 'Self-driving vehicles (SDVs) must be rigorously tested on a wide range of scenarios to ensure safe deployment. The industry typically relies on closed-loop simulation to evaluate how the SDV interacts on a corpus of synthetic and real scenarios and to verify good performance. However, they primarily only test the motion planning module of the system, and only consider behavior variations. It is key to evaluate the full autonomy system in closed-loop, and to understand how variations in sensor data based on scene appearance, such as the shape of actors, affect system performance. In this paper, we propose a framework, Adv3D, that takes real world scenarios and performs closed-loop sensor simulation to evaluate autonomy performance, and finds vehicle shapes that make the scenario more challenging, resulting in autonomy failures and uncomfortable SDV maneuvers. Unlike prior work that add contrived adversarial shapes to vehicle roof-tops or roadside to harm perception performance, we optimize a low-dimensional shape representation to modify the vehicle shape itself in a realistic manner to degrade full autonomy performance (e.g., perception, prediction, motion planning). Moreover, we find that the shape variations found with Adv3D optimized in closed-loop are much more effective than open-loop, demonstrating the importance of finding and testing scene appearance variations that affect full autonomy performance.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/sarva23a.html
  PDF: https://proceedings.mlr.press/v229/sarva23a/sarva23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-sarva23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jay
    family: Sarva
  - given: Jingkang
    family: Wang
  - given: James
    family: Tu
  - given: Yuwen
    family: Xiong
  - given: Sivabalan
    family: Manivasagam
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1316-1334
  id: sarva23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1316
  lastpage: 1334
  published: 2023-12-02 00:00:00 +0000
- title: 'FindThis: Language-Driven Object Disambiguation in Indoor Environments'
  abstract: 'Natural language is naturally ambiguous. In this work, we consider interactions between a user and a mobile service robot tasked with locating a desired object, specified by a language utterance. We present a task FindThis, which addresses the problem of how to disambiguate and locate the particular object instance desired through a dialog with the user. To approach this problem we propose an algorithm, GoFind, which exploits visual attributes of the object that may be intrinsic (e.g., color, shape), or extrinsic (e.g., location, relationships to other entities), expressed in an open vocabulary. GoFind leverages the visual common sense learned by large language models to enable fine-grained object localization and attribute differentiation in a zero-shot manner. We also provide a new visio-linguistic dataset, 3D Objects in Context (3DOC), for evaluating agents on this task consisting of Google Scanned Objects placed in Habitat-Matterport 3D scenes. Finally, we validate our approach on a real robot operating in an unstructured physical office environment using complex fine-grained language instructions.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/majumdar23a.html
  PDF: https://proceedings.mlr.press/v229/majumdar23a/majumdar23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-majumdar23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Arjun
    family: Majumdar
  - given: Fei
    family: Xia
  - given: Brian
    family: Ichter
  - given: Dhruv
    family: Batra
  - given: Leonidas
    family: Guibas
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1335-1347
  id: majumdar23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1335
  lastpage: 1347
  published: 2023-12-02 00:00:00 +0000
- title: 'Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning'
  abstract: 'The offline reinforcement learning (RL) paradigm provides a general recipe to convert static behavior datasets into policies that can perform better than the policy that collected the data. While policy constraints, conservatism, and other methods for mitigating distributional shifts have made offline reinforcement learning more effective, the continuous action setting often necessitates various approximations for applying these techniques. Many of these challenges are greatly alleviated in discrete action settings, where offline RL constraints and regularizers can often be computed more precisely or even exactly. In this paper, we propose an adaptive scheme for action quantization. We use a VQ-VAE to learn state- conditioned action quantization, avoiding the exponential blowup that comes with naïve discretization of the action space. We show that several state-of-the-art offline RL methods such as IQL, CQL, and BRAC improve in performance on benchmarks when combined with our proposed discretization scheme. We further validate our approach on a set of challenging long-horizon complex robotic manipulation tasks in the Robomimic environment, where our discretized offline RL algorithms are able to improve upon their continuous counterparts by 2-3x. Our project page is at saqrl.github.io'
  volume: 229
  URL: https://proceedings.mlr.press/v229/luo23a.html
  PDF: https://proceedings.mlr.press/v229/luo23a/luo23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-luo23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jianlan
    family: Luo
  - given: Perry
    family: Dong
  - given: Jeffrey
    family: Wu
  - given: Aviral
    family: Kumar
  - given: Xinyang
    family: Geng
  - given: Sergey
    family: Levine
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1348-1361
  id: luo23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1348
  lastpage: 1361
  published: 2023-12-02 00:00:00 +0000
- title: 'Batch Differentiable Pose Refinement for In-The-Wild Camera/LiDAR Extrinsic Calibration'
  abstract: 'Accurate camera to LiDAR (Light Detection and Ranging) extrinsic calibration is important for robotic tasks carrying out tight sensor fusion — such as target tracking and odometry. Calibration is typically performed before deployment in controlled conditions using calibration targets, however, this limits scalability and subsequent recalibration. We propose a novel approach for target-free camera-LiDAR calibration using end-to-end direct alignment which doesn’t need calibration targets. Our batched formulation enhances sample efficiency during training and robustness at inference time. We present experimental results, on publicly available real-world data, demonstrating 1.6cm/$0.07^{\circ}$ median accuracy when transferred to unseen sensors from held-out data sequences. We also show state-of-the-art zero-shot transfer to unseen cameras, LiDARs, and environments.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/fu23a.html
  PDF: https://proceedings.mlr.press/v229/fu23a/fu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-fu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Lanke Frank Tarimo
    family: Fu
  - given: Maurice
    family: Fallon
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1362-1377
  id: fu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1362
  lastpage: 1377
  published: 2023-12-02 00:00:00 +0000
- title: 'Fleet Active Learning: A Submodular Maximization Approach'
  abstract: 'In multi-robot systems, robots often gather data to improve the performance of their deep neural networks (DNNs) for perception and planning. Ideally, these robots should select the most informative samples from their local data distributions by employing active learning approaches. However, when the data collection is distributed among multiple robots, redundancy becomes an issue as different robots may select similar data points. To overcome this challenge, we propose a fleet active learning (FAL) framework in which robots collectively select informative data samples to enhance their DNN models. Our framework leverages submodular maximization techniques to prioritize the selection of samples with high information gain. Through an iterative algorithm, the robots coordinate their efforts to collectively select the most valuable samples while minimizing communication between robots. We provide a theoretical analysis of the performance of our proposed framework and show that it is able to approximate the NP-hard optimal solution. We demonstrate the effectiveness of our framework through experiments on real-world perception and classification datasets, which include autonomous driving datasets such as Berkeley DeepDrive. Our results show an improvement by up to $25.0 %$ in classification accuracy, $9.2 %$ in mean average precision and $48.5 %$ in the submodular objective value compared to a completely distributed baseline.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/akcin23a.html
  PDF: https://proceedings.mlr.press/v229/akcin23a/akcin23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-akcin23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Oguzhan
    family: Akcin
  - given: Orhan
    family: Unuvar
  - given: Onat
    family: Ure
  - given: Sandeep P.
    family: Chinchali
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1378-1399
  id: akcin23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1378
  lastpage: 1399
  published: 2023-12-02 00:00:00 +0000
- title: 'Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization'
  abstract: 'Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy under uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named Uncertainty Set Regularizer (USR), to formulate the uncertainty set on the parametric space of a transition function. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark and the Unitree A1 Robot, demonstrating improvements in the robust performance of perturbed testing environments and sim-to-real scenarios.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23d.html
  PDF: https://proceedings.mlr.press/v229/zhang23d/zhang23d.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuan
    family: Zhang
  - given: Jianhong
    family: Wang
  - given: Joschka
    family: Boedecker
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1400-1424
  id: zhang23d
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1400
  lastpage: 1424
  published: 2023-12-02 00:00:00 +0000
- title: 'Context-Aware Deep Reinforcement Learning for Autonomous Robotic Navigation in Unknown Area'
  abstract: 'Mapless navigation refers to a challenging task where a mobile robot must rapidly navigate to a predefined destination using its partial knowledge of the environment, which is updated online along the way, instead of a prior map of the environment. Inspired by the recent developments in deep reinforcement learning (DRL), we propose a learning-based framework for mapless navigation, which employs a context-aware policy network to achieve efficient decision-making (i.e., maximize the likelihood of finding the shortest route towards the target destination), especially in complex and large-scale environments. Specifically, our robot learns to form a context of its belief over the entire known area, which it uses to reason about long-term efficiency and sequence show-term movements. Additionally, we propose a graph rarefaction algorithm to enable more efficient decision-making in large-scale applications. We empirically demonstrate that our approach reduces average travel time by up to $61.4%$ and average planning time by up to $88.2%$ compared to benchmark planners (D*lite and BIT) on hundreds of test scenarios. We also validate our approach both in high-fidelity Gazebo simulations as well as on hardware, highlighting its promising applicability in the real world without further training/tuning.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/liang23a.html
  PDF: https://proceedings.mlr.press/v229/liang23a/liang23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-liang23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jingsong
    family: Liang
  - given: Zhichen
    family: Wang
  - given: Yuhong
    family: Cao
  - given: Jimmy
    family: Chiun
  - given: Mengqi
    family: Zhang
  - given: Guillaume Adrien
    family: Sartoretti
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1425-1436
  id: liang23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1425
  lastpage: 1436
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning to Discern: Imitating Heterogeneous Human Demonstrations with Preference and Representation Learning'
  abstract: 'Practical Imitation Learning (IL) systems rely on large human demonstration datasets for successful policy learning. However, challenges lie in maintaining the quality of collected data and addressing the suboptimal nature of some demonstrations, which can compromise the overall dataset quality and hence the learning outcome. Furthermore, the intrinsic heterogeneity in human behavior can produce equally successful but disparate demonstrations, further exacerbating the challenge of discerning demonstration quality. To address these challenges, this paper introduces Learning to Discern (L2D), an offline imitation learning framework for learning from demonstrations with diverse quality and style. Given a small batch of demonstrations with sparse quality labels, we learn a latent representation for temporally embedded trajectory segments. Preference learning in this latent space trains a quality evaluator that generalizes to new demonstrators exhibiting different styles. Empirically, we show that L2D can effectively assess and learn from varying demonstrations, thereby leading to improved policy performance across a range of tasks in both simulations and on a physical robot.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kuhar23a.html
  PDF: https://proceedings.mlr.press/v229/kuhar23a/kuhar23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kuhar23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sachit
    family: Kuhar
  - given: Shuo
    family: Cheng
  - given: Shivang
    family: Chopra
  - given: Matthew
    family: Bronars
  - given: Danfei
    family: Xu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1437-1449
  id: kuhar23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1437
  lastpage: 1449
  published: 2023-12-02 00:00:00 +0000
- title: 'Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter'
  abstract: 'Robots operating in human-centric environments require the integration of visual grounding and grasping capabilities to effectively manipulate objects based on user instructions. This work focuses on the task of referring grasp synthesis, which predicts a grasp pose for an object referred through natural language in cluttered scenes. Existing approaches often employ multi-stage pipelines that first segment the referred object and then propose a suitable grasp, and are evaluated in private datasets or simulators that do not capture the complexity of natural indoor scenes. To address these limitations, we develop a challenging benchmark based on cluttered indoor scenes from OCID dataset, for which we generate referring expressions and connect them with 4-DoF grasp poses. Further, we propose a novel end-to-end model (CROG) that leverages the visual grounding capabilities of CLIP to learn grasp synthesis directly from image-text pairs. Our results show that vanilla integration of CLIP with pretrained models transfers poorly in our challenging benchmark, while CROG achieves significant improvements both in terms of grounding and grasping. Extensive robot experiments in both simulation and hardware demonstrate the effectiveness of our approach in challenging interactive object grasping scenarios that include clutter.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/tziafas23a.html
  PDF: https://proceedings.mlr.press/v229/tziafas23a/tziafas23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-tziafas23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Georgios
    family: Tziafas
  - given: Yucheng
    family: XU
  - given: Arushi
    family: Goel
  - given: Mohammadreza
    family: Kasaei
  - given: Zhibin
    family: Li
  - given: Hamidreza
    family: Kasaei
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1450-1466
  id: tziafas23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1450
  lastpage: 1466
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning Reusable Manipulation Strategies'
  abstract: 'Humans demonstrate an impressive ability to acquire and generalize manipulation “tricks.” Even from a single demonstration, such as using soup ladles to reach for distant objects, we can apply this skill to new scenarios involving different object positions, sizes, and categories (e.g., forks and hammers). Additionally, we can flexibly combine various skills to devise long-term plans. In this paper, we present a framework that enables machines to acquire such manipulation skills, referred to as “mechanisms,” through a single demonstration and self-play. Our key insight lies in interpreting each demonstration as a sequence of changes in robot-object and object-object contact modes, which provides a scaffold for learning detailed samplers for continuous parameters. These learned mechanisms and samplers can be seamlessly integrated into standard task and motion planners, enabling their compositional use.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/mao23a.html
  PDF: https://proceedings.mlr.press/v229/mao23a/mao23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-mao23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jiayuan
    family: Mao
  - given: Tomás
    family: Lozano-Pérez
  - given: Joshua B.
    family: Tenenbaum
  - given: Leslie Pack
    family: Kaelbling
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1467-1483
  id: mao23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1467
  lastpage: 1483
  published: 2023-12-02 00:00:00 +0000
- title: 'Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards'
  abstract: 'Preference-based reinforcement learning (PbRL) aligns a robot behavior with human preferences via a reward function learned from binary feedback over agent behaviors. We show that encoding environment dynamics in the reward function improves the sample efficiency of PbRL by an order of magnitude. In our experiments we iterate between: (1) encoding environment dynamics in a state-action representation $z^{sa}$ via a self-supervised temporal consistency task, and (2) bootstrapping the preference-based reward function from $z^{sa}$, which results in faster policy learning and better final policy performance. For example, on quadruped-walk, walker-walk, and cheetah-run, with 50 preference labels we achieve the same performance as existing approaches with 500 preference labels, and we recover $83%$ and $66%$ of ground truth reward policy performance versus only $38%$ and $21%$ without environment dynamics. The performance gains demonstrate that explicitly encoding environment dynamics improves preference-learned reward functions.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/metcalf23a.html
  PDF: https://proceedings.mlr.press/v229/metcalf23a/metcalf23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-metcalf23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Katherine
    family: Metcalf
  - given: Miguel
    family: Sarabia
  - given: Natalie
    family: Mackraz
  - given: Barry-John
    family: Theobald
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1484-1532
  id: metcalf23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1484
  lastpage: 1532
  published: 2023-12-02 00:00:00 +0000
- title: 'Im2Contact: Vision-Based Contact Localization Without Touch or Force Sensing'
  abstract: 'Contacts play a critical role in most manipulation tasks. Robots today mainly use proximal touch/force sensors to sense contacts, but the information they provide must be calibrated and is inherently local, with practical applications relying either on extensive surface coverage or restrictive assumptions to resolve ambiguities. We propose a vision-based extrinsic contact localization task: with only a single RGB-D camera view of a robot workspace, identify when and where an object held by the robot contacts the rest of the environment. We show that careful task-attuned design is critical for a neural network trained in simulation to discover solutions that transfer well to a real robot. Our final approach im2contact demonstrates the promise of versatile general-purpose contact perception from vision alone, performing well for localizing various contact types (point, line, or planar; sticking, sliding, or rolling; single or multiple), and even under occlusions in its camera view. Video results can be found at: https://sites.google.com/view/im2contact/home'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kim23b.html
  PDF: https://proceedings.mlr.press/v229/kim23b/kim23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kim23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Leon
    family: Kim
  - given: Yunshuang
    family: Li
  - given: Michael
    family: Posa
  - given: Dinesh
    family: Jayaraman
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1533-1546
  id: kim23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1533
  lastpage: 1546
  published: 2023-12-02 00:00:00 +0000
- title: 'DROID: Learning from Offline Heterogeneous Demonstrations via Reward-Policy Distillation'
  abstract: 'Offline Learning from Demonstrations (OLfD) is valuable in domains where trial-and-error learning is infeasible or specifying a cost function is difficult, such as robotic surgery, autonomous driving, and path-finding for NASA’s Mars rovers. However, two key problems remain challenging in OLfD: 1) heterogeneity: demonstration data can be generated with diverse preferences and strategies, and 2) generalizability: the learned policy and reward must perform well beyond a limited training regime in unseen test settings. To overcome these challenges, we propose Dual Reward and policy Offline Inverse Distillation (DROID), where the key idea is to leverage diversity to improve generalization performance by decomposing common-task and individual-specific strategies and distilling knowledge in both the reward and policy spaces. We ground DROID in a novel and uniquely challenging Mars rover path-planning problem for NASA’s Mars Curiosity Rover. We also curate a novel dataset along 163 Sols (Martian days) and conduct a novel, empirical investigation to characterize heterogeneity in the dataset. We find DROID outperforms prior SOTA OLfD techniques, leading to a $26%$ improvement in modeling expert behaviors and $92%$ closer to the task objective of reaching the final destination. We also benchmark DROID on the OpenAI Gym Cartpole environment and find DROID achieves $55%$ (significantly) better performance modeling heterogeneous demonstrations.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/jayanthi23a.html
  PDF: https://proceedings.mlr.press/v229/jayanthi23a/jayanthi23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-jayanthi23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sravan
    family: Jayanthi
  - given: Letian
    family: Chen
  - given: Nadya
    family: Balabanska
  - given: Van
    family: Duong
  - given: Erik
    family: Scarlatescu
  - given: Ezra
    family: Ameperosa
  - given: Zulfiqar Haider
    family: Zaidi
  - given: Daniel
    family: Martin
  - given: Taylor Keith Del
    family: Matto
  - given: Masahiro
    family: Ono
  - given: Matthew
    family: Gombolay
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1547-1571
  id: jayanthi23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1547
  lastpage: 1571
  published: 2023-12-02 00:00:00 +0000
- title: 'SA6D: Self-Adaptive Few-Shot 6D Pose Estimator for Novel and Occluded Objects'
  abstract: 'To enable meaningful robotic manipulation of objects in the real-world, 6D pose estimation is one of the critical aspects. Most existing approaches have difficulties to extend predictions to scenarios where novel object instances are continuously introduced, especially with heavy occlusions. In this work, we propose a few-shot pose estimation (FSPE) approach called SA6D, which uses a self-adaptive segmentation module to identify the novel target object and construct a point cloud model of the target object using only a small number of cluttered reference images. Unlike existing methods, SA6D does not require object-centric reference images or any additional object information, making it a more generalizable and scalable solution across categories. We evaluate SA6D on real-world tabletop object datasets and demonstrate that SA6D outperforms existing FSPE methods, particularly in cluttered scenes with occlusions, while requiring fewer reference images.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/gao23a.html
  PDF: https://proceedings.mlr.press/v229/gao23a/gao23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-gao23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ning
    family: Gao
  - given: Vien Anh
    family: Ngo
  - given: Hanna
    family: Ziesche
  - given: Gerhard
    family: Neumann
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1572-1595
  id: gao23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1572
  lastpage: 1595
  published: 2023-12-02 00:00:00 +0000
- title: 'Hierarchical Planning for Rope Manipulation using Knot Theory and a Learned Inverse Model'
  abstract: 'This work considers planning the manipulation of deformable 1-dimensional objects, such as ropes or cables, specifically to tie knots. We propose TWISTED: Tying With Inverse model and Search in Topological space Excluding Demos, a hierarchical planning approach which, at the high level, uses ideas from knot-theory to plan a sequence of rope configurations, while at the low level uses a neural-network inverse model to move between the configurations in the high-level plan. To train the neural network, we propose a self-supervised approach, where we learn from random movements of the rope. To focus the random movements on interesting configurations, such as knots, we propose a non-uniform sampling method tailored for this domain. In a simulation, we show that our approach can plan significantly faster and more accurately than baselines. We also show that our plans are robust to parameter changes in the physical simulation, suggesting future applications via sim2real.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/sudry23a.html
  PDF: https://proceedings.mlr.press/v229/sudry23a/sudry23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-sudry23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Matan
    family: Sudry
  - given: Tom
    family: Jurgenson
  - given: Aviv
    family: Tamar
  - given: Erez
    family: Karpas
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1596-1609
  id: sudry23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1596
  lastpage: 1609
  published: 2023-12-02 00:00:00 +0000
- title: 'OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data'
  abstract: 'This work presents OVIR-3D, a straightforward yet effective method for open-vocabulary 3D object instance retrieval without using any 3D data for training. Given a language query, the proposed method is able to return a ranked set of 3D object instance segments based on the feature similarity of the instance and the text query. This is achieved by a multi-view fusion of text-aligned 2D region proposals into 3D space, where the 2D region proposal network could leverage 2D datasets, which are more accessible and typically larger than 3D datasets. The proposed fusion process is efficient as it can be performed in real-time for most indoor 3D scenes and does not require additional training in 3D space. Experiments on public datasets and a real robot show the effectiveness of the method and its potential for applications in robot navigation and manipulation.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/lu23a.html
  PDF: https://proceedings.mlr.press/v229/lu23a/lu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-lu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Shiyang
    family: Lu
  - given: Haonan
    family: Chang
  - given: Eric Pu
    family: Jing
  - given: Abdeslam
    family: Boularias
  - given: Kostas
    family: Bekris
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1610-1620
  id: lu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1610
  lastpage: 1620
  published: 2023-12-02 00:00:00 +0000
- title: 'Efficient Sim-to-real Transfer of Contact-Rich Manipulation Skills with Online Admittance Residual Learning'
  abstract: 'Learning contact-rich manipulation skills is essential. Such skills require the robots to interact with the environment with feasible manipulation trajectories and suitable compliance control parameters to enable safe and stable contact. However, learning these skills is challenging due to data inefficiency in the real world and the sim-to-real gap in simulation. In this paper, we introduce a hybrid offline-online framework to learn robust manipulation skills. We employ model-free reinforcement learning for the offline phase to obtain the robot motion and compliance control parameters in simulation \RV{with domain randomization}. Subsequently, in the online phase, we learn the residual of the compliance control parameters to maximize robot performance-related criteria with force sensor measurements in real-time. To demonstrate the effectiveness and robustness of our approach, we provide comparative results against existing methods for assembly, pivoting, and screwing tasks.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23e.html
  PDF: https://proceedings.mlr.press/v229/zhang23e/zhang23e.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23e.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xiang
    family: Zhang
  - given: Changhao
    family: Wang
  - given: Lingfeng
    family: Sun
  - given: Zheng
    family: Wu
  - given: Xinghao
    family: Zhu
  - given: Masayoshi
    family: Tomizuka
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1621-1639
  id: zhang23e
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1621
  lastpage: 1639
  published: 2023-12-02 00:00:00 +0000
- title: 'Tell Me Where to Go: A Composable Framework for Context-Aware Embodied Robot Navigation'
  abstract: 'Humans have the remarkable ability to navigate through unfamiliar environments by solely relying on our prior knowledge and descriptions of the environment. For robots to perform the same type of navigation, they need to be able to associate natural language descriptions with their associated physical environment with a limited amount of prior knowledge. Recently, Large Language Models (LLMs) have been able to reason over billions of parameters and utilize them in multi-modal chat-based natural language responses. However, LLMs lack real-world awareness and their outputs are not always predictable. In this work, we develop a low-bandwidth framework that solves this lack of real-world generalization by creating an intermediate layer between an LLM and a robot navigation framework in the form of Python code. Our intermediate shoehorns the vast prior knowledge inherent in an LLM model into a series of input and output API instructions that a mobile robot can understand. We evaluate our method across four different environments and command classes on a mobile robot and highlight our framework’s ability to interpret contextual commands.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/biggie23a.html
  PDF: https://proceedings.mlr.press/v229/biggie23a/biggie23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-biggie23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Harel
    family: Biggie
  - given: Ajay Narasimha
    family: Mopidevi
  - given: Dusty
    family: Woods
  - given: Chris
    family: Heckman
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1640-1666
  id: biggie23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1640
  lastpage: 1666
  published: 2023-12-02 00:00:00 +0000
- title: 'Dynamic Multi-Team Racing: Competitive Driving on 1/10-th Scale Vehicles via Learning in Simulation'
  abstract: 'Autonomous racing is a challenging task that requires vehicle handling at the dynamic limits of friction. While single-agent scenarios like Time Trials are solved competitively with classical model-based or model-free feedback control, multi-agent wheel-to-wheel racing poses several challenges including planning over unknown opponent intentions as well as negotiating interactions under dynamic constraints. We propose to address these challenges via a learning-based approach that effectively combines model-based techniques, massively parallel simulation, and self-play reinforcement learning to enable zero-shot sim-to-real transfer of highly dynamic policies. We deploy our algorithm in wheel-to-wheel multi-agent races on scale hardware to demonstrate the efficacy of our approach. Further details and videos can be found on the project website: https://sites.google.com/view/dynmutr/home.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/werner23a.html
  PDF: https://proceedings.mlr.press/v229/werner23a/werner23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-werner23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Peter
    family: Werner
  - given: Tim
    family: Seyde
  - given: Paul
    family: Drews
  - given: Thomas Matrai
    family: Balch
  - given: Igor
    family: Gilitschenski
  - given: Wilko
    family: Schwarting
  - given: Guy
    family: Rosman
  - given: Sertac
    family: Karaman
  - given: Daniela
    family: Rus
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1667-1685
  id: werner23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1667
  lastpage: 1685
  published: 2023-12-02 00:00:00 +0000
- title: 'Stochastic Occupancy Grid Map Prediction in Dynamic Scenes'
  abstract: 'This paper presents two variations of a novel stochastic prediction algorithm that enables mobile robots to accurately and robustly predict the future state of complex dynamic scenes. The proposed algorithm uses a variational autoencoder to predict a range of possible future states of the environment. The algorithm takes full advantage of the motion of the robot itself, the motion of dynamic objects, and the geometry of static objects in the scene to improve prediction accuracy. Three simulated and real-world datasets collected by different robot models are used to demonstrate that the proposed algorithm is able to achieve more accurate and robust prediction performance than other prediction algorithms. Furthermore, a predictive uncertainty-aware planner is proposed to demonstrate the effectiveness of the proposed predictor in simulation and real-world navigation experiments. Implementations are open source at https://github.com/TempleRAIL/SOGMP.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/xie23a.html
  PDF: https://proceedings.mlr.press/v229/xie23a/xie23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-xie23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zhanteng
    family: Xie
  - given: Philip
    family: Dames
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1686-1705
  id: xie23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1686
  lastpage: 1705
  published: 2023-12-02 00:00:00 +0000
- title: 'A Bayesian approach to breaking things: efficiently predicting and repairing failure modes via sampling'
  abstract: 'Before autonomous systems can be deployed in safety-critical applications, we must be able to understand and verify the safety of these systems. For cases where the risk or cost of real-world testing is prohibitive, we propose a simulation-based framework for a) predicting ways in which an autonomous system is likely to fail and b) automatically adjusting the system’s design to preemptively mitigate those failures. We frame this problem through the lens of approximate Bayesian inference and use differentiable simulation for efficient failure case prediction and repair. We apply our approach on a range of robotics and control problems, including optimizing search patterns for robot swarms and reducing the severity of outages in power transmission networks. Compared to optimization-based falsification techniques, our method predicts a more diverse, representative set of failure modes, and we also find that our use of differentiable simulation yields solutions that have up to 10x lower cost and requires up to 2x fewer iterations to converge relative to gradient-free techniques.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/dawson23a.html
  PDF: https://proceedings.mlr.press/v229/dawson23a/dawson23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-dawson23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Charles
    family: Dawson
  - given: Chuchu
    family: Fan
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1706-1722
  id: dawson23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1706
  lastpage: 1722
  published: 2023-12-02 00:00:00 +0000
- title: 'BridgeData V2: A Dataset for Robot Learning at Scale'
  abstract: 'We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research in scalable robot learning. BridgeData V2 contains 53,896 trajectories collected across 24 environments on a publicly available low-cost robot. Unlike many existing robotic manipulation datasets, BridgeData V2 provides enough task and environment variability that skills learned from the data generalize across institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments,we apply 6 state-of-the-art imitation learning and offline reinforcement learning methods to the data and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models. By publicly sharing BridgeData V2 and our pre-trained models, we aim to accelerate research in scalable robot learning methods.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/walke23a.html
  PDF: https://proceedings.mlr.press/v229/walke23a/walke23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-walke23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Homer Rich
    family: Walke
  - given: Kevin
    family: Black
  - given: Tony Z.
    family: Zhao
  - given: Quan
    family: Vuong
  - given: Chongyi
    family: Zheng
  - given: Philippe
    family: Hansen-Estruch
  - given: Andre Wang
    family: He
  - given: Vivek
    family: Myers
  - given: Moo Jin
    family: Kim
  - given: Max
    family: Du
  - given: Abraham
    family: Lee
  - given: Kuan
    family: Fang
  - given: Chelsea
    family: Finn
  - given: Sergey
    family: Levine
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1723-1736
  id: walke23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1723
  lastpage: 1736
  published: 2023-12-02 00:00:00 +0000
- title: 'NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities'
  abstract: 'We present Neural Signal Operated Intelligent Robots (NOIR), a general-purpose, intelligent brain-robot interface system that enables humans to command robots to perform everyday activities through brain signals. Through this interface, humans communicate their intended objects of interest and actions to the robots using electroencephalography (EEG). Our novel system demonstrates success in an expansive array of 20 challenging, everyday household activities, including cooking, cleaning, personal care, and entertainment. The effectiveness of the system is improved by its synergistic integration of robot learning algorithms, allowing for NOIR to adapt to individual users and predict their intentions. Our work enhances the way humans interact with robots, replacing traditional channels of interaction with direct, neural communication.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23f.html
  PDF: https://proceedings.mlr.press/v229/zhang23f/zhang23f.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23f.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ruohan
    family: Zhang
  - given: Sharon
    family: Lee
  - given: Minjune
    family: Hwang
  - given: Ayano
    family: Hiranaka
  - given: Chen
    family: Wang
  - given: Wensi
    family: Ai
  - given: Jin Jie Ryan
    family: Tan
  - given: Shreya
    family: Gupta
  - given: Yilun
    family: Hao
  - given: Gabrael
    family: Levine
  - given: Ruohan
    family: Gao
  - given: Anthony
    family: Norcia
  - given: Li
    family: Fei-Fei
  - given: Jiajun
    family: Wu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1737-1760
  id: zhang23f
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1737
  lastpage: 1760
  published: 2023-12-02 00:00:00 +0000
- title: 'PolarNet: 3D Point Clouds for Language-Guided Robotic Manipulation'
  abstract: 'The ability for robots to comprehend and execute manipulation tasks based on natural language instructions is a long-term goal in robotics. The dominant approaches for language-guided manipulation use 2D image representations, which face difficulties in combining multi-view cameras and inferring precise 3D positions and relationships. To address these limitations, we propose a 3D point cloud based policy called PolarNet for language-guided manipulation. It leverages carefully designed point cloud inputs, efficient point cloud encoders, and multimodal transformers to learn 3D point cloud representations and integrate them with language instructions for action prediction. PolarNet is shown to be effective and data efficient in a variety of experiments conducted on the RLBench benchmark. It outperforms state-of-the-art 2D and 3D approaches in both single-task and multi-task learning. It also achieves promising results on a real robot.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/chen23b.html
  PDF: https://proceedings.mlr.press/v229/chen23b/chen23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-chen23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Shizhe
    family: Chen
  - given: Ricardo Garcia
    family: Pinel
  - given: Cordelia
    family: Schmid
  - given: Ivan
    family: Laptev
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1761-1781
  id: chen23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1761
  lastpage: 1781
  published: 2023-12-02 00:00:00 +0000
- title: 'Stealthy Terrain-Aware Multi-Agent Active Search'
  abstract: 'Stealthy multi-agent active search is the problem of making efficient sequential data-collection decisions to identify an unknown number of sparsely located targets while adapting to new sensing information and concealing the search agents’ location from the targets. This problem is applicable to reconnaissance tasks wherein the safety of the search agents can be compromised as the targets may be adversarial. Prior work usually focuses either on adversarial search, where the risk of revealing the agents’ location to the targets is ignored or evasion strategies where efficient search is ignored. We present the Stealthy Terrain-Aware Reconnaissance (STAR) algorithm, a multi-objective parallelized Thompson sampling-based algorithm that relies on a strong topographical prior to reason over changing visibility risk over the course of the search. The STAR algorithm outperforms existing state-of-the-art multi-agent active search methods on both rate of recovery of targets as well as minimising risk even when subject to noisy observations, communication failures and an unknown number of targets.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/bakshi23a.html
  PDF: https://proceedings.mlr.press/v229/bakshi23a/bakshi23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-bakshi23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Nikhil Angad
    family: Bakshi
  - given: Jeff
    family: Schneider
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1782-1796
  id: bakshi23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1782
  lastpage: 1796
  published: 2023-12-02 00:00:00 +0000
- title: 'A Data-Efficient Visual-Audio Representation with Intuitive Fine-tuning for Voice-Controlled Robots'
  abstract: 'A command-following robot that serves people in everyday life must continually improve itself in deployment domains with minimal help from its end users, instead of engineers. Previous methods are either difficult to continuously improve after the deployment or require a large number of new labels during fine-tuning. Motivated by (self-)supervised contrastive learning, we propose a novel representation that generates an intrinsic reward function for command-following robot tasks by associating images with sound commands. After the robot is deployed in a new domain, the representation can be updated intuitively and data-efficiently by non-experts without any hand-crafted reward functions. We demonstrate our approach on various sound types and robotic tasks, including navigation and manipulation with raw sensor inputs. In simulated and real-world experiments, we show that our system can continually self-improve in previously unseen scenarios given fewer new labeled data, while still achieving better performance over previous methods.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/chang23a.html
  PDF: https://proceedings.mlr.press/v229/chang23a/chang23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-chang23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Peixin
    family: Chang
  - given: Shuijing
    family: Liu
  - given: Tianchen
    family: Ji
  - given: Neeloy
    family: Chakraborty
  - given: Kaiwen
    family: Hong
  - given: Katherine Rose
    family: Driggs-Campbell
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1797-1819
  id: chang23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1797
  lastpage: 1819
  published: 2023-12-02 00:00:00 +0000
- title: 'MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations'
  abstract: 'Imitation learning from a large set of human demonstrations has proved to be an effective paradigm for building capable robot agents. However, the demonstrations can be extremely costly and time-consuming to collect. We introduce MimicGen, a system for automatically synthesizing large-scale, rich datasets from only a small number of human demonstrations by adapting them to new contexts. We use MimicGen to generate over 50K demonstrations across 18 tasks with diverse scene configurations, object instances, and robot arms from just  200 human demonstrations. We show that robot agents can be effectively trained on this generated dataset by imitation learning to achieve strong performance in long-horizon and high-precision tasks, such as multi-part assembly and coffee preparation, across broad initial state distributions. We further demonstrate that the effectiveness and utility of MimicGen data compare favorably to collecting additional human demonstrations, making it a powerful and economical approach towards scaling up robot learning. Datasets, simulation environments, videos, and more at https://mimicgen.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/mandlekar23a.html
  PDF: https://proceedings.mlr.press/v229/mandlekar23a/mandlekar23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-mandlekar23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ajay
    family: Mandlekar
  - given: Soroush
    family: Nasiriany
  - given: Bowen
    family: Wen
  - given: Iretiayo
    family: Akinola
  - given: Yashraj
    family: Narang
  - given: Linxi
    family: Fan
  - given: Yuke
    family: Zhu
  - given: Dieter
    family: Fox
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1820-1864
  id: mandlekar23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1820
  lastpage: 1864
  published: 2023-12-02 00:00:00 +0000
- title: 'Quantifying Assistive Robustness Via the Natural-Adversarial Frontier'
  abstract: 'Our ultimate goal is to build robust policies for robots that assist people. What makes this hard is that people can behave unexpectedly at test time, potentially interacting with the robot outside its training distribution and leading to failures. Even just measuring robustness is a challenge. Adversarial perturbations are the default, but they can paint the wrong picture: they can correspond to human motions that are unlikely to occur during natural interactions with people. A robot policy might fail under small adversarial perturbations but work under large natural perturbations. We propose that capturing robustness in these interactive settings requires constructing and analyzing the entire natural-adversarial frontier: the Pareto-frontier of human policies that are the best trade-offs between naturalness and low robot performance. We introduce RIGID, a method for constructing this frontier by training adversarial human policies that trade off between minimizing robot reward and acting human-like (as measured by a discriminator). On an Assistive Gym task, we use RIGID to analyze the performance of standard collaborative RL, as well as the performance of existing methods meant to increase robustness. We also compare the frontier RIGID identifies with the failures identified in expert adversarial interaction, and with naturally-occurring failures during user interaction. Overall, we find evidence that RIGID can provide a meaningful measure of robustness predictive of deployment performance, and uncover failure cases that are difficult to find manually.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/he23a.html
  PDF: https://proceedings.mlr.press/v229/he23a/he23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-he23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jerry Zhi-Yang
    family: He
  - given: Daniel S.
    family: Brown
  - given: Zackory
    family: Erickson
  - given: Anca
    family: Dragan
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1865-1886
  id: he23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1865
  lastpage: 1886
  published: 2023-12-02 00:00:00 +0000
- title: 'Dynamic Handover: Throw and Catch with Bimanual Hands'
  abstract: 'Humans throw and catch objects all the time. However, such a seemingly common skill introduces a lot of challenges for robots to achieve: The robots need to operate such dynamic actions at high-speed, collaborate precisely, and interact with diverse objects. In this paper, we design a system with two multi-finger hands attached to robot arms to solve this problem. We train our system using Multi-Agent Reinforcement Learning in simulation and perform Sim2Real transfer to deploy on the real robots. To overcome the Sim2Real gap, we provide multiple novel algorithm designs including learning a trajectory prediction model for the object. Such a model can help the robot catcher has a real-time estimation of where the object will be heading, and then react accordingly. We conduct our experiments with multiple objects in the real-world system, and show significant improvements over multiple baselines. Our project page is available at https://binghao-huang.github.io/dynamic_handover/'
  volume: 229
  URL: https://proceedings.mlr.press/v229/huang23d.html
  PDF: https://proceedings.mlr.press/v229/huang23d/huang23d.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-huang23d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Binghao
    family: Huang
  - given: Yuanpei
    family: Chen
  - given: Tianyu
    family: Wang
  - given: Yuzhe
    family: Qin
  - given: Yaodong
    family: Yang
  - given: Nikolay
    family: Atanasov
  - given: Xiaolong
    family: Wang
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1887-1902
  id: huang23d
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1887
  lastpage: 1902
  published: 2023-12-02 00:00:00 +0000
- title: 'Cross-Dataset Sensor Alignment: Making Visual 3D Object Detector Generalizable'
  abstract: 'While camera-based 3D object detection has evolved rapidly, these models are susceptible to overfitting to specific sensor setups. For example, in autonomous driving, most datasets are collected using a single sensor configuration. This paper evaluates the generalization capability of camera-based 3D object detectors, including adapting detectors from one dataset to another and training detectors with multiple datasets. We observe that merely aggregating datasets yields drastic performance drops, contrary to the expected improvements associated with increased training data. To close the gap, we introduce an efficient technique for aligning disparate sensor configurations — a combination of camera intrinsic synchronization, camera extrinsic correction, and ego frame alignment, which collectively enhance cross-dataset performance remarkably. Compared with single dataset baselines, we achieve 42.3 mAP improvement on KITTI, 23.2 mAP improvement on Lyft, 18.5 mAP improvement on nuScenes, 17.3 mAP improvement on KITTI-360, 8.4 mAP improvement on Argoverse2 and 3.9 mAP improvement on Waymo. We hope this comprehensive study can facilitate research on generalizable 3D object detection and associated tasks.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zheng23a.html
  PDF: https://proceedings.mlr.press/v229/zheng23a/zheng23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zheng23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Liangtao
    family: Zheng
  - given: Yicheng
    family: Liu
  - given: Yue
    family: Wang
  - given: Hang
    family: Zhao
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1903-1929
  id: zheng23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1903
  lastpage: 1929
  published: 2023-12-02 00:00:00 +0000
- title: 'REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation'
  abstract: 'Dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance forces on the non-prehensile object, and control a high number of degrees of freedom. Reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advancements in sample-efficient RL and replay buffer bootstrapping. This unique combination allows us to utilize data from different tasks or objects as a starting point for training new tasks, significantly improving learning efficiency. Additionally, our system completes the real-world training cycle by incorporating learned resets via an imitation-based pickup policy and learned reward functions, to eliminate the need for manual reset and reward engineering. We show the benefits of reusing past data as replay buffer initialization for new tasks, for instance, the fast acquisitions of intricate manipulation skills in the real world on a four-fingered robotic hand. https://sites.google.com/view/reboot-dexterous'
  volume: 229
  URL: https://proceedings.mlr.press/v229/hu23a.html
  PDF: https://proceedings.mlr.press/v229/hu23a/hu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-hu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zheyuan
    family: Hu
  - given: Aaron
    family: Rovinsky
  - given: Jianlan
    family: Luo
  - given: Vikash
    family: Kumar
  - given: Abhishek
    family: Gupta
  - given: Sergey
    family: Levine
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1930-1949
  id: hu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1930
  lastpage: 1949
  published: 2023-12-02 00:00:00 +0000
- title: 'Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs'
  abstract: 'We present an Open-Vocabulary 3D Scene Graph (OVSG), a formal framework for grounding a variety of entities, such as object instances, agents, and regions, with free-form text-based queries. Unlike conventional semantic-based object localization approaches, our system facilitates context-aware entity localization, allowing for queries such as “pick up a cup on a kitchen table" or “navigate to a sofa on which someone is sitting". In contrast to existing research on 3D scene graphs, OVSG supports free-form text input and open-vocabulary querying. Through a series of comparative experiments using the ScanNet dataset and a self-collected dataset, we demonstrate that our proposed approach significantly surpasses the performance of previous semantic-based localization techniques. Moreover, we highlight the practical application of OVSG in real-world robot navigation and manipulation experiments. The code and dataset used for evaluation will be made available upon publication.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/chang23b.html
  PDF: https://proceedings.mlr.press/v229/chang23b/chang23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-chang23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Haonan
    family: Chang
  - given: Kowndinya
    family: Boyalakuntla
  - given: Shiyang
    family: Lu
  - given: Siwei
    family: Cai
  - given: Eric Pu
    family: Jing
  - given: Shreesh
    family: Keskar
  - given: Shijie
    family: Geng
  - given: Adeeb
    family: Abbas
  - given: Lifeng
    family: Zhou
  - given: Kostas
    family: Bekris
  - given: Abdeslam
    family: Boularias
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1950-1974
  id: chang23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1950
  lastpage: 1974
  published: 2023-12-02 00:00:00 +0000
- title: 'HomeRobot: Open-Vocabulary Mobile Manipulation'
  abstract: 'HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a $20%$ success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://home-robot-ovmm.github.io/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yenamandra23a.html
  PDF: https://proceedings.mlr.press/v229/yenamandra23a/yenamandra23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yenamandra23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sriram
    family: Yenamandra
  - given: Arun
    family: Ramachandran
  - given: Karmesh
    family: Yadav
  - given: Austin S.
    family: Wang
  - given: Mukul
    family: Khanna
  - given: Theophile
    family: Gervet
  - given: Tsung-Yen
    family: Yang
  - given: Vidhi
    family: Jain
  - given: Alexander
    family: Clegg
  - given: John M.
    family: Turner
  - given: Zsolt
    family: Kira
  - given: Manolis
    family: Savva
  - given: Angel X.
    family: Chang
  - given: Devendra Singh
    family: Chaplot
  - given: Dhruv
    family: Batra
  - given: Roozbeh
    family: Mottaghi
  - given: Yonatan
    family: Bisk
  - given: Chris
    family: Paxton
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 1975-2011
  id: yenamandra23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 1975
  lastpage: 2011
  published: 2023-12-02 00:00:00 +0000
- title: 'PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play'
  abstract: 'Learning from unstructured and uncurated data has become the dominant paradigm for generative approaches in language or vision. Such unstructured and unguided behavior data, commonly known as play, is also easier to collect in robotics but much more difficult to learn from due to its inherently multimodal, noisy, and suboptimal nature. In this paper, we study this problem of learning goal-directed skill policies from unstructured play data which is labeled with language in hindsight. Specifically, we leverage advances in diffusion models to learn a multi-task diffusion model to extract robotic skills from play data. Using a conditional denoising diffusion process in the space of states and actions, we can gracefully handle the complexity and multimodality of play data and generate diverse and interesting robot behaviors. To make diffusion models more useful for skill learning, we encourage robotic agents to acquire a vocabulary of skills by introducing discrete bottlenecks into the conditional behavior generation process. In our experiments, we demonstrate the effectiveness of our approach across a wide variety of environments in both simulation and the real world. Video results available at https://play-fusion.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/chen23c.html
  PDF: https://proceedings.mlr.press/v229/chen23c/chen23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-chen23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Lili
    family: Chen
  - given: Shikhar
    family: Bahl
  - given: Deepak
    family: Pathak
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2012-2029
  id: chen23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2012
  lastpage: 2029
  published: 2023-12-02 00:00:00 +0000
- title: 'Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement'
  abstract: 'We propose a system for rearranging objects in a scene to achieve a desired object-scene placing relationship, such as a book inserted in an open slot of a bookshelf. The pipeline generalizes to novel geometries, poses, and layouts of both scenes and objects, and is trained from demonstrations to operate directly on 3D point clouds. Our system overcomes challenges associated with the existence of many geometrically-similar rearrangement solutions for a given scene. By leveraging an iterative pose de-noising training procedure, we can fit multi-modal demonstration data and produce multi-modal outputs while remaining precise and accurate. We also show the advantages of conditioning on relevant local geometric features while ignoring irrelevant global structure that harms both generalization and precision. We demonstrate our approach on three distinct rearrangement tasks that require handling multi-modality and generalization over object shape and pose in both simulation and the real world. Project website, code, and videos: https://anthonysimeonov.github.io/rpdiff-multi-modal'
  volume: 229
  URL: https://proceedings.mlr.press/v229/simeonov23a.html
  PDF: https://proceedings.mlr.press/v229/simeonov23a/simeonov23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-simeonov23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Anthony
    family: Simeonov
  - given: Ankit
    family: Goyal
  - given: Lucas
    family: Manuelli
  - given: Yen-Chen
    family: Lin
  - given: Alina
    family: Sarmiento
  - given: Alberto Rodriguez
    family: Garcia
  - given: Pulkit
    family: Agrawal
  - given: Dieter
    family: Fox
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2030-2069
  id: simeonov23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2030
  lastpage: 2069
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning Efficient Abstract Planning Models that Choose What to Predict'
  abstract: 'An effective approach to solving long-horizon tasks in robotics domains with continuous state and action spaces is bilevel planning, wherein a high-level search over an abstraction of an environment is used to guide low-level decision-making. Recent work has shown how to enable such bilevel planning by learning abstract models in the form of symbolic operators and neural samplers. In this work, we show that existing symbolic operator learning approaches fall short in many robotics domains where a robot’s actions tend to cause a large number of irrelevant changes in the abstract state. This is primarily because they attempt to learn operators that exactly predict all observed changes in the abstract state. To overcome this issue, we propose to learn operators that ‘choose what to predict’ by only modelling changes necessary for abstract planning to achieve specified goals. Experimentally, we show that our approach learns operators that lead to efficient planning across 10 different hybrid robotics domains, including 4 from the challenging BEHAVIOR-100 benchmark, while generalizing to novel initial states, goals, and objects.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kumar23a.html
  PDF: https://proceedings.mlr.press/v229/kumar23a/kumar23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kumar23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Nishanth
    family: Kumar
  - given: Willie
    family: McClinton
  - given: Rohan
    family: Chitnis
  - given: Tom
    family: Silver
  - given: Tomás
    family: Lozano-Pérez
  - given: Leslie Pack
    family: Kaelbling
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2070-2095
  id: kumar23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2070
  lastpage: 2095
  published: 2023-12-02 00:00:00 +0000
- title: 'DYNAMO-GRASP: DYNAMics-aware Optimization for GRASP Point Detection in Suction Grippers'
  abstract: 'In this research, we introduce a novel approach to the challenge of suction grasp point detection. Our method, exploiting the strengths of physics-based simulation and data-driven modeling, accounts for object dynamics during the grasping process, markedly enhancing the robot’s capability to handle previously unseen objects and scenarios in real-world settings. We benchmark DYNAMO-GRASP against established approaches via comprehensive evaluations in both simulated and real-world environments. DYNAMO-GRASP delivers improved grasping performance with greater consistency in both simulated and real-world settings. Remarkably, in real-world tests with challenging scenarios, our method demonstrates a success rate improvement of up to $48%$ over SOTA methods. Demonstrating a strong ability to adapt to complex and unexpected object dynamics, our method offers robust generalization to real-world challenges. The results of this research set the stage for more reliable and resilient robotic manipulation in intricate real-world situations. Experiment videos, dataset, model, and code are available at: https://sites.google.com/view/dynamo-grasp.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yang23a.html
  PDF: https://proceedings.mlr.press/v229/yang23a/yang23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yang23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Boling
    family: Yang
  - given: Soofiyan
    family: Atar
  - given: Markus
    family: Grotz
  - given: Byron
    family: Boots
  - given: Joshua
    family: Smith
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2096-2112
  id: yang23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2096
  lastpage: 2112
  published: 2023-12-02 00:00:00 +0000
- title: 'HYDRA: Hybrid Robot Actions for Imitation Learning'
  abstract: 'Imitation Learning (IL) is a sample efficient paradigm for robot learning using expert demonstrations. However, policies learned through IL suffer from state distribution shift at test time, due to compounding errors in action prediction which lead to previously unseen states. Choosing an action representation for the policy that minimizes this distribution shift is critical in imitation learning. Prior work propose using temporal action abstractions to reduce compounding errors, but they often sacrifice policy dexterity or require domain-specific knowledge. To address these trade-offs, we introduce HYDRA, a method that leverages a hybrid action space with two levels of action abstractions: sparse high-level waypoints and dense low-level actions. HYDRA dynamically switches between action abstractions at test time to enable both coarse and fine-grained control of a robot. In addition, HYDRA employs action relabeling to increase the consistency of actions in the dataset, further reducing distribution shift. HYDRA outperforms prior imitation learning methods by $30-40%$ on seven challenging simulation and real world environments, involving long-horizon tasks in the real world like making coffee and toasting bread. Videos are found on our website: https://tinyurl.com/3mc6793z'
  volume: 229
  URL: https://proceedings.mlr.press/v229/belkhale23a.html
  PDF: https://proceedings.mlr.press/v229/belkhale23a/belkhale23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-belkhale23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Suneel
    family: Belkhale
  - given: Yuchen
    family: Cui
  - given: Dorsa
    family: Sadigh
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2113-2133
  id: belkhale23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2113
  lastpage: 2133
  published: 2023-12-02 00:00:00 +0000
- title: 'Embodied Lifelong Learning for Task and Motion Planning'
  abstract: 'A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge and proficiency. We formalize this setting with a novel formulation of lifelong learning for task and motion planning (TAMP), which endows our learner with the compositionality of TAMP systems. Exploiting the modularity of TAMP, we develop a mixture of generative models that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across various models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model’s understanding of a state. Our method exhibits substantial improvements (over time and compared to baselines) in planning success on 2D and BEHAVIOR domains.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/mendez-mendez23a.html
  PDF: https://proceedings.mlr.press/v229/mendez-mendez23a/mendez-mendez23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-mendez-mendez23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jorge
    family: Mendez-Mendez
  - given: Leslie Pack
    family: Kaelbling
  - given: Tomás
    family: Lozano-Pérez
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2134-2150
  id: mendez-mendez23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2134
  lastpage: 2150
  published: 2023-12-02 00:00:00 +0000
- title: '4D-Former: Multimodal 4D Panoptic Segmentation'
  abstract: '4D panoptic segmentation is a challenging but practically useful task that requires every point in a LiDAR point-cloud sequence to be assigned a semantic class label, and individual objects to be segmented and tracked over time. Existing approaches utilize only LiDAR inputs which convey limited information in regions with point sparsity. This problem can, however, be mitigated by utilizing RGB camera images which offer appearance-based information that can reinforce the geometry-based LiDAR features. Motivated by this, we propose 4D-Former: a novel method for 4D panoptic segmentation which leverages both LiDAR and image modalities, and predicts semantic masks as well as temporally consistent object masks for the input point-cloud sequence. We encode semantic classes and objects using a set of concise queries which absorb feature information from both data modalities. Additionally, we propose a learned mechanism to associate object tracks over time which reasons over both appearance and spatial location. We apply 4D-Former to the nuScenes and SemanticKITTI datasets where it achieves state-of-the-art results.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/athar23a.html
  PDF: https://proceedings.mlr.press/v229/athar23a/athar23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-athar23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ali
    family: Athar
  - given: Enxu
    family: Li
  - given: Sergio
    family: Casas
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2151-2164
  id: athar23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2151
  lastpage: 2164
  published: 2023-12-02 00:00:00 +0000
- title: 'RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control'
  abstract: 'We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zitkovich23a.html
  PDF: https://proceedings.mlr.press/v229/zitkovich23a/zitkovich23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zitkovich23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Brianna
    family: Zitkovich
  - given: Tianhe
    family: Yu
  - given: Sichun
    family: Xu
  - given: Peng
    family: Xu
  - given: Ted
    family: Xiao
  - given: Fei
    family: Xia
  - given: Jialin
    family: Wu
  - given: Paul
    family: Wohlhart
  - given: Stefan
    family: Welker
  - given: Ayzaan
    family: Wahid
  - given: Quan
    family: Vuong
  - given: Vincent
    family: Vanhoucke
  - given: Huong
    family: Tran
  - given: Radu
    family: Soricut
  - given: Anikait
    family: Singh
  - given: Jaspiar
    family: Singh
  - given: Pierre
    family: Sermanet
  - given: Pannag R.
    family: Sanketi
  - given: Grecia
    family: Salazar
  - given: Michael S.
    family: Ryoo
  - given: Krista
    family: Reymann
  - given: Kanishka
    family: Rao
  - given: Karl
    family: Pertsch
  - given: Igor
    family: Mordatch
  - given: Henryk
    family: Michalewski
  - given: Yao
    family: Lu
  - given: Sergey
    family: Levine
  - given: Lisa
    family: Lee
  - given: Tsang-Wei Edward
    family: Lee
  - given: Isabel
    family: Leal
  - given: Yuheng
    family: Kuang
  - given: Dmitry
    family: Kalashnikov
  - given: Ryan
    family: Julian
  - given: Nikhil J.
    family: Joshi
  - given: Alex
    family: Irpan
  - given: Brian
    family: Ichter
  - given: Jasmine
    family: Hsu
  - given: Alexander
    family: Herzog
  - given: Karol
    family: Hausman
  - given: Keerthana
    family: Gopalakrishnan
  - given: Chuyuan
    family: Fu
  - given: Pete
    family: Florence
  - given: Chelsea
    family: Finn
  - given: Kumar Avinava
    family: Dubey
  - given: Danny
    family: Driess
  - given: Tianli
    family: Ding
  - given: Krzysztof Marcin
    family: Choromanski
  - given: Xi
    family: Chen
  - given: Yevgen
    family: Chebotar
  - given: Justice
    family: Carbajal
  - given: Noah
    family: Brown
  - given: Anthony
    family: Brohan
  - given: Montserrat Gonzalez
    family: Arenas
  - given: Kehang
    family: Han
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2165-2183
  id: zitkovich23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2165
  lastpage: 2183
  published: 2023-12-02 00:00:00 +0000
- title: 'Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control'
  abstract: 'Seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. Although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. We demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/defazio23a.html
  PDF: https://proceedings.mlr.press/v229/defazio23a/defazio23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-defazio23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: David
    family: DeFazio
  - given: Eisuke
    family: Hirota
  - given: Shiqi
    family: Zhang
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2184-2194
  id: defazio23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2184
  lastpage: 2194
  published: 2023-12-02 00:00:00 +0000
- title: 'Waypoint-Based Imitation Learning for Robotic Manipulation'
  abstract: 'While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to $25%$ in simulation and by $4-28%$ on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/shi23b.html
  PDF: https://proceedings.mlr.press/v229/shi23b/shi23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-shi23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Lucy Xiaoyang
    family: Shi
  - given: Archit
    family: Sharma
  - given: Tony Z.
    family: Zhao
  - given: Chelsea
    family: Finn
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2195-2209
  id: shi23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2195
  lastpage: 2209
  published: 2023-12-02 00:00:00 +0000
- title: 'Multi-Resolution Sensing for Real-Time Control with Vision-Language Models'
  abstract: 'Leveraging sensing modalities across diverse spatial and temporal resolutions can improve performance of robotic manipulation tasks. Multi-spatial resolution sensing provides hierarchical information captured at different spatial scales and enables both coarse and precise motions. Simultaneously multi-temporal resolution sensing enables the agent to exhibit high reactivity and real-time control. In this work, we propose a framework for learning generalizable language-conditioned multi-task policies that utilize sensing at different spatial and temporal resolutions using networks of varying capacities to effectively perform real time control of precise and reactive tasks. We leverage off-the-shelf pretrained vision-language models to operate on low-frequency global features along with small non-pretrained models to adapt to high frequency local feedback. Through extensive experiments in 3 domains (coarse, precise and dynamic manipulation tasks), we show that our approach significantly improves ($2\times$ on average) over recent multi-task baselines. Further, our approach generalizes well to visual and geometric variations in target objects and to varying interaction forces.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/saxena23a.html
  PDF: https://proceedings.mlr.press/v229/saxena23a/saxena23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-saxena23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Saumya
    family: Saxena
  - given: Mohit
    family: Sharma
  - given: Oliver
    family: Kroemer
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2210-2228
  id: saxena23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2210
  lastpage: 2228
  published: 2023-12-02 00:00:00 +0000
- title: 'SCALE: Causal Learning and Discovery of Robot Manipulation Skills using Simulation'
  abstract: 'We propose SCALE, an approach for discovering and learning a diverse set of interpretable robot skills from a limited dataset. Rather than learning a single skill which may fail to capture all the modes in the data, we first identify the different modes via causal reasoning and learn a separate skill for each of them. Our main insight is to associate each mode with a unique set of causally relevant context variables that are discovered by performing causal interventions in simulation. This enables data partitioning based on the causal processes that generated the data, and then compressed skills that ignore the irrelevant variables can be trained. We model each robot skill as a Regional Compressed Option, which extends the options framework by associating a causal process and its relevant variables with the option. Modeled as the skill Data Generating Region, each causal process is local in nature and hence valid over only a subset of the context space. We demonstrate our approach for two representative manipulation tasks: block stacking and peg-in-hole insertion under uncertainty. Our experiments show that our approach yields diverse skills that are compact, robust to domain shifts, and suitable for sim-to-real transfer.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/lee23b.html
  PDF: https://proceedings.mlr.press/v229/lee23b/lee23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-lee23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tabitha Edith
    family: Lee
  - given: Shivam
    family: Vats
  - given: Siddharth
    family: Girdhar
  - given: Oliver
    family: Kroemer
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2229-2256
  id: lee23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2229
  lastpage: 2256
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning Robot Manipulation from Cross-Morphology Demonstration'
  abstract: 'Some Learning from Demonstrations (LfD) methods handle small mismatches in the action spaces of the teacher and student. Here we address the casewhere the teacher’s morphology is substantially different from that of the student. Our framework, Morphological Adaptation in Imitation Learning (MAIL), bridges this gap allowing us to train an agent from demonstrations by other agents with significantly different morphologies. MAIL learns from suboptimal demonstrations, so long as they provide some guidance towards a desired solution. We demonstrate MAIL on manipulation tasks with rigid and deformable objects including 3D cloth manipulation interacting with rigid obstacles. We train a visual control policy for a robot with one end-effector using demonstrations from a simulated agent with two end-effectors. MAIL shows up to $24%$ improvement in a normalized performance metric over LfD and non-LfD baselines. It is deployed to a real Franka Panda robot, handles multiple variations in properties for objects (size, rotation, translation), and cloth-specific properties (color, thickness, size, material).'
  volume: 229
  URL: https://proceedings.mlr.press/v229/salhotra23a.html
  PDF: https://proceedings.mlr.press/v229/salhotra23a/salhotra23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-salhotra23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Gautam
    family: Salhotra
  - given: I-Chun Arthur
    family: Liu
  - given: Gaurav S.
    family: Sukhatme
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2257-2277
  id: salhotra23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2257
  lastpage: 2277
  published: 2023-12-02 00:00:00 +0000
- title: 'Synthesizing Navigation Abstractions for Planning with Portable Manipulation Skills'
  abstract: 'We address the problem of efficiently learning high-level abstractions for task-level robot planning. Existing approaches require large amounts of data and fail to generalize learned abstractions to new environments. To address this, we propose to exploit the independence between spatial and non-spatial state variables in the preconditions of manipulation and navigation skills, mirroring the manipulation-navigation split in robotics research. Given a collection of portable manipulation abstractions (i.e., object-centric manipulation skills paired with matching symbolic representations), we derive an algorithm to automatically generate navigation abstractions that support mobile manipulation planning in a novel environment. We apply our approach to simulated data in AI2Thor and on real robot hardware with a coffee preparation task, efficiently generating plannable representations for mobile manipulators in just a few minutes of robot time, significantly outperforming state-of-the-art baselines.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/rosen23a.html
  PDF: https://proceedings.mlr.press/v229/rosen23a/rosen23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-rosen23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Eric
    family: Rosen
  - given: Steven
    family: James
  - given: Sergio
    family: Orozco
  - given: Vedant
    family: Gupta
  - given: Max
    family: Merlin
  - given: Stefanie
    family: Tellex
  - given: George
    family: Konidaris
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2278-2287
  id: rosen23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2278
  lastpage: 2287
  published: 2023-12-02 00:00:00 +0000
- title: 'Transforming a Quadruped into a Guide Robot for the Visually Impaired: Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism'
  abstract: 'This paper explores the principles for transforming a quadrupedal robot into a guide robot for individuals with visual impairments. A guide robot has great potential to resolve the limited availability of guide animals that are accessible to only two to three percent of the potential blind or visually impaired (BVI) users. To build a successful guide robot, our paper explores three key topics: (1) formalizing the navigation mechanism of a guide dog and a human, (2) developing a data-driven model of their interaction, and (3) improving user safety. First, we formalize the wayfinding task of the human-guide robot team using Markov Decision Processes based on the literature and interviews. Then we collect real human-robot interaction data from three visually impaired and six sighted people and develop an interaction model called the "Delayed Harness" to effectively simulate the navigation behaviors of the team. Additionally, we introduce an action shielding mechanism to enhance user safety by predicting and filtering out dangerous actions. We evaluate the developed interaction model and the safety mechanism in simulation, which greatly reduce the prediction errors and the number of collisions, respectively. We also demonstrate the integrated system on an AlienGo robot with a rigid harness, by guiding users over 100+ meter trajectories.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kim23c.html
  PDF: https://proceedings.mlr.press/v229/kim23c/kim23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kim23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: J. Taery
    family: Kim
  - given: Wenhao
    family: Yu
  - given: Yash
    family: Kothari
  - given: Bruce
    family: Walker
  - given: Jie
    family: Tan
  - given: Greg
    family: Turk
  - given: Sehoon
    family: Ha
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2288-2303
  id: kim23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2288
  lastpage: 2303
  published: 2023-12-02 00:00:00 +0000
- title: 'A Bayesian Approach to Robust Inverse Reinforcement Learning'
  abstract: 'We consider a Bayesian approach to offline model-based inverse reinforcement learning (IRL). The proposed framework differs from existing offline model-based IRL approaches by performing simultaneous estimation of the expert’s reward function and subjective model of environment dynamics. We make use of a class of prior distributions which parameterizes how accurate the expert’s model of the environment is to develop efficient algorithms to estimate the expert’s reward and subjective dynamics in high-dimensional settings. Our analysis reveals a novel insight that the estimated policy exhibits robust performance when the expert is believed (a priori) to have a highly accurate model of the environment. We verify this observation in the MuJoCo environments and show that our algorithms outperform state-of-the-art offline IRL algorithms.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wei23a.html
  PDF: https://proceedings.mlr.press/v229/wei23a/wei23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wei23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ran
    family: Wei
  - given: Siliang
    family: Zeng
  - given: Chenliang
    family: Li
  - given: Alfredo
    family: Garcia
  - given: Anthony D
    family: McDonald
  - given: Mingyi
    family: Hong
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2304-2322
  id: wei23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2304
  lastpage: 2322
  published: 2023-12-02 00:00:00 +0000
- title: 'ChainedDiffuser: Unifying Trajectory Diffusion and Keypose Prediction for Robotic Manipulation'
  abstract: 'We present ChainedDiffuser, a policy architecture that unifies action keypose prediction and trajectory diffusion generation for learning robot manipulation from demonstrations. Our main innovation is to use a global transformer-based action predictor to predict actions at keyframes, a task that requires multi- modal semantic scene understanding, and to use a local trajectory diffuser to predict trajectory segments that connect predicted macro-actions. ChainedDiffuser sets a new record on established manipulation benchmarks, and outperforms both state-of-the-art keypose (macro-action) prediction models that use motion plan- ners for trajectory prediction, and trajectory diffusion policies that do not predict keyframe macro-actions. We conduct experiments in both simulated and real-world environments and demonstrate ChainedDiffuser’s ability to solve a wide range of manipulation tasks involving interactions with diverse objects.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/xian23a.html
  PDF: https://proceedings.mlr.press/v229/xian23a/xian23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-xian23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zhou
    family: Xian
  - given: Nikolaos
    family: Gkanatsios
  - given: Theophile
    family: Gervet
  - given: Tsung-Wei
    family: Ke
  - given: Katerina
    family: Fragkiadaki
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2323-2339
  id: xian23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2323
  lastpage: 2339
  published: 2023-12-02 00:00:00 +0000
- title: 'IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors'
  abstract: 'Imitation learning has been applied to a range of robotic tasks, but can struggle when robots encounter edge cases that are not represented in the training data (i.e., distribution shift). Interactive fleet learning (IFL) mitigates distribution shift by allowing robots to access remote human supervisors during task execution and learn from them over time, but different supervisors may demonstrate the task in different ways. Recent work proposes Implicit Behavior Cloning (IBC), which is able to represent multimodal demonstrations using energy-based models (EBMs). In this work, we propose Implicit Interactive Fleet Learning (IIFL), an algorithm that builds on IBC for interactive imitation learning from multiple heterogeneous human supervisors. A key insight in IIFL is a novel approach for uncertainty quantification in EBMs using Jeffreys divergence. While IIFL is more computationally expensive than explicit methods, results suggest that IIFL achieves a 2.8x higher success rate in simulation experiments and a 4.5x higher return on human effort in a physical block pushing task over (Explicit) IFL, IBC, and other baselines.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/datta23a.html
  PDF: https://proceedings.mlr.press/v229/datta23a/datta23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-datta23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Gaurav
    family: Datta
  - given: Ryan
    family: Hoque
  - given: Anrui
    family: Gu
  - given: Eugen
    family: Solowjow
  - given: Ken
    family: Goldberg
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2340-2356
  id: datta23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2340
  lastpage: 2356
  published: 2023-12-02 00:00:00 +0000
- title: 'CAT: Closed-loop Adversarial Training for Safe End-to-End Driving'
  abstract: 'Driving safety is a top priority for autonomous vehicles. Orthogonal to prior work handling accident-prone traffic events by algorithm designs at the policy level, we investigate a Closed-loop Adversarial Training (CAT) framework for safe end-to-end driving in this paper through the lens of environment augmentation. CAT aims to continuously improve the safety of driving agents by training the agent on safety-critical scenarios that are dynamically generated over time. A novel resampling technique is developed to turn log-replay real-world driving scenarios into safety-critical ones via probabilistic factorization, where the adversarial traffic generation is modeled as the multiplication of standard motion prediction sub-problems. Consequently, CAT can launch more efficient physical attacks compared to existing safety-critical scenario generation methods and yields a significantly less computational cost in the iterative learning pipeline. We incorporate CAT into the MetaDrive simulator and validate our approach on hundreds of driving scenarios imported from real-world driving datasets. Experimental results demonstrate that CAT can effectively generate adversarial scenarios countering the agent being trained. After training, the agent can achieve superior driving safety in both log-replay and safety-critical traffic scenarios on the held-out test set. Code and data are available at: https://metadriverse.github.io/cat'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23g.html
  PDF: https://proceedings.mlr.press/v229/zhang23g/zhang23g.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23g.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Linrui
    family: Zhang
  - given: Zhenghao
    family: Peng
  - given: Quanyi
    family: Li
  - given: Bolei
    family: Zhou
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2357-2372
  id: zhang23g
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2357
  lastpage: 2372
  published: 2023-12-02 00:00:00 +0000
- title: 'Neural Graph Control Barrier Functions Guided Distributed Collision-avoidance Multi-agent Control'
  abstract: 'We consider the problem of designing distributed collision-avoidance multi-agent control in large-scale environments with potentially moving obstacles, where a large number of agents are required to maintain safety using only local information and reach their goals. This paper addresses the problem of collision avoidance, scalability, and generalizability by introducing graph control barrier functions (GCBFs) for distributed control. The newly introduced GCBF is based on the well-established CBF theory for safety guarantees but utilizes a graph structure for scalable and generalizable decentralized control. We use graph neural networks to learn both neural a GCBF certificate and distributed control. We also extend the framework from handling state-based models to directly taking point clouds from LiDAR for more practical robotics settings. We demonstrated the efficacy of GCBF in a variety of numerical experiments, where the number, density, and traveling distance of agents, as well as the number of unseen and uncontrolled obstacles increase. Empirical results show that GCBF outperforms leading methods such as MAPPO and multi-agent distributed CBF (MDCBF). Trained with only $16$ agents, GCBF can achieve up to $3$ times improvement of success rate (agents reach goals and never encountered in any collisions) on $<500$ agents, and still maintain more than $50%$ success rates for $>\!1000$ agents when other methods completely fail.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23h.html
  PDF: https://proceedings.mlr.press/v229/zhang23h/zhang23h.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23h.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Songyuan
    family: Zhang
  - given: Kunal
    family: Garg
  - given: Chuchu
    family: Fan
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2373-2392
  id: zhang23h
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2373
  lastpage: 2392
  published: 2023-12-02 00:00:00 +0000
- title: 'STERLING: Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience'
  abstract: 'Terrain awareness, i.e., the ability to identify and distinguish different types of terrain, is a critical ability that robots must have to succeed at autonomous off-road navigation. Current approaches that provide robots with this awareness either rely on labeled data which is expensive to collect, engineered features and cost functions that may not generalize, or expert human demonstrations which may not be available. Towards endowing robots with terrain awareness without these limitations, we introduce Self-supervised TErrain Representation LearnING (STERLING), a novel approach for learning terrain representations that relies solely on easy-to-collect, unconstrained (e.g., non-expert), and unlabelled robot experience, with no additional constraints on data collection. STERLING employs a novel multi-modal self-supervision objective through non-contrastive representation learning to learn relevant terrain representations for terrain-aware navigation. Through physical robot experiments in off-road environments, we evaluate STERLING features on the task of preference-aligned visual navigation and find that STERLING features perform on par with fully-supervised approaches and outperform other state-of-the-art methods with respect to preference alignment. Additionally, we perform a large-scale experiment of autonomously hiking a 3-mile long trail which STERLING completes successfully with only two manual interventions, demonstrating its robustness to real-world off-road conditions.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/karnan23a.html
  PDF: https://proceedings.mlr.press/v229/karnan23a/karnan23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-karnan23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Haresh
    family: Karnan
  - given: Elvin
    family: Yang
  - given: Daniel
    family: Farkash
  - given: Garrett
    family: Warnell
  - given: Joydeep
    family: Biswas
  - given: Peter
    family: Stone
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2393-2413
  id: karnan23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2393
  lastpage: 2413
  published: 2023-12-02 00:00:00 +0000
- title: 'Towards General Single-Utensil Food Acquisition with Human-Informed Actions'
  abstract: 'Food acquisition with common general-purpose utensils is a necessary component of robot applications like in-home assistive feeding. Learning acquisition policies in this space is difficult in part because any model will need to contend with extensive state and actions spaces. Food is extremely diverse and generally difficult to simulate, and acquisition actions like skewers, scoops, wiggles, and twirls can be parameterized in myriad ways. However, food’s visual diversity can belie a degree of physical homogeneity, and many foods allow flexibility in how they are acquired. Due to these facts, our key insight is that a small subset of actions is sufficient to acquire a wide variety of food items. In this work, we present a methodology for identifying such a subset from limited human trajectory data. We first develop an over-parameterized action space of robot acquisition trajectories that capture the variety of human food acquisition technique. By mapping human trajectories into this space and clustering, we construct a discrete set of 11 actions. We demonstrate that this set is capable of acquiring a variety of food items with $\geq80%$ success rate, a rate that users have said is sufficient for in-home robot-assisted feeding. Furthermore, since this set is so small, we also show that we can use online learning to determine a sufficiently optimal action for a previously-unseen food item over the course of a single meal.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/gordon23a.html
  PDF: https://proceedings.mlr.press/v229/gordon23a/gordon23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-gordon23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ethan Kroll
    family: Gordon
  - given: Amal
    family: Nanavati
  - given: Ramya
    family: Challa
  - given: Bernie Hao
    family: Zhu
  - given: Taylor Annette Kessler
    family: Faulkner
  - given: Siddhartha
    family: Srinivasa
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2414-2428
  id: gordon23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2414
  lastpage: 2428
  published: 2023-12-02 00:00:00 +0000
- title: 'ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD Map Construction'
  abstract: 'We propose a novel end-to-end pipeline for online long-range vectorized high-definition (HD) map construction using on-board camera sensors. The vectorized representation of HD maps, employing polylines and polygons to represent map elements, is widely used by downstream tasks. However, previous schemes designed with reference to dynamic object detection overlook the structural constraints within linear map elements, resulting in performance degradation in long-range scenarios. In this paper, we exploit the properties of map elements to improve the performance of map construction. We extract more accurate bird’s eye view (BEV) features guided by their linear structure, and then propose a hierarchical sparse map representation to further leverage the scalability of vectorized map elements, and design a progressive decoding mechanism and a supervision strategy based on this representation. Our approach, ScalableMap, demonstrates superior performance on the nuScenes dataset, especially in long-range scenarios, surpassing previous state-of-the-art model by 6.5 mAP while achieving 18.3 FPS.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yu23b.html
  PDF: https://proceedings.mlr.press/v229/yu23b/yu23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yu23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jingyi
    family: Yu
  - given: Zizhao
    family: Zhang
  - given: Shengfu
    family: Xia
  - given: Jizhang
    family: Sang
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2429-2443
  id: yu23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2429
  lastpage: 2443
  published: 2023-12-02 00:00:00 +0000
- title: 'Tuning Legged Locomotion Controllers via Safe Bayesian Optimization'
  abstract: 'This paper presents a data-driven strategy to streamline the deployment of model-based controllers in legged robotic hardware platforms. Our approach leverages a model-free safe learning algorithm to automate the tuning of control gains, addressing the mismatch between the simplified model used in the control formulation and the real system. This method substantially mitigates the risk of hazardous interactions with the robot by sample-efficiently optimizing parameters within a probably safe region. Additionally, we extend the applicability of our approach to incorporate the different gait parameters as contexts, leading to a safe, sample-efficient exploration algorithm capable of tuning a motion controller for diverse gait patterns. We validate our method through simulation and hardware experiments, where we demonstrate that the algorithm obtains superior performance on tuning a model-based motion controller for multiple gaits safely.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/widmer23a.html
  PDF: https://proceedings.mlr.press/v229/widmer23a/widmer23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-widmer23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Daniel
    family: Widmer
  - given: Dongho
    family: Kang
  - given: Bhavya
    family: Sukhija
  - given: Jonas
    family: Hübotter
  - given: Andreas
    family: Krause
  - given: Stelian
    family: Coros
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2444-2464
  id: widmer23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2444
  lastpage: 2464
  published: 2023-12-02 00:00:00 +0000
- title: 'TraCo: Learning Virtual Traffic Coordinator for Cooperation with Multi-Agent Reinforcement Learning'
  abstract: 'Multi-agent reinforcement learning (MARL) has emerged as a popular technique in diverse domains due to its ability to automate system controller design and facilitate continuous intelligence learning. For instance, traffic flow is often trained with MARL to enable intelligent simulations for autonomous driving. However, The existing MARL algorithm only characterizes the relative degree of each agent’s contribution to the team, and cannot express the contribution that the team needs from the agent. Especially in the field of autonomous driving, the team changes over time, and the agent needs to act directly according to the needs of the team. To address these limitations, we propose an innovative method inspired by realistic traffic coordinators called the Traffic Coordinator Network (TraCo). Our approach leverages a combination of cross-attention and counterfactual advantage function, allowing us to extract distinctive characteristics of domain agents and accurately quantify the contribution that a team needs from an agent. Through experiments conducted on four traffic tasks, we demonstrate that our method outperforms existing approaches, yielding superior performance. Furthermore, our approach enables the emergence of rich and diverse social behaviors among vehicles within the traffic flow.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/liu23f.html
  PDF: https://proceedings.mlr.press/v229/liu23f/liu23f.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-liu23f.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Weiwei
    family: Liu
  - given: Wei
    family: Jing
  - given: lingping
    family: Gao
  - given: Ke
    family: Guo
  - given: Gang
    family: Xu
  - given: Yong
    family: Liu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2465-2477
  id: liu23f
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2465
  lastpage: 2477
  published: 2023-12-02 00:00:00 +0000
- title: 'Enabling Efficient, Reliable Real-World Reinforcement Learning with Approximate Physics-Based Models'
  abstract: 'We focus on developing efficient and reliable policy optimization strategies for robot learning with real-world data.  In recent years, policy gradient methods have emerged as a promising paradigm for training control policies in simulation.  However, these approaches often remain too data inefficient or unreliable to train on real robotic hardware. In this paper we introduce a novel policy gradient-based policy optimization framework which systematically leverages a (possibly highly simplified) first-principles model and enables learning precise control policies with limited amounts of real-world data. Our approach $1)$ uses the derivatives of the model to produce sample-efficient estimates of the policy gradient and $2)$ uses the model to design a low-level tracking controller, which is embedded in the policy class. Theoretical analysis provides insight into how the presence of this feedback controller addresses overcomes key limitations of stand-alone policy gradient methods, while hardware experiments with a small car and quadruped demonstrate that our approach can learn precise control strategies reliably and with only minutes of real-world data.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/westenbroek23a.html
  PDF: https://proceedings.mlr.press/v229/westenbroek23a/westenbroek23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-westenbroek23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tyler
    family: Westenbroek
  - given: Jacob
    family: Levy
  - given: David
    family: Fridovich-Keil
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2478-2497
  id: westenbroek23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2478
  lastpage: 2497
  published: 2023-12-02 00:00:00 +0000
- title: 'Large Language Models as General Pattern Machines'
  abstract: 'We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences–from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics–from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/mirchandani23a.html
  PDF: https://proceedings.mlr.press/v229/mirchandani23a/mirchandani23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-mirchandani23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Suvir
    family: Mirchandani
  - given: Fei
    family: Xia
  - given: Pete
    family: Florence
  - given: Brian
    family: Ichter
  - given: Danny
    family: Driess
  - given: Montserrat Gonzalez
    family: Arenas
  - given: Kanishka
    family: Rao
  - given: Dorsa
    family: Sadigh
  - given: Andy
    family: Zeng
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2498-2518
  id: mirchandani23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2498
  lastpage: 2518
  published: 2023-12-02 00:00:00 +0000
- title: 'One-shot Imitation Learning via Interaction Warping'
  abstract: 'Learning robot policies from few demonstrations is crucial in open-ended applications. We propose a new method, Interaction Warping, for one-shot learning SE(3) robotic manipulation policies. We infer the 3D mesh of each object in the environment using shape warping, a technique for aligning point clouds across object instances. Then, we represent manipulation actions as keypoints on objects, which can be warped with the shape of the object. We show successful one-shot imitation learning on three simulated and real-world object re-arrangement tasks. We also demonstrate the ability of our method to predict object meshes and robot grasps in the wild. Webpage: https://shapewarping.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/biza23a.html
  PDF: https://proceedings.mlr.press/v229/biza23a/biza23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-biza23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ondrej
    family: Biza
  - given: Skye
    family: Thompson
  - given: Kishore Reddy
    family: Pagidi
  - given: Abhinav
    family: Kumar
  - given: Elise van der
    family: Pol
  - given: Robin
    family: Walters
  - given: Thomas
    family: Kipf
  - given: Jan-Willem van de
    family: Meent
  - given: Lawson L. S.
    family: Wong
  - given: Robert
    family: Platt
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2519-2536
  id: biza23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2519
  lastpage: 2536
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning to See Physical Properties with Active Sensing Motor Policies'
  abstract: 'To plan efficient robot locomotion, we must use the information about a terrain’s physics that can be inferred from color images. To this end, we train a visual perception module that predicts terrain properties using labels from a small amount of real-world proprioceptive locomotion. To ensure label precision, we introduce Active Sensing Motor Policies (ASMP). These policies are trained to prefer motor skills that facilitate accurately estimating the environment’s physics, like swiping a foot to observe friction. The estimated labels supervise a vision model that infers physical properties directly from color images and can be reused for different tasks. Leveraging a pretrained vision backbone, we demonstrate robust generalization in image space, enabling path planning from overhead imagery despite using only ground camera images for training.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/margolis23a.html
  PDF: https://proceedings.mlr.press/v229/margolis23a/margolis23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-margolis23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Gabriel B.
    family: Margolis
  - given: Xiang
    family: Fu
  - given: Yandong
    family: Ji
  - given: Pulkit
    family: Agrawal
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2537-2548
  id: margolis23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2537
  lastpage: 2548
  published: 2023-12-02 00:00:00 +0000
- title: 'General In-hand Object Rotation with Vision and Touch'
  abstract: 'We introduce Rotateit, a system that enables fingertip-based object rotation along multiple axes by leveraging multimodal sensory inputs. Our system is trained in simulation, where it has access to ground-truth object shapes and physical properties. Then we distill it to operate on realistic yet noisy simulated visuotactile and proprioceptive sensory inputs. These multimodal inputs are fused via a visuotactile transformer, enabling online inference of object shapes and physical properties during deployment. We show significant performance improvements over prior methods and highlight the importance of visual and tactile sensing.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/qi23a.html
  PDF: https://proceedings.mlr.press/v229/qi23a/qi23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-qi23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Haozhi
    family: Qi
  - given: Brent
    family: Yi
  - given: Sudharshan
    family: Suresh
  - given: Mike
    family: Lambeta
  - given: Yi
    family: Ma
  - given: Roberto
    family: Calandra
  - given: Jitendra
    family: Malik
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2549-2564
  id: qi23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2549
  lastpage: 2564
  published: 2023-12-02 00:00:00 +0000
- title: 'Imitating Task and Motion Planning with Visuomotor Transformers'
  abstract: 'Imitation learning is a powerful tool for training robot manipulation policies, allowing them to learn from expert demonstrations without manual programming or trial-and-error. However, common methods of data collection, such as human supervision, scale poorly, as they are time-consuming and labor-intensive. In contrast, Task and Motion Planning (TAMP) can autonomously generate large-scale datasets of diverse demonstrations. In this work, we show that the combination of large-scale datasets generated by TAMP supervisors and flexible Transformer models to fit them is a powerful paradigm for robot manipulation. We present a novel imitation learning system called OPTIMUS that trains large-scale visuomotor Transformer policies by imitating a TAMP agent. We conduct a thorough study of the design decisions required to imitate TAMP and demonstrate that OPTIMUS can solve a wide variety of challenging vision-based manipulation tasks with over 70 different objects, ranging from long-horizon pick-and-place tasks, to shelf and articulated object manipulation, achieving $70$ to $80%$ success rates. Video results and code at https://mihdalal.github.io/optimus/'
  volume: 229
  URL: https://proceedings.mlr.press/v229/dalal23a.html
  PDF: https://proceedings.mlr.press/v229/dalal23a/dalal23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-dalal23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Murtaza
    family: Dalal
  - given: Ajay
    family: Mandlekar
  - given: Caelan Reed
    family: Garrett
  - given: Ankur
    family: Handa
  - given: Ruslan
    family: Salakhutdinov
  - given: Dieter
    family: Fox
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2565-2593
  id: dalal23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2565
  lastpage: 2593
  published: 2023-12-02 00:00:00 +0000
- title: 'Curiosity-Driven Learning of Joint Locomotion and Manipulation Tasks'
  abstract: 'Learning complex locomotion and manipulation tasks presents significant challenges, often requiring extensive engineering of, e.g., reward functions or curricula to provide meaningful feedback to the Reinforcement Learning (RL) algorithm. This paper proposes an intrinsically motivated RL approach to reduce task-specific engineering. The desired task is encoded in a single sparse reward, i.e., a reward of “+1" is given if the task is achieved. Intrinsic motivation enables learning by guiding exploration toward the sparse reward signal. Specifically, we adapt the idea of Random Network Distillation (RND) to the robotics domain to learn holistic motion control policies involving simultaneous locomotion and manipulation. We investigate opening doors as an exemplary task for robotic ap- plications. A second task involving package manipulation from a table to a bin highlights the generalization capabilities of the presented approach. Finally, the resulting RL policies are executed in real-world experiments on a wheeled-legged robot in biped mode. We experienced no failure in our experiments, which consisted of opening push doors (over 15 times in a row) and manipulating packages (over 5 times in a row).'
  volume: 229
  URL: https://proceedings.mlr.press/v229/schwarke23a.html
  PDF: https://proceedings.mlr.press/v229/schwarke23a/schwarke23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-schwarke23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Clemens
    family: Schwarke
  - given: Victor
    family: Klemm
  - given: Matthijs van der
    family: Boon
  - given: Marko
    family: Bjelonic
  - given: Marco
    family: Hutter
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2594-2610
  id: schwarke23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2594
  lastpage: 2610
  published: 2023-12-02 00:00:00 +0000
- title: 'Towards Scalable Coverage-Based Testing of Autonomous Vehicles'
  abstract: 'To deploy autonomous vehicles(AVs) in the real world, developers must understand the conditions in which the system can operate safely. To do this in a scalable manner, AVs are often tested in simulation on parameterized scenarios. In this context, it’s important to build a testing framework that partitions the scenario parameter space into safe, unsafe, and unknown regions. Existing approaches rely on discretizing continuous parameter spaces into bins, which scales poorly to high-dimensional spaces and cannot describe regions with arbitrary shape. In this work, we introduce a problem formulation which avoids discretization – by modeling the probability of meeting safety requirements everywhere, the parameter space can be paritioned using a probability threshold. Based on our formulation, we propose GUARD as a testing framework which leverages Gaussian Processes to model probability and levelset algorithms to efficiently generate tests. Moreover, we introduce a set of novel evaluation metrics for coverage-based testing frameworks to capture the key objectives of testing. In our evaluation suite of diverse high-dimensional scenarios, GUARD significantly outperforms existing approaches. By proposing an efficient, accurate, and scalable testing framework, our work is a step towards safely deploying autonomous vehicles at scale.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/tu23a.html
  PDF: https://proceedings.mlr.press/v229/tu23a/tu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-tu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: James
    family: Tu
  - given: Simon
    family: Suo
  - given: Chris
    family: Zhang
  - given: Kelvin
    family: Wong
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2611-2623
  id: tu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2611
  lastpage: 2623
  published: 2023-12-02 00:00:00 +0000
- title: 'PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining'
  abstract: 'A rich representation is key to general robotic manipulation, but existing approaches to representation learning require large amounts of multimodal demonstrations. In this work we propose PLEX, a transformer-based architecture that learns from a small amount of task-agnostic visuomotor trajectories and a much larger amount of task-conditioned object manipulation videos – a type of data available in quantity. PLEX uses visuomotor trajectories to induce a latent feature space and to learn task-agnostic manipulation routines, while diverse video-only demonstrations teach PLEX how to plan in the induced latent feature space for a wide variety of tasks. Experiments showcase PLEX’s generalization on Meta-World and SOTA performance in challenging Robosuite environments. In particular, using relative positional encoding in PLEX’s transformers greatly helps in low-data regimes of learning from human-collected demonstrations.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/thomas23a.html
  PDF: https://proceedings.mlr.press/v229/thomas23a/thomas23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-thomas23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Garrett
    family: Thomas
  - given: Ching-An
    family: Cheng
  - given: Ricky
    family: Loynd
  - given: Felipe Vieira
    family: Frujeri
  - given: Vibhav
    family: Vineet
  - given: Mihai
    family: Jalobeanu
  - given: Andrey
    family: Kolobov
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2624-2641
  id: thomas23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2624
  lastpage: 2641
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning Lyapunov-Stable Polynomial Dynamical Systems Through Imitation'
  abstract: 'Imitation learning is a paradigm to address complex motion planning problems by learning a policy to imitate an expert’s behavior. However, relying solely on the expert’s data might lead to unsafe actions when the robot deviates from the demonstrated trajectories. Stability guarantees have previously been provided utilizing nonlinear dynamical systems, acting as high-level motion planners, in conjunction with the Lyapunov stability theorem. Yet, these methods are prone to inaccurate policies, high computational cost, sample inefficiency, or quasi stability when replicating complex and highly nonlinear trajectories. To mitigate this problem, we present an approach for learning a globally stable nonlinear dynamical system as a motion planning policy. We model the nonlinear dynamical system as a parametric polynomial and learn the polynomial’s coefficients jointly with a Lyapunov candidate. To showcase its success, we compare our method against the state of the art in simulation and conduct real-world experiments with the Kinova Gen3 Lite manipulator arm. Our experiments demonstrate the sample efficiency and reproduction accuracy of our method for various expert trajectories, while remaining stable in the face of perturbations.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/abyaneh23a.html
  PDF: https://proceedings.mlr.press/v229/abyaneh23a/abyaneh23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-abyaneh23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Amin
    family: Abyaneh
  - given: Hsiu-Chin
    family: Lin
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2642-2662
  id: abyaneh23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2642
  lastpage: 2662
  published: 2023-12-02 00:00:00 +0000
- title: 'MUTEX: Learning Unified Policies from Multimodal Task Specifications'
  abstract: 'Humans use different modalities, such as speech, text, images, videos, etc., to communicate their intent and goals with teammates. For robots to become better assistants, we aim to endow them with the ability to follow instructions and understand tasks specified by their human partners. Most robotic policy learning methods have focused on one single modality of task specification while ignoring the rich cross-modal information. We present MUTEX, a unified approach to policy learning from multimodal task specifications. It trains a transformer-based architecture to facilitate cross-modal reasoning, combining masked modeling and cross-modal matching objectives in a two-stage training procedure. After training, MUTEX can follow a task specification in any of the six learned modalities (video demonstrations, goal images, text goal descriptions, text instructions, speech goal descriptions, and speech instructions) or a combination of them. We systematically evaluate the benefits of MUTEX in a newly designed dataset with 100 tasks in simulation and 50 tasks in the real world, annotated with multiple instances of task specifications in different modalities, and observe improved performance over methods trained specifically for any single modality. More information at https://ut-austin-rpl.github.io/MUTEX/'
  volume: 229
  URL: https://proceedings.mlr.press/v229/shah23b.html
  PDF: https://proceedings.mlr.press/v229/shah23b/shah23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-shah23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Rutav
    family: Shah
  - given: Roberto
    family: Martín-Martín
  - given: Yuke
    family: Zhu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2663-2682
  id: shah23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2663
  lastpage: 2682
  published: 2023-12-02 00:00:00 +0000
- title: 'Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning'
  abstract: 'Navigation in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. Humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics — e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. Language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. Therefore, in this paper we study how the “semantic guesswork” produced by language models can be utilized as a guiding heuristic for planning algorithms. Our method, Language Frontier Guide (LFG), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. We evaluate LFG in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/shah23c.html
  PDF: https://proceedings.mlr.press/v229/shah23c/shah23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-shah23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Dhruv
    family: Shah
  - given: Michael Robert
    family: Equi
  - given: Błażej
    family: Osiński
  - given: Fei
    family: Xia
  - given: Brian
    family: Ichter
  - given: Sergey
    family: Levine
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2683-2699
  id: shah23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2683
  lastpage: 2699
  published: 2023-12-02 00:00:00 +0000
- title: 'A Data-efficient Neural ODE Framework for Optimal Control of Soft Manipulators'
  abstract: 'This paper introduces a novel approach for modeling continuous forward kinematic models of soft continuum robots by employing Augmented Neural ODE (ANODE), a cutting-edge family of deep neural network models. To the best of our knowledge, this is the first application of ANODE in modeling soft continuum robots. This formulation introduces auxiliary dimensions, allowing the system’s states to evolve in the augmented space which provides a richer set of dynamics that the model can learn, increasing the flexibility and accuracy of the model. Our methodology achieves exceptional sample efficiency, training the continuous forward kinematic model using only 25 scattered data points. Additionally, we design and implement a fully parallel Model Predictive Path Integral (MPPI)-based controller running on a GPU, which efficiently manages a non-convex objective function. Through a set of experiments, we showed that the proposed framework (ANODE+MPPI) significantly outperforms state-of-the-art learning-based methods such as FNN and RNN in unseen-before scenarios and marginally outperforms them in seen-before scenarios.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kasaei23a.html
  PDF: https://proceedings.mlr.press/v229/kasaei23a/kasaei23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kasaei23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Mohammadreza
    family: Kasaei
  - given: Keyhan Kouhkiloui
    family: Babarahmati
  - given: Zhibin
    family: Li
  - given: Mohsen
    family: Khadem
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2700-2713
  id: kasaei23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2700
  lastpage: 2713
  published: 2023-12-02 00:00:00 +0000
- title: 'Language Conditioned Traffic Generation'
  abstract: 'Simulation forms the backbone of modern self-driving development. Simulators help develop, test, and improve driving systems without putting humans, vehicles, or their environment at risk. However, simulators face a major challenge: They rely on realistic, scalable, yet interesting content. While recent advances in rendering and scene reconstruction make great strides in creating static scene assets, modeling their layout, dynamics, and behaviors remains challenging. In this work, we turn to language as a source of supervision for dynamic traffic scene generation. Our model, LCTGen, combines a large language model with a transformer-based decoder architecture that selects likely map locations from a dataset of maps, and produces an initial traffic distribution, as well as the dynamics of each vehicle. LCTGen outperforms prior work in both unconditional and conditional traffic scene generation in terms of realism and fidelity.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/tan23a.html
  PDF: https://proceedings.mlr.press/v229/tan23a/tan23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-tan23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Shuhan
    family: Tan
  - given: Boris
    family: Ivanovic
  - given: Xinshuo
    family: Weng
  - given: Marco
    family: Pavone
  - given: Philipp
    family: Kraehenbuehl
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2714-2752
  id: tan23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2714
  lastpage: 2752
  published: 2023-12-02 00:00:00 +0000
- title: 'CALAMARI: Contact-Aware and Language conditioned spatial Action MApping for contact-RIch manipulation'
  abstract: 'Making contact with purpose is a central part of robot manipulation and remains essential for many household tasks – from sweeping dust into a dustpan, to wiping tables; from erasing whiteboards, to applying paint. In this work, we investigate learning language-conditioned, vision-based manipulation policies wherein the action representation is in fact, contact itself – predicting contact formations at which tools grasped by the robot should meet an observable surface. Our approach, Contact-Aware and Language conditioned spatial Action MApping for contact-RIch manipulation (CALAMARI), exhibits several advantages including (i) benefiting from existing visual-language models for pretrained spatial features, grounding instructions to behaviors, and for sim2real transfer; and (ii) factorizing perception and control over a natural boundary (i.e. contact) into two modules that synergize with each other, whereby action predictions can be aligned per pixel with image observations, and low-level controllers can optimize motion trajectories that maintain contact while avoiding penetration. Experiments show that CALAMARI outperforms existing state-of-the-art model architectures for a broad range of contact-rich tasks, and pushes new ground on embodiment-agnostic generalization to unseen objects with varying elasticity, geometry, and colors in both simulated and real-world settings.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wi23a.html
  PDF: https://proceedings.mlr.press/v229/wi23a/wi23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wi23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Youngsun
    family: Wi
  - given: Mark Van der
    family: Merwe
  - given: Pete
    family: Florence
  - given: Andy
    family: Zeng
  - given: Nima
    family: Fazeli
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2753-2771
  id: wi23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2753
  lastpage: 2771
  published: 2023-12-02 00:00:00 +0000
- title: 'Generalization of Heterogeneous Multi-Robot Policies via Awareness and Communication of Capabilities'
  abstract: 'Recent advances in multi-agent reinforcement learning (MARL) are enabling impressive coordination in heterogeneous multi-robot teams. However, existing approaches often overlook the challenge of generalizing learned policies to teams of new compositions, sizes, and robots. While such generalization might not be important in teams of virtual agents that can retrain policies on-demand, it is pivotal in multi-robot systems that are deployed in the real-world and must readily adapt to inevitable changes. As such, multi-robot policies must remain robust to team changes – an ability we call adaptive teaming. In this work, we investigate if awareness and communication of robot capabilities can provide such generalization by conducting detailed experiments involving an established multi-robot test bed. We demonstrate that shared decentralized policies, that enable robots to be both aware of and communicate their capabilities, can achieve adaptive teaming by implicitly capturing the fundamental relationship between collective capabilities and effective coordination. Videos of trained policies can be viewed at https://sites.google.com/view/cap-comm .'
  volume: 229
  URL: https://proceedings.mlr.press/v229/howell23a.html
  PDF: https://proceedings.mlr.press/v229/howell23a/howell23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-howell23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Pierce
    family: Howell
  - given: Max
    family: Rudolph
  - given: Reza Joseph
    family: Torbati
  - given: Kevin
    family: Fu
  - given: Harish
    family: Ravichandar
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2772-2790
  id: howell23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2772
  lastpage: 2790
  published: 2023-12-02 00:00:00 +0000
- title: 'CAJun: Continuous Adaptive Jumping using a Learned Centroidal Controller'
  abstract: 'We present CAJun, a novel hierarchical learning and control framework that enables legged robots to jump continuously with adaptive jumping distances. CAJun consists of a high-level centroidal policy and a low-level leg controller. In particular, we use reinforcement learning (RL) to train the centroidal policy, which specifies the gait timing, base velocity, and swing foot position for the leg controller. The leg controller optimizes motor commands for the swing and stance legs according to the gait timing to track the swing foot target and base velocity commands. Additionally, we reformulate the stance leg optimizer in the leg controller to speed up policy training by an order of magnitude. Our system combines the versatility of learning with the robustness of optimal control. We show that after 20 minutes of training on a single GPU, CAJun can achieve continuous, long jumps with adaptive distances on a Go1 robot with small sim-to-real gaps. Moreover, the robot can jump across gaps with a maximum width of 70cm, which is over $40%$ wider than existing methods.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yang23b.html
  PDF: https://proceedings.mlr.press/v229/yang23b/yang23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yang23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuxiang
    family: Yang
  - given: Guanya
    family: Shi
  - given: Xiangyun
    family: Meng
  - given: Wenhao
    family: Yu
  - given: Tingnan
    family: Zhang
  - given: Jie
    family: Tan
  - given: Byron
    family: Boots
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2791-2806
  id: yang23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2791
  lastpage: 2806
  published: 2023-12-02 00:00:00 +0000
- title: 'Multi-Predictor Fusion: Combining Learning-based and Rule-based Trajectory Predictors'
  abstract: 'Trajectory prediction modules are key enablers for safe and efficient planning of autonomous vehicles (AVs), particularly in highly interactive traffic scenarios. Recently, learning-based trajectory predictors have experienced considerable success in providing state-of-the-art performance due to their ability to learn multimodal behaviors of other agents from data. In this paper, we present an algorithm called multi-predictor fusion (MPF) that augments the performance of learning-based predictors by imbuing them with motion planners that are tasked with satisfying logic-based rules. MPF probabilistically combines learning- and rule-based predictors by mixing trajectories from both standalone predictors in accordance with a belief distribution that reflects the online performance of each predictor. In our results, we show that MPF outperforms the two standalone predictors on various metrics and delivers the most consistent performance.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/veer23a.html
  PDF: https://proceedings.mlr.press/v229/veer23a/veer23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-veer23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Sushant
    family: Veer
  - given: Apoorva
    family: Sharma
  - given: Marco
    family: Pavone
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2807-2820
  id: veer23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2807
  lastpage: 2820
  published: 2023-12-02 00:00:00 +0000
- title: 'Neural Field Dynamics Model for Granular Object Piles Manipulation'
  abstract: 'We present a learning-based dynamics model for granular material manipulation. Drawing inspiration from computer graphics’ Eulerian approach, our method adopts a fully convolutional neural network that operates on a density field-based representation of object piles, allowing it to exploit the spatial locality of inter-object interactions through the convolution operations. This approach greatly improves the learning and computation efficiency compared to existing latent or particle-based methods and sidesteps the need for state estimation, making it directly applicable to real-world settings. Furthermore, our differentiable action rendering module makes the model fully differentiable and can be directly integrated with a gradient-based algorithm for curvilinear trajectory optimization. We evaluate our model with a wide array of piles manipulation tasks both in simulation and real-world experiments and demonstrate that it significantly exceeds existing methods in both accuracy and computation efficiency. More details can be found at https://sites.google.com/view/nfd-corl23/'
  volume: 229
  URL: https://proceedings.mlr.press/v229/xue23a.html
  PDF: https://proceedings.mlr.press/v229/xue23a/xue23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-xue23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Shangjie
    family: Xue
  - given: Shuo
    family: Cheng
  - given: Pujith
    family: Kachana
  - given: Danfei
    family: Xu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2821-2837
  id: xue23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2821
  lastpage: 2837
  published: 2023-12-02 00:00:00 +0000
- title: 'AR2-D2: Training a Robot Without a Robot'
  abstract: 'Diligently gathered human demonstrations serve as the unsung heroes empowering the progression of robot learning. Today, demonstrations are collected by training people to use specialized controllers, which (tele-)operate robots to manipulate a small number of objects. By contrast, we introduce AR2-D2: a system for collecting demonstrations which (1) does not require people with specialized training, (2) does not require any real robots during data collection, and therefore, (3) enables manipulation of diverse objects with a real robot. AR2-D2 is a framework in the form of an iOS app that people can use to record a video of themselves manipulating any object while simultaneously capturing essential data modalities for training a real robot. We show that data collected via our system enables the training of behavior cloning agents in manipulating real objects. Our experiments further show that training with our AR data is as effective as training with real-world robot demonstrations. Moreover, our user study indicates that users find AR2-D2 intuitive to use and require no training in contrast to four other frequently employed methods for collecting robot demonstrations.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/duan23a.html
  PDF: https://proceedings.mlr.press/v229/duan23a/duan23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-duan23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jiafei
    family: Duan
  - given: Yi Ru
    family: Wang
  - given: Mohit
    family: Shridhar
  - given: Dieter
    family: Fox
  - given: Ranjay
    family: Krishna
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2838-2848
  id: duan23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2838
  lastpage: 2848
  published: 2023-12-02 00:00:00 +0000
- title: 'Affordance-Driven Next-Best-View Planning for Robotic Grasping'
  abstract: 'Grasping occluded objects in cluttered environments is an essential component in complex robotic manipulation tasks. In this paper, we introduce an AffordanCE-driven Next-Best-View planning policy (ACE-NBV) that tries to find a feasible grasp for target object via continuously observing scenes from new viewpoints. This policy is motivated by the observation that the grasp affordances of an occluded object can be better-measured under the view when the view-direction are the same as the grasp view. Specifically, our method leverages the paradigm of novel view imagery to predict the grasps affordances under previously unobserved view, and select next observation view based on the highest imagined grasp quality of the target object. The experimental results in simulation and on a real robot demonstrate the effectiveness of the proposed affordance-driven next-best-view planning policy. Project page: https://sszxc.net/ace-nbv/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23i.html
  PDF: https://proceedings.mlr.press/v229/zhang23i/zhang23i.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23i.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xuechao
    family: Zhang
  - given: Dong
    family: Wang
  - given: Sun
    family: Han
  - given: Weichuang
    family: Li
  - given: Bin
    family: Zhao
  - given: Zhigang
    family: Wang
  - given: Xiaoming
    family: Duan
  - given: Chongrong
    family: Fang
  - given: Xuelong
    family: Li
  - given: Jianping
    family: He
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2849-2862
  id: zhang23i
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2849
  lastpage: 2862
  published: 2023-12-02 00:00:00 +0000
- title: 'PairwiseNet: Pairwise Collision Distance Learning for High-dof Robot Systems'
  abstract: 'Motion planning for robot manipulation systems operating in complex environments remains a challenging problem. It requires the evaluation of both the collision distance and its derivative. Owing to its computational complexity, recent studies have attempted to utilize data-driven approaches to learn the collision distance. However, their performance degrades significantly for complicated high-dof systems, such as multi-arm robots. Additionally, the model must be retrained every time the environment undergoes even slight changes. In this paper, we propose PairwiseNet, a model that estimates the minimum distance between two geometric shapes and overcomes many of the limitations of current models. By dividing the problem of global collision distance learning into smaller pairwise sub-problems, PairwiseNet can be used to efficiently calculate the global collision distance. PairwiseNet can be deployed without further modifications or training for any system comprised of the same shape elements (as those in the training dataset). Experiments with multi-arm manipulation systems of various dof indicate that our model achieves significant performance improvements concerning several performance metrics, especially the false positive rate with the collision-free guaranteed threshold. Results further demonstrate that our single trained PairwiseNet model is applicable to all multi-arm systems used in the evaluation. The code is available at https://github.com/kjh6526/PairwiseNet.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kim23d.html
  PDF: https://proceedings.mlr.press/v229/kim23d/kim23d.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kim23d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jihwan
    family: Kim
  - given: Frank C.
    family: Park
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2863-2877
  id: kim23d
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2863
  lastpage: 2877
  published: 2023-12-02 00:00:00 +0000
- title: 'Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching'
  abstract: 'Gradient-based methods enable efficient search capabilities in high dimensions. However, in order to apply them effectively in offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL), we require a more careful consideration of how uncertainty estimation interplays with first-order methods that attempt to minimize them. We study smoothed distance to data as an uncertainty metric, and claim that it has two beneficial properties: (i) it allows gradient-based methods that attempt to minimize uncertainty to drive iterates to data as smoothing is annealed, and (ii) it facilitates analysis of model bias with Lipschitz constants. As distance to data can be expensive to compute online, we consider settings where we need amortize this computation. Instead of learning the distance however, we propose to learn its gradients directly as an oracle for first-order optimizers. We show these gradients can be efficiently learned with score-matching techniques by leveraging the equivalence between distance to data and data likelihood. Using this insight, we propose Score-Guided Planning (SGP), a planning algorithm for offline RL that utilizes score-matching to enable first-order planning in high-dimensional problems, where zeroth-order methods were unable to scale, and ensembles were unable to overcome local minima. Website: https://sites.google.com/view/score-guided-planning/home'
  volume: 229
  URL: https://proceedings.mlr.press/v229/suh23a.html
  PDF: https://proceedings.mlr.press/v229/suh23a/suh23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-suh23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: H.J. Terry
    family: Suh
  - given: Glen
    family: Chou
  - given: Hongkai
    family: Dai
  - given: Lujie
    family: Yang
  - given: Abhishek
    family: Gupta
  - given: Russ
    family: Tedrake
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2878-2904
  id: suh23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2878
  lastpage: 2904
  published: 2023-12-02 00:00:00 +0000
- title: 'Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models'
  abstract: 'Long-horizon tasks, usually characterized by complex subtask dependencies, present a significant challenge in manipulation planning. Skill chaining is a practical approach to solving unseen tasks by combining learned skill priors. However, such methods are myopic if sequenced greedily and face scalability issues with search-based planning strategy. To address these challenges, we introduce Generative Skill Chaining (GSC), a probabilistic framework that learns skill-centric diffusion models and composes their learned distributions to generate long-horizon plans during inference. GSC samples from all skill models in parallel to efficiently solve unseen tasks while enforcing geometric constraints. We evaluate the method on various long-horizon tasks and demonstrate its capability in reasoning about action dependencies, constraint handling, and generalization, along with its ability to replan in the face of perturbations. We show results in simulation and on real robot to validate the efficiency and scalability of GSC, highlighting its potential for advancing long-horizon task planning. More details are available at: https://generative-skill-chaining.github.io/'
  volume: 229
  URL: https://proceedings.mlr.press/v229/mishra23a.html
  PDF: https://proceedings.mlr.press/v229/mishra23a/mishra23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-mishra23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Utkarsh Aashu
    family: Mishra
  - given: Shangjie
    family: Xue
  - given: Yongxin
    family: Chen
  - given: Danfei
    family: Xu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2905-2925
  id: mishra23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2905
  lastpage: 2925
  published: 2023-12-02 00:00:00 +0000
- title: 'Online Learning for Obstacle Avoidance'
  abstract: 'We approach the fundamental problem of obstacle avoidance for robotic systems via the lens of online learning. In contrast to prior work that either assumes worst-case realizations of uncertainty in the environment or a stationary stochastic model of uncertainty, we propose a method that is efficient to implement and provably grants instance-optimality with respect to perturbations of trajectories generated from an open-loop planner (in the sense of minimizing worst-case regret). The resulting policy adapts online to realizations of uncertainty and provably compares well with the best obstacle avoidance policy in hindsight from a rich class of policies. The method is validated in simulation on a dynamical system environment and compared to baseline open-loop planning and robust Hamilton-Jacobi reachability techniques. Further, it is implemented on a hardware example where a quadruped robot traverses a dense obstacle field and encounters input disturbances due to time delays, model uncertainty, and dynamics nonlinearities.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/snyder23a.html
  PDF: https://proceedings.mlr.press/v229/snyder23a/snyder23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-snyder23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: David
    family: Snyder
  - given: Meghan
    family: Booker
  - given: Nathaniel
    family: Simon
  - given: Wenhan
    family: Xia
  - given: Daniel
    family: Suo
  - given: Elad
    family: Hazan
  - given: Anirudha
    family: Majumdar
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2926-2954
  id: snyder23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2926
  lastpage: 2954
  published: 2023-12-02 00:00:00 +0000
- title: 'Polybot: Training One Policy Across Robots While Embracing Variability'
  abstract: 'Reusing large datasets is crucial to scale vision-based robotic manipulators to everyday scenarios due to the high cost of collecting robotic datasets. However, robotic platforms possess varying control schemes, camera viewpoints, kinematic configurations, and end-effector morphologies, posing significant challenges when transferring manipulation skills from one platform to another. To tackle this problem, we propose a set of key design decisions to train a single policy for deployment on multiple robotic platforms. Our framework first aligns the observation and action spaces of our policy across embodiments via utilizing wrist cameras and a unified, but modular codebase. To bridge the remaining domain shift, we align our policy’s internal representations across embodiments via contrastive learning. We evaluate our method on a dataset collected over 60 hours spanning 6 tasks and 3 robots with varying joint configurations and sizes: the WidowX 250S, Franka Emika Panda, and Sawyer. Our results demonstrate significant improvements in success rate and sample efficiency for our policy when using new task data collected on a different robot, validating our proposed design decisions. More details and videos can be found on our project website: https://sites.google.com/view/cradle-multirobot'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yang23c.html
  PDF: https://proceedings.mlr.press/v229/yang23c/yang23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yang23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jonathan Heewon
    family: Yang
  - given: Dorsa
    family: Sadigh
  - given: Chelsea
    family: Finn
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2955-2974
  id: yang23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2955
  lastpage: 2974
  published: 2023-12-02 00:00:00 +0000
- title: 'RoboPianist: Dexterous Piano Playing with Deep Reinforcement Learning'
  abstract: 'Replicating human-like dexterity in robot hands represents one of the largest open problems in robotics. Reinforcement learning is a promising approach that has achieved impressive progress in the last few years; however, the class of problems it has typically addressed corresponds to a rather narrow definition of dexterity as compared to human capabilities. To address this gap, we investigate piano-playing, a skill that challenges even the human limits of dexterity, as a means to test high-dimensional control, and which requires high spatial and temporal precision, and complex finger coordination and planning. We introduce RoboPianist, a system that enables simulated anthropomorphic hands to learn an extensive repertoire of 150 piano pieces where traditional model-based optimization struggles. We additionally introduce an open-sourced environment, benchmark of tasks, interpretable evaluation metrics, and open challenges for future study. Our website featuring videos, code, and datasets is available at https://kzakka.com/robopianist/'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zakka23a.html
  PDF: https://proceedings.mlr.press/v229/zakka23a/zakka23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zakka23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Kevin
    family: Zakka
  - given: Philipp
    family: Wu
  - given: Laura
    family: Smith
  - given: Nimrod
    family: Gileadi
  - given: Taylor
    family: Howell
  - given: Xue Bin
    family: Peng
  - given: Sumeet
    family: Singh
  - given: Yuval
    family: Tassa
  - given: Pete
    family: Florence
  - given: Andy
    family: Zeng
  - given: Pieter
    family: Abbeel
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2975-2994
  id: zakka23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2975
  lastpage: 2994
  published: 2023-12-02 00:00:00 +0000
- title: 'Revisiting Depth-guided Methods for Monocular 3D Object Detection by Hierarchical Balanced Depth'
  abstract: 'Monocular 3D object detection has seen significant advancements with the incorporation of depth information. However, there remains a considerable performance gap compared to LiDAR-based methods, largely due to inaccurate depth estimation. We argue that this issue stems from the commonly used pixel-wise depth map loss, which inherently creates the imbalance of loss weighting between near and distant objects. To address these challenges, we propose MonoHBD (Monocular Hierarchical Balanced Depth), a comprehensive solution with the hierarchical mechanism. We introduce the Hierarchical Depth Map (HDM) structure that incorporates depth bins and depth offsets to enhance the localization accuracy for objects. Leveraging RoIAlign, our Balanced Depth Extractor (BDE) module captures both scene-level depth relationships and object-specific depth characteristics while considering the geometry properties through the inclusion of camera calibration parameters. Furthermore, we propose a novel depth map loss that regularizes object-level depth features to mitigate imbalanced loss propagation. Our model reaches state-of-the-art results on the KITTI 3D object detection benchmark while supporting real-time detection. Excessive ablation studies are also conducted to prove the efficacy of our proposed modules.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/chen23d.html
  PDF: https://proceedings.mlr.press/v229/chen23d/chen23d.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-chen23d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yi-Rong
    family: Chen
  - given: Ching-Yu
    family: Tseng
  - given: Yi-Syuan
    family: Liou
  - given: Tsung-Han
    family: Wu
  - given: Winston H.
    family: Hsu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 2995-3009
  id: chen23d
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 2995
  lastpage: 3009
  published: 2023-12-02 00:00:00 +0000
- title: 'Heteroscedastic Gaussian Processes and Random Features: Scalable Motion Primitives with Guarantees'
  abstract: 'Heteroscedastic Gaussian processes (HGPs) are kernel-based, non-parametric models that can be used to infer nonlinear functions with time-varying noise. In robotics, they can be employed for learning from demonstration as motion primitives, i.e. as a model of the trajectories to be executed by the robot. HGPs provide variance estimates around the reference signal modeling the trajectory, capturing both the predictive uncertainty and the motion variability. However, similarly to standard Gaussian processes they suffer from a cubic complexity in the number of training points, due to the inversion of the kernel matrix. The uncertainty can be leveraged for more complex learning tasks, such as inferring the variable impedance profile required from a robotic manipulator. However, suitable approximations are needed to make HGPs scalable, at the price of potentially worsening the posterior mean and variance profiles. Motivated by these observations, we study the combination of HGPs and random features, which are a popular, data-independent approximation strategy of kernel functions. In a theoretical analysis, we provide novel guarantees on the approximation error of the HGP posterior due to random features. Moreover, we validate this scalable motion primitive on real robot data, related to the problem of variable impedance learning. In this way, we show that random features offer a viable and theoretically sound alternative for speeding up the trajectory processing, without sacrificing accuracy.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/caldarelli23a.html
  PDF: https://proceedings.mlr.press/v229/caldarelli23a/caldarelli23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-caldarelli23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Edoardo
    family: Caldarelli
  - given: Antoine
    family: Chatalic
  - given: Adrià
    family: Colomé
  - given: Lorenzo
    family: Rosasco
  - given: Carme
    family: Torras
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3010-3029
  id: caldarelli23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3010
  lastpage: 3029
  published: 2023-12-02 00:00:00 +0000
- title: 'Human-in-the-Loop Task and Motion Planning for Imitation Learning'
  abstract: 'Imitation learning from human demonstrations can teach robots complex manipulation skills, but is time-consuming and labor intensive. In contrast, Task and Motion Planning (TAMP) systems are automated and excel at solving long-horizon tasks, but they are difficult to apply to contact-rich tasks. In this paper, we present Human-in-the-Loop Task and Motion Planning (HITL-TAMP), a novel system that leverages the benefits of both approaches. The system employs a TAMP-gated control mechanism, which selectively gives and takes control to and from a human teleoperator. This enables the human teleoperator to manage a fleet of robots, maximizing data collection efficiency. The collected human data is then combined with an imitation learning framework to train a TAMP-gated policy, leading to superior performance compared to training on full task demonstrations. We compared HITL-TAMP to a conventional teleoperation system — users gathered more than 3x the number of demos given the same time budget. Furthermore, proficient agents ($75%$+ success) could be trained from just 10 minutes of non-expert teleoperation data. Finally, we collected 2.1K demos with HITL-TAMP across 12 contact-rich, long-horizon tasks and show that the system often produces near-perfect agents. Videos and additional results at https://hitltamp.github.io .'
  volume: 229
  URL: https://proceedings.mlr.press/v229/mandlekar23b.html
  PDF: https://proceedings.mlr.press/v229/mandlekar23b/mandlekar23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-mandlekar23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ajay
    family: Mandlekar
  - given: Caelan Reed
    family: Garrett
  - given: Danfei
    family: Xu
  - given: Dieter
    family: Fox
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3030-3060
  id: mandlekar23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3030
  lastpage: 3060
  published: 2023-12-02 00:00:00 +0000
- title: 'Gesture-Informed Robot Assistance via Foundation Models'
  abstract: 'Gestures serve as a fundamental and significant mode of non-verbal communication among humans. Deictic gestures (such as pointing towards an object), in particular, offer valuable means of efficiently expressing intent in situations where language is inaccessible, restricted, or highly specialized. As a result, it is essential for robots to comprehend gestures in order to infer human intentions and establish more effective coordination with them. Prior work often rely on a rigid hand-coded library of gestures along with their meanings. However, interpretation of gestures is often context-dependent, requiring more flexibility and common-sense reasoning. In this work, we propose a framework, GIRAF, for more flexibly interpreting gesture and language instructions by leveraging the power of large language models. Our framework is able to accurately infer human intent and contextualize the meaning of their gestures for more effective human-robot collaboration. We instantiate the framework for three table-top manipulation tasks and demonstrate that it is both effective and preferred by users. We further demonstrate GIRAF’s ability on reasoning about diverse types of gestures by curating a GestureInstruct dataset consisting of 36 different task scenarios. GIRAF achieved $81%$ success rate on finding the correct plan for tasks in GestureInstruct. Videos and datasets can be found on our project website: https://tinyurl.com/giraf23'
  volume: 229
  URL: https://proceedings.mlr.press/v229/lin23a.html
  PDF: https://proceedings.mlr.press/v229/lin23a/lin23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-lin23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Li-Heng
    family: Lin
  - given: Yuchen
    family: Cui
  - given: Yilun
    family: Hao
  - given: Fei
    family: Xia
  - given: Dorsa
    family: Sadigh
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3061-3082
  id: lin23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3061
  lastpage: 3082
  published: 2023-12-02 00:00:00 +0000
- title: 'TactileVAD: Geometric Aliasing-Aware Dynamics for High-Resolution Tactile Control'
  abstract: 'Touch-based control is a promising approach to dexterous manipulation. However, existing tactile control methods often overlook tactile geometric aliasing which can compromise control performance and reliability. This type of aliasing occurs when different contact locations yield similar tactile signatures. To address this, we propose TactileVAD, a generative decoder-only linear latent dynamics formulation compatible with standard control methods that is capable of resolving geometric aliasing. We evaluate TactileVAD on two mechanically-distinct tactile sensors, SoftBubbles (pointcloud data) and Gelslim 3.0 (RGB data), showcasing its effectiveness in handling different sensing modalities. Additionally, we introduce the tactile cartpole, a novel benchmarking setup to evaluate the ability of a control method to respond to disturbances based on tactile input. Evaluations comparing TactileVAD to baselines suggest that our method is better able to achieve goal tactile configurations and hand poses.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/oller23a.html
  PDF: https://proceedings.mlr.press/v229/oller23a/oller23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-oller23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Miquel
    family: Oller
  - given: Dmitry
    family: Berenson
  - given: Nima
    family: Fazeli
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3083-3099
  id: oller23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3083
  lastpage: 3099
  published: 2023-12-02 00:00:00 +0000
- title: 'FastRLAP: A System for Learning High-Speed Driving via Deep RL and Autonomous Practicing'
  abstract: 'We present a system that enables an autonomous small-scale RC car to drive aggressively from visual observations using reinforcement learning (RL). Our system, FastRLAP, trains autonomously in the real world, without human interventions, and without requiring any simulation or expert demonstrations. Our system integrates a number of important components to make this possible: we initialize the representations for the RL policy and value function from a large prior dataset of other robots navigating in other environments (at low speed), which provides a navigation-relevant representation. From here, a sample-efficient online RL method uses a single low-speed user-provided demonstration to determine the desired driving course, extracts a set of navigational checkpoints, and autonomously practices driving through these checkpoints, resetting automatically on collision or failure. Perhaps surprisingly, we find that with appropriate initialization and choice of algorithm, our system can learn to drive over a variety of racing courses with less than 20 minutes of online training. The resulting policies exhibit emergent aggressive driving skills, such as timing braking and acceleration around turns and avoiding areas which impede the robot’s motion, approaching the performance of a human driver using a similar first-person interface over the course of training.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/stachowicz23a.html
  PDF: https://proceedings.mlr.press/v229/stachowicz23a/stachowicz23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-stachowicz23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Kyle
    family: Stachowicz
  - given: Dhruv
    family: Shah
  - given: Arjun
    family: Bhorkar
  - given: Ilya
    family: Kostrikov
  - given: Sergey
    family: Levine
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3100-3111
  id: stachowicz23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3100
  lastpage: 3111
  published: 2023-12-02 00:00:00 +0000
- title: 'Energy-based Potential Games for Joint Motion Forecasting and Control'
  abstract: 'This work uses game theory as a mathematical framework to address interaction modeling in multi-agent motion forecasting and control. Despite its interpretability, applying game theory to real-world robotics, like automated driving, faces challenges such as unknown game parameters. To tackle these, we establish a connection between differential games, optimal control, and energy-based models, demonstrating how existing approaches can be unified under our proposed Energy-based Potential Game formulation. Building upon this, we introduce a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, acting as an inductive bias. The analysis provides empirical evidence that the game-theoretic layer adds interpretability and improves the predictive performance of various neural network backbones using two simulations and two real-world driving datasets.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/diehl23a.html
  PDF: https://proceedings.mlr.press/v229/diehl23a/diehl23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-diehl23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Christopher
    family: Diehl
  - given: Tobias
    family: Klosek
  - given: Martin
    family: Krueger
  - given: Nils
    family: Murzyn
  - given: Timo
    family: Osterburg
  - given: Torsten
    family: Bertram
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3112-3141
  id: diehl23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3112
  lastpage: 3141
  published: 2023-12-02 00:00:00 +0000
- title: 'Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play'
  abstract: 'Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vision and torque-based models by an average of 1.7X. Finally, we provide a detailed analysis on factors critical to T-Dex including the importance of play data, architectures, and representation learning.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/guzey23a.html
  PDF: https://proceedings.mlr.press/v229/guzey23a/guzey23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-guzey23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Irmak
    family: Guzey
  - given: Ben
    family: Evans
  - given: Soumith
    family: Chintala
  - given: Lerrel
    family: Pinto
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3142-3166
  id: guzey23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3142
  lastpage: 3166
  published: 2023-12-02 00:00:00 +0000
- title: 'ADU-Depth: Attention-based Distillation with Uncertainty Modeling for Depth Estimation'
  abstract: 'Monocular depth estimation is challenging due to its inherent ambiguity and ill-posed nature, yet it is quite important to many applications. While recent works achieve limited accuracy by designing increasingly complicated networks to extract features with limited spatial geometric cues from a single RGB image, we intend to introduce spatial cues by training a teacher network that leverages left-right image pairs as inputs and transferring the learned 3D geometry-aware knowledge to the monocular student network. Specifically, we present a novel knowledge distillation framework, named ADU-Depth, with the goal of leveraging the well-trained teacher network to guide the learning of the student network, thus boosting the precise depth estimation with the help of extra spatial scene information. To enable domain adaptation and ensure effective and smooth knowledge transfer from teacher to student, we apply both attention-adapted feature distillation and focal-depth-adapted response distillation in the training stage. In addition, we explicitly model the uncertainty of depth estimation to guide distillation in both feature space and result space to better produce 3D-aware knowledge from monocular observations and thus enhance the learning for hard-to-predict image regions. Our extensive experiments on the real depth estimation datasets KITTI and DrivingStereo demonstrate the effectiveness of the proposed method, which ranked 1st on the challenging KITTI online benchmark.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wu23c.html
  PDF: https://proceedings.mlr.press/v229/wu23c/wu23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wu23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: ZiZhang
    family: Wu
  - given: Zhuozheng
    family: Li
  - given: Zhi-Gang
    family: Fan
  - given: Yunzhe
    family: Wu
  - given: Xiaoquan
    family: Wang
  - given: Rui
    family: Tang
  - given: Jian
    family: Pu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3167-3179
  id: wu23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3167
  lastpage: 3179
  published: 2023-12-02 00:00:00 +0000
- title: 'Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning'
  abstract: 'Robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. Existing work focuses primarily on single-level rearrangement planning and, even if multiple levels exist, dependency relations among substructures are geometrically simpler, like tower stacking. We propose Structural Concept Learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. It is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. We compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding to achieve better performance, flexibility, and efficiency. The dataset, demonstration videos, supplementary details, and code implementation are available at: https://manavkulshrestha.github.io/scl'
  volume: 229
  URL: https://proceedings.mlr.press/v229/kulshrestha23a.html
  PDF: https://proceedings.mlr.press/v229/kulshrestha23a/kulshrestha23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-kulshrestha23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Manav
    family: Kulshrestha
  - given: Ahmed H.
    family: Qureshi
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3180-3193
  id: kulshrestha23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3180
  lastpage: 3193
  published: 2023-12-02 00:00:00 +0000
- title: 'Few-Shot In-Context Imitation Learning via Implicit Graph Alignment'
  abstract: 'Consider the following problem: given a few demonstrations of a task across a few different objects, how can a robot learn to perform that same task on new, previously unseen objects? This is challenging because the large variety of objects within a class makes it difficult to infer the task-relevant relationship between the new objects and the objects in the demonstrations. We address this by formulating imitation learning as a conditional alignment problem between graph representations of objects. Consequently, we show that this conditioning allows for in-context learning, where a robot can perform a task on a set of new objects immediately after the demonstrations, without any prior knowledge about the object class or any further training. In our experiments, we explore and validate our design choices, and we show that our method is highly effective for few-shot learning of several real-world, everyday tasks, whilst outperforming baselines. Videos are available on our project webpage at https://www.robot-learning.uk/implicit-graph-alignment.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/vosylius23a.html
  PDF: https://proceedings.mlr.press/v229/vosylius23a/vosylius23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-vosylius23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Vitalis
    family: Vosylius
  - given: Edward
    family: Johns
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3194-3213
  id: vosylius23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3194
  lastpage: 3213
  published: 2023-12-02 00:00:00 +0000
- title: 'Topology-Matching Normalizing Flows for Out-of-Distribution Detection in Robot Learning'
  abstract: 'To facilitate reliable deployments of autonomous robots in the real world, Out-of-Distribution (OOD) detection capabilities are often required. A powerful approach for OOD detection is based on density estimation with Normalizing Flows (NFs). However, we find that prior work with NFs attempts to match the complex target distribution topologically with naïve base distributions leading to adverse implications. In this work, we circumvent this topological mismatch using an expressive class-conditional base distribution trained with an information-theoretic objective to match the required topology. The proposed method enjoys the merits of wide compatibility with existing learned models without any performance degradation and minimum computation overhead while enhancing OOD detection capabilities. We demonstrate superior results in density estimation and 2D object detection benchmarks in comparison with extensive baselines. Moreover, we showcase the applicability of the method with a real-robot deployment.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/feng23b.html
  PDF: https://proceedings.mlr.press/v229/feng23b/feng23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-feng23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jianxiang
    family: Feng
  - given: Jongseok
    family: Lee
  - given: Simon
    family: Geisler
  - given: Stephan
    family: Günnemann
  - given: Rudolph
    family: Triebel
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3214-3241
  id: feng23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3214
  lastpage: 3241
  published: 2023-12-02 00:00:00 +0000
- title: 'Compositional Diffusion-Based Continuous Constraint Solvers'
  abstract: 'This paper introduces an approach for learning to solve continuous constraint satisfaction problems (CCSP) in robotic reasoning and planning. Previous methods primarily rely on hand-engineering or learning generators for specific constraint types and then rejecting the value assignments when other constraints are violated. By contrast, our model, the compositional diffusion continuous constraint solver (Diffusion-CCSP) derives global solutions to CCSPs by representing them as factor graphs and combining the energies of diffusion models trained to sample for individual constraint types. Diffusion-CCSP exhibits strong generalization to novel combinations of known constraints, and it can be integrated into a task and motion planner to devise long-horizon plans that include actions with both discrete and continuous parameters.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yang23d.html
  PDF: https://proceedings.mlr.press/v229/yang23d/yang23d.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yang23d.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zhutian
    family: Yang
  - given: Jiayuan
    family: Mao
  - given: Yilun
    family: Du
  - given: Jiajun
    family: Wu
  - given: Joshua B.
    family: Tenenbaum
  - given: Tomás
    family: Lozano-Pérez
  - given: Leslie Pack
    family: Kaelbling
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3242-3265
  id: yang23d
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3242
  lastpage: 3265
  published: 2023-12-02 00:00:00 +0000
- title: 'Precise Robotic Needle-Threading with Tactile Perception and Reinforcement Learning'
  abstract: 'This work presents a novel tactile perception-based method, named T-NT, for performing the needle-threading task, an application of deformable linear object (DLO) manipulation. This task is divided into two main stages: Tail-end Finding and Tail-end Insertion. In the first stage, the agent traces the contour of the thread twice using vision-based tactile sensors mounted on the gripper fingers. The two-run tracing is to locate the tail-end of the thread. In the second stage, it employs a tactile-guided reinforcement learning (RL) model to drive the robot to insert the thread into the target needle eyelet. The RL model is trained in a Unity-based simulated environment. The simulation environment supports tactile rendering which can produce realistic tactile images and thread modeling. During insertion, the position of the poke point and the center of the eyelet are obtained through a pre-trained segmentation model, Grounded-SAM, which predicts the masks for both the needle eye and thread imprints. These positions are then fed into the reinforcement learning model, aiding in a smoother transition to real-world applications. Extensive experiments on real robots are conducted to demonstrate the efficacy of our method. More experiments and videos can be found in the supplementary materials and on the website: https://sites.google.com/view/tac-needlethreading.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yu23c.html
  PDF: https://proceedings.mlr.press/v229/yu23c/yu23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yu23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zhenjun
    family: Yu
  - given: Wenqiang
    family: Xu
  - given: Siqiong
    family: Yao
  - given: Jieji
    family: Ren
  - given: Tutian
    family: Tang
  - given: Yutong
    family: Li
  - given: Guoying
    family: Gu
  - given: Cewu
    family: Lu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3266-3276
  id: yu23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3266
  lastpage: 3276
  published: 2023-12-02 00:00:00 +0000
- title: 'Cold Diffusion on the Replay Buffer: Learning to Plan from Known Good States'
  abstract: 'Learning from demonstrations (LfD) has successfully trained robots to exhibit remarkable generalization capabilities. However, many powerful imitation techniques do not prioritize the feasibility of the robot behaviors they generate. In this work, we explore the feasibility of plans produced by LfD. As in prior work, we employ a temporal diffusion model with fixed start and goal states to facilitate imitation through in-painting. Unlike previous studies, we apply cold diffusion to ensure the optimization process is directed through the agent’s replay buffer of previously visited states. This routing approach increases the likelihood that the final trajectories will predominantly occupy the feasible region of the robot’s state space. We test this method in simulated robotic environments with obstacles and observe a significant improvement in the agent’s ability to avoid these obstacles during planning.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wang23e.html
  PDF: https://proceedings.mlr.press/v229/wang23e/wang23e.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wang23e.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zidan
    family: Wang
  - given: Takeru
    family: Oba
  - given: Takuma
    family: Yoneda
  - given: Rui
    family: Shen
  - given: Matthew
    family: Walter
  - given: Bradly C.
    family: Stadie
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3277-3291
  id: wang23e
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3277
  lastpage: 3291
  published: 2023-12-02 00:00:00 +0000
- title: 'Self-Improving Robots: End-to-End Autonomous Visuomotor Reinforcement Learning'
  abstract: 'In imitation and reinforcement learning (RL), the cost of human supervision limits the amount of data that the robots can be trained on. While RL offers a framework for building self-improving robots that can learn via trial-and-error autonomously, practical realizations end up requiring extensive human supervision for reward function design and repeated resetting of the environment between episodes of interactions. In this work, we propose MEDAL++, a novel design for self-improving robotic systems: given a small set of expert demonstrations at the start, the robot autonomously practices the task by learning to both do and undo the task, simultaneously inferring the reward function from the demonstrations. The policy and reward function are learned end-to-end from high-dimensional visual inputs, bypassing the need for explicit state estimation or task-specific pre-training for visual encoders used in prior work. We first evaluate our proposed system on a simulated non-episodic benchmark EARL, finding that MEDAL++ is both more data efficient and gets up to $30%$ better final performance compared to state-of-the-art vision-based methods. Our real-robot experiments show that MEDAL++ can be applied to manipulation problems in larger environments than those considered in prior work, and autonomous self-improvement can improve the success rate by $30%$ to $70%$ over behavioral cloning on just the expert data.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/sharma23b.html
  PDF: https://proceedings.mlr.press/v229/sharma23b/sharma23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-sharma23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Archit
    family: Sharma
  - given: Ahmed M.
    family: Ahmed
  - given: Rehaan
    family: Ahmad
  - given: Chelsea
    family: Finn
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3292-3308
  id: sharma23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3292
  lastpage: 3308
  published: 2023-12-02 00:00:00 +0000
- title: 'Equivariant Reinforcement Learning under Partial Observability'
  abstract: 'Incorporating inductive biases is a promising approach for tackling challenging robot learning domains with sample-efficient solutions. This paper identifies partially observable domains where symmetries can be a useful inductive bias for efficient learning. Specifically, by encoding the equivariance regarding specific group symmetries into the neural networks, our actor-critic reinforcement learning agents can reuse solutions in the past for related scenarios. Consequently, our equivariant agents outperform non-equivariant approaches significantly in terms of sample efficiency and final performance, demonstrated through experiments on a range of robotic tasks in simulation and real hardware.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/nguyen23a.html
  PDF: https://proceedings.mlr.press/v229/nguyen23a/nguyen23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-nguyen23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Hai Huu
    family: Nguyen
  - given: Andrea
    family: Baisero
  - given: David
    family: Klee
  - given: Dian
    family: Wang
  - given: Robert
    family: Platt
  - given: Christopher
    family: Amato
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3309-3320
  id: nguyen23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3309
  lastpage: 3320
  published: 2023-12-02 00:00:00 +0000
- title: 'UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding'
  abstract: 'This paper explores the development of UniFolding, a sample-efficient, scalable, and generalizable robotic system for unfolding and folding various garments. UniFolding employs the proposed UFONet neural network to integrate unfolding and folding decisions into a single policy model that is adaptable to different garment types and states. The design of UniFolding is based on a garment’s partial point cloud, which aids in generalization and reduces sensitivity to variations in texture and shape. The training pipeline prioritizes low-cost, sample-efficient data collection. Training data is collected via a human-centric process with offline and online stages. The offline stage involves human unfolding and folding actions via Virtual Reality, while the online stage utilizes human-in-the-loop learning to fine-tune the model in a real-world setting. The system is tested on two garment types: long-sleeve and short-sleeve shirts. Performance is evaluated on 20 shirts with significant variations in textures, shapes, and materials. More experiments and videos can be found in the supplementary materials and on the website: https://unifolding.robotflow.ai.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/xue23b.html
  PDF: https://proceedings.mlr.press/v229/xue23b/xue23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-xue23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Han
    family: Xue
  - given: Yutong
    family: Li
  - given: Wenqiang
    family: Xu
  - given: Huanyu
    family: Li
  - given: Dongzhe
    family: Zheng
  - given: Cewu
    family: Lu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3321-3341
  id: xue23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3321
  lastpage: 3341
  published: 2023-12-02 00:00:00 +0000
- title: 'A Universal Semantic-Geometric Representation for Robotic Manipulation'
  abstract: 'Robots rely heavily on sensors, especially RGB and depth cameras, to perceive and interact with the world. RGB cameras record 2D images with rich semantic information while missing precise spatial information. On the other side, depth cameras offer critical 3D geometry data but capture limited semantics. Therefore, integrating both modalities is crucial for learning representations for robotic perception and control. However, current research predominantly focuses on only one of these modalities, neglecting the benefits of incorporating both. To this end, we present Semantic-Geometric Representation (SGR), a universal perception module for robotics that leverages the rich semantic information of large-scale pre-trained 2D models and inherits the merits of 3D spatial reasoning. Our experiments demonstrate that SGR empowers the agent to successfully complete a diverse range of simulated and real-world robotic manipulation tasks, outperforming state-of-the-art methods significantly in both single-task and multi-task settings. Furthermore, SGR possesses the capability to generalize to novel semantic attributes, setting it apart from the other methods. Project website: https://semantic-geometric-representation.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23j.html
  PDF: https://proceedings.mlr.press/v229/zhang23j/zhang23j.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23j.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tong
    family: Zhang
  - given: Yingdong
    family: Hu
  - given: Hanchen
    family: Cui
  - given: Hang
    family: Zhao
  - given: Yang
    family: Gao
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3342-3363
  id: zhang23j
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3342
  lastpage: 3363
  published: 2023-12-02 00:00:00 +0000
- title: 'LabelFormer: Object Trajectory Refinement for Offboard Perception from LiDAR Point Clouds'
  abstract: 'A major bottleneck to scaling-up training of self-driving perception systems are the human annotations required for supervision. A promising alternative is to leverage “auto-labelling" offboard perception models that are trained to automatically generate annotations from raw LiDAR point clouds at a fraction of the cost. Auto-labels are most commonly generated via a two-stage approach – first objects are detected and tracked over time, and then each object trajectory is passed to a learned refinement model to improve accuracy. Since existing refinement models are overly complex and lack advanced temporal reasoning capabilities, in this work we propose LabelFormer, a simple, efficient, and effective trajectory-level refinement approach. Our approach first encodes each frame’s observations separately, then exploits self-attention to reason about the trajectory with full temporal context, and finally decodes the refined object size and per-frame poses. Evaluation on both urban and highway datasets demonstrates that LabelFormer outperforms existing works by a large margin. Finally, we show that training on a dataset augmented with auto-labels generated by our method leads to improved downstream detection performance compared to existing methods. Please visit the project website for details https://waabi.ai/labelformer/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yang23e.html
  PDF: https://proceedings.mlr.press/v229/yang23e/yang23e.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yang23e.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Anqi Joyce
    family: Yang
  - given: Sergio
    family: Casas
  - given: Nikita
    family: Dvornik
  - given: Sean
    family: Segal
  - given: Yuwen
    family: Xiong
  - given: Jordan Sir Kwang
    family: Hu
  - given: Carter
    family: Fang
  - given: Raquel
    family: Urtasun
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3364-3383
  id: yang23e
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3364
  lastpage: 3383
  published: 2023-12-02 00:00:00 +0000
- title: 'Language-Conditioned Path Planning'
  abstract: 'Contact is at the core of robotic manipulation. At times, it is desired (e.g. manipulation and grasping), and at times, it is harmful (e.g. when avoiding obstacles). However, traditional path planning algorithms focus solely on collision-free paths, limiting their applicability in contact-rich tasks. To address this limitation, we propose the domain of Language-Conditioned Path Planning, where contact-awareness is incorporated into the path planning problem. As a first step in this domain, we propose Language-Conditioned Collision Functions (LACO), a novel approach that learns a collision function using only a single-view image, language prompt, and robot configuration. LACO predicts collisions between the robot and the environment, enabling flexible, conditional path planning without the need for manual object annotations, point cloud data, or ground-truth object meshes. In both simulation and the real world, we demonstrate that LACO can facilitate complex, nuanced path plans that allow for interaction with objects that are safe to collide, rather than prohibiting any collision.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/xie23b.html
  PDF: https://proceedings.mlr.press/v229/xie23b/xie23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-xie23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Amber
    family: Xie
  - given: Youngwoon
    family: Lee
  - given: Pieter
    family: Abbeel
  - given: Stephen
    family: James
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3384-3396
  id: xie23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3384
  lastpage: 3396
  published: 2023-12-02 00:00:00 +0000
- title: 'Open-World Object Manipulation using Pre-Trained Vision-Language Models'
  abstract: 'For robots to follow instructions from people, they must be able to connect the rich semantic information in human vocabulary, e.g. “can you get me the pink stuffed whale?” to their sensory observations and actions. This brings up a notably difficult challenge for robots: while robot learning approaches allow robots to learn many different behaviors from first-hand experience, it is impractical for robots to have first-hand experiences that span all of this semantic information. We would like a robot’s policy to be able to perceive and pick up the pink stuffed whale, even if it has never seen any data interacting with a stuffed whale before. Fortunately, static data on the internet has vast semantic information, and this information is captured in pre-trained vision-language models. In this paper, we study whether we can interface robot policies with these pre-trained models, with the aim of allowing robots to complete instructions involving object categories that the robot has never seen first-hand. We develop a simple approach, which we call Manipulation of Open-World Objects (MOO), which leverages a pre-trained vision-language model to extract object-identifying information from the language command and image, and conditions the robot policy on the current image, the instruction, and the extracted object information. In a variety of experiments on a real mobile manipulator, we find that MOO generalizes zero-shot to a wide range of novel object categories and environments. In addition, we show how MOO generalizes to other, non-language-based input modalities to specify the object of interest such as finger pointing, and how it can be further extended to enable open-world navigation and manipulation. The project’s website and evaluation videos can be found at https://robot-moo.github.io/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/stone23a.html
  PDF: https://proceedings.mlr.press/v229/stone23a/stone23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-stone23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Austin
    family: Stone
  - given: Ted
    family: Xiao
  - given: Yao
    family: Lu
  - given: Keerthana
    family: Gopalakrishnan
  - given: Kuang-Huei
    family: Lee
  - given: Quan
    family: Vuong
  - given: Paul
    family: Wohlhart
  - given: Sean
    family: Kirmani
  - given: Brianna
    family: Zitkovich
  - given: Fei
    family: Xia
  - given: Chelsea
    family: Finn
  - given: Karol
    family: Hausman
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3397-3417
  id: stone23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3397
  lastpage: 3417
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning Generalizable Manipulation Policies with Object-Centric 3D Representations'
  abstract: 'We introduce GROOT, an imitation learning method for learning robust policies with object-centric and 3D priors. GROOT builds policies that generalize beyond their initial training conditions for vision-based manipulation. It constructs object-centric 3D representations that are robust toward background changes and camera views and reason over these representations using a transformer-based policy. Furthermore, we introduce a segmentation correspondence model that allows policies to generalize to new objects at test time. Through comprehensive experiments, we validate the robustness of GROOT policies against perceptual variations in simulated and real-world environments. GROOT’s performance excels in generalization over background changes, camera viewpoint shifts, and the presence of new object instances, whereas both state-of-the-art end-to-end learning methods and object proposal-based approaches fall short. We also extensively evaluate GROOT policies on real robots, where we demonstrate the efficacy under very wild changes in setup. More videos and model details can be found in the appendix and the project website https://ut-austin-rpl.github.io/GROOT.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhu23b.html
  PDF: https://proceedings.mlr.press/v229/zhu23b/zhu23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhu23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yifeng
    family: Zhu
  - given: Zhenyu
    family: Jiang
  - given: Peter
    family: Stone
  - given: Yuke
    family: Zhu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3418-3433
  id: zhu23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3418
  lastpage: 3433
  published: 2023-12-02 00:00:00 +0000
- title: 'AdaptSim: Task-Driven Simulation Adaptation for Sim-to-Real Transfer'
  abstract: 'Simulation parameter settings such as contact models and object geometry approximations are critical to training robust manipulation policies capable of transferring from simulation to real-world deployment. There is often an irreducible gap between simulation and reality: attempting to match the dynamics between simulation and reality may be infeasible and may not lead to policies that perform well in reality for a specific task. We propose AdaptSim, a new task-driven adaptation framework for sim-to-real transfer that aims to optimize task performance in target (real) environments. First, we meta-learn an adaptation policy in simulation using reinforcement learning for adjusting the simulation parameter distribution based on the current policy’s performance in a target environment. We then perform iterative real-world adaptation by inferring new simulation parameter distributions for policy training. Our extensive simulation and hardware experiments demonstrate AdaptSim achieving 1-3x asymptotic performance and 2x real data efficiency when adapting to different environments, compared to methods based on Sys-ID and directly training the task policy in target environments.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/ren23b.html
  PDF: https://proceedings.mlr.press/v229/ren23b/ren23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-ren23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Allen Z.
    family: Ren
  - given: Hongkai
    family: Dai
  - given: Benjamin
    family: Burchfiel
  - given: Anirudha
    family: Majumdar
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3434-3452
  id: ren23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3434
  lastpage: 3452
  published: 2023-12-02 00:00:00 +0000
- title: 'Dexterous Functional Grasping'
  abstract: 'While there have been significant strides in dexterous manipulation, most of it is limited to benchmark tasks like in-hand reorientation which are of limited utility in the real world. The main benefit of dexterous hands over two-fingered ones is their ability to pickup tools and other objects (including thin ones) and grasp them firmly in order to apply force. However, this task requires both a complex understanding of functional affordances as well as precise low-level control. While prior work obtains affordances from human data this approach doesn’t scale to low-level control. Similarly, simulation training cannot give the robot an understanding of real-world semantics. In this paper, we aim to combine the best of both worlds to accomplish functional grasping for in-the-wild objects. We use a modular approach. First, affordances are obtained by matching corresponding regions of different objects and then a low-level policy trained in sim is run to grasp it. We propose a novel application of eigengrasps to reduce the search space of RL using a small amount of human data and find that it leads to more stable and physically realistic motion. We find that eigengrasp action space beats baselines in simulation and outperforms hardcoded grasping in real and matches or outperforms a trained human teleoperator. Videos at https://dexfunc.github.io/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/agarwal23a.html
  PDF: https://proceedings.mlr.press/v229/agarwal23a/agarwal23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-agarwal23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ananye
    family: Agarwal
  - given: Shagun
    family: Uppal
  - given: Kenneth
    family: Shaw
  - given: Deepak
    family: Pathak
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3453-3467
  id: agarwal23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3453
  lastpage: 3467
  published: 2023-12-02 00:00:00 +0000
- title: 'REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction'
  abstract: 'The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs. To leverage the power of LLMs for robot failure explanation, we introduce REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations. The failure explanation can further guide a language-based planner to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset with a variety of tasks and failure scenarios. We demonstrate that the LLM-based framework is able to generate informative failure explanations that assist successful correction planning.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/liu23g.html
  PDF: https://proceedings.mlr.press/v229/liu23g/liu23g.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-liu23g.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Zeyi
    family: Liu
  - given: Arpit
    family: Bahety
  - given: Shuran
    family: Song
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3468-3484
  id: liu23g
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3468
  lastpage: 3484
  published: 2023-12-02 00:00:00 +0000
- title: 'Task Generalization with Stability Guarantees via Elastic Dynamical System Motion Policies'
  abstract: 'Dynamical System (DS) based Learning from Demonstration (LfD) allows learning of reactive motion policies with stability and convergence guarantees from a few trajectories. Yet, current DS learning techniques lack the flexibility to generalize to new task instances as they overlook explicit task parameters that inherently change the underlying demonstrated trajectories. In this work, we propose Elastic-DS, a novel DS learning and generalization approach that embeds task parameters into the Gaussian Mixture Model (GMM) based Linear Parameter Varying (LPV) DS formulation. Central to our approach is the Elastic-GMM, a GMM constrained to SE(3) task-relevant frames. Given a new task instance/context, the Elastic-GMM is transformed with Laplacian Editing and used to re-estimate the LPV-DS policy. Elastic-DS is compositional in nature and can be used to construct flexible multi-step tasks. We showcase its strength on a myriad of simulated and real-robot experiments while preserving desirable control-theoretic guarantees.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/li23b.html
  PDF: https://proceedings.mlr.press/v229/li23b/li23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-li23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Tianyu
    family: Li
  - given: Nadia
    family: Figueroa
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3485-3517
  id: li23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3485
  lastpage: 3517
  published: 2023-12-02 00:00:00 +0000
- title: 'Push Past Green: Learning to Look Behind Plant Foliage by Moving It'
  abstract: 'Autonomous agriculture applications (e.g., inspection, phenotyping, plucking fruits) require manipulating the plant foliage to look behind the leaves and the branches. Partial visibility, extreme clutter, thin structures, and unknown geometry and dynamics for plants make such manipulation challenging. We tackle these challenges through data-driven methods. We use self-supervision to train SRPNet, a neural network that predicts what space is revealed on execution of a candidate action on a given plant. We use SRPNet with the cross-entropy method to predict actions that are effective at revealing space beneath plant foliage. Furthermore, as SRPNet does not just predict how much space is revealed but also where it is revealed, we can execute a sequence of actions that incrementally reveal more and more space beneath the plant foliage. We experiment with a synthetic (vines) and a real plant (Dracaena) on a physical test-bed across 5 settings including 2 settings that test generalization to novel plant configurations. Our experiments reveal the effectiveness of our overall method, PPG, over a competitive hand-crafted exploration method, and the effectiveness of SRPNet over a hand-crafted dynamics model and relevant ablations. Project website with execution videos, code, data, and models: https://sites.google.com/view/pushingfoliage/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhang23k.html
  PDF: https://proceedings.mlr.press/v229/zhang23k/zhang23k.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhang23k.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xiaoyu
    family: Zhang
  - given: Saurabh
    family: Gupta
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3518-3535
  id: zhang23k
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3518
  lastpage: 3535
  published: 2023-12-02 00:00:00 +0000
- title: 'XSkill: Cross Embodiment Skill Discovery'
  abstract: 'Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/xu23a.html
  PDF: https://proceedings.mlr.press/v229/xu23a/xu23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-xu23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Mengda
    family: Xu
  - given: Zhenjia
    family: Xu
  - given: Cheng
    family: Chi
  - given: Manuela
    family: Veloso
  - given: Shuran
    family: Song
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3536-3555
  id: xu23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3536
  lastpage: 3555
  published: 2023-12-02 00:00:00 +0000
- title: 'SayTap: Language to Quadrupedal Locomotion'
  abstract: 'Large language models (LLMs) have demonstrated the potential to perform high-level planning. Yet, it remains a challenge for LLMs to comprehend low-level commands, such as joint angle targets or motor torques. This paper proposes an approach to use foot contact patterns as an interface that bridges human commands in natural language and a locomotion controller that outputs these low-level commands. This results in an interactive system for quadrupedal robots that allows the users to craft diverse locomotion behaviors flexibly. We contribute an LLM prompt design, a reward function, and a method to expose the controller to the feasible distribution of contact patterns. The results are a controller capable of achieving diverse locomotion patterns that can be transferred to real robot hardware. Compared with other design choices, the proposed approach enjoys more than $50%$ success rate in predicting the correct contact patterns and can solve 10 more tasks out of a total of 30 tasks. (https://saytap.github.io)'
  volume: 229
  URL: https://proceedings.mlr.press/v229/tang23a.html
  PDF: https://proceedings.mlr.press/v229/tang23a/tang23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-tang23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yujin
    family: Tang
  - given: Wenhao
    family: Yu
  - given: Jie
    family: Tan
  - given: Heiga
    family: Zen
  - given: Aleksandra
    family: Faust
  - given: Tatsuya
    family: Harada
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3556-3570
  id: tang23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3556
  lastpage: 3570
  published: 2023-12-02 00:00:00 +0000
- title: 'SLAP: Spatial-Language Attention Policies'
  abstract: 'Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an $80%$ success rate in the real world across eight tasks with a single model, and a $47.5%$ success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of $30%$ over prior work ($20%$ given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io'
  volume: 229
  URL: https://proceedings.mlr.press/v229/parashar23a.html
  PDF: https://proceedings.mlr.press/v229/parashar23a/parashar23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-parashar23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Priyam
    family: Parashar
  - given: Vidhi
    family: Jain
  - given: Xiaohan
    family: Zhang
  - given: Jay
    family: Vakil
  - given: Sam
    family: Powers
  - given: Yonatan
    family: Bisk
  - given: Chris
    family: Paxton
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3571-3596
  id: parashar23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3571
  lastpage: 3596
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning Human Contribution Preferences in Collaborative Human-Robot Tasks'
  abstract: 'In human-robot collaboration, both human and robotic agents must work together to achieve a set of shared objectives. However, each team member may have individual preferences, or constraints, for how they would like to contribute to the task. Effective teams align their actions to optimize task performance while satisfying each team member’s constraints to the greatest extent possible. We propose a framework for representing human and robot contribution constraints in collaborative human-robot tasks. Additionally, we present an approach for learning a human partner’s contribution constraint online during a collaborative interaction. We evaluate our approach using a variety of simulated human partners in a collaborative decluttering task. Our results demonstrate that our method improves team performance over baselines with some, but not all, simulated human partners. Furthermore, we conducted a pilot user study to gather preliminary insights into the effectiveness of our approach on task performance and collaborative fluency. Preliminary results suggest that pilot users performed fluently with our method, motivating further investigation into considering preferences that emerge from collaborative interactions.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhao23b.html
  PDF: https://proceedings.mlr.press/v229/zhao23b/zhao23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhao23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Michelle D
    family: Zhao
  - given: Reid
    family: Simmons
  - given: Henny
    family: Admoni
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3597-3618
  id: zhao23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3597
  lastpage: 3618
  published: 2023-12-02 00:00:00 +0000
- title: 'M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place'
  abstract: 'With the advent of large language models and large-scale robotic datasets, there has been tremendous progress in high-level decision-making for object manipulation. These generic models are able to interpret complex tasks using language commands, but they often have difficulties generalizing to out-of-distribution objects due to the inability of low-level action primitives. In contrast, existing task-specific models excel in low-level manipulation of unknown objects, but only work for a single type of action. To bridge this gap, we present M2T2, a single model that supplies different types of low-level actions that work robustly on arbitrary objects in cluttered scenes. M2T2 is a transformer model which reasons about contact points and predicts valid gripper poses for different action modes given a raw point cloud of the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2 achieves zero-shot sim2real transfer on the real robot, outperforming the baseline system with state-of-the-art task-specific models by about $19%$ in overall performance and $37.5%$ in challenging scenes were the object needs to be re-oriented for collision-free placement. M2T2 also achieves state-of-the-art results on a subset of language conditioned tasks in RLBench. Videos of robot experiments on unseen objects in both real world and simulation are available at m2-t2.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/yuan23a.html
  PDF: https://proceedings.mlr.press/v229/yuan23a/yuan23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-yuan23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Wentao
    family: Yuan
  - given: Adithyavairavan
    family: Murali
  - given: Arsalan
    family: Mousavian
  - given: Dieter
    family: Fox
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3619-3630
  id: yuan23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3619
  lastpage: 3630
  published: 2023-12-02 00:00:00 +0000
- title: 'Learning to Drive Anywhere'
  abstract: 'Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose GeCo, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity, geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across the inherently imbalanced data distributions and location-dependent events. We demonstrate the benefits of our GeCo agent across multiple datasets, cities, and scalable deployment paradigms, i.e., centralized, semi-supervised, and distributed agent training. Specifically, GeCo outperforms CIL baselines by over $14%$ in open-loop evaluation and $30%$ in closed-loop testing on CARLA.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/zhu23c.html
  PDF: https://proceedings.mlr.press/v229/zhu23c/zhu23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-zhu23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Ruizhao
    family: Zhu
  - given: Peng
    family: Huang
  - given: Eshed
    family: Ohn-Bar
  - given: Venkatesh
    family: Saligrama
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3631-3653
  id: zhu23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3631
  lastpage: 3653
  published: 2023-12-02 00:00:00 +0000
- title: 'MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning'
  abstract: 'We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing methods for high-dimensional model-based offline RL are not suitable for offline-to-online fine-tuning due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty. We find that our approach successfully solves tasks from the MetaWorld benchmark, as well as the Franka Kitchen robot manipulation environment completely from images. To our knowledge, MOTO is the first and only method to solve this environment from pixels.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/rafailov23a.html
  PDF: https://proceedings.mlr.press/v229/rafailov23a/rafailov23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-rafailov23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Rafael
    family: Rafailov
  - given: Kyle Beltran
    family: Hatch
  - given: Victor
    family: Kolev
  - given: John D.
    family: Martin
  - given: Mariano
    family: Phielipp
  - given: Chelsea
    family: Finn
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3654-3671
  id: rafailov23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3654
  lastpage: 3671
  published: 2023-12-02 00:00:00 +0000
- title: 'Ready, Set, Plan! Planning to Goal Sets Using Generalized Bayesian Inference'
  abstract: 'Many robotic tasks can have multiple and diverse solutions and, as such, are naturally expressed as goal sets. Examples include navigating to a room, finding a feasible placement location for an object, or opening a drawer enough to reach inside. Using a goal set as a planning objective requires that a model for the objective be explicitly given by the user. However, some goals are intractable to model, leading to uncertainty over the goal (e.g. stable grasping of an object). In this work, we propose a technique for planning directly to a set of sampled goal configurations. We formulate a planning as inference problem with a novel goal likelihood evaluated against the goal samples. To handle the intractable goal likelihood, we employ Generalized Bayesian Inference to approximate the trajectory distribution. The result is a fully differentiable cost which generalizes across a diverse range of goal set objectives for which samples can be obtained. We show that by considering all goal samples throughout the planning process, our method reliably finds plans on manipulation and navigation problems where heuristic approaches fail.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/pavlasek23a.html
  PDF: https://proceedings.mlr.press/v229/pavlasek23a/pavlasek23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-pavlasek23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jana
    family: Pavlasek
  - given: Stanley Robert
    family: Lewis
  - given: Balakumar
    family: Sundaralingam
  - given: Fabio
    family: Ramos
  - given: Tucker
    family: Hermans
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3672-3686
  id: pavlasek23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3672
  lastpage: 3686
  published: 2023-12-02 00:00:00 +0000
- title: 'Online Model Adaptation with Feedforward Compensation'
  abstract: 'To cope with distribution shifts or non-stationarity in system dynamics, online adaptation algorithms have been introduced to update offline-learned prediction models in real-time. Existing online adaptation methods focus on optimizing the prediction model by utilizing feedback from the latest prediction error. Unfortunately, this feedback-based approach is susceptible to forgetting past information. This work proposes an online adaptation method with feedforward compensation, which uses critical data samples from a memory buffer, instead of the latest samples, to optimize the prediction model. We prove that the proposed approach achieves a smaller error bound compared to previously utilized methods in slow time-varying systems. We conducted experiments on several prediction tasks, which clearly illustrate the superiority of the proposed feedforward adaptation method. Furthermore, our feedforward adaptation technique is capable of estimating an uncertainty bound for predictions.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/abuduweili23a.html
  PDF: https://proceedings.mlr.press/v229/abuduweili23a/abuduweili23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-abuduweili23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Abulikemu
    family: Abuduweili
  - given: Changliu
    family: Liu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3687-3709
  id: abuduweili23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3687
  lastpage: 3709
  published: 2023-12-02 00:00:00 +0000
- title: 'Generating Transferable Adversarial Simulation Scenarios for Self-Driving via Neural Rendering'
  abstract: 'Self-driving software pipelines include components that are learned from a significant number of training examples, yet it remains challenging to evaluate the overall system’s safety and generalization performance. Together with scaling up the real-world deployment of autonomous vehicles, it is of critical importance to automatically find simulation scenarios where the driving policies will fail. We propose a method that efficiently generates adversarial simulation scenarios for autonomous driving by solving an optimal control problem that aims to maximally perturb the policy from its nominal trajectory. Given an image-based driving policy, we show that we can inject new objects in a neural rendering representation of the deployment scene, and optimize their texture in order to generate adversarial sensor inputs to the policy. We demonstrate that adversarial scenarios discovered purely in the neural renderer (surrogate scene) can often be successfully transferred to the deployment scene, without further optimization. We demonstrate this transfer occurs both in simulated and real environments, provided the learned surrogate scene is sufficiently close to the deployment scene.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/abeysirigoonawardena23a.html
  PDF: https://proceedings.mlr.press/v229/abeysirigoonawardena23a/abeysirigoonawardena23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-abeysirigoonawardena23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yasasa
    family: Abeysirigoonawardena
  - given: Kevin
    family: Xie
  - given: Chuhan
    family: Chen
  - given: Salar Hosseini
    family: Khorasgani
  - given: Ruiting
    family: Chen
  - given: Ruiqi
    family: Wang
  - given: Florian
    family: Shkurti
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3710-3731
  id: abeysirigoonawardena23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3710
  lastpage: 3731
  published: 2023-12-02 00:00:00 +0000
- title: 'STOW: Discrete-Frame Segmentation and Tracking of Unseen Objects for Warehouse Picking Robots'
  abstract: 'Segmentation and tracking of unseen object instances in discrete frames pose a significant challenge in dynamic industrial robotic contexts, such as distribution warehouses. Here, robots must handle object rearrangements, including shifting, removal, and partial occlusion by new items, and track these items after substantial temporal gaps. The task is further complicated when robots encounter objects beyond their training sets, thereby requiring the ability to segment and track previously unseen items. Considering that continuous observation is often inaccessible in such settings, our task involves working with a discrete set of frames separated by indefinite periods, during which substantial changes to the scene may occur. This task also translates to domestic robotic applications, such as table rearrangement. To address these demanding challenges, we introduce new synthetic and real-world datasets that replicate these industrial and household scenarios. Furthermore, we propose a novel paradigm for joint segmentation and tracking in discrete frames, alongside a transformer module that facilitates efficient inter-frame communication. Our approach significantly outperforms recent methods in our experiments. For additional results and videos, please visit https://sites.google.com/view/stow-corl23. Code and dataset will be released.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/li23c.html
  PDF: https://proceedings.mlr.press/v229/li23c/li23c.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-li23c.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yi
    family: Li
  - given: Muru
    family: Zhang
  - given: Markus
    family: Grotz
  - given: Kaichun
    family: Mo
  - given: Dieter
    family: Fox
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3732-3748
  id: li23c
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3732
  lastpage: 3748
  published: 2023-12-02 00:00:00 +0000
- title: 'DORT: Modeling Dynamic Objects in Recurrent for Multi-Camera 3D Object Detection and Tracking'
  abstract: 'Recent multi-camera 3D object detectors usually leverage temporal information to construct multi-view stereo that alleviates the ill-posed depth estimation. However, they typically assume all the objects are static and directly aggregate features across frames. This work begins with a theoretical and empirical analysis to reveal that ignoring the motion of moving objects can result in serious localization bias. Therefore, we propose to model Dynamic Objects in RecurrenT (DORT) to tackle this problem. In contrast to previous global BirdEye-View (BEV) methods, DORT extracts object-wise local volumes for motion estimation that also alleviates the heavy computational burden. By iteratively refining the estimated object motion and location, the preceding features can be precisely aggregated to the current frame to mitigate the aforementioned adverse effects. The simple framework has two significant appealing properties. It is flexible and practical that can be plugged into most camera-based 3D object detectors. As there are predictions of object motion in the loop, it can easily track objects across frames according to their nearest center distances. Without bells and whistles, DORT outperforms all the previous methods on the nuScenes detection and tracking benchmarks with $62.8%$ NDS and $57.6%$ AMOTA, respectively. The source code will be available at https://github.com/OpenRobotLab/DORT.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/lian23a.html
  PDF: https://proceedings.mlr.press/v229/lian23a/lian23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-lian23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Qing
    family: LIAN
  - given: Tai
    family: Wang
  - given: Dahua
    family: Lin
  - given: Jiangmiao
    family: Pang
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3749-3765
  id: lian23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3749
  lastpage: 3765
  published: 2023-12-02 00:00:00 +0000
- title: 'Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition'
  abstract: 'We present a framework for robot skill acquisition, which 1) efficiently scale up data generation of language-labelled robot data and 2) effectively distills this data down into a robust multi-task language-conditioned visuo-motor policy. For (1), we use a large language model (LLM) to guide high-level planning, and sampling-based robot planners (e.g. motion or grasp samplers) for generating diverse and rich manipulation trajectories. To robustify this data-collection process, the LLM also infers a code-snippet for the success condition of each task, simultaneously enabling the data-collection process to detect failure and retry as well as the automatic labeling of trajectories with success/failure. For (2), we extend the diffusion policy single-task behavior-cloning approach to multi-task settings with language conditioning. Finally, we propose a new multi-task benchmark with 18 tasks across five domains to test long-horizon behavior, common-sense reasoning, tool-use, and intuitive physics. We find that our distilled policy successfully learned the robust retrying behavior in its data collection procedure, while improving absolute success rates by $33.2%$ on average across five domains. Code, data, and additional qualitative results are available on https://www.cs.columbia.edu/ huy/scalingup/.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/ha23a.html
  PDF: https://proceedings.mlr.press/v229/ha23a/ha23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-ha23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Huy
    family: Ha
  - given: Pete
    family: Florence
  - given: Shuran
    family: Song
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3766-3777
  id: ha23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3766
  lastpage: 3777
  published: 2023-12-02 00:00:00 +0000
- title: 'Marginalized Importance Sampling for Off-Environment Policy Evaluation'
  abstract: 'Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies prior to deploying them in the real world. Our approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy’s occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separately. The first term is learned with direct supervision and the second term has a small magnitude, thus making it computationally efficient. We analyze the sample complexity as well as error propagation of our two step-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim environments such as Cartpole, Reacher, and Half-Cheetah. Our results show that our method generalizes well across a variety of Sim2Sim gap, target policies and offline data collection policies. We also demonstrate the performance of our algorithm on a Sim2Real task of validating the performance of a 7 DoF robotic arm using offline data along with the Gazebo simulator.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/katdare23a.html
  PDF: https://proceedings.mlr.press/v229/katdare23a/katdare23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-katdare23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Pulkit
    family: Katdare
  - given: Nan
    family: Jiang
  - given: Katherine Rose
    family: Driggs-Campbell
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3778-3788
  id: katdare23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3778
  lastpage: 3788
  published: 2023-12-02 00:00:00 +0000
- title: 'Policy Stitching: Learning Transferable Robot Policies'
  abstract: 'Training robots with reinforcement learning (RL) typically involves heavy interactions with the environment, and the acquired skills are often sensitive to changes in task environments and robot kinematics. Transfer RL aims to leverage previous knowledge to accelerate learning of new tasks or new body configurations. However, existing methods struggle to generalize to novel robot-task combinations and scale to realistic tasks due to complex architecture design or strong regularization that limits the capacity of the learned policy. We propose Policy Stitching, a novel framework that facilitates robot transfer learning for novel combinations of robots and tasks. Our key idea is to apply modular policy design and align the latent representations between the modular interfaces. Our method allows direct stitching of the robot and task modules trained separately to form a new policy for fast adaptation. Our simulated and real-world experiments on various 3D manipulation tasks demonstrate the superior zero-shot and few-shot transfer learning performances of our method.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/jian23a.html
  PDF: https://proceedings.mlr.press/v229/jian23a/jian23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-jian23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Pingcheng
    family: Jian
  - given: Easop
    family: Lee
  - given: Zachary
    family: Bell
  - given: Michael M.
    family: Zavlanos
  - given: Boyuan
    family: Chen
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3789-3808
  id: jian23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3789
  lastpage: 3808
  published: 2023-12-02 00:00:00 +0000
- title: 'Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation'
  abstract: 'Many real-world manipulation tasks consist of a series of subtasks that are significantly different from one another. Such long-horizon, complex tasks highlight the potential of dexterous hands, which possess adaptability and versatility, capable of seamlessly transitioning between different modes of functionality without the need for re-grasping or external tools. However, the challenges arise due to the high-dimensional action space of dexterous hand and complex compositional dynamics of the long-horizon tasks. We present Sequential Dexterity, a general system based on reinforcement learning (RL) that chains multiple dexterous policies for achieving long-horizon task goals. The core of the system is a transition feasibility function that progressively finetunes the sub-policies for enhancing chaining success rate, while also enables autonomous policy-switching for recovery from failures and bypassing redundant stages. Despite being trained only in simulation with a few task objects, our system demonstrates generalization capability to novel object shapes and is able to zero-shot transfer to a real-world robot equipped with a dexterous hand. Code and videos are available at https://sequential-dexterity.github.io.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/chen23e.html
  PDF: https://proceedings.mlr.press/v229/chen23e/chen23e.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-chen23e.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yuanpei
    family: Chen
  - given: Chen
    family: Wang
  - given: Li
    family: Fei-Fei
  - given: Karen
    family: Liu
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3809-3829
  id: chen23e
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3809
  lastpage: 3829
  published: 2023-12-02 00:00:00 +0000
- title: 'Deception Game: Closing the Safety-Learning Loop in Interactive Robot Autonomy'
  abstract: 'An outstanding challenge for the widespread deployment of robotic systems like autonomous vehicles is ensuring safe interaction with humans without sacrificing performance. Existing safety methods often neglect the robot’s ability to learn and adapt at runtime, leading to overly conservative behavior. This paper proposes a new closed-loop paradigm for synthesizing safe control policies that explicitly account for the robot’s evolving uncertainty and its ability to quickly respond to future scenarios as they arise, by jointly considering the physical dynamics and the robot’s learning algorithm. We leverage adversarial reinforcement learning for tractable safety analysis under high-dimensional learning dynamics and demonstrate our framework’s ability to work with both Bayesian belief propagation and implicit learning through large pre-trained neural trajectory predictors.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/hu23b.html
  PDF: https://proceedings.mlr.press/v229/hu23b/hu23b.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-hu23b.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Haimin
    family: Hu
  - given: Zixu
    family: Zhang
  - given: Kensuke
    family: Nakamura
  - given: Andrea
    family: Bajcsy
  - given: Jaime Fernández
    family: Fisac
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3830-3850
  id: hu23b
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3830
  lastpage: 3850
  published: 2023-12-02 00:00:00 +0000
- title: 'Improving Behavioural Cloning with Positive Unlabeled Learning'
  abstract: 'Learning control policies offline from pre-recorded datasets is a promising avenue for solving challenging real-world problems. However, available datasets are typically of mixed quality, with a limited number of the trajectories that we would consider as positive examples; i.e., high-quality demonstrations. Therefore, we propose a novel iterative learning algorithm for identifying expert trajectories in unlabeled mixed-quality robotics datasets given a minimal set of positive examples, surpassing existing algorithms in terms of accuracy. We show that applying behavioral cloning to the resulting filtered dataset outperforms several competitive offline reinforcement learning and imitation learning baselines. We perform experiments on a range of simulated locomotion tasks and on two challenging manipulation tasks on a real robotic system; in these experiments, our method showcases state-of-the-art performance. Our website: https://sites.google.com/view/offline-policy-learning-pubc.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/wang23f.html
  PDF: https://proceedings.mlr.press/v229/wang23f/wang23f.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-wang23f.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Qiang
    family: Wang
  - given: Robert
    family: McCarthy
  - given: David Cordova
    family: Bulens
  - given: Kevin
    family: McGuinness
  - given: Noel E.
    family: O’Connor
  - given: Francisco Roldan
    family: Sanchez
  - given: Nico
    family: Gürtler
  - given: Felix
    family: Widmaier
  - given: Stephen J.
    family: Redmond
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3851-3869
  id: wang23f
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3851
  lastpage: 3869
  published: 2023-12-02 00:00:00 +0000
- title: '$$-MDF: An Attention-based Multimodal Differentiable Filter for Robot State Estimation'
  abstract: 'Differentiable Filters are recursive Bayesian estimators that derive the state transition and measurement models from data alone. Their data-driven nature eschews the need for explicit analytical models, while remaining algorithmic components of the filtering process intact. As a result, the gain mechanism – a critical component of the filtering process – remains non-differentiable and cannot be adjusted to the specific nature of the task or context. In this paper, we propose an attention-based Multimodal Differentiable Filter ($\alpha$-MDF) which utilizes modern attention mechanisms to learn multimodal latent representations. Unlike previous differentiable filter frameworks, $\alpha$-MDF substitutes the traditional gain, e.g., the Kalman gain, with a neural attention mechanism. The approach generates specialized, context-dependent gains that can effectively combine multiple input modalities and observed variables. We validate $\alpha$-MDF on a diverse set of robot state estimation tasks in real world and simulation. Our results show $\alpha$-MDF achieves significant reductions in state estimation errors, demonstrating nearly 4-fold improvements compared to state-of-the-art sensor fusion strategies for rigid body robots. Additionally, the $\alpha$-MDF consistently outperforms differentiable filter baselines by up to $45%$ in soft robotics tasks. The project is available at alpha-mdf.github.io and the codebase is at github.com/ir-lab/alpha-MDF'
  volume: 229
  URL: https://proceedings.mlr.press/v229/liu23h.html
  PDF: https://proceedings.mlr.press/v229/liu23h/liu23h.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-liu23h.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Xiao
    family: Liu
  - given: Yifan
    family: Zhou
  - given: Shuhei
    family: Ikemoto
  - given: Heni Ben
    family: Amor
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3870-3893
  id: liu23h
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3870
  lastpage: 3893
  published: 2023-12-02 00:00:00 +0000
- title: 'Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control'
  abstract: 'Our goal is for robots to follow natural language instructions like “put the towel next to the microwave.” But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desired change between the start and goal images that the instruction corresponds to. We then train a policy on this embedding: the policy benefits from all the unlabeled data, but the aligned embedding provides an *interface* for language to steer the policy. We show instruction following across a variety of manipulation tasks in different scenes, with generalization to language instructions outside of the labeled data.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/myers23a.html
  PDF: https://proceedings.mlr.press/v229/myers23a/myers23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-myers23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Vivek
    family: Myers
  - given: Andre Wang
    family: He
  - given: Kuan
    family: Fang
  - given: Homer Rich
    family: Walke
  - given: Philippe
    family: Hansen-Estruch
  - given: Ching-An
    family: Cheng
  - given: Mihai
    family: Jalobeanu
  - given: Andrey
    family: Kolobov
  - given: Anca
    family: Dragan
  - given: Sergey
    family: Levine
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3894-3908
  id: myers23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3894
  lastpage: 3908
  published: 2023-12-02 00:00:00 +0000
- title: 'Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions'
  abstract: 'In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/chebotar23a.html
  PDF: https://proceedings.mlr.press/v229/chebotar23a/chebotar23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-chebotar23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Yevgen
    family: Chebotar
  - given: Quan
    family: Vuong
  - given: Karol
    family: Hausman
  - given: Fei
    family: Xia
  - given: Yao
    family: Lu
  - given: Alex
    family: Irpan
  - given: Aviral
    family: Kumar
  - given: Tianhe
    family: Yu
  - given: Alexander
    family: Herzog
  - given: Karl
    family: Pertsch
  - given: Keerthana
    family: Gopalakrishnan
  - given: Julian
    family: Ibarz
  - given: Ofir
    family: Nachum
  - given: Sumedh Anand
    family: Sontakke
  - given: Grecia
    family: Salazar
  - given: Huong T.
    family: Tran
  - given: Jodilyn
    family: Peralta
  - given: Clayton
    family: Tan
  - given: Deeksha
    family: Manjunath
  - given: Jaspiar
    family: Singh
  - given: Brianna
    family: Zitkovich
  - given: Tomas
    family: Jackson
  - given: Kanishka
    family: Rao
  - given: Chelsea
    family: Finn
  - given: Sergey
    family: Levine
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3909-3928
  id: chebotar23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3909
  lastpage: 3928
  published: 2023-12-02 00:00:00 +0000
- title: 'Preference learning for guiding the tree search in continuous POMDPs'
  abstract: 'A robot operating in a partially observable environment must perform sensing actions to achieve a goal, such as clearing the objects in front of a shelf to better localize a target object at the back, and estimate its shape for grasping. A POMDP is a principled framework for enabling robots to perform such information-gathering actions. Unfortunately, while robot manipulation domains involve high-dimensional and continuous observation and action spaces, most POMDP solvers are limited to discrete spaces. Recently, POMCPOW has been proposed for continuous POMDPs, which handles continuity using sampling and progressive widening. However, for robot manipulation problems involving camera observations and multiple objects, POMCPOW is too slow to be practical. We take inspiration from the recent work in learning to guide task and motion planning to propose a framework that learns to guide POMCPOW from past planning experience. Our method uses preference learning that utilizes both success and failure trajectories, where the preference label is given by the results of the tree search. We demonstrate the efficacy of our framework in several continuous partially observable robotics domains, including real-world manipulation, where our framework explicitly reasons about the uncertainty in off-the-shelf segmentation and pose estimation algorithms.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/ahn23a.html
  PDF: https://proceedings.mlr.press/v229/ahn23a/ahn23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-ahn23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Jiyong
    family: Ahn
  - given: Sanghyeon
    family: Son
  - given: Dongryung
    family: Lee
  - given: Jisu
    family: Han
  - given: Dongwon
    family: Son
  - given: Beomjoon
    family: Kim
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3929-3948
  id: ahn23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3929
  lastpage: 3948
  published: 2023-12-02 00:00:00 +0000
- title: 'Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation'
  abstract: '3D perceptual representations are well suited for robot manipulation as they easily encode occlusions and simplify spatial reasoning. Many manipulation tasks require high spatial precision in end-effector pose prediction, which typically demands high-resolution 3D feature grids that are computationally expensive to process. As a result, most manipulation policies operate directly in 2D, foregoing 3D inductive biases. In this paper, we introduce Act3D, a manipulation policy transformer that represents the robot’s workspace using a 3D feature field with adaptive resolutions dependent on the task at hand. The model lifts 2D pre-trained features to 3D using sensed depth, and attends to them to compute features for sampled 3D points. It samples 3D point grids in a coarse to fine manner, featurizes them using relative-position attention, and selects where to focus the next round of point sampling. In this way, it efficiently computes 3D action maps of high spatial resolution. Act3D sets a new state-of-the-art in RLBench, an established manipulation benchmark, where it achieves $10%$ absolute improvement over the previous SOTA 2D multi-view policy on 74 RLBench tasks and $22%$ absolute improvement with 3x less compute over the previous SOTA 3D policy. We quantify the importance of relative spatial attention, large-scale vision-language pre-trained 2D backbones, and weight tying across coarse-to-fine attentions in ablative experiments.'
  volume: 229
  URL: https://proceedings.mlr.press/v229/gervet23a.html
  PDF: https://proceedings.mlr.press/v229/gervet23a/gervet23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-gervet23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Theophile
    family: Gervet
  - given: Zhou
    family: Xian
  - given: Nikolaos
    family: Gkanatsios
  - given: Katerina
    family: Fragkiadaki
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3949-3965
  id: gervet23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3949
  lastpage: 3965
  published: 2023-12-02 00:00:00 +0000
- title: 'Simultaneous Learning of Contact and Continuous Dynamics'
  abstract: 'Robotic manipulation can greatly benefit from the data efficiency, robustness, and predictability of model-based methods if robots can quickly generate models of novel objects they encounter. This is especially difficult when effects like complex joint friction lack clear first-principles models and are usually ignored by physics simulators. Further, numerically-stiff contact dynamics can make common model-building approaches struggle. We propose a method to simultaneously learn contact and continuous dynamics of a novel, possibly multi-link object by observing its motion through contact-rich trajectories. We formulate a system identification process with a loss that infers unmeasured contact forces, penalizing their violation of physical constraints and laws of motion given current model parameters. Our loss is unlike prediction-based losses used in differentiable simulation. Using a new dataset of real articulated object trajectories and an existing cube toss dataset, our method outperforms differentiable simulation and end-to-end alternatives with more data efficiency. See our project page for code, datasets, and media: https://sites.google.com/view/continuous-contact-nets/home'
  volume: 229
  URL: https://proceedings.mlr.press/v229/bianchini23a.html
  PDF: https://proceedings.mlr.press/v229/bianchini23a/bianchini23a.pdf
  edit: https://github.com/mlresearch//v229/edit/gh-pages/_posts/2023-12-02-bianchini23a.md
  series: 'Proceedings of Machine Learning Research'
  container-title: 'Proceedings of The 7th Conference on Robot Learning'
  publisher: 'PMLR'
  author: 
  - given: Bibit
    family: Bianchini
  - given: Mathew
    family: Halm
  - given: Michael
    family: Posa
  editor: 
  - given: Jie
    family: Tan
  - given: Marc
    family: Toussaint
  - given: Kourosh
    family: Darvish
  page: 3966-3978
  id: bianchini23a
  issued:
    date-parts: 
      - 2023
      - 12
      - 2
  firstpage: 3966
  lastpage: 3978
  published: 2023-12-02 00:00:00 +0000
