
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Papers</title>
        <style>
            table {
                width: 100%;
                border-collapse: collapse;
            }
            th, td {
                border: 1px solid #ddd;
                padding: 8px;
                word-wrap: break-word;
                max-width: 200px;
            }
            th {
                background-color: #f2f2f2;
            }
            td:nth-child(4) {
                width: 50%; /* 使abstract列的宽度是其他列的两倍 */
            }
        </style>
    </head>
    <body>
        <h1>Papers</h1>
        <table>
            <tr>
                <th>Title</th>
                <th>Authors</th>
                <th>Year</th>
                <th>Abstract</th>
                <th>URL</th>
            </tr>
    
            <tr>
                <td>RoboTube: Learning Household Manipulation from Human Videos with Simulated Twin Environments</td>
                <td>H, a, o, y, u,  , X, i, o, n, g, ,,  , H, a, o, y, u, a, n,  , F, u, ,,  , J, i, e, y, i,  , Z, h, a, n, g, ,,  , C, h, e, n,  , B, a, o, ,,  , Q, i, a, n, g,  , Z, h, a, n, g, ,,  , Y, o, n, g, x, i,  , H, u, a, n, g, ,,  , W, e, n, q, i, a, n, g,  , X, u, ,,  , A, n, i, m, e, s, h,  , G, a, r, g, ,,  , C, e, w, u,  , L, u</td>
                <td>corl2022</td>
                <td>We aim to build a useful, reproducible, democratized benchmark for learning household robotic manipulation from human videos. To realize this goal, a diverse, high-quality human video dataset curated specifically for robots is desired. To evaluate the learning progress, a simulated twin environment that resembles the appearance and the dynamics of the physical world would help roboticists and AI researchers validate their algorithms convincingly and efficiently before testing on a real robot. Hence, we present RoboTube, a human video dataset, and its digital twins for learning various robotic manipulation tasks. RoboTube video dataset contains 5,000 video demonstrations recorded with multi-view RGB-D cameras of human-performing everyday household tasks including manipulation of rigid objects, articulated objects, deformable objects, and bimanual manipulation. RT-sim, as the simulated twin environments, consists of 3D scanned, photo-realistic objects, minimizing the visual domain gap between the physical world and the simulated environment. After extensively benchmarking existing methods in the field of robot learning from videos, the empirical results suggest that knowledge and models learned from the RoboTube video dataset can be deployed, benchmarked, and reproduced in RT-sim and be transferred to a real robot. We hope RoboTube can lower the barrier to robotics research for beginners while facilitating reproducible research in the community. More experiments and videos can be found in the supplementary materials and on the website: https://sites.google.com/view/robotube</td>
                <td><a href="https://proceedings.mlr.press/v205/xiong23a.html">https://proceedings.mlr.press/v205/xiong23a.html</a></td>
            </tr>
        
            <tr>
                <td>Training Robots to Evaluate Robots: Example-Based Interactive Reward Functions for Policy Learning</td>
                <td>K, u, n,  , H, u, a, n, g, ,,  , E, d, w, a, r, d,  , S, .,  , H, u, ,,  , D, i, n, e, s, h,  , J, a, y, a, r, a, m, a, n</td>
                <td>corl2022</td>
                <td>Physical interactions can often help reveal information that is not readily apparent. For example, we may tug at a table leg to evaluate whether it is built well, or turn a water bottle upside down to check that it is watertight. We propose to train robots to acquire such interactive behaviors automatically, for the purpose of evaluating the result of an attempted robotic skill execution. These evaluations in turn serve as "interactive reward functions" (IRFs) for training reinforcement learning policies to perform the target skill, such as screwing the table leg tightly. In addition, even after task policies are fully trained, IRFs can serve as verification mechanisms that improve online task execution. For any given task, our IRFs can be conveniently trained using only examples of successful outcomes, and no further specification is needed to train the task policy thereafter. In our evaluations on door locking and weighted block stacking in simulation, and screw tightening on a real robot, IRFs enable large performance improvements, even outperforming baselines with access to demonstrations or carefully engineered rewards.</td>
                <td><a href="https://proceedings.mlr.press/v205/huang23a.html">https://proceedings.mlr.press/v205/huang23a.html</a></td>
            </tr>
        
            <tr>
                <td>Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior</td>
                <td>G, a, b, r, i, e, l,  , B, .,  , M, a, r, g, o, l, i, s, ,,  , P, u, l, k, i, t,  , A, g, r, a, w, a, l</td>
                <td>corl2022</td>
                <td>Learned locomotion policies can rapidly adapt to diverse environments similar to those experienced during training but lack a mechanism for fast tuning when they fail in an out-of-distribution test environment. This necessitates a slow and iterative cycle of reward and environment redesign to achieve good performance on a new task. As an alternative, we propose learning a single policy that encodes a structured family of locomotion strategies that solve training tasks in different ways, resulting in Multiplicity of Behavior (MoB). Different strategies generalize differently and can be chosen in real-time for new tasks or environments, bypassing the need for time-consuming retraining. We release a fast, robust open-source MoB locomotion controller, Walk These Ways, that can execute diverse gaits with variable footswing, posture, and speed, unlocking diverse downstream tasks: crouching, hopping, high-speed running, stair traversal, bracing against shoves, rhythmic dance, and more. Video and code release: https://gmargo11.github.io/walk-these-ways</td>
                <td><a href="https://proceedings.mlr.press/v205/margolis23a.html">https://proceedings.mlr.press/v205/margolis23a.html</a></td>
            </tr>
        
            <tr>
                <td>Watch and Match: Supercharging Imitation with Regularized Optimal Transport</td>
                <td>S, i, d, d, h, a, n, t,  , H, a, l, d, a, r, ,,  , V, a, i, b, h, a, v,  , M, a, t, h, u, r, ,,  , D, e, n, i, s,  , Y, a, r, a, t, s, ,,  , L, e, r, r, e, l,  , P, i, n, t, o</td>
                <td>corl2022</td>
                <td>Imitation learning holds tremendous promise in learning policies efficiently for complex decision making problems. Current state-of-the-art algorithms often use inverse reinforcement learning (IRL), where given a set of expert demonstrations, an agent alternatively infers a reward function and the associated optimal policy. However, such IRL approaches often require substantial online interactions for complex control problems. In this work, we present Regularized Optimal Transport (ROT), a new imitation learning algorithm that builds on recent advances in optimal transport based trajectory-matching. Our key technical insight is that adaptively combining trajectory-matching rewards with behavior cloning can significantly accelerate imitation even with only a few demonstrations. Our experiments on 20 visual control tasks across the DeepMind Control Suite, the OpenAI Robotics Suite, and the Meta-World Benchmark demonstrate an average of 7.8x faster imitation to reach 90% of expert performance compared to prior state-of-the-art methods. On real-world robotic manipulation, with just one demonstration and an hour of online training, ROT achieves an average success rate of 90.1% across 14 tasks.</td>
                <td><a href="https://proceedings.mlr.press/v205/haldar23a.html">https://proceedings.mlr.press/v205/haldar23a.html</a></td>
            </tr>
        
            <tr>
                <td>Offline Reinforcement Learning for Visual Navigation</td>
                <td>D, h, r, u, v,  , S, h, a, h, ,,  , A, r, j, u, n,  , B, h, o, r, k, a, r, ,,  , H, r, i, s, h, i, t,  , L, e, e, n, ,,  , I, l, y, a,  , K, o, s, t, r, i, k, o, v, ,,  , N, i, c, h, o, l, a, s,  , R, h, i, n, e, h, a, r, t, ,,  , S, e, r, g, e, y,  , L, e, v, i, n, e</td>
                <td>corl2022</td>
                <td>Reinforcement learning can enable robots to navigate to distant goals while optimizing user-specified reward functions, including preferences for following lanes, staying on paved paths, or avoiding freshly mowed grass. However, online learning from trial-and-error for real-world robots is logistically challenging, and methods that instead can utilize existing datasets of robotic navigation data could be significantly more scalable and enable broader generalization. In this paper, we present ReViND, the first offline RL system for robotic navigation that can leverage previously collected data to optimize user-specified reward functions in the real-world. We evaluate our system for off-road navigation without any additional data collection or fine-tuning, and show that it can navigate to distant goals using only offline training from this dataset, and exhibit behaviors that qualitatively differ based on the user-specified reward function.</td>
                <td><a href="https://proceedings.mlr.press/v205/shah23a.html">https://proceedings.mlr.press/v205/shah23a.html</a></td>
            </tr>
        
            <tr>
                <td>Graph Inverse Reinforcement Learning from Diverse Videos</td>
                <td>S, a, t, e, e, s, h,  , K, u, m, a, r, ,,  , J, o, n, a, t, h, a, n,  , Z, a, m, o, r, a, ,,  , N, i, c, k, l, a, s,  , H, a, n, s, e, n, ,,  , R, i, s, h, a, b, h,  , J, a, n, g, i, r, ,,  , X, i, a, o, l, o, n, g,  , W, a, n, g</td>
                <td>corl2022</td>
                <td>Research on Inverse Reinforcement Learning (IRL) from third-person videos has shown encouraging results on removing the need for manual reward design for robotic tasks. However, most prior works are still limited by training from a relatively restricted domain of videos. In this paper, we argue that the true potential of third-person IRL lies in increasing the diversity of videos for better scaling. To learn a reward function from diverse videos, we propose to perform graph abstraction on the videos followed by temporal matching in the graph space to measure the task progress. Our insight is that a task can be described by entity interactions that form a graph, and this graph abstraction can help remove irrelevant information such as textures, resulting in more robust reward functions. We evaluate our approach, GraphIRL, on cross-embodiment learning in X-MAGICAL and learning from human demonstrations for real-robot manipulation. We show significant improvements in robustness to diverse video demonstrations over previous approaches, and even achieve better results than manual reward design on a real robot pushing task. Videos are available at https://sateeshkumar21.github.io/GraphIRL/.</td>
                <td><a href="https://proceedings.mlr.press/v205/kumar23a.html">https://proceedings.mlr.press/v205/kumar23a.html</a></td>
            </tr>
        
            <tr>
                <td>Inferring Smooth Control: Monte Carlo Posterior Policy Iteration with Gaussian Processes</td>
                <td>J, o, e,  , W, a, t, s, o, n, ,,  , J, a, n,  , P, e, t, e, r, s</td>
                <td>corl2022</td>
                <td>Monte Carlo methods have become increasingly relevant for control of non-differentiable systems, approximate dynamics models, and learning from data.These methods scale to high-dimensional spaces and are effective at the non-convex optimization often seen in robot learning. We look at sample-based methods from the perspective of inference-based control, specifically posterior policy iteration. From this perspective, we highlight how Gaussian noise priors produce rough control actions that are unsuitable for physical robot deployment. Considering smoother Gaussian process priors, as used in episodic reinforcement learning and motion planning, we demonstrate how smoother model predictive control can be achieved using online sequential inference. This inference is realized through an efficient factorization of the action distribution, and novel means of optimizing the likelihood temperature for to improve importance sampling accuracy. We evaluate this approach on several high-dimensional robot control tasks, matching the sample efficiency of prior heuristic methods while also ensuring smoothness. Simulation results can be seen at monte-carlo-ppi.github.io.</td>
                <td><a href="https://proceedings.mlr.press/v205/watson23a.html">https://proceedings.mlr.press/v205/watson23a.html</a></td>
            </tr>
        
            <tr>
                <td>BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation</td>
                <td>C, h, e, n, g, s, h, u,  , L, i, ,,  , R, u, o, h, a, n,  , Z, h, a, n, g, ,,  , J, o, s, i, a, h,  , W, o, n, g, ,,  , C, e, m,  , G, o, k, m, e, n, ,,  , S, a, n, j, a, n, a,  , S, r, i, v, a, s, t, a, v, a, ,,  , R, o, b, e, r, t, o,  , M, a, r, t, í, n, -, M, a, r, t, í, n, ,,  , C, h, e, n,  , W, a, n, g, ,,  , G, a, b, r, a, e, l,  , L, e, v, i, n, e, ,,  , M, i, c, h, a, e, l,  , L, i, n, g, e, l, b, a, c, h, ,,  , J, i, a, n, k, a, i,  , S, u, n, ,,  , M, o, n, a,  , A, n, v, a, r, i, ,,  , M, i, n, j, u, n, e,  , H, w, a, n, g, ,,  , M, a, n, a, s, i,  , S, h, a, r, m, a, ,,  , A, r, m, a, n,  , A, y, d, i, n, ,,  , D, h, r, u, v, a,  , B, a, n, s, a, l, ,,  , S, a, m, u, e, l,  , H, u, n, t, e, r, ,,  , K, y, u, -, Y, o, u, n, g,  , K, i, m, ,,  , A, l, a, n,  , L, o, u, ,,  , C, a, l, e, b,  , R,  , M, a, t, t, h, e, w, s, ,,  , I, v, a, n,  , V, i, l, l, a, -, R, e, n, t, e, r, i, a, ,,  , J, e, r, r, y,  , H, u, a, y, a, n, g,  , T, a, n, g, ,,  , C, l, a, i, r, e,  , T, a, n, g, ,,  , F, e, i,  , X, i, a, ,,  , S, i, l, v, i, o,  , S, a, v, a, r, e, s, e, ,,  , H, y, o, w, o, n,  , G, w, e, o, n, ,,  , K, a, r, e, n,  , L, i, u, ,,  , J, i, a, j, u, n,  , W, u, ,,  , L, i,  , F, e, i, -, F, e, i</td>
                <td>corl2022</td>
                <td>We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on "what do you want robots to do for you?". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 5,000 objects annotated with rich physical and semantic properties. The second is OmniGibson, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K’s human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.</td>
                <td><a href="https://proceedings.mlr.press/v205/li23a.html">https://proceedings.mlr.press/v205/li23a.html</a></td>
            </tr>
        
            <tr>
                <td>Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from Demonstrations</td>
                <td>Y, a, n, w, e, i,  , W, a, n, g, ,,  , N, a, d, i, a,  , F, i, g, u, e, r, o, a, ,,  , S, h, e, n,  , L, i, ,,  , A, n, k, i, t,  , S, h, a, h, ,,  , J, u, l, i, e,  , S, h, a, h</td>
                <td>corl2022</td>
                <td>Learning from demonstration (LfD) has successfully solved tasks featuring a long time horizon. However, when the problem complexity also includes human-in-the-loop perturbations, state-of-the-art approaches do not guarantee the successful reproduction of a task. In this work, we identify the roots of this challenge as the failure of a learned continuous policy to satisfy the discrete plan implicit in the demonstration. By utilizing modes (rather than subgoals) as the discrete abstraction and motion policies with both mode invariance and goal reachability properties, we prove our learned continuous policy can simulate any discrete plan specified by a linear temporal logic (LTL) formula. Consequently, an imitator is robust to both task- and motion-level perturbations and guaranteed to achieve task success.</td>
                <td><a href="https://proceedings.mlr.press/v205/wang23a.html">https://proceedings.mlr.press/v205/wang23a.html</a></td>
            </tr>
        
            <tr>
                <td>Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks</td>
                <td>K, u, a, n,  , F, a, n, g, ,,  , P, a, t, r, i, c, k,  , Y, i, n, ,,  , A, s, h, v, i, n,  , N, a, i, r, ,,  , H, o, m, e, r,  , R, i, c, h,  , W, a, l, k, e, ,,  , G, e, n, g, c, h, e, n,  , Y, a, n, ,,  , S, e, r, g, e, y,  , L, e, v, i, n, e</td>
                <td>corl2022</td>
                <td>The use of broad datasets has proven to be crucial for generalization for a wide range of fields. However, how to effectively make use of diverse multi-task data for novel downstream tasks still remains a grand challenge in reinforcement learning and robotics. To tackle this challenge, we introduce a framework that acquires goal-conditioned policies for unseen temporally extended tasks via offline reinforcement learning on broad data, in combination with online fine-tuning guided by subgoals in a learned lossy representation space. When faced with a novel task goal, our framework uses an affordance model to plan a sequence of lossy representations as subgoals that decomposes the original task into easier problems. Learned from the broad prior data, the lossy representation emphasizes task-relevant information about states and goals while abstracting away redundant contexts that hinder generalization. It thus enables subgoal planning for unseen tasks, provides a compact input to the policy, and facilitates reward shaping during fine-tuning. We show that our framework can be pre-trained on large-scale datasets of robot experience from prior work and efficiently fine-tuned for novel tasks, entirely from visual inputs without any manual reward engineering.</td>
                <td><a href="https://proceedings.mlr.press/v205/fang23a.html">https://proceedings.mlr.press/v205/fang23a.html</a></td>
            </tr>
        
            <tr>
                <td>Real-time Mapping of Physical Scene Properties with an Autonomous Robot Experimenter</td>
                <td>I, a, i, n,  , H, a, u, g, h, t, o, n, ,,  , E, d, g, a, r,  , S, u, c, a, r, ,,  , A, n, d, r, e,  , M, o, u, t, o, n, ,,  , E, d, w, a, r, d,  , J, o, h, n, s, ,,  , A, n, d, r, e, w,  , D, a, v, i, s, o, n</td>
                <td>corl2022</td>
                <td>Neural fields can be trained from scratch to represent the shape and appearance of 3D scenes efficiently. It has also been shown that they can densely map correlated properties such as semantics, via sparse interactions from a human labeller. In this work, we show that a robot can densely annotate a scene with arbitrary discrete or continuous physical properties via its own fully-autonomous experimental interactions, as it simultaneously scans and maps it with an RGB-D camera. A variety of scene interactions are possible, including poking with force sensing to determine rigidity, measuring local material type with single-pixel spectroscopy or predicting force distributions by pushing. Sparse experimental interactions are guided by entropy to enable high efficiency, with tabletop scene properties densely mapped from scratch in a few minutes from a few tens of interactions.</td>
                <td><a href="https://proceedings.mlr.press/v205/haughton23a.html">https://proceedings.mlr.press/v205/haughton23a.html</a></td>
            </tr>
        
            <tr>
                <td>Robust Trajectory Prediction against Adversarial Attacks</td>
                <td>Y, u, l, o, n, g,  , C, a, o, ,,  , D, a, n, f, e, i,  , X, u, ,,  , X, i, n, s, h, u, o,  , W, e, n, g, ,,  , Z, h, u, o, q, i, n, g,  , M, a, o, ,,  , A, n, i, m, a,  , A, n, a, n, d, k, u, m, a, r, ,,  , C, h, a, o, w, e, i,  , X, i, a, o, ,,  , M, a, r, c, o,  , P, a, v, o, n, e</td>
                <td>corl2022</td>
                <td>Trajectory prediction using deep neural networks (DNNs) is an essential component of autonomous driving (AD) systems.  However, these methods are vulnerable to adversarial attacks, leading to serious consequences such as collisions. In this work, we identify two key ingredients to defend trajectory prediction models against adversarial attacks including (1) designing effective adversarial training methods and (2) adding domain-specific data augmentation to mitigate the performance degradation on clean data. We demonstrate that our method is able to improve the performance by 46% on adversarial data and at the cost of only 3% performance degradation on clean data, compared to the model trained with clean data. Additionally, compared to existing robust methods, our method can improve performance by 21% on adversarial examples and 9% on clean data. Our robust model is evaluated with a planner to study its downstream impacts. We demonstrate that our model can significantly reduce the severe accident rates (e.g., collisions and off-road driving).</td>
                <td><a href="https://proceedings.mlr.press/v205/cao23a.html">https://proceedings.mlr.press/v205/cao23a.html</a></td>
            </tr>
        
            <tr>
                <td>Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion</td>
                <td>Z, i, p, e, n, g,  , F, u, ,,  , X, u, x, i, n,  , C, h, e, n, g, ,,  , D, e, e, p, a, k,  , P, a, t, h, a, k</td>
                <td>corl2022</td>
                <td>An attached arm can significantly increase the applicability of legged robots to several mobile manipulation tasks that are not possible for the wheeled or tracked counterparts. The standard modular control pipeline for such legged manipulators is to decouple the controller into that of manipulation and locomotion. However, this is ineffective. It requires immense engineering to support coordination between the arm and legs, and error can propagate across modules causing non-smooth unnatural motions. It is also biological implausible given evidence for strong motor synergies across limbs. In this work, we propose to learn a unified policy for whole-body control of a legged manipulator using reinforcement learning. We propose Regularized Online Adaptation to bridge the Sim2Real gap for high-DoF control, and Advantage Mixing exploiting the causal dependency in the action space to overcome local minima during training the whole-body system. We also present a simple design for a low-cost legged manipulator, and find that our unified policy can demonstrate dynamic and agile behaviors across several task setups. Videos are at https://maniploco.github.io</td>
                <td><a href="https://proceedings.mlr.press/v205/fu23a.html">https://proceedings.mlr.press/v205/fu23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity</td>
                <td>W, e, n, x, u, a, n,  , Z, h, o, u, ,,  , D, a, v, i, d,  , H, e, l, d</td>
                <td>corl2022</td>
                <td>A simple gripper can solve more complex manipulation tasks if it can utilize the external environment such as pushing the object against the table or a vertical wall, known as "Extrinsic Dexterity." Previous work in extrinsic dexterity usually has careful assumptions about contacts which impose restrictions on robot design, robot motions, and the variations of the physical parameters. In this work, we develop a system based on reinforcement learning (RL) to address these limitations. We study the task of "Occluded Grasping" which aims to grasp the object in configurations that are initially occluded; the robot needs to move the object into a configuration from which these grasps can be achieved. We present a system with model-free RL that successfully achieves this task using a simple gripper with extrinsic dexterity. The policy learns emergent behaviors of pushing the object against the wall to rotate and then grasp it without additional reward terms on extrinsic dexterity. We discuss important components of the system including the design of the RL problem, multi-grasp training and selection, and policy generalization with automatic curriculum. Most importantly, the policy trained in simulation is zero-shot transferred to a physical robot. It demonstrates dynamic and contact-rich motions with a simple gripper that generalizes across objects with various size, density, surface friction, and shape with a 78% success rate. </td>
                <td><a href="https://proceedings.mlr.press/v205/zhou23a.html">https://proceedings.mlr.press/v205/zhou23a.html</a></td>
            </tr>
        
            <tr>
                <td>PRISM: Probabilistic Real-Time Inference in Spatial World Models</td>
                <td>A, t, a, n, a, s,  , M, i, r, c, h, e, v, ,,  , B, a, r, i, s,  , K, a, y, a, l, i, b, a, y, ,,  , A, h, m, e, d,  , A, g, h, a, ,,  , P, a, t, r, i, c, k,  , v, a, n,  , d, e, r,  , S, m, a, g, t, ,,  , D, a, n, i, e, l,  , C, r, e, m, e, r, s, ,,  , J, u, s, t, i, n,  , B, a, y, e, r</td>
                <td>corl2022</td>
                <td>We introduce PRISM, a method for real-time filtering in a probabilistic generative model of agent motion and visual perception. Previous approaches either lack uncertainty estimates for the map and agent state, do not run in real-time, do not have a dense scene representation or do not model agent dynamics. Our solution reconciles all of these aspects. We start from a predefined state-space model which combines differentiable rendering and 6-DoF dynamics. Probabilistic inference in this model amounts to simultaneous localisation and mapping (SLAM) and is intractable. We use a series of approximations to Bayesian inference to arrive at probabilistic map and state estimates. We take advantage of well-established methods and closed-form updates, preserving accuracy and enabling real-time capability. The proposed solution runs at 10Hz real-time and is similarly accurate to state-of-the-art SLAM in small to medium-sized indoor environments, with high-speed UAV and handheld camera agents (Blackbird, EuRoC and TUM-RGBD). </td>
                <td><a href="https://proceedings.mlr.press/v205/mirchev23a.html">https://proceedings.mlr.press/v205/mirchev23a.html</a></td>
            </tr>
        
            <tr>
                <td>Instruction-driven history-aware policies for robotic manipulations</td>
                <td>P, i, e, r, r, e, -, L, o, u, i, s,  , G, u, h, u, r, ,,  , S, h, i, z, h, e,  , C, h, e, n, ,,  , R, i, c, a, r, d, o,  , G, a, r, c, i, a,  , P, i, n, e, l, ,,  , M, a, k, a, r, a, n, d,  , T, a, p, a, s, w, i, ,,  , I, v, a, n,  , L, a, p, t, e, v, ,,  , C, o, r, d, e, l, i, a,  , S, c, h, m, i, d</td>
                <td>corl2022</td>
                <td>In human environments, robots are expected to accomplish a variety of manipulation tasks given simple natural language instructions. Yet, robotic manipulation is extremely challenging as it requires fine-grained motor control, long-term memory as well as generalization to previously unseen tasks and environments. To address these challenges, we propose a unified transformer-based approach that takes into account multiple inputs. In particular, our transformer architecture integrates (i) natural language instructions and (ii) multi-view scene observations while (iii) keeping track of the full history of observations and actions. Such an approach enables learning dependencies between history and instructions and improves manipulation precision using multiple views. We evaluate our method on the challenging RLBench benchmark and on a real-world robot. Notably, our approach scales to 74 diverse RLBench tasks and outperforms the state of the art. We also address instruction-conditioned tasks and demonstrate excellent generalization to previously unseen variations. </td>
                <td><a href="https://proceedings.mlr.press/v205/guhur23a.html">https://proceedings.mlr.press/v205/guhur23a.html</a></td>
            </tr>
        
            <tr>
                <td>Embedding Synthetic Off-Policy Experience for Autonomous Driving via Zero-Shot Curricula</td>
                <td>E, l, i,  , B, r, o, n, s, t, e, i, n, ,,  , S, i, r, i, s, h,  , S, r, i, n, i, v, a, s, a, n, ,,  , S, u, p, r, a, t, i, k,  , P, a, u, l, ,,  , A, m, a, n,  , S, i, n, h, a, ,,  , M, a, t, t, h, e, w,  , O, ’, K, e, l, l, y, ,,  , P, a, y, a, m,  , N, i, k, d, e, l, ,,  , S, h, i, m, o, n,  , W, h, i, t, e, s, o, n</td>
                <td>corl2022</td>
                <td>ML-based motion planning is a promising approach to produce agents that exhibit complex behaviors, and automatically adapt to novel environments. In the context of autonomous driving, it is common to treat all available training data equally. However, this approach produces agents that do not perform robustly in safety-critical settings, an issue that cannot be addressed by simply adding more data to the training set – we show that an agent trained using only a 10% subset of the data performs just as well as an agent trained on the entire dataset. We present a method to predict the inherent difficulty of a driving situation given data collected from a fleet of autonomous vehicles deployed on public roads. We then demonstrate that this difficulty score can be used in a zero-shot transfer to generate curricula for an imitation-learning based planning agent. Compared to training on the entire unbiased training dataset, we show that prioritizing difficult driving scenarios both reduces collisions by 15% and increases route adherence by 14% in closed-loop evaluation, all while using only 10% of the training data.</td>
                <td><a href="https://proceedings.mlr.press/v205/bronstein23a.html">https://proceedings.mlr.press/v205/bronstein23a.html</a></td>
            </tr>
        
            <tr>
                <td>LEADER: Learning Attention over Driving Behaviors for Planning under Uncertainty</td>
                <td>M, o, h, a, m, a, d,  , H, o, s, e, i, n,  , D, a, n, e, s, h, ,,  , P, a, n, p, a, n,  , C, a, i, ,,  , D, a, v, i, d,  , H, s, u</td>
                <td>corl2022</td>
                <td>Uncertainty in human behaviors poses a significant challenge to autonomous driving in crowded urban environments. The partially observable Markov decision process (POMDP) offers a principled general framework for decision making under uncertainty and achieves real-time performance for complex tasks by leveraging Monte Carlo sampling. However, sampling may miss rare, but critical events, leading to potential safety concerns. To tackle this challenge, we propose a new algorithm, LEarning Attention over Driving bEhavioRs (LEADER), which learns to attend to critical human behaviors during planning. LEADER learns a neural network generator to provide attention over human behaviors; it integrates the attention into a belief-space planner through importance sampling, which biases planning towards critical events. To train the attention generator, we form a minimax game between the generator and the planner. By solving this minimax game, LEADER learns to perform risk-aware planning without explicit human effort on data labeling.</td>
                <td><a href="https://proceedings.mlr.press/v205/danesh23a.html">https://proceedings.mlr.press/v205/danesh23a.html</a></td>
            </tr>
        
            <tr>
                <td>i-Sim2Real: Reinforcement Learning of Robotic Policies in Tight Human-Robot Interaction Loops</td>
                <td>S, a, m, i, n, d, a,  , W, i, s, h, w, a, j, i, t, h,  , A, b, e, y, r, u, w, a, n, ,,  , L, a, u, r, a,  , G, r, a, e, s, s, e, r, ,,  , D, a, v, i, d,  , B,  , D, ’, A, m, b, r, o, s, i, o, ,,  , A, v, i,  , S, i, n, g, h, ,,  , A, n, i, s, h,  , S, h, a, n, k, a, r, ,,  , A, l, e, x,  , B, e, w, l, e, y, ,,  , D, e, e, p, a, l, i,  , J, a, i, n, ,,  , K, r, z, y, s, z, t, o, f,  , M, a, r, c, i, n,  , C, h, o, r, o, m, a, n, s, k, i, ,,  , P, a, n, n, a, g,  , R,  , S, a, n, k, e, t, i</td>
                <td>corl2022</td>
                <td>Sim-to-real transfer is a powerful paradigm for robotic reinforcement learning. The ability to train policies in simulation enables safe exploration and large-scale data collection quickly at low cost. However, prior works in sim-to-real transfer of robotic policies typically do not involve any human-robot interaction because accurately simulating human behavior is an open problem. In this work, our goal is to leverage the power of simulation to train robotic policies that are proficient at interacting with humans upon deployment. But there is a chicken and egg problem — how to gather examples of a human interacting with a physical robot so as to model human behavior in simulation without already having a robot that is able to interact with a human? Our proposed method, Iterative-Sim-to-Real (i-S2R), attempts to address this. i-S2R bootstraps from a simple model of human behavior and alternates between training in simulation and deploying in the real world. In each iteration, both the human behavior model and the policy are refined. For all training we apply a new evolutionary search algorithm called Blackbox Gradient Sensing (BGS). We evaluate our method on a real world robotic table tennis setting, where the objective for the robot is to play cooperatively with a human player for as long as possible. Table tennis is a high-speed, dynamic task that requires the two players to react quickly to each other’s moves, making for a challenging test bed for research on human-robot interaction. We present results on an industrial robotic arm that is able to cooperatively play table tennis with human players, achieving rallies of 22 successive hits on average and 150 at best. Further, for 80% of players, rally lengths are 70% to 175% longer compared to the sim-to-real plus fine-tuning (S2R+FT) baseline. For videos of our system in action please see https://sites.google.com/view/is2r.</td>
                <td><a href="https://proceedings.mlr.press/v205/abeyruwan23a.html">https://proceedings.mlr.press/v205/abeyruwan23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning</td>
                <td>J, a, n,  , A, c, h, t, e, r, h, o, l, d, ,,  , M, a, r, k, u, s,  , K, r, i, m, m, e, l, ,,  , J, o, e, r, g,  , S, t, u, e, c, k, l, e, r</td>
                <td>corl2022</td>
                <td>Problems which require both long-horizon planning and continuous control capabilities pose significant challenges to existing reinforcement learning agents. In this paper we introduce a novel hierarchical reinforcement learning agent which links temporally extended skills for continuous control with a forward model in a symbolic discrete abstraction of the environment’s state for planning. We term our agent SEADS for Symbolic Effect-Aware Diverse Skills. We formulate an objective and corresponding algorithm which leads to unsupervised learning of a diverse set of skills through intrinsic motivation given a known state abstraction. The skills are jointly learned with the symbolic forward model which captures the effect of skill execution in the state abstraction. After training, we can leverage the skills as symbolic actions using the forward model for long-horizon planning and subsequently execute the plan using the learned continuous-action control skills. The proposed algorithm learns skills and forward models that can be used to solve complex tasks which require both continuous control and long-horizon planning capabilities with high success rate. It compares favorably with other flat and hierarchical reinforcement learning baseline agents and is successfully demonstrated with a real robot.</td>
                <td><a href="https://proceedings.mlr.press/v205/achterhold23a.html">https://proceedings.mlr.press/v205/achterhold23a.html</a></td>
            </tr>
        
            <tr>
                <td>Meta-Learning Priors for Safe Bayesian Optimization</td>
                <td>J, o, n, a, s,  , R, o, t, h, f, u, s, s, ,,  , C, h, r, i, s, t, o, p, h, e, r,  , K, o, e, n, i, g, ,,  , A, l, i, s, a,  , R, u, p, e, n, y, a, n, ,,  , A, n, d, r, e, a, s,  , K, r, a, u, s, e</td>
                <td>corl2022</td>
                <td>In robotics, optimizing controller parameters under safety constraints is an important challenge. Safe Bayesian optimization (BO) quantifies uncertainty in the objective and constraints to safely guide exploration in such settings. Hand-designing a suitable probabilistic model can be challenging however. In the presence of unknown safety constraints, it is crucial to choose reliable model hyper-parameters to avoid safety violations. Here, we propose a data-driven approach to this problem by em meta-learning priors for safe BO from offline data. We build on a meta-learning algorithm, F-PACOH, capable of providing reliable uncertainty quantification in settings of data scarcity. As core contribution, we develop a novel framework for choosing safety-compliant priors in a data-riven manner via empirical uncertainty metrics and a frontier search algorithm. On benchmark functions and a high-precision motion system, we demonstrate that our meta-learnt priors accelerate convergence of safe BO approaches while maintaining safety. </td>
                <td><a href="https://proceedings.mlr.press/v205/rothfuss23a.html">https://proceedings.mlr.press/v205/rothfuss23a.html</a></td>
            </tr>
        
            <tr>
                <td>Planning Paths through Occlusions in Urban Environments</td>
                <td>Y, u, t, a, o,  , H, a, n, ,,  , Y, o, u, y, a,  , X, i, a, ,,  , G, u, o, -, J, u, n,  , Q, i, ,,  , M, a, r, k,  , C, a, m, p, b, e, l, l</td>
                <td>corl2022</td>
                <td>This paper presents a novel framework for planning in unknown and occluded urban spaces. We specifically focus on turns and intersections where occlusions significantly impact navigability. Our approach uses an inpainting model to fill in a sparse, occluded, semantic lidar point cloud and plans dynamically feasible paths for a vehicle to traverse through the open and inpainted spaces. We demonstrate our approach using a car’s lidar data with real-time occlusions, and show that by inpainting occluded areas, we can plan longer paths, with more turn options compared to without inpainting; in addition, our approach more closely follows paths derived from a planner with no occlusions (called the ground truth) compared to other state of the art approaches.</td>
                <td><a href="https://proceedings.mlr.press/v205/han23a.html">https://proceedings.mlr.press/v205/han23a.html</a></td>
            </tr>
        
            <tr>
                <td>Rethinking Optimization with Differentiable Simulation from a Global Perspective</td>
                <td>R, i, k, a,  , A, n, t, o, n, o, v, a, ,,  , J, i, n, g, y, u, n,  , Y, a, n, g, ,,  , K, r, i, s, h, n, a,  , M, u, r, t, h, y,  , J, a, t, a, v, a, l, l, a, b, h, u, l, a, ,,  , J, e, a, n, n, e, t, t, e,  , B, o, h, g</td>
                <td>corl2022</td>
                <td>Differentiable simulation is a promising toolkit for fast gradient-based policy optimization and system identification. However, existing approaches to differentiable simulation have largely tackled scenarios where obtaining smooth gradients has been relatively easy, such as systems with mostly smooth dynamics. In this work, we study the challenges that differentiable simulation presents when it is not feasible to expect that a single descent reaches a global optimum, which is often a problem in contact-rich scenarios. We analyze the optimization landscapes of diverse scenarios that contain both rigid bodies and deformable objects. In dynamic environments with highly deformable objects and fluids, differentiable simulators produce rugged landscapes with nonetheless useful gradients in some parts of the space. We propose a method that combines Bayesian optimization with semi-local ’leaps’ to obtain a global search method that can use gradients effectively, while also maintaining robust performance in regions with noisy gradients. We show that our approach outperforms several gradient-based and gradient-free baselines on an extensive set of experiments in simulation, and also validate the method using experiments with a real robot and deformables.</td>
                <td><a href="https://proceedings.mlr.press/v205/antonova23a.html">https://proceedings.mlr.press/v205/antonova23a.html</a></td>
            </tr>
        
            <tr>
                <td>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</td>
                <td>b, r, i, a, n,  , i, c, h, t, e, r, ,,  , A, n, t, h, o, n, y,  , B, r, o, h, a, n, ,,  , Y, e, v, g, e, n,  , C, h, e, b, o, t, a, r, ,,  , C, h, e, l, s, e, a,  , F, i, n, n, ,,  , K, a, r, o, l,  , H, a, u, s, m, a, n, ,,  , A, l, e, x, a, n, d, e, r,  , H, e, r, z, o, g, ,,  , D, a, n, i, e, l,  , H, o, ,,  , J, u, l, i, a, n,  , I, b, a, r, z, ,,  , A, l, e, x,  , I, r, p, a, n, ,,  , E, r, i, c,  , J, a, n, g, ,,  , R, y, a, n,  , J, u, l, i, a, n, ,,  , D, m, i, t, r, y,  , K, a, l, a, s, h, n, i, k, o, v, ,,  , S, e, r, g, e, y,  , L, e, v, i, n, e, ,,  , Y, a, o,  , L, u, ,,  , C, a, r, o, l, i, n, a,  , P, a, r, a, d, a, ,,  , K, a, n, i, s, h, k, a,  , R, a, o, ,,  , P, i, e, r, r, e,  , S, e, r, m, a, n, e, t, ,,  , A, l, e, x, a, n, d, e, r,  , T,  , T, o, s, h, e, v, ,,  , V, i, n, c, e, n, t,  , V, a, n, h, o, u, c, k, e, ,,  , F, e, i,  , X, i, a, ,,  , T, e, d,  , X, i, a, o, ,,  , P, e, n, g,  , X, u, ,,  , M, e, n, g, y, u, a, n,  , Y, a, n, ,,  , N, o, a, h,  , B, r, o, w, n, ,,  , M, i, c, h, a, e, l,  , A, h, n, ,,  , O, m, a, r,  , C, o, r, t, e, s, ,,  , N, i, c, o, l, a, s,  , S, i, e, v, e, r, s, ,,  , C, l, a, y, t, o, n,  , T, a, n, ,,  , S, i, c, h, u, n,  , X, u, ,,  , D, i, e, g, o,  , R, e, y, e, s, ,,  , J, a, r, e, k,  , R, e, t, t, i, n, g, h, o, u, s, e, ,,  , J, o, r, n, e, l, l,  , Q, u, i, a, m, b, a, o, ,,  , P, e, t, e, r,  , P, a, s, t, o, r, ,,  , L, i, n, d, a,  , L, u, u, ,,  , K, u, a, n, g, -, H, u, e, i,  , L, e, e, ,,  , Y, u, h, e, n, g,  , K, u, a, n, g, ,,  , S, a, l, l, y,  , J, e, s, m, o, n, t, h, ,,  , N, i, k, h, i, l,  , J, .,  , J, o, s, h, i, ,,  , K, y, l, e,  , J, e, f, f, r, e, y, ,,  , R, o, s, a, r, i, o,  , J, a, u, r, e, g, u, i,  , R, u, a, n, o, ,,  , J, a, s, m, i, n, e,  , H, s, u, ,,  , K, e, e, r, t, h, a, n, a,  , G, o, p, a, l, a, k, r, i, s, h, n, a, n, ,,  , B, y, r, o, n,  , D, a, v, i, d, ,,  , A, n, d, y,  , Z, e, n, g, ,,  , C, h, u, y, u, a, n,  , K, e, l, l, y,  , F, u</td>
                <td>corl2022</td>
                <td>Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so  that  the  language model  provides  high-level  knowledge about the procedures for performing complex and temporally extended instructions,  while  value  functions  associated  with  these  skills  provide  the  grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project’s website, video, and open source can be found at say-can.github.io.</td>
                <td><a href="https://proceedings.mlr.press/v205/ichter23a.html">https://proceedings.mlr.press/v205/ichter23a.html</a></td>
            </tr>
        
            <tr>
                <td>MidasTouch: Monte-Carlo inference over distributions across sliding touch</td>
                <td>S, u, d, h, a, r, s, h, a, n,  , S, u, r, e, s, h, ,,  , Z, i, l, i, n,  , S, i, ,,  , S, t, u, a, r, t,  , A, n, d, e, r, s, o, n, ,,  , M, i, c, h, a, e, l,  , K, a, e, s, s, ,,  , M, u, s, t, a, f, a,  , M, u, k, a, d, a, m</td>
                <td>corl2022</td>
                <td>We present MidasTouch, a tactile perception system for online global localization of a vision-based touch sensor sliding on an object surface. This framework takes in posed tactile images over time, and outputs an evolving distribution of sensor pose on the object’s surface, without the need for visual priors. Our key insight is to estimate local surface geometry with tactile sensing, learn a compact representation for it, and disambiguate these signals over a long time horizon. The backbone of MidasTouch is a Monte-Carlo particle filter, with a measurement model based on a tactile code network learned from tactile simulation. This network, inspired by LIDAR place recognition, compactly summarizes local surface geometries. These generated codes are efficiently compared against a precomputed tactile codebook per-object, to update the pose distribution. We further release the YCB-Slide dataset of real-world and simulated forceful sliding interactions between a vision-based tactile sensor and standard YCB objects. While single-touch localization can be inherently ambiguous, we can quickly localize our sensor by traversing salient surface geometries. Project page: https://suddhu.github.io/midastouch-tactile/</td>
                <td><a href="https://proceedings.mlr.press/v205/suresh23a.html">https://proceedings.mlr.press/v205/suresh23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Visuo-Haptic Skewering Strategies for Robot-Assisted Feeding</td>
                <td>P, r, i, y, a,  , S, u, n, d, a, r, e, s, a, n, ,,  , S, u, n, e, e, l,  , B, e, l, k, h, a, l, e, ,,  , D, o, r, s, a,  , S, a, d, i, g, h</td>
                <td>corl2022</td>
                <td>Acquiring food items with a fork poses an immense challenge to a robot-assisted feeding system, due to the wide range of material properties and visual appearances present across food groups. Deformable foods necessitate different skewering strategies than firm ones, but inferring such characteristics for several previously unseen items on a plate remains nontrivial. Our key insight is to leverage visual and haptic observations during interaction with an item to rapidly and reactively plan skewering motions. We learn a generalizable, multimodal representation for a food item from raw sensory inputs which informs the optimal skewering strategy. Given this representation, we propose a zero-shot framework to sense visuo-haptic properties of a previously unseen item and reactively skewer it, all within a single interaction. Real-robot experiments with foods of varying levels of visual and textural diversity demonstrate that our multimodal policy outperforms baselines which do not exploit both visual and haptic cues or do not reactively plan. Across 6 plates of different food items, our proposed framework achieves 71% success over 69 skewering attempts total. Supplementary material, code, and videos can be found on our website: https://sites.google.com/view/hapticvisualnet-corl22/home.</td>
                <td><a href="https://proceedings.mlr.press/v205/sundaresan23a.html">https://proceedings.mlr.press/v205/sundaresan23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations</td>
                <td>C, h, e, n, h, a, o,  , L, i, ,,  , M, a, r, i, n,  , V, l, a, s, t, e, l, i, c, a, ,,  , S, e, b, a, s, t, i, a, n,  , B, l, a, e, s, ,,  , J, o, n, a, s,  , F, r, e, y, ,,  , F, e, l, i, x,  , G, r, i, m, m, i, n, g, e, r, ,,  , G, e, o, r, g,  , M, a, r, t, i, u, s</td>
                <td>corl2022</td>
                <td>Learning agile skills is one of the main challenges in robotics. To this end, reinforcement learning approaches have achieved impressive results. These methods require explicit task information in terms of a reward function or an expert that can be queried in simulation to provide a target control output, which limits their applicability. In this work, we propose a generative adversarial method for inferring reward functions from partial and potentially physically incompatible demonstrations for successful skill acquirement where reference or expert demonstrations are not easily accessible. Moreover, we show that by using a Wasserstein GAN formulation and transitions from demonstrations with rough and partial information as input, we are able to extract policies that are robust and capable of imitating demonstrated behaviors. Finally, the obtained skills such as a backflip are tested on an agile quadruped robot called Solo 8 and present faithful replication of hand-held human demonstrations.</td>
                <td><a href="https://proceedings.mlr.press/v205/li23b.html">https://proceedings.mlr.press/v205/li23b.html</a></td>
            </tr>
        
            <tr>
                <td>Evo-NeRF: Evolving NeRF for Sequential Robot Grasping of Transparent Objects</td>
                <td>J, u, s, t, i, n,  , K, e, r, r, ,,  , L, e, t, i, a, n,  , F, u, ,,  , H, u, a, n, g,  , H, u, a, n, g, ,,  , Y, a, h, a, v,  , A, v, i, g, a, l, ,,  , M, a, t, t, h, e, w,  , T, a, n, c, i, k, ,,  , J, e, f, f, r, e, y,  , I, c, h, n, o, w, s, k, i, ,,  , A, n, g, j, o, o,  , K, a, n, a, z, a, w, a, ,,  , K, e, n,  , G, o, l, d, b, e, r, g</td>
                <td>corl2022</td>
                <td>Sequential robot grasping of transparent objects, where a robot removes objects one by one from a workspace, is important in many industrial and household scenarios. We propose Evolving NeRF (Evo-NeRF), leveraging recent speedups in NeRF training and further extending it to rapidly train the NeRF representation concurrently to image capturing. Evo-NeRF terminates training early when a sufficient task confidence is achieved, evolves the NeRF weights from grasp to grasp to rapidly adapt to object removal, and applies additional geometry regularizations to make the reconstruction smoother and faster. General purpose grasp planners such as Dex-Net may underperform with NeRF outputs because there can be unreliable geometry from rapidly trained NeRFs. To mitigate this distribution shift, we propose a Radiance-Adjusted Grasp Network (RAG-Net), a grasping network adapted to NeRF’s characteristics through training on depth rendered from NeRFs of synthetic scenes. In experiments, a physical YuMi robot using Evo-NeRF and RAG-Net achieves an 89% grasp success rate over 27 trials on single objects, with early capture termination providing a 41% speed improvement with no loss in reliability. In a sequential grasping task on 6 scenes, Evo-NeRF reusing network weights clears 72% of the objects, retaining similar performance as reconstructing the NeRF from scratch (76%) but using 61% less sensing time. See https://sites.google.com/view/evo-nerf for more materials.</td>
                <td><a href="https://proceedings.mlr.press/v205/kerr23a.html">https://proceedings.mlr.press/v205/kerr23a.html</a></td>
            </tr>
        
            <tr>
                <td>Fleet-DAgger: Interactive Robot Fleet Learning with Scalable Human Supervision</td>
                <td>R, y, a, n,  , H, o, q, u, e, ,,  , L, a, w, r, e, n, c, e,  , Y, u, n, l, i, a, n, g,  , C, h, e, n, ,,  , S, a, t, v, i, k,  , S, h, a, r, m, a, ,,  , K, a, r, t, h, i, k,  , D, h, a, r, m, a, r, a, j, a, n, ,,  , B, r, i, j, e, n,  , T, h, a, n, a, n, j, e, y, a, n, ,,  , P, i, e, t, e, r,  , A, b, b, e, e, l, ,,  , K, e, n,  , G, o, l, d, b, e, r, g</td>
                <td>corl2022</td>
                <td>Commercial and industrial deployments of robot fleets at Amazon, Nimble, Plus One, Waymo, and Zoox query remote human teleoperators when robots are at risk or unable to make task progress. With continual learning, interventions from the remote pool of humans can also be used to improve the robot fleet control policy over time. A central question is how to effectively allocate limited human attention. Prior work addresses this in the single-robot, single-human setting; we formalize the Interactive Fleet Learning (IFL) setting, in which multiple robots interactively query and learn from multiple human supervisors. We propose Return on Human Effort (ROHE) as a new metric and Fleet-DAgger, a family of IFL algorithms. We present an open-source IFL benchmark suite of GPU-accelerated Isaac Gym environments for standardized evaluation and development of IFL algorithms. We compare a novel Fleet-DAgger algorithm to 4 baselines with 100 robots in simulation. We also perform a physical block-pushing experiment with 4 ABB YuMi robot arms and 2 remote humans. Experiments suggest that the allocation of humans to robots significantly affects the performance of the fleet, and that the novel Fleet-DAgger algorithm can achieve up to 8.8x higher ROHE than baselines. See https://tinyurl.com/fleet-dagger for supplemental material.</td>
                <td><a href="https://proceedings.mlr.press/v205/hoque23a.html">https://proceedings.mlr.press/v205/hoque23a.html</a></td>
            </tr>
        
            <tr>
                <td>RAP: Risk-Aware Prediction for Robust Planning</td>
                <td>H, a, r, u, k, i,  , N, i, s, h, i, m, u, r, a, ,,  , J, e, a, n,  , M, e, r, c, a, t, ,,  , B, l, a, k, e,  , W, u, l, f, e, ,,  , R, o, w, a, n,  , T, h, o, m, a, s,  , M, c, A, l, l, i, s, t, e, r, ,,  , A, d, r, i, e, n,  , G, a, i, d, o, n</td>
                <td>corl2022</td>
                <td>Robust planning in interactive scenarios requires predicting the uncertain future to make risk-aware decisions. Unfortunately, due to long-tail safety-critical events, the risk is often under-estimated by finite-sampling approximations of probabilistic motion forecasts. This can lead to overconfident and unsafe robot behavior, even with robust planners. Instead of assuming full prediction coverage that robust planners require, we propose to make prediction itself risk-aware. We introduce a new prediction objective to learn a risk-biased distribution over trajectories, so that risk evaluation simplifies to an expected cost estimation under this biased distribution. This reduces sample complexity of the risk estimation during online planning, which is needed for safe real-time performance. Evaluation results in a didactic simulation environment and on a real-world dataset demonstrate the effectiveness of our approach. The code and a demo are available.</td>
                <td><a href="https://proceedings.mlr.press/v205/nishimura23a.html">https://proceedings.mlr.press/v205/nishimura23a.html</a></td>
            </tr>
        
            <tr>
                <td>Topological Semantic Graph Memory for Image-Goal Navigation</td>
                <td>N, u, r, i,  , K, i, m, ,,  , O, b, i, n,  , K, w, o, n, ,,  , H, w, i, y, e, o, n,  , Y, o, o, ,,  , Y, u, n, h, o,  , C, h, o, i, ,,  , J, e, o, n, g, h, o,  , P, a, r, k, ,,  , S, o, n, g, h, w, a, i,  , O, h</td>
                <td>corl2022</td>
                <td>A novel framework is proposed to incrementally collect landmark-based graph memory and use the collected memory for image goal navigation. Given a target image to search, an embodied robot utilizes semantic memory to find the target in an unknown environment. In this paper, we present a topological semantic graph memory (TSGM), which consists of (1) a graph builder that takes the observed RGB-D image to construct a topological semantic graph, (2) a cross graph mixer module that takes the collected nodes to get contextual information, and (3) a memory decoder that takes the contextual memory as an input to find an action to the target. On the task of an image goal navigation, TSGM significantly outperforms competitive baselines by +5.0-9.0% on the success rate and +7.0-23.5% on SPL, which means that the TSGM finds efficient paths. Additionally, we demonstrate our method on a mobile robot in real-world image goal scenarios.</td>
                <td><a href="https://proceedings.mlr.press/v205/kim23a.html">https://proceedings.mlr.press/v205/kim23a.html</a></td>
            </tr>
        
            <tr>
                <td>Legged Locomotion in Challenging Terrains using Egocentric Vision</td>
                <td>A, n, a, n, y, e,  , A, g, a, r, w, a, l, ,,  , A, s, h, i, s, h,  , K, u, m, a, r, ,,  , J, i, t, e, n, d, r, a,  , M, a, l, i, k, ,,  , D, e, e, p, a, k,  , P, a, t, h, a, k</td>
                <td>corl2022</td>
                <td>Animals are capable of precise and agile locomotion using vision. Replicating this ability has been a long-standing goal in robotics. The traditional approach has been to decompose this problem into elevation mapping and foothold planning phases. The elevation mapping, however, is susceptible to failure and large noise artifacts, requires specialized hardware, and is biologically implausible. In this paper, we present the first end-to-end locomotion system capable of traversing stairs, curbs, stepping stones, and gaps. We show this result on a medium-sized quadruped robot using a single front-facing depth camera. The small size of the robot necessitates discovering specialized gait patterns not seen elsewhere. The egocentric camera requires the policy to remember past information to estimate the terrain under its hind feet. We train our policy in simulation. Training has two phases - first, we train a policy using reinforcement learning with a cheap-to-compute variant of depth image and then in phase 2 distill it into the final policy that uses depth using supervised learning. The resulting policy transfers to the real world and is able to run in real-time on the limited compute of the robot. It can traverse a large variety of terrain while being robust to perturbations like pushes, slippery surfaces, and rocky terrain. Videos are at https://vision-locomotion.github.io</td>
                <td><a href="https://proceedings.mlr.press/v205/agarwal23a.html">https://proceedings.mlr.press/v205/agarwal23a.html</a></td>
            </tr>
        
            <tr>
                <td>Real-World Robot Learning with Masked Visual Pre-training</td>
                <td>I, l, i, j, a,  , R, a, d, o, s, a, v, o, v, i, c, ,,  , T, e, t, e,  , X, i, a, o, ,,  , S, t, e, p, h, e, n,  , J, a, m, e, s, ,,  , P, i, e, t, e, r,  , A, b, b, e, e, l, ,,  , J, i, t, e, n, d, r, a,  , M, a, l, i, k, ,,  , T, r, e, v, o, r,  , D, a, r, r, e, l, l</td>
                <td>corl2022</td>
                <td>In this work, we explore self-supervised visual pre-training on images from diverse, in-the-wild videos for real-world robotic tasks. Like prior work, our visual representations are pre-trained via a masked autoencoder (MAE), frozen, and then passed into a learnable control module. Unlike prior work, we show that the pre-trained representations are effective across a range of real-world robotic tasks and embodiments. We find that our encoder consistently outperforms CLIP (up to 75%), supervised ImageNet pre-training (up to 81%), and training from scratch (up to 81%). Finally, we train a 307M parameter vision transformer on a massive collection of 4.5M images from the Internet and egocentric videos, and demonstrate clearly the benefits of scaling visual pre-training for robot learning.</td>
                <td><a href="https://proceedings.mlr.press/v205/radosavovic23a.html">https://proceedings.mlr.press/v205/radosavovic23a.html</a></td>
            </tr>
        
            <tr>
                <td>SE(2)-Equivariant Pushing Dynamics Models for Tabletop Object Manipulations</td>
                <td>S, e, u, n, g, y, e, o, n,  , K, i, m, ,,  , B, y, e, o, n, g, d, o,  , L, i, m, ,,  , Y, o, n, g, h, y, e, o, n,  , L, e, e, ,,  , F, r, a, n, k,  , C, .,  , P, a, r, k</td>
                <td>corl2022</td>
                <td>For tabletop object manipulation tasks, learning an accurate pushing dynamics model, which predicts the objects’ motions when a robot pushes an object, is very important. In this work, we claim that an ideal pushing dynamics model should have the SE(2)-equivariance property, i.e., if tabletop objects’ poses and pushing action are transformed by some same planar rigid-body transformation, then the resulting motion should also be the result of the same transformation. Existing state-of-the-art data-driven approaches do not have this equivariance property, resulting in less-than-desirable learning performances. In this paper, we propose a new neural network architecture that by construction has the above equivariance property. Through extensive empirical validations, we show that the proposed model shows significantly improved learning performances over the existing methods. Also, we verify that our pushing dynamics model can be used for various downstream pushing manipulation tasks such as the object moving, singulation, and grasping in both simulation and real robot experiments. Code is available at https://github.com/seungyeon-k/SQPDNet-public.</td>
                <td><a href="https://proceedings.mlr.press/v205/kim23b.html">https://proceedings.mlr.press/v205/kim23b.html</a></td>
            </tr>
        
            <tr>
                <td>Vision-based Uneven BEV Representation Learning with Polar Rasterization and Surface Estimation</td>
                <td>Z, h, i,  , L, i, u, ,,  , S, h, a, o, y, u,  , C, h, e, n, ,,  , X, i, a, o, j, i, e,  , G, u, o, ,,  , X, i, n, g, g, a, n, g,  , W, a, n, g, ,,  , T, i, a, n, h, e, n, g,  , C, h, e, n, g, ,,  , H, o, n, g, m, e, i,  , Z, h, u, ,,  , Q, i, a, n,  , Z, h, a, n, g, ,,  , W, e, n, y, u,  , L, i, u, ,,  , Y, i,  , Z, h, a, n, g</td>
                <td>corl2022</td>
                <td>In this work, we propose PolarBEV  for vision-based uneven BEV representation learning. To adapt to the foreshortening effect of camera imaging, we rasterize the BEV space both angularly and radially, and introduce polar embedding decomposition to model the associations among polar grids.  Polar grids are rearranged to an array-like regular representation for efficient processing. Besides, to determine the 2D-to-3D correspondence, we iteratively update the BEV surface based on a hypothetical plane, and adopt height-based  feature transformation. PolarBEV keeps real-time inference speed on a single 2080Ti GPU, and outperforms other methods for both BEV semantic segmentation and BEV instance segmentation. Thorough ablations  are presented to validate the design. The code will be released for facilitating further research.</td>
                <td><a href="https://proceedings.mlr.press/v205/liu23a.html">https://proceedings.mlr.press/v205/liu23a.html</a></td>
            </tr>
        
            <tr>
                <td>HERD: Continuous Human-to-Robot Evolution for Learning from Human Demonstration</td>
                <td>X, i, n, g, y, u,  , L, i, u, ,,  , D, e, e, p, a, k,  , P, a, t, h, a, k, ,,  , K, r, i, s,  , M, .,  , K, i, t, a, n, i</td>
                <td>corl2022</td>
                <td>The ability to learn from human demonstration endows robots with the ability to automate various tasks. However, directly learning from human demonstration is challenging since the structure of the human hand can be very different from the desired robot gripper. In this work, we show that manipulation skills can be transferred from a human to a robot through the use of micro-evolutionary reinforcement learning, where a five-finger human dexterous hand robot gradually evolves into a commercial robot, while repeated interacting in a physics simulator to continuously update the policy that is first learned from human demonstration. To deal with the high dimensions of robot parameters, we propose an algorithm for multi-dimensional evolution path searching that allows joint optimization of both the robot evolution path and the policy. Through experiments on human object manipulation datasets, we show that our framework can efficiently transfer the expert human agent policy trained from human demonstrations in diverse modalities to target commercial robots.</td>
                <td><a href="https://proceedings.mlr.press/v205/liu23b.html">https://proceedings.mlr.press/v205/liu23b.html</a></td>
            </tr>
        
            <tr>
                <td>PlanT: Explainable Planning Transformers via Object-Level Representations</td>
                <td>K, a, t, r, i, n,  , R, e, n, z, ,,  , K, a, s, h, y, a, p,  , C, h, i, t, t, a, ,,  , O, t, n, i, e, l, -, B, o, g, d, a, n,  , M, e, r, c, e, a, ,,  , A, .,  , S, o, p, h, i, a,  , K, o, e, p, k, e, ,,  , Z, e, y, n, e, p,  , A, k, a, t, a, ,,  , A, n, d, r, e, a, s,  , G, e, i, g, e, r</td>
                <td>corl2022</td>
                <td>Planning an optimal route in a complex environment requires efficient reasoning about the surrounding scene. While human drivers prioritize important objects and ignore details not relevant to the decision, learning-based planners typically extract features from dense, high-dimensional grid representations containing all vehicle and road context information. In this paper, we propose PlanT, a novel approach for planning in the context of self-driving that uses a standard transformer architecture. PlanT is based on imitation learning with a compact object-level input representation. On the Longest6 benchmark for CARLA, PlanT outperforms all prior methods (matching the driving score of the expert) while being 5.3× faster than equivalent pixel-based planning baselines during inference. Combining PlanT with an off-the-shelf perception module provides a sensor-based driving system that is more than 10 points better in terms of driving score than the existing state of the art. Furthermore, we propose an evaluation protocol to quantify the ability of planners to identify relevant objects, providing insights regarding their decision-making. Our results indicate that PlanT can focus on the most relevant object in the scene, even when this object is geometrically distant.</td>
                <td><a href="https://proceedings.mlr.press/v205/renz23a.html">https://proceedings.mlr.press/v205/renz23a.html</a></td>
            </tr>
        
            <tr>
                <td>Where To Start? Transferring Simple Skills to Complex Environments</td>
                <td>V, i, t, a, l, i, s,  , V, o, s, y, l, i, u, s, ,,  , E, d, w, a, r, d,  , J, o, h, n, s</td>
                <td>corl2022</td>
                <td>Robot learning provides a number of ways to teach robots simple skills, such as grasping. However, these skills are usually trained in open, clutter-free environments, and therefore would likely cause undesirable collisions in more complex, cluttered environments. In this work, we introduce an affordance model based on a graph representation of an environment, which is optimised during deployment to find suitable robot configurations to start a skill from, such that the skill can be executed without any collisions. We demonstrate that our method can generalise a priori acquired skills to previously unseen cluttered and constrained environments, in simulation and in the real world, for both a grasping and a placing task. </td>
                <td><a href="https://proceedings.mlr.press/v205/vosylius23a.html">https://proceedings.mlr.press/v205/vosylius23a.html</a></td>
            </tr>
        
            <tr>
                <td>Efficient and Stable Off-policy Training via Behavior-aware Evolutionary Learning</td>
                <td>M, a, i, y, u, e,  , C, h, e, n, ,,  , G, u, a, n, g, y, i,  , H, e</td>
                <td>corl2022</td>
                <td>Applying reinforcement learning (RL) algorithms to real-world continuos control problems faces many challenges in terms of sample efficiency, stability and exploration. Off-policy RL algorithms show great sample efficiency but can be unstable to train and require effective exploration techniques for sparse reward environments. A simple yet effective approach to address these challenges is to train a population of policies and ensemble them in certain ways. In this work, a novel population based evolutionary training framework inspired by evolution strategies (ES) called Behavior-aware Evolutionary Learning (BEL) is proposed. The main idea is to train a population of behaviorally diverse policies in parallel and conduct selection with simple linear recombination. BEL consists of two mechanisms called behavior-regularized perturbation (BRP) and behavior-targeted training (BTT) to accomplish stable and fine control of the population behavior divergence. Experimental studies showed that BEL not only has superior sample efficiency and stability compared to existing methods, but can also produce diverse agents in sparse reward environments. Due to the parallel implementation, BEL also exhibits relatively good computation efficiency, making it a practical and competitive method to train policies for real-world robots.</td>
                <td><a href="https://proceedings.mlr.press/v205/chen23a.html">https://proceedings.mlr.press/v205/chen23a.html</a></td>
            </tr>
        
            <tr>
                <td>LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action</td>
                <td>D, h, r, u, v,  , S, h, a, h, ,,  , B, ł, a, ż, e, j,  , O, s, i, ń, s, k, i, ,,  , b, r, i, a, n,  , i, c, h, t, e, r, ,,  , S, e, r, g, e, y,  , L, e, v, i, n, e</td>
                <td>corl2022</td>
                <td>Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. LM-Nav extracts landmarks names from an instruction, grounds them in the world via the image-language model, and then reaches them via the (vision-only) navigation model. We instantiate LM-Nav on a real-world  mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions.</td>
                <td><a href="https://proceedings.mlr.press/v205/shah23b.html">https://proceedings.mlr.press/v205/shah23b.html</a></td>
            </tr>
        
            <tr>
                <td>BusyBot: Learning to Interact, Reason, and Plan in a BusyBoard Environment</td>
                <td>Z, e, y, i,  , L, i, u, ,,  , Z, h, e, n, j, i, a,  , X, u, ,,  , S, h, u, r, a, n,  , S, o, n, g</td>
                <td>corl2022</td>
                <td>We introduce BusyBoard, a toy-inspired robot learning environment that leverages a diverse set of articulated objects and inter-object functional relations to provide rich visual feedback for robot interactions. Based on this environment, we introduce a learning framework, BusyBot, which allows an agent to jointly acquire three fundamental capabilities (interaction, reasoning, and planning) in an integrated and self-supervised manner. With the rich sensory feedback provided by BusyBoard, BusyBot first learns a policy to efficiently interact with the environment; then with data collected using the policy, BusyBot reasons the inter-object functional relations through a causal discovery network; and finally by combining the learned interaction policy and relation reasoning skill, the agent is able to perform goal-conditioned manipulation tasks. We evaluate BusyBot in both simulated and real-world environments, and validate its generalizability to unseen objects and relations.</td>
                <td><a href="https://proceedings.mlr.press/v205/liu23c.html">https://proceedings.mlr.press/v205/liu23c.html</a></td>
            </tr>
        
            <tr>
                <td>NeuralGrasps: Learning Implicit Representations for Grasps of Multiple Robotic Hands</td>
                <td>N, i, n, a, d,  , K, h, a, r, g, o, n, k, a, r, ,,  , N, e, i, l,  , S, o, n, g, ,,  , Z, e, s, h, e, n, g,  , X, u, ,,  , B,  , P, r, a, b, h, a, k, a, r, a, n, ,,  , Y, u,  , X, i, a, n, g</td>
                <td>corl2022</td>
                <td>We introduce a neural implicit representation for grasps of objects from multiple robotic hands. Different grasps across multiple robotic hands are encoded into a shared latent space. Each latent vector is learned to decode to the 3D shape of an object and the 3D shape of a robotic hand in a grasping pose in terms of the signed distance functions of the two 3D shapes. In addition, the distance metric in the latent space is learned to preserve the similarity between grasps across different robotic hands, where the similarity of grasps is defined according to contact regions of the robotic hands. This property enables our method to transfer grasps between different grippers including a human hand, and grasp transfer has the potential to share grasping skills between robots and enable robots to learn grasping skills from humans. Furthermore, the encoded signed distance functions of objects and grasps in our implicit representation can be used for 6D object pose estimation with grasping contact optimization from partial point clouds, which enables robotic grasping in the real world.</td>
                <td><a href="https://proceedings.mlr.press/v205/khargonkar23a.html">https://proceedings.mlr.press/v205/khargonkar23a.html</a></td>
            </tr>
        
            <tr>
                <td>Frame Mining: a Free Lunch for Learning Robotic Manipulation from 3D Point Clouds</td>
                <td>M, i, n, g, h, u, a,  , L, i, u, ,,  , X, u, a, n, l, i, n,  , L, i, ,,  , Z, h, a, n,  , L, i, n, g, ,,  , Y, a, n, g, y, a, n,  , L, i, ,,  , H, a, o,  , S, u</td>
                <td>corl2022</td>
                <td> We study how choices of input point cloud coordinate frames impact learning of manipulation skills from 3D point clouds. There exist a variety of coordinate frame choices to normalize captured robot-object-interaction point clouds. We find that different frames have a profound effect on agent learning performance, and the trend is similar across 3D backbone networks. In particular, the end-effector frame and the target-part frame achieve higher training efficiency than the commonly used world frame and robot-base frame in many tasks, intuitively because they provide helpful alignments among point clouds across time steps and thus can simplify visual module learning. Moreover, the well-performing frames vary across tasks, and some tasks may benefit from multiple frame candidates. We thus propose FrameMiners to adaptively select candidate frames and fuse their merits in a task-agnostic manner. Experimentally, FrameMiners achieves on-par or significantly higher performance than the best single-frame version on five fully physical manipulation tasks adapted from ManiSkill and OCRTOC. Without changing existing camera placements or adding extra cameras, point cloud frame mining can serve as a free lunch to improve 3D manipulation learning.</td>
                <td><a href="https://proceedings.mlr.press/v205/liu23d.html">https://proceedings.mlr.press/v205/liu23d.html</a></td>
            </tr>
        
            <tr>
                <td>SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation</td>
                <td>Y, i,  , W, e, i, ,,  , L, i, n, q, i, n, g,  , Z, h, a, o, ,,  , W, e, n, z, h, a, o,  , Z, h, e, n, g, ,,  , Z, h, e, n, g,  , Z, h, u, ,,  , Y, o, n, g, m, i, n, g,  , R, a, o, ,,  , G, u, a, n,  , H, u, a, n, g, ,,  , J, i, w, e, n,  , L, u, ,,  , J, i, e,  , Z, h, o, u</td>
                <td>corl2022</td>
                <td>Depth estimation from images serves as the fundamental step of 3D perception for autonomous driving and is an economical alternative to expensive depth sensors like LiDAR. The temporal photometric consistency enables self-supervised depth estimation without labels, further facilitating its application. However, most existing methods predict the depth solely based on each monocular image and ignore the correlations among multiple surrounding cameras, which are typically available for modern self-driving vehicles. In this paper, we propose a SurroundDepth method to incorporate the information from multiple surrounding views to predict depth maps across cameras. Specifically, we employ a joint network to process all the surrounding views and propose a cross-view transformer to effectively fuse the information from multiple views. We apply cross-view self-attention to efficiently enable the global interactions between multi-camera feature maps. Different from self-supervised monocular depth estimation, we are able to predict real-world scales given multi-camera extrinsic matrices. To achieve this goal, we adopt two-frame structure-from-motion to extract scale-aware pseudo depths to pretrain the models. Further, instead of predicting the ego-motion of each individual camera, we estimate a universal ego-motion of the vehicle and transfer it to each view to achieve multi-view consistency. In experiments, our method achieves the state-of-the-art performance on the challenging multi-camera depth estimation datasets DDAD and nuScenes. </td>
                <td><a href="https://proceedings.mlr.press/v205/wei23a.html">https://proceedings.mlr.press/v205/wei23a.html</a></td>
            </tr>
        
            <tr>
                <td>One-Shot Transfer of Affordance Regions? AffCorrs!</td>
                <td>D, e, n, i, s,  , H, a, d, j, i, v, e, l, i, c, h, k, o, v, ,,  , S, i, c, e, l, u, k, w, a, n, d, a,  , Z, w, a, n, e, ,,  , L, o, u, r, d, e, s,  , A, g, a, p, i, t, o, ,,  , M, a, r, c,  , P, e, t, e, r,  , D, e, i, s, e, n, r, o, t, h, ,,  , D, i, m, i, t, r, i, o, s,  , K, a, n, o, u, l, a, s</td>
                <td>corl2022</td>
                <td>In this work, we tackle one-shot visual search of object parts.  Given a single reference image of an object with annotated affordance regions, we segment semantically corresponding parts within a target scene.  We propose AffCorrs, an unsupervised model that combines the properties of pre-trained DINO-ViT’s image descriptors and cyclic correspondences.  We use AffCorrs to find corresponding affordances both for intra- and inter-class one-shot part segmentation. This task is more difficult than supervised alternatives, but enables future work such as learning affordances via imitation and assisted teleoperation.</td>
                <td><a href="https://proceedings.mlr.press/v205/hadjivelichkov23a.html">https://proceedings.mlr.press/v205/hadjivelichkov23a.html</a></td>
            </tr>
        
            <tr>
                <td>Lidar Line Selection with Spatially-Aware Shapley Value for Cost-Efficient Depth Completion</td>
                <td>K, a, m, i, l,  , A, d, a, m, c, z, e, w, s, k, i, ,,  , C, h, r, i, s, t, o, s,  , S, a, k, a, r, i, d, i, s, ,,  , V, a, i, s, h, a, k, h,  , P, a, t, i, l, ,,  , L, u, c,  , V, a, n,  , G, o, o, l</td>
                <td>corl2022</td>
                <td>Lidar is a vital sensor for estimating the depth of a scene. Typical spinning lidars emit pulses arranged in several horizontal lines and the monetary cost of the sensor increases with the number of these lines. In this work, we present the new problem of optimizing the positioning of lidar lines to find the most effective configuration for the depth completion task. We propose a solution to reduce the number of lines while retaining the up-to-the-mark quality of depth completion. Our method consists of two components, (1) line selection based on the marginal contribution of a line computed via the Shapley value and (2) incorporating line position spread to take into account its need to arrive at image-wide depth completion. Spatially-aware Shapley values (SaS) succeed in selecting line subsets that yield a depth accuracy comparable to the full lidar input while using just half of the lines.</td>
                <td><a href="https://proceedings.mlr.press/v205/adamczewski23a.html">https://proceedings.mlr.press/v205/adamczewski23a.html</a></td>
            </tr>
        
            <tr>
                <td>Iterative Interactive Modeling for Knotting Plastic Bags</td>
                <td>C, h, o, n, g, k, a, i,  , G, a, o, ,,  , Z, e, k, u, n,  , L, i, ,,  , H, a, i, c, h, u, a, n,  , G, a, o, ,,  , F, e, n, g,  , C, h, e, n</td>
                <td>corl2022</td>
                <td>Deformable object manipulation has great research significance for the robotic community and numerous applications in daily life. In this work, we study how to knot plastic bags that are randomly dropped from the air with a dual-arm robot based on image input. The complex initial configuration and terrible material properties of plastic bags pose challenges to reliable perception and planning. Directly knotting it from random initial states is difficult. To tackle this problem, we propose Iterative Interactive Modeling (IIM) to first adjust the plastic bag to a standing pose with imitation learning to establish a high-confidence keypoint skeleton model, then perform a set of learned motion primitives to knot it. We leverage spatial action maps to accomplish the iterative pick-and-place action and a graph convolutional network to evaluate the adjusted pose during the IIM process. In experiments, we achieve an 85.0% success rate in knotting 4 different plastic bags, including one with no demonstration.</td>
                <td><a href="https://proceedings.mlr.press/v205/gao23a.html">https://proceedings.mlr.press/v205/gao23a.html</a></td>
            </tr>
        
            <tr>
                <td>Tailoring Visual Object Representations to Human Requirements: A Case Study with a Recycling Robot</td>
                <td>D, e, b, a, s, m, i, t, a,  , G, h, o, s, e, ,,  , M, i, c, h, a, l,  , A, d, a, m,  , L, e, w, k, o, w, i, c, z, ,,  , K, a, l, e, b,  , G, e, z, a, h, e, g, n, ,,  , J, u, l, i, a, n,  , L, e, e, ,,  , T, i, m, o, t, h, y,  , A, d, a, m, s, o, n, ,,  , M, a, r, y, n, e, l,  , V, a, z, q, u, e, z, ,,  , B, r, i, a, n,  , S, c, a, s, s, e, l, l, a, t, i</td>
                <td>corl2022</td>
                <td>Robots are well-suited to alleviate the burden of repetitive and tedious manipulation tasks. In many applications though, a robot may be asked to interact with a wide variety of objects, making it hard or even impossible to pre-program visual object classifiers suitable for the task of interest. In this work, we study the problem of learning a classifier for visual objects based on a few examples provided by humans. We frame this problem from the perspective of learning a suitable visual object representation that allows us to distinguish the desired object category from others. Our proposed approach integrates human supervision into the representation learning process by combining contrastive learning with an additional loss function that brings the representations of human examples close to each other in the latent space. Our experiments show that our proposed method performs better than self-supervised and fully supervised learning methods in offline evaluations and can also be used in real-time by a robot in a simplified recycling domain, where recycling streams contain a variety of objects.</td>
                <td><a href="https://proceedings.mlr.press/v205/ghose23a.html">https://proceedings.mlr.press/v205/ghose23a.html</a></td>
            </tr>
        
            <tr>
                <td>DexPoint: Generalizable Point Cloud Reinforcement Learning for Sim-to-Real Dexterous Manipulation</td>
                <td>Y, u, z, h, e,  , Q, i, n, ,,  , B, i, n, g, h, a, o,  , H, u, a, n, g, ,,  , Z, h, a, o, -, H, e, n, g,  , Y, i, n, ,,  , H, a, o,  , S, u, ,,  , X, i, a, o, l, o, n, g,  , W, a, n, g</td>
                <td>corl2022</td>
                <td>We propose a sim-to-real framework for dexterous manipulation which can generalize to new objects of the same category in the real world. The key of our framework is to train the manipulation policy with point cloud inputs and dexterous hands. We propose two new techniques to enable joint learning on multiple objects and sim-to-real generalization: (i) using imagined hand point clouds as augmented inputs; and (ii) designing novel contact-based rewards. We empirically evaluate our method using an Allegro Hand to grasp novel objects in both simulation and real world. To the best of our knowledge, this is the first policy learning-based framework that achieves such generalization results with dexterous hands. Our project page is available at https://yzqin.github.io/dexpoint.</td>
                <td><a href="https://proceedings.mlr.press/v205/qin23a.html">https://proceedings.mlr.press/v205/qin23a.html</a></td>
            </tr>
        
            <tr>
                <td>Interpretable Self-Aware Neural Networks for Robust Trajectory Prediction</td>
                <td>M, a, s, h, a,  , I, t, k, i, n, a, ,,  , M, y, k, e, l,  , K, o, c, h, e, n, d, e, r, f, e, r</td>
                <td>corl2022</td>
                <td>Although neural networks have seen tremendous success as predictive models in a variety of domains, they can be overly confident in their predictions on out-of-distribution (OOD) data. To be viable for safety-critical applications, like autonomous vehicles, neural networks must accurately estimate their epistemic or model uncertainty, achieving a level of system self-awareness. Techniques for epistemic uncertainty quantification often require OOD data during training or multiple neural network forward passes during inference. These approaches may not be suitable for real-time performance on high-dimensional inputs. Furthermore, existing methods lack interpretability of the estimated uncertainty, which limits their usefulness both to engineers for further system development and to downstream modules in the autonomy stack. We propose the use of evidential deep learning to estimate the epistemic uncertainty over a low-dimensional, interpretable latent space in a trajectory prediction setting. We introduce an interpretable paradigm for trajectory prediction that distributes the uncertainty among the semantic concepts: past agent behavior, road structure, and social context. We validate our approach on real-world autonomous driving data, demonstrating superior performance over state-of-the-art baselines.</td>
                <td><a href="https://proceedings.mlr.press/v205/itkina23a.html">https://proceedings.mlr.press/v205/itkina23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Generalizable Dexterous Manipulation from Human Grasp Affordance</td>
                <td>Y, u, e, h, -, H, u, a,  , W, u, ,,  , J, i, a, s, h, u, n,  , W, a, n, g, ,,  , X, i, a, o, l, o, n, g,  , W, a, n, g</td>
                <td>corl2022</td>
                <td>Dexterous manipulation with a multi-finger hand is one of the most challenging problems in robotics. While recent progress in imitation learning has largely improved the sample efficiency compared to Reinforcement Learning, the learned policy can hardly generalize to manipulate novel objects, given limited expert demonstrations. In this paper, we propose to learn dexterous manipulation using large-scale demonstrations with diverse 3D objects in a category, which are generated from a human grasp affordance model. This generalizes the policy to novel object instances within the same category. To train the policy, we propose a novel imitation learning objective jointly with a geometric representation learning objective using our demonstrations. By experimenting with relocating diverse objects in simulation, we show that our approach outperforms baselines with a large margin when manipulating novel objects. We also ablate the importance of 3D object representation learning for manipulation. We include videos and code on the project website: https://kristery.github.io/ILAD/ .</td>
                <td><a href="https://proceedings.mlr.press/v205/wu23a.html">https://proceedings.mlr.press/v205/wu23a.html</a></td>
            </tr>
        
            <tr>
                <td>CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation</td>
                <td>J, i, n, g, k, a, n, g,  , W, a, n, g, ,,  , S, i, v, a, b, a, l, a, n,  , M, a, n, i, v, a, s, a, g, a, m, ,,  , Y, u, n,  , C, h, e, n, ,,  , Z, e,  , Y, a, n, g, ,,  , I, o, a, n,  , A, n, d, r, e, i,  , B, â, r, s, a, n, ,,  , A, n, q, i,  , J, o, y, c, e,  , Y, a, n, g, ,,  , W, e, i, -, C, h, i, u,  , M, a, ,,  , R, a, q, u, e, l,  , U, r, t, a, s, u, n</td>
                <td>corl2022</td>
                <td>Realistic simulation is key to enabling safe and scalable development of self-driving vehicles. A core component is simulating the sensors so that the entire autonomy system can be tested in simulation. Sensor simulation involves modeling traffic participants, such as vehicles, with high-quality appearance and articulated geometry, and rendering them in real-time. The self-driving industry has employed artists to build these assets. However, this is expensive, slow, and may not reflect reality. Instead, reconstructing assets automatically from sensor data collected in the wild would provide a better path to generating a diverse and large set that provides good real-world coverage. However, current reconstruction approaches struggle on in-the-wild sensor data, due to its sparsity and noise. To tackle these issues, we present CADSim which combines part-aware object-class priors via a small set of CAD models with differentiable rendering to automatically reconstruct vehicle geometry, including articulated wheels, with high-quality appearance. Our experiments show our approach recovers more accurate shape from sparse data compared to existing approaches. Importantly, it also trains and renders efficiently. We demonstrate our reconstructed vehicles in a wide range of applications, including accurate testing of autonomy perception systems.</td>
                <td><a href="https://proceedings.mlr.press/v205/wang23b.html">https://proceedings.mlr.press/v205/wang23b.html</a></td>
            </tr>
        
            <tr>
                <td>Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models</td>
                <td>H, u, y,  , H, a, ,,  , S, h, u, r, a, n,  , S, o, n, g</td>
                <td>corl2022</td>
                <td> We study open-world 3D scene understanding, a family of tasks that require agents to reason about their 3D environment with an open-set vocabulary and out-of-domain visual inputs – a critical skill for robots to operate in the unstructured 3D world. Towards this end, we propose Semantic Abstraction (SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D spatial capabilities, while maintaining their zero-shot robustness. We achieve this abstraction using relevancy maps extracted from CLIP and learn 3D spatial and geometric reasoning skills on top of those abstractions in a semantic-agnostic manner. We demonstrate the usefulness of SemAbs on two open-world 3D scene understanding tasks: 1) completing partially observed objects and 2) localizing hidden objects from language descriptions. Experiments show that SemAbs can generalize to novel vocabulary, materials/lighting, classes, and domains (i.e., real-world scans) from training on limited 3D synthetic data. </td>
                <td><a href="https://proceedings.mlr.press/v205/ha23a.html">https://proceedings.mlr.press/v205/ha23a.html</a></td>
            </tr>
        
            <tr>
                <td>VideoDex: Learning Dexterity from Internet Videos</td>
                <td>K, e, n, n, e, t, h,  , S, h, a, w, ,,  , S, h, i, k, h, a, r,  , B, a, h, l, ,,  , D, e, e, p, a, k,  , P, a, t, h, a, k</td>
                <td>corl2022</td>
                <td>To build general robotic agents that can operate in many environments, it is often imperative for the robot to collect experience in the real world.  However, this is often not feasible due to safety, time and hardware restrictions.  We thus propose leveraging the next best thing as real world experience: internet videos of humans using their hands.  Visual priors, such as visual features, are often learned from videos, but we believe that more information from videos can be utilized as a stronger prior.  We build a learning algorithm, Videodex, that leverages visual, action and physical priors from human video datasets to guide robot behavior.  These action and physical priors in the neural network dictate the typical human behavior for a particular robot task.   We test our approach on a robot arm and dexterous hand based system and show strong results on many different manipulation tasks, outperforming various state-of-the-art methods. For videos and supplemental material visit our website at https://video-dex.github.io.</td>
                <td><a href="https://proceedings.mlr.press/v205/shaw23a.html">https://proceedings.mlr.press/v205/shaw23a.html</a></td>
            </tr>
        
            <tr>
                <td>Last-Mile Embodied Visual Navigation</td>
                <td>J, u, s, t, i, n,  , W, a, s, s, e, r, m, a, n, ,,  , K, a, r, m, e, s, h,  , Y, a, d, a, v, ,,  , G, i, r, i, s, h,  , C, h, o, w, d, h, a, r, y, ,,  , A, b, h, i, n, a, v,  , G, u, p, t, a, ,,  , U, n, n, a, t,  , J, a, i, n</td>
                <td>corl2022</td>
                <td>Realistic long-horizon tasks like image-goal navigation involve exploratory and exploitative phases. Assigned with an image of the goal, an embodied agent must explore to discover the goal, i.e., search efficiently using learned priors. Once the goal is discovered, the agent must accurately calibrate the last-mile of navigation to the goal. As with any robust system, switches between exploratory goal discovery and exploitative last-mile navigation enable better recovery from errors. Following these intuitive guide rails, we propose SLING to improve the performance of existing image-goal navigation systems. Entirely complementing prior methods, we focus on last-mile navigation and leverage the underlying geometric structure of the problem with neural descriptors. With simple but effective switches, we can easily connect SLING with heuristic, reinforcement learning, and neural modular policies. On a standardized image-goal navigation benchmark (Hahn et al. 2021), we improve performance across policies, scenes, and episode complexity, raising the state-of-the-art from 45% to 55% success rate. Beyond photorealistic simulation, we conduct real-robot experiments in three physical scenes and find these improvements to transfer well to real environments.</td>
                <td><a href="https://proceedings.mlr.press/v205/wasserman23a.html">https://proceedings.mlr.press/v205/wasserman23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Preconditions of Hybrid Force-Velocity Controllers for Contact-Rich Manipulation</td>
                <td>J, a, c, k, y,  , L, i, a, n, g, ,,  , X, i, a, n, y, i,  , C, h, e, n, g, ,,  , O, l, i, v, e, r,  , K, r, o, e, m, e, r</td>
                <td>corl2022</td>
                <td>Robots need to manipulate objects in constrained environments like shelves and cabinets when assisting humans in everyday settings like homes and offices. These constraints make manipulation difficult by reducing grasp accessibility, so robots need to use non-prehensile strategies that leverage object-environment contacts to perform manipulation tasks. To tackle the challenge of planning and controlling contact-rich behaviors in such settings, this work uses Hybrid Force-Velocity Controllers (HFVCs) as the skill representation and plans skill sequences with learned preconditions. While HFVCs naturally enable robust and compliant contact-rich behaviors, solvers that synthesize them have traditionally relied on precise object models and closed-loop feedback on object pose, which are difficult to obtain in constrained environments due to occlusions. We first relax HFVCs’ need for precise models and feedback with our HFVC synthesis framework, then learn a point-cloud-based precondition function to classify where HFVC executions will still be successful despite modeling inaccuracies. Finally, we use the learned precondition in a search-based task planner to complete contact-rich manipulation tasks in a shelf domain. Our method achieves a task success rate of $73.2%$, outperforming the $51.5%$ achieved by a baseline without the learned precondition. While the precondition function is trained in simulation, it can also transfer to a real-world setup without further fine-tuning. See supplementary materials and videos at https://sites.google.com/view/constrained-manipulation/. </td>
                <td><a href="https://proceedings.mlr.press/v205/liang23a.html">https://proceedings.mlr.press/v205/liang23a.html</a></td>
            </tr>
        
            <tr>
                <td>Cross-Domain Transfer via Semantic Skill Imitation</td>
                <td>K, a, r, l,  , P, e, r, t, s, c, h, ,,  , R, u, t, a,  , D, e, s, a, i, ,,  , V, i, k, a, s, h,  , K, u, m, a, r, ,,  , F, r, a, n, z, i, s, k, a,  , M, e, i, e, r, ,,  , J, o, s, e, p, h,  , J,  , L, i, m, ,,  , D, h, r, u, v,  , B, a, t, r, a, ,,  , A, k, s, h, a, r, a,  , R, a, i</td>
                <td>corl2022</td>
                <td>We propose an approach for semantic imitation, which uses demonstrations from a source domain, e.g. human videos, to accelerate reinforcement learning (RL) in a different target domain, e.g. a robotic manipulator in a simulated kitchen. Instead of imitating low-level actions like joint velocities, our approach imitates the sequence of demonstrated semantic skills like "opening the microwave" or "turning on the stove". This allows us to transfer demonstrations across environments (e.g. real-world to simulated kitchen) and agent embodiments (e.g. bimanual human demonstration to robotic arm).  We evaluate on three challenging cross-domain learning problems and match the performance of demonstration-accelerated RL approaches that require in-domain demonstrations. In a simulated kitchen environment, our approach learns long-horizon robot manipulation tasks, using less than 3 minutes of human video demonstrations from a real-world kitchen. This enables scaling robot learning via the reuse of demonstrations, e.g. collected as human videos, for learning in any number of target domains.</td>
                <td><a href="https://proceedings.mlr.press/v205/pertsch23a.html">https://proceedings.mlr.press/v205/pertsch23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Neuro-Symbolic Skills for Bilevel Planning</td>
                <td>T, o, m,  , S, i, l, v, e, r, ,,  , A, s, h, a, y,  , A, t, h, a, l, y, e, ,,  , J, o, s, h, u, a,  , B, .,  , T, e, n, e, n, b, a, u, m, ,,  , T, o, m, á, s,  , L, o, z, a, n, o, -, P, é, r, e, z, ,,  , L, e, s, l, i, e,  , P, a, c, k,  , K, a, e, l, b, l, i, n, g</td>
                <td>corl2022</td>
                <td>Decision-making is challenging in robotics environments with continuous object-centric states, continuous actions, long horizons, and sparse feedback. Hierarchical approaches, such as task and motion planning (TAMP), address these challenges by decomposing decision-making into two or more levels of abstraction. In a setting where demonstrations and symbolic predicates are given, prior work has shown how to learn symbolic operators and neural samplers for TAMP with manually designed parameterized policies. Our main contribution is a method for learning parameterized polices in combination with operators and samplers. These components are packaged into modular neuro-symbolic skills and sequenced together with search-then-sample TAMP to solve new tasks. In experiments in four robotics domains, we show that our approach — bilevel planning with neuro-symbolic skills — can solve a wide range of tasks with varying initial states, goals, and objects, outperforming six baselines and ablations.</td>
                <td><a href="https://proceedings.mlr.press/v205/silver23a.html">https://proceedings.mlr.press/v205/silver23a.html</a></td>
            </tr>
        
            <tr>
                <td>MegaPose: 6D Pose Estimation of Novel Objects via Render & Compare</td>
                <td>Y, a, n, n,  , L, a, b, b, é, ,,  , L, u, c, a, s,  , M, a, n, u, e, l, l, i, ,,  , A, r, s, a, l, a, n,  , M, o, u, s, a, v, i, a, n, ,,  , S, t, e, p, h, e, n,  , T, y, r, e, e, ,,  , S, t, a, n,  , B, i, r, c, h, f, i, e, l, d, ,,  , J, o, n, a, t, h, a, n,  , T, r, e, m, b, l, a, y, ,,  , J, u, s, t, i, n,  , C, a, r, p, e, n, t, i, e, r, ,,  , M, a, t, h, i, e, u,  , A, u, b, r, y, ,,  , D, i, e, t, e, r,  , F, o, x, ,,  , J, o, s, e, f,  , S, i, v, i, c</td>
                <td>corl2022</td>
                <td>We introduce MegaPose, a method to estimate the 6D pose of novel objects, that is, objects unseen during training. At inference time, the method only assumes knowledge of (i) a region of interest displaying the object in the image and (ii) a CAD model of the observed object. The contributions of this work are threefold. First, we present a 6D pose refiner based on a render&compare strategy which can be applied to novel objects. The shape and coordinate system of the novel object are provided as inputs to the network by rendering multiple synthetic views of the object’s CAD model. Second, we introduce a novel approach for coarse pose estimation which leverages a network trained to classify whether the pose error between a synthetic rendering and an observed image of the same object can be corrected by the refiner. Third, we introduce a large-scale synthetic dataset of photorealistic images of thousands of objects with diverse visual and shape properties and show that this diversity is crucial to obtain good generalization performance on novel objects. We train our approach on this large synthetic dataset and apply it without retraining to hundreds of novel objects in real images from several pose estimation benchmarks. Our approach achieves state-of-the-art performance on the ModelNet and YCB-Video datasets. An extensive evaluation on the 7 core datasets of the BOP challenge demonstrates that our approach achieves performance competitive with existing approaches that require access to the target objects during training. Code, dataset and trained models are available on the project page: https://megapose6d.github.io/.</td>
                <td><a href="https://proceedings.mlr.press/v205/labbe23a.html">https://proceedings.mlr.press/v205/labbe23a.html</a></td>
            </tr>
        
            <tr>
                <td>Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer</td>
                <td>H, a, o,  , S, h, a, o, ,,  , L, e, t, i, a, n,  , W, a, n, g, ,,  , R, u, o, b, i, n, g,  , C, h, e, n, ,,  , H, o, n, g, s, h, e, n, g,  , L, i, ,,  , Y, u,  , L, i, u</td>
                <td>corl2022</td>
                <td>Large-scale deployment of autonomous vehicles has been continually delayed due to safety concerns. On the one hand, comprehensive scene understanding is indispensable, a lack of which would result in vulnerability to rare but complex traffic situations, such as the sudden emergence of unknown objects. However, reasoning from a global context requires access to sensors of multiple types and adequate fusion of multi-modal sensor signals, which is difficult to achieve. On the other hand, the lack of interpretability in learning models also hampers the safety with unverifiable failure causes. In this paper, we propose a safety-enhanced autonomous driving framework, named Interpretable Sensor Fusion Transformer (InterFuser), to fully process and fuse information from multi-modal multi-view sensors for achieving comprehensive scene understanding and adversarial event detection. Besides, intermediate interpretable features are generated from our framework, which provide more semantics and are exploited to better constrain actions to be within the safe sets. We conducted extensive experiments on CARLA benchmarks, where our model outperforms prior methods, ranking the first on the public CARLA Leaderboard.</td>
                <td><a href="https://proceedings.mlr.press/v205/shao23a.html">https://proceedings.mlr.press/v205/shao23a.html</a></td>
            </tr>
        
            <tr>
                <td>A Dual Representation Framework for Robot Learning with Human Guidance</td>
                <td>R, u, o, h, a, n,  , Z, h, a, n, g, ,,  , D, h, r, u, v, a,  , B, a, n, s, a, l, ,,  , Y, i, l, u, n,  , H, a, o, ,,  , A, y, a, n, o,  , H, i, r, a, n, a, k, a, ,,  , J, i, a, l, u,  , G, a, o, ,,  , C, h, e, n,  , W, a, n, g, ,,  , R, o, b, e, r, t, o,  , M, a, r, t, í, n, -, M, a, r, t, í, n, ,,  , L, i,  , F, e, i, -, F, e, i, ,,  , J, i, a, j, u, n,  , W, u</td>
                <td>corl2022</td>
                <td>The ability to interactively learn skills from human guidance and adjust behavior according to human preference is crucial to accelerating robot learning. But human guidance is an expensive resource, calling for methods that can learn efficiently. In this work, we argue that learning is more efficient if the agent is equipped with a high-level, symbolic representation. We propose a dual representation framework for robot learning from human guidance. The dual representation used by the robotic agent includes one for learning a sensorimotor control policy, and the other, in the form of a symbolic scene graph, for encoding the task-relevant information that motivates human input. We propose two novel learning algorithms based on this framework for learning from human evaluative feedback and from preference. In five continuous control tasks in simulation and in the real world, we demonstrate that our algorithms lead to significant improvement in task performance and learning speed. Additionally, these algorithms require less human effort and are qualitatively preferred by users.</td>
                <td><a href="https://proceedings.mlr.press/v205/zhang23a.html">https://proceedings.mlr.press/v205/zhang23a.html</a></td>
            </tr>
        
            <tr>
                <td>Proactive slip control by learned slip model and trajectory adaptation</td>
                <td>K, i, y, a, n, o, u, s, h,  , N, a, z, a, r, i, ,,  , W, i, l, l, o, w,  , M, a, n, d, i, l, ,,  , A, m, i, r,  , M, a, s, o, u, d,  , G, h, a, l, a, m, z, a, n,  , E, s, f, a, h, a, n, i</td>
                <td>corl2022</td>
                <td>This paper presents a novel control approach to dealing with a slip during robotic manipulative movements. Slip is a major cause of failure in many robotic grasping and manipulation tasks. Existing works use increased gripping forces to avoid/control slip. However, this may not be feasible, e.g., because (i) the robot cannot increase the gripping force– the max gripping force has already applied or (ii) an increased force yields a damaged grasped object, such as soft fruit. Moreover, the robot fixes the gripping force when it forms a stable grasp on the surface of an object, and changing the gripping force during manipulative movements in real-time may not be feasible, e.g., with the Franka robot. Hence, controlling the slip by changing gripping forces is not an effective control policy in many settings. We propose a novel control approach to slip avoidance including a learned action-conditioned slip predictor and a constrained optimizer avoiding an expected slip given the desired robot actions. We show the effectiveness of this receding horizon controller in a series of test cases in real robot experimentation. Our experimental results show our proposed data-driven predictive controller can control slip for objects unseen in training.  </td>
                <td><a href="https://proceedings.mlr.press/v205/nazari23a.html">https://proceedings.mlr.press/v205/nazari23a.html</a></td>
            </tr>
        
            <tr>
                <td>Transferring Hierarchical Structures with Dual Meta Imitation Learning</td>
                <td>C, h, o, n, g, k, a, i,  , G, a, o, ,,  , Y, i, z, h, o, u,  , J, i, a, n, g, ,,  , F, e, n, g,  , C, h, e, n</td>
                <td>corl2022</td>
                <td>Hierarchical Imitation Learning (HIL) is an effective way for robots to learn sub-skills from long-horizon unsegmented demonstrations. However, the learned hierarchical structure lacks the mechanism to transfer across multi-tasks or to new tasks, which makes them have to learn from scratch when facing a new situation. Transferring and reorganizing modular sub-skills require fast adaptation ability of the whole hierarchical structure. In this work, we propose Dual Meta Imitation Learning (DMIL), a hierarchical meta imitation learning method where the high-level network and sub-skills are iteratively meta-learned with model-agnostic meta-learning. DMIL uses the likelihood of state-action pairs from each sub-skill as the supervision for the high-level network adaptation and uses the adapted high-level network to determine different data set for each sub-skill adaptation. We theoretically prove the convergence of the iterative training process of DMIL and establish the connection between DMIL and Expectation-Maximization algorithm. Empirically, we achieve state-of-the-art few-shot imitation learning performance on the Meta-world benchmark and competitive results on long-horizon tasks in Kitchen environments.</td>
                <td><a href="https://proceedings.mlr.press/v205/gao23b.html">https://proceedings.mlr.press/v205/gao23b.html</a></td>
            </tr>
        
            <tr>
                <td>Motion Style Transfer: Modular Low-Rank Adaptation for Deep Motion Forecasting</td>
                <td>P, a, r, t, h,  , K, o, t, h, a, r, i, ,,  , D, a, n, y, a,  , L, i, ,,  , Y, u, e, j, i, a, n, g,  , L, i, u, ,,  , A, l, e, x, a, n, d, r, e,  , A, l, a, h, i</td>
                <td>corl2022</td>
                <td>Deep motion forecasting models have achieved great success when trained on a massive amount of data. Yet, they often perform poorly when training data is limited. To address this challenge, we propose a transfer learning approach for efficiently adapting pre-trained forecasting models to new domains, such as unseen agent types and scene contexts. Unlike the conventional fine-tuning approach that updates the whole encoder, our main idea is to reduce the amount of tunable parameters that can precisely account for the target domain-specific motion style. To this end, we introduce two components that exploit our prior knowledge of motion style shifts: (i) a low-rank motion style adapter that projects and adjusts the style features at a low-dimensional bottleneck; and (ii) a modular adapter strategy that disentangles the features of scene context and motion history to facilitate a fine-grained choice of adaptation layers. Through extensive experimentation, we show that our proposed adapter design, coined MoSA, outperforms prior methods on several forecasting benchmarks.</td>
                <td><a href="https://proceedings.mlr.press/v205/kothari23a.html">https://proceedings.mlr.press/v205/kothari23a.html</a></td>
            </tr>
        
            <tr>
                <td>Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation</td>
                <td>M, o, h, i, t,  , S, h, r, i, d, h, a, r, ,,  , L, u, c, a, s,  , M, a, n, u, e, l, l, i, ,,  , D, i, e, t, e, r,  , F, o, x</td>
                <td>corl2022</td>
                <td>Transformers have revolutionized vision and natural language processing with their ability to scale with large datasets. But in robotic manipulation, data is both limited and expensive. Can manipulation still benefit from Transformers with the right problem formulation? We investigate this question with PerAct, a language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation. PerAct encodes language goals and RGB-D voxel observations with a Perceiver Transformer, and outputs discretized actions by “detecting the next best voxel action”. Unlike frameworks that operate on 2D images, the voxelized 3D observation and action space provides a strong structural prior for efficiently learning 6-DoF actions. With this formulation, we train a single multi-task Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks (with 18 variations) from just a few demonstrations per task. Our results show that PerAct significantly outperforms unstructured image-to-action agents and 3D ConvNet baselines for a wide range of tabletop tasks.</td>
                <td><a href="https://proceedings.mlr.press/v205/shridhar23a.html">https://proceedings.mlr.press/v205/shridhar23a.html</a></td>
            </tr>
        
            <tr>
                <td>Synthesizing Adversarial Visual Scenarios for Model-Based Robotic Control</td>
                <td>S, h, u, b, h, a, n, k, a, r,  , A, g, a, r, w, a, l, ,,  , S, a, n, d, e, e, p,  , P, .,  , C, h, i, n, c, h, a, l, i</td>
                <td>corl2022</td>
                <td>Today’s robots often interface data-driven perception and planning models with classical model-predictive controllers (MPC). Often, such learned perception/planning models produce erroneous waypoint predictions on out-of-distribution (OoD) or even adversarial visual inputs, which increase control cost. However, today’s methods to train robust perception models are largely task-agnostic – they augment a dataset using random image transformations or adversarial examples targeted at the vision model in isolation. As such, they often introduce pixel perturbations that are ultimately benign for control. In contrast to prior work that synthesizes adversarial examples for single-step vision tasks, our key contribution is to synthesize adversarial scenarios tailored to multi-step, model-based control. To do so, we use differentiable MPC methods to calculate the sensitivity of a model-based controller to errors in state estimation. We show that re-training vision models on these adversarial datasets improves control performance on OoD test scenarios by up to 36.2% compared to standard task-agnostic data augmentation. We demonstrate our method on examples of robotic navigation, manipulation in RoboSuite, and control of an autonomous air vehicle.</td>
                <td><a href="https://proceedings.mlr.press/v205/agarwal23b.html">https://proceedings.mlr.press/v205/agarwal23b.html</a></td>
            </tr>
        
            <tr>
                <td>CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation</td>
                <td>W, e, n, h, a, o,  , D, i, n, g, ,,  , H, a, o, h, o, n, g,  , L, i, n, ,,  , B, o,  , L, i, ,,  , D, i, n, g,  , Z, h, a, o</td>
                <td>corl2022</td>
                <td>Generating safety-critical scenarios, which are crucial yet difficult to collect, provides an effective way to evaluate the robustness of autonomous driving systems. However, the diversity of scenarios and efficiency of generation methods are heavily restricted by the rareness and structure of safety-critical scenarios. Therefore, existing generative models that only estimate distributions from observational data are not satisfying to solve this problem. In this paper, we integrate causality as a prior into the scenario generation and propose a flow-based generative framework, Causal Autoregressive Flow (CausalAF). CausalAF encourages the generative model to uncover and follow the causal relationship among generated objects via novel causal masking operations instead of searching the sample only from observational data. By learning the cause-and-effect mechanism of how the generated scenario causes risk situations rather than just learning correlations from data, CausalAF significantly improves learning efficiency. Extensive experiments on three heterogeneous traffic scenarios illustrate that CausalAF requires much fewer optimization resources to effectively generate safety-critical scenarios. We also show that using generated scenarios as additional training samples empirically improves the robustness of autonomous driving algorithms.</td>
                <td><a href="https://proceedings.mlr.press/v205/ding23a.html">https://proceedings.mlr.press/v205/ding23a.html</a></td>
            </tr>
        
            <tr>
                <td>Volumetric-based Contact Point Detection for 7-DoF Grasping</td>
                <td>J, u, n, h, a, o,  , C, a, i, ,,  , J, i, n, g, c, h, e, n, g,  , S, u, ,,  , Z, i, d, a,  , Z, h, o, u, ,,  , H, u, i,  , C, h, e, n, g, ,,  , Q, i, f, e, n, g,  , C, h, e, n, ,,  , M, i, c, h, a, e, l,  , Y,  , W, a, n, g</td>
                <td>corl2022</td>
                <td>In this paper, we propose a novel grasp pipeline based on contact point detection on the truncated signed distance function (TSDF) volume to achieve closed-loop 7-degree-of-freedom (7-DoF) grasping on cluttered environments. The key aspects of our method are that 1) the proposed pipeline exploits the TSDF volume in terms of multi-view fusion, contact-point sampling and evaluation, and collision checking, which provides reliable and collision-free 7-DoF gripper poses with real-time performance; 2) the contact-based pose representation effectively eliminates the ambiguity introduced by the normal-based methods, which provides a more precise and flexible solution. Extensive simulated and real-robot experiments demonstrate that the proposed pipeline can select more antipodal and stable grasp poses and outperforms normal-based baselines in terms of the grasp success rate in both simulated and physical scenarios. Code and data are available at https://github.com/caijunhao/vcpd</td>
                <td><a href="https://proceedings.mlr.press/v205/cai23a.html">https://proceedings.mlr.press/v205/cai23a.html</a></td>
            </tr>
        
            <tr>
                <td>SE(3)-Equivariant Relational Rearrangement with Neural Descriptor Fields</td>
                <td>A, n, t, h, o, n, y,  , S, i, m, e, o, n, o, v, ,,  , Y, i, l, u, n,  , D, u, ,,  , Y, e, n, -, C, h, e, n,  , L, i, n, ,,  , A, l, b, e, r, t, o,  , R, o, d, r, i, g, u, e, z,  , G, a, r, c, i, a, ,,  , L, e, s, l, i, e,  , P, a, c, k,  , K, a, e, l, b, l, i, n, g, ,,  , T, o, m, á, s,  , L, o, z, a, n, o, -, P, é, r, e, z, ,,  , P, u, l, k, i, t,  , A, g, r, a, w, a, l</td>
                <td>corl2022</td>
                <td>We present a framework for specifying tasks involving spatial relations between objects using only 5-10 demonstrations and then executing such tasks given point cloud observations of a novel pair of objects in arbitrary initial poses. Our approach structures these rearrangement tasks by assigning a consistent local coordinate frame to the task-relevant object parts, localizing the corresponding coordinate frame on unseen object instances, and executing an action that brings these frames into alignment. We propose an optimization method that uses multiple Neural Descriptor Fields (NDFs) and a single annotated 3D keypoint to assign a set of consistent coordinate frames to the task-relevant object parts. We also propose an energy-based learning scheme to model the joint configuration of the objects that satisfies a desired relational task. We validate our pipeline on three multi-object rearrangement tasks in simulation and on a real robot. Results show that our method can infer relative transformations that satisfy the desired relation between novel objects in unseen initial poses using just a few demonstrations.</td>
                <td><a href="https://proceedings.mlr.press/v205/simeonov23a.html">https://proceedings.mlr.press/v205/simeonov23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Multi-Objective Curricula for Robotic Policy Learning</td>
                <td>J, i, k, u, n,  , K, a, n, g, ,,  , M, i, a, o,  , L, i, u, ,,  , A, b, h, i, n, a, v,  , G, u, p, t, a, ,,  , C, h, r, i, s, t, o, p, h, e, r,  , P, a, l, ,,  , X, u, e,  , L, i, u, ,,  , J, i, e,  , F, u</td>
                <td>corl2022</td>
                <td>Various automatic curriculum learning (ACL) methods have been proposed to improve the sample efficiency and final performance of robots’ policies learning. They are designed to control how a robotic agent collects data, which is inspired by how humans gradually adapt their learning processes to their capabilities. In this paper, we propose a unified automatic curriculum learning framework to create multi-objective but coherent curricula that are generated by a set of parametric curriculum modules. Each curriculum module is instantiated as a neural network and is responsible for generating a particular curriculum. In order to coordinate those potentially conflicting modules in unified parameter space, we propose a multi-task hyper-net learning framework that uses a single hyper-net to parameterize all those curriculum modules. We evaluate our method on a series of robotic manipulation tasks and demonstrate its superiority over other state-of-the-art ACL methods in terms of sample efficiency and final performance.</td>
                <td><a href="https://proceedings.mlr.press/v205/kang23a.html">https://proceedings.mlr.press/v205/kang23a.html</a></td>
            </tr>
        
            <tr>
                <td>Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation</td>
                <td>J, o, a, n, n, e,  , T, r, u, o, n, g, ,,  , M, a, x,  , R, u, d, o, l, p, h, ,,  , N, a, o, k, i,  , H, a, r, r, i, s, o, n,  , Y, o, k, o, y, a, m, a, ,,  , S, o, n, i, a,  , C, h, e, r, n, o, v, a, ,,  , D, h, r, u, v,  , B, a, t, r, a, ,,  , A, k, s, h, a, r, a,  , R, a, i</td>
                <td>corl2022</td>
                <td>If we want to train robots in simulation before deploying them in reality, it seems natural and almost self-evident to presume that reducing the sim2real gap involves creating simulators of increasing fidelity (since reality is what it is). We challenge this assumption and present a contrary hypothesis – sim2real transfer of robots may be improved with lower (not higher) fidelity simulation. We conduct a systematic large-scale evaluation of this hypothesis on the problem of visual navigation – in the real world, and on 2 different simulators (Habitat and iGibson) using 3 different robots (A1, AlienGo, Spot). Our results show that, contrary to expectation, adding fidelity does not help with learning; performance is poor due to slow simulation speed (preventing large-scale learning) and overfitting to inaccuracies in simulation physics. Instead, building simple models of the robot motion using real-world data can improve learning and generalization.</td>
                <td><a href="https://proceedings.mlr.press/v205/truong23a.html">https://proceedings.mlr.press/v205/truong23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Dense Visual Descriptors using Image Augmentations for Robot Manipulation Tasks</td>
                <td>C, h, r, i, s, t, i, a, n,  , G, r, a, f, ,,  , D, a, v, i, d,  , B, .,  , A, d, r, i, a, n, ,,  , J, o, s, h, u, a,  , W, e, i, l, ,,  , M, i, r, o, s, l, a, v,  , G, a, b, r, i, e, l, ,,  , P, h, i, l, i, p, p,  , S, c, h, i, l, l, i, n, g, e, r, ,,  , M, a, r, k, u, s,  , S, p, i, e, s, ,,  , H, e, i, k, o,  , N, e, u, m, a, n, n, ,,  , A, n, d, r, a, s,  , G, a, b, o, r,  , K, u, p, c, s, i, k</td>
                <td>corl2022</td>
                <td>We propose a self-supervised training approach for learning view-invariant dense visual descriptors using image augmentations. Unlike existing works, which often require complex datasets, such as registered RGBD sequences, we train on an unordered set of RGB images. This allows for learning from a single camera view, e.g., in an existing robotic cell with a fix-mounted camera. We create synthetic views and dense pixel correspondences using data augmentations. We find our descriptors are competitive to the existing methods, despite the simpler data recording and setup requirements. We show that training on synthetic correspondences provides descriptor consistency across a broad range of camera views. We compare against training with geometric correspondence from multiple views and provide ablation studies. We also show a robotic bin-picking experiment using descriptors learned from a fix-mounted camera for defining grasp preferences.</td>
                <td><a href="https://proceedings.mlr.press/v205/graf23a.html">https://proceedings.mlr.press/v205/graf23a.html</a></td>
            </tr>
        
            <tr>
                <td>Proactive Robot Assistance via Spatio-Temporal Object Modeling</td>
                <td>M, a, i, t, h, i, l, i,  , P, a, t, e, l, ,,  , S, o, n, i, a,  , C, h, e, r, n, o, v, a</td>
                <td>corl2022</td>
                <td>Proactive robot assistance enables a robot to anticipate and provide for a user’s needs without being explicitly asked. We formulate proactive assistance as the problem of the robot anticipating temporal patterns of object movements associated with everyday user routines, and proactively assisting the user by placing objects to adapt the environment to their needs. We introduce a generative graph neural network to learn a unified spatio-temporal predictive model of object dynamics from temporal sequences of object arrangements. We additionally contribute the Household Object Movements from Everyday Routines (HOMER) dataset, which tracks household objects associated with human activities of daily living across 50+ days for five simulated households. Our model outperforms the leading baseline in predicting object movement, correctly predicting locations for 11.1% more objects and wrongly predicting locations for 11.5% fewer objects used by the human user.</td>
                <td><a href="https://proceedings.mlr.press/v205/patel23a.html">https://proceedings.mlr.press/v205/patel23a.html</a></td>
            </tr>
        
            <tr>
                <td>R3M: A Universal Visual Representation for Robot Manipulation</td>
                <td>S, u, r, a, j,  , N, a, i, r, ,,  , A, r, a, v, i, n, d,  , R, a, j, e, s, w, a, r, a, n, ,,  , V, i, k, a, s, h,  , K, u, m, a, r, ,,  , C, h, e, l, s, e, a,  , F, i, n, n, ,,  , A, b, h, i, n, a, v,  , G, u, p, t, a</td>
                <td>corl2022</td>
                <td>We study how visual representations pre-trained on diverse human video data can enable data-efficient learning of downstream robotic manipulation tasks. Concretely, we pre-train a visual representation using the Ego4D human video dataset using a combination of time-contrastive learning, video-language alignment, and an L1 penalty to encourage sparse and compact representations. The resulting representation, R3M, can be used as a frozen perception module for downstream policy learning. Across a suite of 12 simulated robot manipulation tasks, we find that R3M improves task success by over 20% compared to training from scratch and by over 10% compared to state-of-the-art visual representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika Panda arm to learn a range of manipulation tasks in a real, cluttered apartment given just 20 demonstrations. </td>
                <td><a href="https://proceedings.mlr.press/v205/nair23a.html">https://proceedings.mlr.press/v205/nair23a.html</a></td>
            </tr>
        
            <tr>
                <td>Towards Capturing the Temporal Dynamics for Trajectory Prediction: a Coarse-to-Fine Approach</td>
                <td>X, i, a, o, s, o, n, g,  , J, i, a, ,,  , L, i,  , C, h, e, n, ,,  , P, e, n, g, h, a, o,  , W, u, ,,  , J, i, a,  , Z, e, n, g, ,,  , J, u, n, c, h, i,  , Y, a, n, ,,  , H, o, n, g, y, a, n, g,  , L, i, ,,  , Y, u,  , Q, i, a, o</td>
                <td>corl2022</td>
                <td> Trajectory prediction is one of the basic tasks in the autonomous driving field, which aims to predict the future position of other agents around the ego vehicle so that a safe yet efficient driving plan could be generated in the downstream module. Recently, deep learning based methods dominate the field. State-of-the-art (SOTA) methods usually follow an encoder-decoder paradigm. Specifically, the encoder is responsible for extracting information from agents’ history states and HD-Map and providing a representation vector for each agent. Taking these vectors as input, the decoder predicts multi-step future positions for each agent, which is usually accomplished by a single multi-layer perceptron (MLP) to directly output a Tx2 tensor. Though models with adoptation of MLP decoder have dominated the leaderboard of multiple datasets, ‘the elephant in the room is that the temporal correlation among future time-steps is ignored since there is no direct relation among output neurons of a MLP. In this work, we examine this design choice and investigate several ways to apply the temporal inductive bias into the generation of future trajectories on top of a SOTA encoder. We find that simply using autoregressive RNN to generate future positions would lead to significant performance drop even with techniques such as history highway and teacher forcing. Instead, taking scratch trajectories generated by MLP as input, an additional refinement module based on structures with temporal prior such as RNN or 1D-CNN could remarkably boost the accuracy. Furthermore, we examine several objective functions to  emphasize the temporal priors. By the combination of aforementioned techniques to introduce the temporal prior, we improve the top-ranked method’s performance by a large margin and achieve SOTA result on the Waymo Open Motion Challenge.</td>
                <td><a href="https://proceedings.mlr.press/v205/jia23a.html">https://proceedings.mlr.press/v205/jia23a.html</a></td>
            </tr>
        
            <tr>
                <td>Human-Robot Commensality: Bite Timing Prediction for Robot-Assisted Feeding in Groups</td>
                <td>J, a, n,  , O, n, d, r, a, s, ,,  , A, b, r, a, r,  , A, n, w, a, r, ,,  , T, o, n, g,  , W, u, ,,  , F, a, n, j, u, n,  , B, u, ,,  , M, a, l, t, e,  , J, u, n, g, ,,  , J, o, r, g, e,  , J, o, s, e,  , O, r, t, i, z, ,,  , T, a, p, o, m, a, y, u, k, h,  , B, h, a, t, t, a, c, h, a, r, j, e, e</td>
                <td>corl2022</td>
                <td>We develop data-driven models to predict when a robot should feed during social dining scenarios. Being able to eat independently with friends and family is considered one of the most memorable and important activities for people with mobility limitations. While existing robotic systems for feeding people with mobility limitations focus on solitary dining, commensality, the act of eating together, is often the practice of choice. Sharing meals with others introduces the problem of socially appropriate bite timing for a robot, i.e. the appropriate timing for the robot to feed without disrupting the social dynamics of a shared meal. Our key insight is that bite timing strategies that take into account the delicate balance of social cues can lead to seamless interactions during robot-assisted feeding in a social dining scenario. We approach this problem by collecting a Human-Human Commensality Dataset (HHCD) containing 30 groups of three people eating together. We use this dataset to analyze human-human commensality behaviors and develop bite timing prediction models in social dining scenarios. We also transfer these models to human-robot commensality scenarios. Our user studies show that prediction improves when our algorithm uses multimodal social signaling cues between diners to model bite timing. The HHCD dataset, videos of user studies, and code are available at https://emprise.cs.cornell.edu/hrcom/</td>
                <td><a href="https://proceedings.mlr.press/v205/ondras23a.html">https://proceedings.mlr.press/v205/ondras23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Control Admissibility Models with Graph Neural Networks for Multi-Agent Navigation</td>
                <td>C, h, e, n, n, i, n, g,  , Y, u, ,,  , H, o, n, g, z, h, a, n,  , Y, u, ,,  , S, i, c, u, n,  , G, a, o</td>
                <td>corl2022</td>
                <td>Deep reinforcement learning in continuous domains focuses on learning control policies that map states to distributions over actions that ideally concentrate on the optimal choices in each step. In multi-agent navigation problems, the optimal actions depend heavily on the agents’ density. Their interaction patterns grow exponentially with respect to such density, making it hard for learning-based methods to generalize. We propose to switch the learning objectives from predicting the optimal actions to predicting sets of admissible actions, which we call control admissibility models (CAMs), such that they can be easily composed and used for online inference for an arbitrary number of agents. We design CAMs using graph neural networks and develop training methods that optimize the CAMs in the standard model-free setting, with the additional benefit of eliminating the need for reward engineering typically required to balance collision avoidance and goal-reaching requirements. We evaluate the proposed approach in multi-agent navigation environments. We show that the CAM models can be trained in environments with only a few agents and be easily composed for deployment in dense environments with hundreds of agents, achieving better performance than state-of-the-art methods. </td>
                <td><a href="https://proceedings.mlr.press/v205/yu23a.html">https://proceedings.mlr.press/v205/yu23a.html</a></td>
            </tr>
        
            <tr>
                <td>Socially-Attentive Policy Optimization in Multi-Agent Self-Driving System</td>
                <td>Z, i, p, e, n, g,  , D, a, i, ,,  , T, i, a, n, z, e,  , Z, h, o, u, ,,  , K, u, n,  , S, h, a, o, ,,  , D, a, v, i, d,  , H, e, n, r, y,  , M, g, u, n, i, ,,  , B, i, n,  , W, a, n, g, ,,  , J, i, a, n, y, e,  , H, A, O</td>
                <td>corl2022</td>
                <td>As increasing numbers of autonomous vehicles (AVs) are being deployed, it is important to construct a multi-agent self-driving (MASD) system for navigating traffic flows of AVs. In an MASD system, AVs not only navigate themselves to pursue their own goals, but also interact with each other to prevent congestion or collision, especially in scenarios like intersection or lane merging. Multi-agent reinforcement learning (MARL) provides an appealing alternative to generate safe and efficient actions for multiple AVs. However, current MARL methods are limited to describe scenarios where agents interact in either a cooperative of competitive fashion within one episode. Ordinarily, the agents’ objectives are defined with a global or team reward function, which fail to deal with the dynamic social preferences (SPs) and mixed motives like human drivers in traffic interactions. To this end, we propose a novel MARL method called Socially-Attentive Policy Optimization (SAPO), which incorporates: (a) a self-attention module to select the most interactive traffic participant for each AV, and (b) a social-aware integration mechanism to integrate objectives of interacting AVs by estimating the dynamic social preferences from their observations. SAPO solves the problem of how to improve the safety and efficiency of MASD systems, by enabling AVs to learn socially-compatible behaviors. Simulation experiments show that SAPO can successfully capture and utilize the variation of the SPs of AVs to achieve superior performance, compared with baselines in MASD scenarios.</td>
                <td><a href="https://proceedings.mlr.press/v205/dai23a.html">https://proceedings.mlr.press/v205/dai23a.html</a></td>
            </tr>
        
            <tr>
                <td>Reciprocal MIND MELD: Improving Learning From Demonstration via Personalized, Reciprocal Teaching</td>
                <td>M, a, r, i, a, h,  , L,  , S, c, h, r, u, m, ,,  , E, r, i, n,  , H, e, d, l, u, n, d, -, B, o, t, t, i, ,,  , M, a, t, t, h, e, w,  , G, o, m, b, o, l, a, y</td>
                <td>corl2022</td>
                <td>Endowing robots with the ability to learn novel tasks via demonstrations will increase the accessibility of robots for non-expert, non-roboticists. However, research has shown that humans can be poor teachers, making it difficult for robots to effectively learn from humans. If the robot could instruct humans how to provide better demonstrations, then humans might be able to effectively teach a broader range of novel, out-of-distribution tasks. In this work, we introduce Reciprocal MIND MELD, a framework in which the robot learns the way in which a demonstrator is suboptimal and utilizes this information to provide feedback to the demonstrator to improve upon their demonstrations. We additionally develop an Embedding Predictor Network which learns to predict the demonstrator’s suboptimality online without the need for optimal labels. In a series of human-subject experiments in a driving simulator domain, we demonstrate that robotic feedback can effectively improve human demonstrations in two dimensions of suboptimality (p < .001) and that robotic feedback translates into better learning outcomes for a robotic agent on novel tasks (p = .045).</td>
                <td><a href="https://proceedings.mlr.press/v205/schrum23a.html">https://proceedings.mlr.press/v205/schrum23a.html</a></td>
            </tr>
        
            <tr>
                <td>Motion Policy Networks</td>
                <td>A, d, a, m,  , F, i, s, h, m, a, n, ,,  , A, d, i, t, h, y, a, v, a, i, r, a, v, a, n,  , M, u, r, a, l, i, ,,  , C, l, e, m, e, n, s,  , E, p, p, n, e, r, ,,  , B, r, y, a, n,  , P, e, e, l, e, ,,  , B, y, r, o, n,  , B, o, o, t, s, ,,  , D, i, e, t, e, r,  , F, o, x</td>
                <td>corl2022</td>
                <td>Collision-free motion generation in unknown environments is a core building block for robot manipulation. Generating such motions is challenging due to multiple objectives; not only should the solutions be optimal, the motion generator itself must be fast enough for real-time performance and reliable enough for practical deployment. A wide variety of methods have been proposed ranging from local controllers to global planners, often being combined to offset their shortcomings. We present an end-to-end neural model called Motion Policy Networks (M$\pi$Nets) to generate collision-free, smooth motion from just a single depth camera observation. M$\pi$Nets are trained on over 3 million motion planning problems in more than 500,000 environments. Our experiments show that M$\pi$Nets are significantly faster than global planners while exhibiting the reactivity needed to deal with dynamic scenes. They are 46% better than prior neural planners and more robust than local control policies. Despite being only trained in simulation, M$\pi$Nets transfer well to the real robot with noisy partial point clouds. Videos and code are available at https://mpinets.github.io</td>
                <td><a href="https://proceedings.mlr.press/v205/fishman23a.html">https://proceedings.mlr.press/v205/fishman23a.html</a></td>
            </tr>
        
            <tr>
                <td>Decentralized Data Collection for Robotic Fleet Learning: A Game-Theoretic Approach</td>
                <td>O, g, u, z, h, a, n,  , A, k, c, i, n, ,,  , P, o, -, h, a, n,  , L, i, ,,  , S, h, u, b, h, a, n, k, a, r,  , A, g, a, r, w, a, l, ,,  , S, a, n, d, e, e, p,  , P, .,  , C, h, i, n, c, h, a, l, i</td>
                <td>corl2022</td>
                <td>Fleets of networked autonomous vehicles (AVs) collect terabytes of sensory data, which is often transmitted to central servers (the “cloud”) for training machine learning (ML) models. Ideally, these fleets should upload all their data, especially from rare operating contexts, in order to train robust ML models. However, this is infeasible due to prohibitive network bandwidth and data labeling costs. Instead, we propose a cooperative data sampling strategy where geo-distributed AVs collaborate to collect a diverse ML training dataset in the cloud. Since the AVs have a shared objective but minimal information about each other’s local data distribution and perception model, we can naturally cast cooperative data collection as an $N$-player mathematical game. We show that our cooperative sampling strategy uses minimal information to converge to a centralized oracle policy with complete information about all AVs. Moreover, we theoretically characterize the performance benefits of our game-theoretic strategy compared to greedy sampling. Finally, we experimentally demonstrate that our method outperforms standard benchmarks by up to $21.9%$ on 4 perception datasets, including for autonomous driving in adverse weather conditions. Crucially, our experimental results on real-world datasets closely align with our theoretical guarantees.</td>
                <td><a href="https://proceedings.mlr.press/v205/akcin23a.html">https://proceedings.mlr.press/v205/akcin23a.html</a></td>
            </tr>
        
            <tr>
                <td>CoBEVT: Cooperative Bird’s Eye View Semantic Segmentation with Sparse Transformers</td>
                <td>R, u, n, s, h, e, n, g,  , X, u, ,,  , Z, h, e, n, g, z, h, o, n, g,  , T, u, ,,  , H, a, o,  , X, i, a, n, g, ,,  , W, e, i,  , S, h, a, o, ,,  , B, o, l, e, i,  , Z, h, o, u, ,,  , J, i, a, q, i,  , M, a</td>
                <td>corl2022</td>
                <td>Bird’s eye view (BEV) semantic segmentation plays a crucial role in spatial sensing for autonomous driving. Although recent literature has made significant progress on BEV map understanding, they are all based on single-agent camera-based systems. These solutions sometimes have difficulty handling occlusions or detecting distant objects in complex traffic scenes. Vehicle-to-Vehicle (V2V) communication technologies have enabled autonomous vehicles to share sensing information, dramatically improving the perception performance and range compared to single-agent systems. In this paper, we propose CoBEVT, the first generic multi-agent multi-camera perception framework that can cooperatively generate BEV map predictions. To efficiently fuse camera features from multi-view and multi-agent data in an underlying Transformer architecture, we design a fused axial attention module (FAX), which captures sparsely local and global spatial interactions across views and agents. The extensive experiments on the V2V perception dataset, OPV2V, demonstrate that CoBEVT achieves state-of-the-art performance for cooperative BEV semantic segmentation. Moreover, CoBEVT is shown to be generalizable to other tasks, including 1) BEV segmentation with single-agent multi-camera and 2) 3D object detection with multi-agent LiDAR systems, achieving state-of-the-art performance with real-time inference speed. The code is available at https://github.com/DerrickXuNu/CoBEVT.</td>
                <td><a href="https://proceedings.mlr.press/v205/xu23a.html">https://proceedings.mlr.press/v205/xu23a.html</a></td>
            </tr>
        
            <tr>
                <td>Selective Object Rearrangement in Clutter</td>
                <td>B, i, n, g, j, i, e,  , T, a, n, g, ,,  , G, a, u, r, a, v,  , S, .,  , S, u, k, h, a, t, m, e</td>
                <td>corl2022</td>
                <td>We propose an image-based, learned method for selective tabletop object rearrangement in clutter using a parallel jaw gripper. Our method consists of three stages: graph-based object sequencing (which object to move), feature-based action selection (whether to push or grasp, and at what position and orientation) and a visual correspondence-based placement policy (where to place a grasped object). Experiments show that this decomposition works well in challenging settings requiring the robot to begin with an initially cluttered scene, selecting only the objects that need to be rearranged while discarding others, and dealing with cases where the goal location for an object is already occupied – making it the first system to address all these concurrently in a purely image-based setting. We also achieve an $\sim$ 8% improvement in task success rate over the previously best reported result that handles both translation and orientation in less restrictive (un-cluttered, non-selective) settings. We demonstrate zero-shot transfer of our system solely trained in simulation to a real robot selectively rearranging up to everyday objects, many unseen during learning, on a crowded tabletop. Videos:https://sites.google.com/view/selective-rearrangement</td>
                <td><a href="https://proceedings.mlr.press/v205/tang23a.html">https://proceedings.mlr.press/v205/tang23a.html</a></td>
            </tr>
        
            <tr>
                <td>Transformers Are Adaptable Task Planners</td>
                <td>V, i, d, h, i,  , J, a, i, n, ,,  , Y, i, x, i, n,  , L, i, n, ,,  , E, r, i, c,  , U, n, d, e, r, s, a, n, d, e, r, ,,  , Y, o, n, a, t, a, n,  , B, i, s, k, ,,  , A, k, s, h, a, r, a,  , R, a, i</td>
                <td>corl2022</td>
                <td>Every home is different, and every person likes things done in their particular way. Therefore, home robots of the future need to both reason about the sequential nature of day-to-day tasks and generalize to user’s preferences. To this end, we propose a Transformer Task Planner (TTP) that learns high-level actions from demonstrations by leveraging object attribute-based representations. TTP can be pre-trained on multiple preferences and shows generalization to unseen preferences using a single demonstration as a prompt in a simulated dishwasher loading task. Further, we demonstrate real-world dish rearrangement using TTP with a Franka Panda robotic arm, prompted using a single human demonstration.</td>
                <td><a href="https://proceedings.mlr.press/v205/jain23a.html">https://proceedings.mlr.press/v205/jain23a.html</a></td>
            </tr>
        
            <tr>
                <td>ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from Point Clouds</td>
                <td>D, a, n, i, e, l,  , S, e, i, t, a, ,,  , Y, u, f, e, i,  , W, a, n, g, ,,  , S, a, r, t, h, a, k,  , J,  , S, h, e, t, t, y, ,,  , E, d, w, a, r, d,  , Y, a, o,  , L, i, ,,  , Z, a, c, k, o, r, y,  , E, r, i, c, k, s, o, n, ,,  , D, a, v, i, d,  , H, e, l, d</td>
                <td>corl2022</td>
                <td>Point clouds are a widely available and canonical data modality which convey the 3D geometry of a scene. Despite significant progress in classification and segmentation from point clouds, policy learning from such a modality remains challenging, and most prior works in imitation learning focus on learning policies from images or state information. In this paper, we propose a novel framework for learning policies from point clouds for robotic manipulation with tools. We use a novel neural network, ToolFlowNet, which predicts dense per-point flow on the tool that the robot controls, and then uses the flow to derive the transformation that the robot should execute. We apply this framework to imitation learning of challenging deformable object manipulation tasks with continuous movement of tools, including scooping and pouring, and demonstrate significantly improved performance over baselines which do not use flow. We perform physical scooping experiments with ToolFlowNet and find that we can attain 82% scooping success. See https://sites.google.com/view/point-cloud-policy/home for supplementary material.</td>
                <td><a href="https://proceedings.mlr.press/v205/seita23a.html">https://proceedings.mlr.press/v205/seita23a.html</a></td>
            </tr>
        
            <tr>
                <td>Fusing Priori and Posteriori Metrics for Automatic Dataset Annotation of Planar Grasping</td>
                <td>H, a, o,  , S, h, a, ,,  , L, a, i,  , Q, i, a, n, e, n, ,,  , H, o, n, g, x, i, a, n, g,  , Y, u, ,,  , R, o, n, g,  , X, i, o, n, g, ,,  , Y, u, e,  , W, a, n, g</td>
                <td>corl2022</td>
                <td>Grasp detection based on deep learning has been a research hot spot in recent years.  The performance of grasping detection models relies on high-quality, large-scale grasp datasets. Taking comprehensive consideration of quality, extendability, and annotation cost, metric-based simulation methodology is the most promising way to generate grasp annotation. As experts in grasping, human intuitively tends to make grasp decision based both on priori and posteriori knowledge. Inspired by that, a combination of priori and posteriori grasp metrics is intuitively helpful to improve annotation quality. In this paper, we build a hybrid metric group involving both priori and posteriori metrics and propose a grasp evaluator to merge those metrics to approximate human grasp decision capability. Centered on the evaluator, we have constructed an automatic grasp annotation framework, through which a large-scale, high-quality, low annotation cost planar grasp dataset GMD is automatically generated.</td>
                <td><a href="https://proceedings.mlr.press/v205/sha23a.html">https://proceedings.mlr.press/v205/sha23a.html</a></td>
            </tr>
        
            <tr>
                <td>PoET: Pose Estimation Transformer for Single-View, Multi-Object 6D Pose Estimation</td>
                <td>T, h, o, m, a, s,  , G, e, o, r, g,  , J, a, n, t, o, s, ,,  , M, o, h, a, m, e, d,  , A, m, i, n,  , H, a, m, d, a, d, ,,  , W, o, l, f, g, a, n, g,  , G, r, a, n, i, g, ,,  , S, t, e, p, h, a, n,  , W, e, i, s, s, ,,  , J, a, n,  , S, t, e, i, n, b, r, e, n, e, r</td>
                <td>corl2022</td>
                <td>Accurate 6D object pose estimation is an important task for a variety of robotic applications such as grasping or localization. It is a challenging task due to object symmetries, clutter and occlusion, but it becomes more challenging when additional information, such as depth and 3D models, is not provided. We present a transformer-based approach that takes an RGB image as input and predicts a 6D pose for each object in the image. Besides the image, our network does not require any additional information such as depth maps or 3D object models. First, the image is passed through an object detector to generate feature maps and to detect objects. Then, the feature maps are fed into a transformer with the detected bounding boxes as additional information. Afterwards, the output object queries are processed by a separate translation and rotation head. We achieve state-of-the-art results for RGB-only approaches on the challenging YCB-V dataset. We illustrate the suitability of the resulting model as pose sensor for a 6-DoF state estimation task. Code is available at https://github.com/aau-cns/poet.</td>
                <td><a href="https://proceedings.mlr.press/v205/jantos23a.html">https://proceedings.mlr.press/v205/jantos23a.html</a></td>
            </tr>
        
            <tr>
                <td>Out-of-Dynamics Imitation Learning from Multimodal Demonstrations</td>
                <td>Y, i, w, e, n,  , Q, i, u, ,,  , J, i, a, l, o, n, g,  , W, u, ,,  , Z, h, a, n, g, j, i, e,  , C, a, o, ,,  , M, i, n, g, s, h, e, n, g,  , L, o, n, g</td>
                <td>corl2022</td>
                <td>Existing imitation learning works mainly assume that the demonstrator who collects demonstrations shares the same dynamics as the imitator. However, the assumption limits the usage of imitation learning, especially when collecting demonstrations for the imitator is difficult. In this paper, we study out-of-dynamics imitation learning (OOD-IL), which relaxes the assumption to that the demonstrator and the imitator have the same state spaces but could have different action spaces and dynamics. OOD-IL enables imitation learning to utilize demonstrations from a wide range of demonstrators but introduces a new challenge: some demonstrations cannot be achieved by the imitator due to the different dynamics. Prior works try to filter out such demonstrations by feasibility measurements, but ignore the fact that the demonstrations exhibit a multimodal distribution since the different demonstrators may take different policies in different dynamics. We develop a better transferability measurement to tackle this newly-emerged challenge. We firstly design a novel sequence-based contrastive clustering algorithm to cluster demonstrations from the same mode to avoid the mutual interference of demonstrations from different modes, and then learn the transferability of each demonstration with an adversarial-learning based algorithm in each cluster. Experiment results on several MuJoCo environments, a driving environment, and a simulated robot environment show that the proposed transferability measurement more accurately finds and down-weights non-transferable demonstrations and outperforms prior works on the final imitation learning performance. We show the videos of our experiment results on our website.</td>
                <td><a href="https://proceedings.mlr.press/v205/qiu23a.html">https://proceedings.mlr.press/v205/qiu23a.html</a></td>
            </tr>
        
            <tr>
                <td>Domain Adaptation and Generalization: A Low-Complexity Approach</td>
                <td>J, o, s, h, u, a,  , N, i, e, m, e, i, j, e, r, ,,  , J, ö, r, g,  , P, e, t, e, r,  , S, c, h, ä, f, e, r</td>
                <td>corl2022</td>
                <td>Well-performing deep learning methods are essential in today’s perception of robotic systems such as autonomous driving vehicles. Ongoing research is due to the real-life demands for robust deep learning models against numerous domain changes and cheap training processes to avoid costly manual-labeling efforts. These requirements are addressed by unsupervised domain adaptation methods, in particular for synthetic to real-world domain changes. Recent top-performing approaches are hybrids consisting of multiple adaptation technologies and complex training processes.   In contrast, this work proposes EasyAdap, a simple and easy-to-use unsupervised domain adaptation method achieving near state-of-the-art performance on the synthetic to real-world domain change. Our evaluation consists of a comparison to numerous top-performing methods, and it shows the competitiveness and further potential of domain adaptation and domain generalization capabilities of our method. We contribute and focus on an extensive discussion revealing possible reasons for domain generalization capabilities, which is necessary to satisfy real-life application’s demands. </td>
                <td><a href="https://proceedings.mlr.press/v205/niemeijer23a.html">https://proceedings.mlr.press/v205/niemeijer23a.html</a></td>
            </tr>
        
            <tr>
                <td>COACH: Cooperative Robot Teaching</td>
                <td>C, u, n, j, u, n,  , Y, u, ,,  , Y, i, q, i, n, g,  , X, u, ,,  , L, i, n, f, e, n, g,  , L, i, ,,  , D, a, v, i, d,  , H, s, u</td>
                <td>corl2022</td>
                <td>Knowledge and skills can transfer from human teachers to human students. However, such direct transfer is often not scalable for physical tasks, as they require one-to-one interaction, and human teachers are not available in sufficient numbers.  Machine learning enables robots to become experts and play the role of teachers to help in this situation.  In this work, we formalize cooperative robot teaching as a Markov game, consisting of four key elements: the target task, the student model, the teacher model, and the interactive teaching-learning process.  Under a moderate assumption, the Markov game reduces to a partially observable Markov decision process, with an efficient approximate solution. We illustrate our approach on two cooperative tasks, one in a simulated video game and one with a real robot.</td>
                <td><a href="https://proceedings.mlr.press/v205/yu23b.html">https://proceedings.mlr.press/v205/yu23b.html</a></td>
            </tr>
        
            <tr>
                <td>TrackletMapper: Ground Surface Segmentation and Mapping from Traffic Participant Trajectories</td>
                <td>J, a, n, n, i, k,  , Z, ü, r, n, ,,  , S, e, b, a, s, t, i, a, n,  , W, e, b, e, r, ,,  , W, o, l, f, r, a, m,  , B, u, r, g, a, r, d</td>
                <td>corl2022</td>
                <td>Robustly classifying ground infrastructure such as roads and street crossings is an essential task for mobile robots operating alongside pedestrians. While many semantic segmentation datasets are available for autonomous vehicles, models trained on such datasets exhibit a large domain gap when deployed on robots operating in pedestrian spaces. Manually annotating images recorded from pedestrian viewpoints is both expensive and time-consuming. To overcome this challenge, we propose \textit{TrackletMapper}, a framework for annotating ground surface types such as sidewalks, roads, and street crossings from object tracklets without requiring human-annotated data. To this end, we project the robot ego-trajectory and the paths of other traffic participants into the ego-view camera images, creating sparse semantic annotations for multiple types of ground surfaces from which a ground segmentation model can be trained. We further show that the model can be self-distilled for additional performance benefits by aggregating a ground surface map and projecting it into the camera images, creating a denser set of training annotations compared to the sparse tracklet annotations. We qualitatively and quantitatively attest our findings on a novel large-scale dataset for mobile robots operating in pedestrian areas. Code and dataset will be made available upon acceptance of the manuscript.</td>
                <td><a href="https://proceedings.mlr.press/v205/zurn23a.html">https://proceedings.mlr.press/v205/zurn23a.html</a></td>
            </tr>
        
            <tr>
                <td>HUM3DIL: Semi-supervised Multi-modal 3D HumanPose Estimation for Autonomous Driving</td>
                <td>A, n, d, r, e, i,  , Z, a, n, f, i, r, ,,  , M, i, h, a, i,  , Z, a, n, f, i, r, ,,  , A, l, e, x,  , G, o, r, b, a, n, ,,  , J, i, n, g, w, e, i,  , J, i, ,,  , Y, i, n,  , Z, h, o, u, ,,  , D, r, a, g, o, m, i, r,  , A, n, g, u, e, l, o, v, ,,  , C, r, i, s, t, i, a, n,  , S, m, i, n, c, h, i, s, e, s, c, u</td>
                <td>corl2022</td>
                <td>Autonomous driving is an exciting new industry, posing important research questions. Within the perception module, 3D human pose estimation is an emerging technology, which can enable the autonomous vehicle to perceive and understand the subtle and complex behaviors of pedestrians. While hardware systems and sensors have dramatically improved over the decades – with cars potentially boasting complex LiDAR and vision systems and with a growing expansion of the available body of dedicated datasets for this newly available information – not much work has been done to harness these novel signals for the core problem of 3D human pose estimation. Our method, which we coin HUM3DIL (HUMan 3D from Images and LiDAR), efficiently uses of these complementary signals, in a semi-supervised fashion and outperforms existing methods with a large margin. It is a fast and compact model for onboard deployment. Specifically, we embed LiDAR points into pixel-aligned multi-modal features, which we pass through a sequence of Transformer refinement stages. Quantitative experiments on the Waymo Open Dataset support these claims, where we achieve state-of-the-art results on the task of 3D pose estimation.</td>
                <td><a href="https://proceedings.mlr.press/v205/zanfir23a.html">https://proceedings.mlr.press/v205/zanfir23a.html</a></td>
            </tr>
        
            <tr>
                <td>Laplace Approximation Based Epistemic Uncertainty Estimation in 3D Object Detection</td>
                <td>P, e, n, g,  , Y, u, n, ,,  , M, i, n, g,  , L, i, u</td>
                <td>corl2022</td>
                <td>Understanding the uncertainty of predictions is a desirable feature for perceptual modules in critical robotic applications. 3D object detectors are neural networks with high-dimensional output space. It suffers from poor calibration in classification and lacks reliable uncertainty estimation in regression. To provide a reliable epistemic uncertainty estimation, we tailor Laplace approximation for 3D object detectors, and propose an Uncertainty Separation and Aggregation pipeline for Bayesian inference. The proposed Laplace-approximation approach can easily convert a deterministic 3D object detector into a Bayesian neural network capable of estimating epistemic uncertainty. The experiment results on the KITTI dataset empirically validate the effectiveness of our proposed methods, and demonstrate that Laplace approximation performs better uncertainty quality than Monte-Carlo Dropout, DeepEnsembles, and deterministic models.</td>
                <td><a href="https://proceedings.mlr.press/v205/yun23a.html">https://proceedings.mlr.press/v205/yun23a.html</a></td>
            </tr>
        
            <tr>
                <td>Towards Online 3D Bin Packing: Learning Synergies between Packing and Unpacking via DRL</td>
                <td>S, h, u, a, i,  , S, o, n, g, ,,  , S, h, u, o,  , Y, a, n, g, ,,  , R, a, n,  , S, o, n, g, ,,  , S, h, i, l, e, i,  , C, h, u, ,,  , y, i, b, i, n,  , L, i, ,,  , W, e, i,  , Z, h, a, n, g</td>
                <td>corl2022</td>
                <td>There is an emerging research interest in addressing the online 3D bin packing problem (3D-BPP), which has a wide range of applications in logistics industry. However, neither heuristic methods nor those based on deep reinforcement learning (DRL) outperform human packers in real logistics scenarios. One important reason is that humans can make corrections after performing inappropriate packing actions by unpacking incorrectly packed items. Inspired by such an unpacking mechanism, we present a DRL-based packing-and-unpacking network (PUN) to learn the synergies between the two actions for the online 3D-BPP. Experimental results demonstrate that PUN achieves the state-of-the-art performance and the supplementary video shows that the system based on PUN can reliably complete the online 3D bin packing task in the real world.</td>
                <td><a href="https://proceedings.mlr.press/v205/song23a.html">https://proceedings.mlr.press/v205/song23a.html</a></td>
            </tr>
        
            <tr>
                <td>Reinforcement learning with Demonstrations from Mismatched Task under Sparse Reward</td>
                <td>Y, a, n, j, i, a, n, g,  , G, u, o, ,,  , J, i, n, g, y, u, e,  , G, a, o, ,,  , Z, h, e, n, g,  , W, u, ,,  , C, h, e, n, g, m, i, n, g,  , S, h, i, ,,  , J, i, a, n, y, u,  , C, h, e, n</td>
                <td>corl2022</td>
                <td>Reinforcement learning often suffer from the sparse reward issue in real-world robotics problems. Learning from demonstration (LfD) is an effective way to eliminate this problem, which leverages collected expert data to aid online learning. Prior works often assume that the learning agent and the expert aim to accomplish the same task, which requires collecting new data for every new task. In this paper, we consider the case where the target task is mismatched from but similar with that of the expert. Such setting can be challenging and we found existing LfD methods may encounter a phenomenon called reward signal backward propagation blockages so that the agent cannot be effectively guided by the demonstrations from mismatched task. We propose conservative reward shaping from demonstration (CRSfD), which shapes the sparse rewards using estimated expert value function. To accelerate learning processes, CRSfD guides the agent to conservatively explore around demonstrations. Experimental results of robot manipulation tasks show that our approach outperforms baseline LfD methods when transferring demonstrations collected in a single task to other different but similar tasks.</td>
                <td><a href="https://proceedings.mlr.press/v205/guo23a.html">https://proceedings.mlr.press/v205/guo23a.html</a></td>
            </tr>
        
            <tr>
                <td>Graph network simulators can learn discontinuous, rigid contact dynamics</td>
                <td>K, e, l, s, e, y,  , R,  , A, l, l, e, n, ,,  , T, a, t, i, a, n, a,  , L, o, p, e, z,  , G, u, e, v, a, r, a, ,,  , Y, u, l, i, a,  , R, u, b, a, n, o, v, a, ,,  , K, i, m,  , S, t, a, c, h, e, n, f, e, l, d, ,,  , A, l, v, a, r, o,  , S, a, n, c, h, e, z, -, G, o, n, z, a, l, e, z, ,,  , P, e, t, e, r,  , B, a, t, t, a, g, l, i, a, ,,  , T, o, b, i, a, s,  , P, f, a, f, f</td>
                <td>corl2022</td>
                <td>Recent years have seen a rise in techniques for modeling discontinuous dynamics, such as rigid contact or switching motion modes, using deep learning. A common claim is that deep networks are incapable of accurately modeling rigid-body dynamics without explicit modules for handling contacts, due to the continuous nature of how deep networks are parameterized. Here we investigate this claim with experiments on established real and simulated datasets and show that general-purpose graph network simulators, with no contact-specific assumptions, can learn and predict contact discontinuities. Furthermore, contact dynamics learned by graph network simulators capture real-world cube tossing trajectories more accurately than highly engineered robotics simulators, even when provided with only 8 – 16 trajectories. Overall, this suggests that rigid-body dynamics do not pose a fundamental challenge for deep networks with the appropriate general architecture and parameterization.  Instead, our work opens new directions for considering when deep learning-based models might be preferable to traditional simulation environments for accurately modeling real-world contact dynamics.</td>
                <td><a href="https://proceedings.mlr.press/v205/allen23a.html">https://proceedings.mlr.press/v205/allen23a.html</a></td>
            </tr>
        
            <tr>
                <td>Particle-Based Score Estimation for State Space Model Learning in Autonomous Driving</td>
                <td>A, n, g, a, d,  , S, i, n, g, h, ,,  , O, m, a, r,  , M, a, k, h, l, o, u, f, ,,  , M, a, x, i, m, i, l, i, a, n,  , I, g, l, ,,  , J, o, a, o,  , M, e, s, s, i, a, s, ,,  , A, r, n, a, u, d,  , D, o, u, c, e, t, ,,  , S, h, i, m, o, n,  , W, h, i, t, e, s, o, n</td>
                <td>corl2022</td>
                <td>Multi-object state estimation is a fundamental problem for robotic applications where a robot must interact with other moving objects. Typically, other objects’ relevant state features are not directly observable, and must instead be inferred from observations. Particle filtering can perform such inference given approximate transition and observation models. However, these models are often unknown a priori, yielding a difficult parameter estimation problem since observations jointly carry transition and observation noise. In this work, we consider learning maximum-likelihood parameters using particle methods. Recent methods addressing this problem typically differentiate through time in a particle filter, which requires workarounds to the non-differentiable resampling step, that yield biased or high variance gradient estimates. By contrast, we exploit Fisher’s identity to obtain a particle-based approximation of the score function (the gradient of the log likelihood) that yields a low variance estimate while only requiring stepwise differentiation through the transition and observation models. We apply our method to real data collected from autonomous vehicles (AVs) and show that it learns better models than existing techniques and is more stable in training, yielding an effective smoother for tracking the trajectories of vehicles around an AV.</td>
                <td><a href="https://proceedings.mlr.press/v205/singh23a.html">https://proceedings.mlr.press/v205/singh23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning with Muscles: Benefits for Data-Efficiency and Robustness in Anthropomorphic Tasks</td>
                <td>I, s, a, b, e, l, l,  , W, o, c, h, n, e, r, ,,  , P, i, e, r, r, e,  , S, c, h, u, m, a, c, h, e, r, ,,  , G, e, o, r, g,  , M, a, r, t, i, u, s, ,,  , D, i, e, t, e, r,  , B, ü, c, h, l, e, r, ,,  , S, y, n,  , S, c, h, m, i, t, t, ,,  , D, a, n, i, e, l,  , H, a, e, u, f, l, e</td>
                <td>corl2022</td>
                <td>Humans are able to outperform robots in terms of robustness, versatility, and learning of new tasks in a wide variety of movements. We hypothesize that highly nonlinear muscle dynamics play a large role in providing inherent stability, which is favorable to learning. While recent advances have been made in applying modern learning techniques to muscle-actuated systems both in simulation as well as in robotics, so far, no detailed analysis has been performed to show the benefits of muscles in this setting. Our study closes this gap by investigating core robotics challenges and comparing the performance of different actuator morphologies in terms of data-efficiency, hyperparameter sensitivity, and robustness.</td>
                <td><a href="https://proceedings.mlr.press/v205/wochner23a.html">https://proceedings.mlr.press/v205/wochner23a.html</a></td>
            </tr>
        
            <tr>
                <td>Bayesian Reinforcement Learning for Single-Episode Missions in Partially Unknown Environments</td>
                <td>M, a, t, t, h, e, w,  , B, u, d, d, ,,  , P, a, u, l,  , D, u, c, k, w, o, r, t, h, ,,  , N, i, c, k,  , H, a, w, e, s, ,,  , B, r, u, n, o,  , L, a, c, e, r, d, a</td>
                <td>corl2022</td>
                <td>We consider planning for mobile robots conducting missions in real-world domains where a priori unknown dynamics affect the robot’s costs and transitions. We study single-episode missions where it is crucial that the robot appropriately trades off exploration and exploitation, such that the learning of the environment dynamics is just enough to effectively complete the mission. Thus, we propose modelling unknown dynamics using Gaussian processes, which provide a principled Bayesian framework for incorporating online observations made by the robot, and using them to predict the dynamics in unexplored areas. We then formulate the problem of mission planning in Markov decision processes under Gaussian process predictions as Bayesian model-based reinforcement learning. This allows us to employ solution techniques that plan more efficiently than previous Gaussian process planning methods are able to. We empirically evaluate the benefits of our formulation in an underwater autonomous vehicle navigation task and robot mission planning in a realistic simulation of a nuclear environment.</td>
                <td><a href="https://proceedings.mlr.press/v205/budd23a.html">https://proceedings.mlr.press/v205/budd23a.html</a></td>
            </tr>
        
            <tr>
                <td>VIOLA: Imitation Learning for Vision-Based Manipulation with Object Proposal Priors</td>
                <td>Y, i, f, e, n, g,  , Z, h, u, ,,  , A, b, h, i, s, h, e, k,  , J, o, s, h, i, ,,  , P, e, t, e, r,  , S, t, o, n, e, ,,  , Y, u, k, e,  , Z, h, u</td>
                <td>corl2022</td>
                <td>We introduce VIOLA, an object-centric imitation learning approach to learning closed-loop visuomotor policies for robot manipulation. Our approach constructs object-centric representations based on general object proposals from a pre-trained vision model. VIOLA uses a transformer-based policy to reason over these representations and attend to the task-relevant visual factors for action prediction. Such object-based structural priors improve deep imitation learning algorithm’s robustness against object variations and environmental perturbations. We quantitatively evaluate VIOLA in simulation and on real robots. VIOLA outperforms the state-of-the-art imitation learning methods by 45.8% in success rate. It has also been deployed successfully on a physical robot to solve challenging long-horizon tasks, such as dining table arrangement and coffee making. More videos and model details can be found in supplementary material and the project website:  https://ut-austin-rpl.github.io/VIOLA/.</td>
                <td><a href="https://proceedings.mlr.press/v205/zhu23a.html">https://proceedings.mlr.press/v205/zhu23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Riemannian Stable Dynamical Systems via Diffeomorphisms</td>
                <td>J, i, e, c, h, a, o,  , Z, h, a, n, g, ,,  , H, a, d, i,  , B, e, i, k,  , M, o, h, a, m, m, a, d, i, ,,  , L, e, o, n, e, l,  , R, o, z, o</td>
                <td>corl2022</td>
                <td>Dexterous and autonomous robots should be capable of executing elaborated dynamical motions skillfully. Learning techniques may be leveraged to build models of such dynamic skills. To accomplish this, the learning model needs to encode a stable vector field that resembles the desired motion dynamics. This is challenging as the robot state does not evolve on a Euclidean space, and therefore the stability guarantees and vector field encoding need to account for the geometry arising from, for example, the orientation representation. To tackle this problem, we propose learning Riemannian stable dynamical systems (RSDS) from demonstrations, allowing us to account for different geometric constraints resulting from the dynamical system state representation. Our approach provides Lyapunov-stability guarantees on Riemannian manifolds that are enforced on the desired motion dynamics via diffeomorphisms built on neural manifold ODEs. We show that our Riemannian approach makes it possible to learn stable dynamical systems displaying complicated vector fields on both illustrative examples and real-world manipulation tasks, where Euclidean approximations fail.</td>
                <td><a href="https://proceedings.mlr.press/v205/zhang23b.html">https://proceedings.mlr.press/v205/zhang23b.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Robust Real-World Dexterous Grasping Policies via Implicit Shape Augmentation</td>
                <td>Q, i, u, y, u,  , C, h, e, n, ,,  , K, a, r, l,  , V, a, n,  , W, y, k, ,,  , Y, u, -, W, e, i,  , C, h, a, o, ,,  , W, e, i,  , Y, a, n, g, ,,  , A, r, s, a, l, a, n,  , M, o, u, s, a, v, i, a, n, ,,  , A, b, h, i, s, h, e, k,  , G, u, p, t, a, ,,  , D, i, e, t, e, r,  , F, o, x</td>
                <td>corl2022</td>
                <td>Dexterous robotic hands have the capability to interact with a wide variety of household objects. However, learning robust real world grasping policies for arbitrary objects has proven challenging due to the difficulty of generating high quality training data. In this work, we propose a learning system (\emph{ISAGrasp}) for leveraging a small number of human demonstrations to bootstrap the generation of a much larger dataset containing successful grasps on a variety of novel objects.  Our key insight is to use a correspondence-aware implicit generative model to deform object meshes and demonstrated human grasps in order to create a diverse dataset for supervised learning, while maintaining semantic realism. We use this dataset to train a robust grasping policy in simulation which can be deployed in the real world. We demonstrate grasping performance with a four-fingered Allegro hand in both simulation and the real world, and show this method can handle entirely new semantic classes and achieve a 79% success rate on grasping unseen objects in the real world. </td>
                <td><a href="https://proceedings.mlr.press/v205/chen23b.html">https://proceedings.mlr.press/v205/chen23b.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Visualization Policies of Augmented Reality for Human-Robot Collaboration</td>
                <td>K, i, s, h, a, n,  , D, h, a, n, a, n, j, a, y,  , C, h, a, n, d, a, n, ,,  , J, a, c, k,  , A, l, b, e, r, t, s, o, n, ,,  , S, h, i, q, i,  , Z, h, a, n, g</td>
                <td>corl2022</td>
                <td>In human-robot collaboration domains, augmented reality (AR) technologies have enabled people to visualize the state of robots. Current AR-based visualization policies are designed manually, which requires a lot of human efforts and domain knowledge. When too little information is visualized, human users find the AR interface not useful; when too much information is visualized, they find it difficult to process the visualized information. In this paper, we develop an intelligent AR agent that learns visualization policies (what to visualize, when, and how) from demonstrations. We created a Unity-based platform for simulating warehouse environments where human-robot teammates work on collaborative delivery tasks. We have collected a dataset that includes demonstrations of visualizing robots’ current and planned behaviors. Our results from experiments with real human participants show that, compared with competitive baselines from the literature, our learned visualization strategies significantly increase the efficiency of human-robot teams in delivery tasks, while reducing the distraction level of human users.</td>
                <td><a href="https://proceedings.mlr.press/v205/chandan23a.html">https://proceedings.mlr.press/v205/chandan23a.html</a></td>
            </tr>
        
            <tr>
                <td>Deep Black-Box Reinforcement Learning with Movement Primitives</td>
                <td>F, a, b, i, a, n,  , O, t, t, o, ,,  , O, n, u, r,  , C, e, l, i, k, ,,  , H, o, n, g, y, i,  , Z, h, o, u, ,,  , H, a, n, n, a,  , Z, i, e, s, c, h, e, ,,  , V, i, e, n,  , A, n, h,  , N, g, o, ,,  , G, e, r, h, a, r, d,  , N, e, u, m, a, n, n</td>
                <td>corl2022</td>
                <td>Episode-based reinforcement learning (ERL) algorithms treat reinforcement learning (RL) as a black-box optimization problem where we learn to select a parameter vector of a controller, often represented as a movement primitive, for a given task descriptor called a context. ERL offers several distinct benefits in comparison to step-based RL. It generates smooth control trajectories, can handle non-Markovian reward definitions, and the resulting exploration in parameter space is well suited for solving sparse reward settings. Yet, the high dimensionality of the movement primitive parameters has so far hampered the effective use of deep RL methods. In this paper, we present a new algorithm for deep ERL. It is based on differentiable trust region layers, a successful on-policy deep RL algorithm. These layers allow us to specify trust regions for the policy update that are solved exactly for each state using convex optimization, which enables policies learning with the high precision required for the ERL. We compare our ERL algorithm to state-of-the-art step-based algorithms in many complex simulated robotic control tasks. In doing so, we investigate different reward formulations - dense, sparse, and non-Markovian. While step-based algorithms perform well only on dense rewards, ERL performs favorably on sparse and non-Markovian rewards. Moreover, our results show that the sparse and the non-Markovian rewards are also often better suited to define the desired behavior, allowing us to obtain considerably higher quality policies compared to step-based RL.</td>
                <td><a href="https://proceedings.mlr.press/v205/otto23a.html">https://proceedings.mlr.press/v205/otto23a.html</a></td>
            </tr>
        
            <tr>
                <td>Discriminator-Guided Model-Based Offline Imitation Learning</td>
                <td>W, e, n, j, i, a,  , Z, h, a, n, g, ,,  , H, a, o, r, a, n,  , X, u, ,,  , H, a, o, y, i,  , N, i, u, ,,  , P, e, n, g,  , C, h, e, n, g, ,,  , M, i, n, g,  , L, i, ,,  , H, e, m, i, n, g,  , Z, h, a, n, g, ,,  , G, u, y, u, e,  , Z, h, o, u, ,,  , X, i, a, n, y, u, a, n,  , Z, h, a, n</td>
                <td>corl2022</td>
                <td>Offline imitation learning (IL) is a powerful method to solve decision-making problems from expert demonstrations without reward labels. Existing offline IL methods suffer from severe performance degeneration under limited expert data. Including a learned dynamics model can potentially improve the state-action space coverage of expert data, however, it also faces challenging issues like model approximation/generalization errors and suboptimality of rollout data. In this paper, we propose the Discriminator-guided Model-based offline Imitation Learning (DMIL) framework, which introduces a discriminator to simultaneously distinguish the dynamics correctness and sub-optimality of model rollout data against real expert demonstrations. DMIL adopts a novel cooperative-yet-adversarial learning strategy, which uses the discriminator to guide and couple the learning process of the policy and dynamics model, resulting in improved model performance and robustness. Our framework can also be extended to the case when demonstrations contain a large proportion of suboptimal data. Experimental results show that DMIL and its extension achieve superior performance and robustness compared to state-of-the-art offline IL methods under small datasets.</td>
                <td><a href="https://proceedings.mlr.press/v205/zhang23c.html">https://proceedings.mlr.press/v205/zhang23c.html</a></td>
            </tr>
        
            <tr>
                <td>Representation Learning for Object Detection from Unlabeled Point Cloud Sequences</td>
                <td>X, i, a, n, g, r, u,  , H, u, a, n, g, ,,  , Y, u, e,  , W, a, n, g, ,,  , V, i, t, o, r,  , C, a, m, p, a, g, n, o, l, o,  , G, u, i, z, i, l, i, n, i, ,,  , R, a, r, e, s,  , A, n, d, r, e, i,  , A, m, b, r, u, s, ,,  , A, d, r, i, e, n,  , G, a, i, d, o, n, ,,  , J, u, s, t, i, n,  , S, o, l, o, m, o, n</td>
                <td>corl2022</td>
                <td>Although unlabeled 3D data is easy to collect, state-of-the-art machine learning techniques for 3D object detection still rely on difficult-to-obtain manual annotations. To reduce dependence on the expensive and error-prone process of manual labeling, we propose a technique for representation learning from unlabeled LiDAR point cloud sequences. Our key insight is that moving objects can be reliably detected from point cloud sequences without the need for human-labeled 3D bounding boxes. In a single LiDAR frame extracted from a sequence, the set of moving objects provides sufficient supervision for single-frame object detection. By designing appropriate pretext tasks, we learn point cloud features that generalize to both moving and static unseen objects. We apply these features to object detection, achieving strong performance on self-supervised representation learning and unsupervised object detection tasks. </td>
                <td><a href="https://proceedings.mlr.press/v205/huang23b.html">https://proceedings.mlr.press/v205/huang23b.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Interpretable BEV Based VIO without Deep Neural Networks</td>
                <td>Z, e, x, i,  , C, h, e, n, ,,  , H, a, o, z, h, e,  , D, u, ,,  , X, u, e, c, h, e, n, g,  , X, U, ,,  , R, o, n, g,  , X, i, o, n, g, ,,  , Y, i, y, i,  , L, i, a, o, ,,  , Y, u, e,  , W, a, n, g</td>
                <td>corl2022</td>
                <td>Monocular visual-inertial odometry (VIO) is a critical problem in robotics and autonomous driving. Traditional methods solve this problem based on filtering or optimization. While being fully interpretable, they rely on manual interference and empirical parameter tuning. On the other hand, learning-based approaches allow for end-to-end training but require a large number of training data to learn millions of parameters. However, the non-interpretable and heavy models hinder the generalization ability. In this paper, we propose a fully differentiable, and interpretable, bird-eye-view (BEV) based VIO model for robots with local planar motion that can be trained without deep neural networks. Specifically, we first adopt Unscented Kalman Filter as a differentiable layer to predict the pitch and roll, where the covariance matrices of noise are learned to filter out the noise of the IMU raw data.  Second, the refined pitch and roll are adopted to retrieve a gravity-aligned BEV image of each frame using differentiable camera projection. Finally, a differentiable pose estimator is utilized to estimate the remaining 3 DoF poses between the BEV frames: leading to a 5 DoF pose estimation. Our method allows for learning the covariance matrices end-to-end supervised by the pose estimation loss, demonstrating superior performance to empirical baselines. Experimental results on synthetic and real-world datasets demonstrate that our simple approach is competitive with state-of-the-art methods and generalizes well on unseen scenes.</td>
                <td><a href="https://proceedings.mlr.press/v205/chen23c.html">https://proceedings.mlr.press/v205/chen23c.html</a></td>
            </tr>
        
            <tr>
                <td>Solving Complex Manipulation Tasks with Model-Assisted Model-Free Reinforcement Learning</td>
                <td>J, i, a, n, s, h, u,  , H, u, ,,  , P, a, u, l,  , W, e, n, g</td>
                <td>corl2022</td>
                <td>In this paper, we propose a novel deep reinforcement learning approach for improving the sample efficiency of a model-free actor-critic method by using a learned model to encourage exploration. The basic idea consists in generating artificial transitions with noisy actions, which can be used to update the critic. To counteract the model bias, we introduce a high initialization for the critic and two filters for the artificial transitions. Finally, we evaluate our approach with the TD3 algorithm on different robotic tasks and demonstrate that it achieves a better performance with higher sample efficiency than several other model-based and model-free methods.</td>
                <td><a href="https://proceedings.mlr.press/v205/hu23a.html">https://proceedings.mlr.press/v205/hu23a.html</a></td>
            </tr>
        
            <tr>
                <td>Robustness Certification of Visual Perception Models via Camera Motion Smoothing</td>
                <td>H, a, n, j, i, a, n, g,  , H, u, ,,  , Z, u, x, i, n,  , L, i, u, ,,  , L, i, n, y, i,  , L, i, ,,  , J, i, a, c, h, e, n, g,  , Z, h, u, ,,  , D, i, n, g,  , Z, h, a, o</td>
                <td>corl2022</td>
                <td>A vast literature shows that the learning-based visual perception model is sensitive to adversarial noises, but few works consider the robustness of robotic perception models under widely-existing camera motion perturbations. To this end, we study the robustness of the visual perception model under camera motion perturbations to investigate the influence of camera motion on robotic perception. Specifically, we propose a motion smoothing technique for arbitrary image classification models, whose robustness under camera motion perturbations could be certified. The proposed robustness certification framework based on camera motion smoothing provides effective and scalable robustness guarantees for visual perception modules so that they are applicable to wide robotic applications. As far as we are aware, this is the first work to provide robustness certification for the deep perception module against camera motions, which improves the trustworthiness of robotic perception. A realistic indoor robotic dataset with a dense point cloud map for the entire room, MetaRoom, is introduced for the challenging certifiable robust perception task. We conduct extensive experiments to validate the certification approach via motion smoothing against camera motion perturbations. Our framework guarantees the certified accuracy of 81.7% against camera translation perturbation along depth direction within -0.1m   0.1m. We also validate the effectiveness of our method on the real-world robot by conducting hardware experiments on the robotic arm with an eye-in-hand camera. The code is available at https://github.com/HanjiangHu/camera-motion-smoothing.</td>
                <td><a href="https://proceedings.mlr.press/v205/hu23b.html">https://proceedings.mlr.press/v205/hu23b.html</a></td>
            </tr>
        
            <tr>
                <td>GLSO: Grammar-guided Latent Space Optimization for Sample-efficient Robot Design Automation</td>
                <td>J, i, a, h, e, n, g,  , H, u, ,,  , J, u, l, i, a, n,  , W, h, i, t, m, a, n, ,,  , H, o, w, i, e,  , C, h, o, s, e, t</td>
                <td>corl2022</td>
                <td>Robots have been used in all sorts of automation, and yet the design of robots remains mainly a manual task. We seek to provide design tools to automate the design of robots themselves. An important challenge in robot design automation is the large and complex design search space which grows exponentially with the number of components, making optimization difficult and sample inefficient. In this work, we present Grammar-guided Latent Space Optimization (GLSO), a framework that transforms design automation into a low-dimensional continuous optimization problem by training a graph variational autoencoder (VAE) to learn a mapping between the graph-structured design space and a continuous latent space. This transformation allows optimization to be conducted in a continuous latent space, where sample efficiency can be significantly boosted by applying algorithms such as Bayesian Optimization. GLSO guides training of the VAE using graph grammar rules and robot world space features, such that the learned latent space focus on valid robots and is easier for the optimization algorithm to explore. Importantly, the trained VAE can be reused to search for designs specialized to multiple different tasks without retraining. We evaluate GLSO by designing robots for a set of locomotion tasks in simulation, and demonstrate that our method outperforms related state-of-the-art robot design automation methods.</td>
                <td><a href="https://proceedings.mlr.press/v205/hu23c.html">https://proceedings.mlr.press/v205/hu23c.html</a></td>
            </tr>
        
            <tr>
                <td>Masked World Models for Visual Control</td>
                <td>Y, o, u, n, g, g, y, o,  , S, e, o, ,,  , D, a, n, i, j, a, r,  , H, a, f, n, e, r, ,,  , H, a, o,  , L, i, u, ,,  , F, a, n, g, c, h, e, n,  , L, i, u, ,,  , S, t, e, p, h, e, n,  , J, a, m, e, s, ,,  , K, i, m, i, n,  , L, e, e, ,,  , P, i, e, t, e, r,  , A, b, b, e, e, l</td>
                <td>corl2022</td>
                <td>Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling approach achieves state-of-the-art performance on a variety of visual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7% success rate on 50 visual robotic manipulation tasks from Meta-world, while the baseline achieves 67.9%. Code is available on the project website: https://sites.google.com/view/mwm-rl.</td>
                <td><a href="https://proceedings.mlr.press/v205/seo23a.html">https://proceedings.mlr.press/v205/seo23a.html</a></td>
            </tr>
        
            <tr>
                <td>On-Robot Learning With Equivariant Models</td>
                <td>D, i, a, n,  , W, a, n, g, ,,  , M, i, n, g, x, i,  , J, i, a, ,,  , X, u, p, e, n, g,  , Z, h, u, ,,  , R, o, b, i, n,  , W, a, l, t, e, r, s, ,,  , R, o, b, e, r, t,  , P, l, a, t, t</td>
                <td>corl2022</td>
                <td>Recently, equivariant neural network models have been shown to improve sample efficiency for tasks in computer vision and reinforcement learning. This paper explores this idea in the context of on-robot policy learning in which a policy must be learned entirely on a physical robotic system without reference to a model, a simulator, or an offline dataset. We focus on applications of Equivariant SAC to robotic manipulation and explore a number of variations of the algorithm. Ultimately, we demonstrate the ability to learn several non-trivial manipulation tasks completely through on-robot experiences in less than an hour or two of wall clock time. </td>
                <td><a href="https://proceedings.mlr.press/v205/wang23c.html">https://proceedings.mlr.press/v205/wang23c.html</a></td>
            </tr>
        
            <tr>
                <td>Neural Geometric Fabrics: Efficiently Learning High-Dimensional Policies from Demonstration</td>
                <td>M, a, n, d, y,  , X, i, e, ,,  , A, n, k, u, r,  , H, a, n, d, a, ,,  , S, t, e, p, h, e, n,  , T, y, r, e, e, ,,  , D, i, e, t, e, r,  , F, o, x, ,,  , H, a, r, i, s, h,  , R, a, v, i, c, h, a, n, d, a, r, ,,  , N, a, t, h, a, n,  , D, .,  , R, a, t, l, i, f, f, ,,  , K, a, r, l,  , V, a, n,  , W, y, k</td>
                <td>corl2022</td>
                <td>Learning dexterous manipulation policies for multi-fingered robots has been a long-standing challenge in robotics. Existing methods either limit themselves to highly constrained problems and smaller models to achieve extreme sample efficiency or sacrifice sample efficiency to gain capacity to solve more complex tasks with deep neural networks. In this work, we develop a structured approach to sample-efficient learning of dexterous manipulation skills from demonstrations by leveraging recent advances in robot motion generation and control. Specifically, our policy structure is induced by Geometric Fabrics - a recent framework that generalizes classical mechanical systems to allow for flexible design of expressive robot motions. To avoid the cumbersome manual design required by existing motion generators, we introduce Neural Geometric Fabric (NGF) - a framework that learns Geometric Fabric-based policies from data. NGF policies are provably stable and capable of encoding speed-invariant geometries of complex motions in multiple task spaces simultaneously. We demonstrate that NGFs can learn to perform a variety of dexterous manipulation tasks on a 23-DoF hand-arm physical robotic platform purely from demonstrations. Results from comprehensive comparative and ablative experiments show that NGF’s structure and action spaces help learn acceleration-based policies that consistently outperform state-of-the-art baselines like Riemannian Motion Policies (RMPs), and other commonly used networks, such as feed-forward and recurrent neural networks. More importantly, we demonstrate that NGFs do not rely on often-used and expertly-designed operational-space controllers, promoting an advancement towards efficiently learning safe, stable, and high-dimensional controllers.</td>
                <td><a href="https://proceedings.mlr.press/v205/xie23a.html">https://proceedings.mlr.press/v205/xie23a.html</a></td>
            </tr>
        
            <tr>
                <td>See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation</td>
                <td>H, a, o,  , L, i, ,,  , Y, i, z, h, i,  , Z, h, a, n, g, ,,  , J, u, n, z, h, e,  , Z, h, u, ,,  , S, h, a, o, x, i, o, n, g,  , W, a, n, g, ,,  , M, i, c, h, e, l, l, e,  , A,  , L, e, e, ,,  , H, u, a, z, h, e,  , X, u, ,,  , E, d, w, a, r, d,  , A, d, e, l, s, o, n, ,,  , L, i,  , F, e, i, -, F, e, i, ,,  , R, u, o, h, a, n,  , G, a, o, ,,  , J, i, a, j, u, n,  , W, u</td>
                <td>corl2022</td>
                <td>Humans use all of their senses to accomplish different tasks in everyday activities. In contrast, existing work on robotic manipulation mostly relies on one, or occasionally two modalities, such as vision and touch. In this work, we systematically study how visual, auditory, and tactile perception can jointly help robots to solve complex manipulation tasks. We build a robot system that can see with a camera, hear with a contact microphone, and feel with a vision-based tactile sensor, with all three sensory modalities fused with a self-attention model. Results on two challenging tasks, dense packing and pouring, demonstrate the necessity and power of multisensory perception for robotic manipulation: vision displays the global status of the robot but can often suffer from occlusion, audio provides immediate feedback of key moments that are even invisible, and touch offers precise local geometry for decision making. Leveraging all three modalities, our robotic system significantly outperforms prior methods.</td>
                <td><a href="https://proceedings.mlr.press/v205/li23c.html">https://proceedings.mlr.press/v205/li23c.html</a></td>
            </tr>
        
            <tr>
                <td>Inferring Versatile Behavior from Demonstrations by Matching Geometric Descriptors</td>
                <td>N, i, k, l, a, s,  , F, r, e, y, m, u, t, h, ,,  , N, i, c, o, l, a, s,  , S, c, h, r, e, i, b, e, r, ,,  , A, l, e, k, s, a, n, d, a, r,  , T, a, r, a, n, o, v, i, c, ,,  , P, h, i, l, i, p, p,  , B, e, c, k, e, r, ,,  , G, e, r, h, a, r, d,  , N, e, u, m, a, n, n</td>
                <td>corl2022</td>
                <td>Humans intuitively solve tasks in versatile ways, varying their behavior in terms of trajectory-based planning and for individual steps. Thus, they can easily generalize and adapt to new and changing environments. Current Imitation Learning algorithms often only consider unimodal expert demonstrations and act in a state-action-based setting, making it difficult for them to imitate human behavior in case of versatile demonstrations. Instead, we combine a mixture of movement primitives with a distribution matching objective to learn versatile behaviors that match the expert’s behavior and versatility. To facilitate generalization to novel task configurations, we do not directly match the agent’s and expert’s trajectory distributions but rather work with concise geometric descriptors which generalize well to unseen task configurations. We empirically validate our method on various robot tasks using versatile human demonstrations and compare to imitation learning algorithms in a state-action setting as well as a trajectory-based setting. We find that the geometric descriptors greatly help in generalizing to new task configurations and that combining them with our distribution-matching objective is crucial for representing and reproducing versatile behavior.</td>
                <td><a href="https://proceedings.mlr.press/v205/freymuth23a.html">https://proceedings.mlr.press/v205/freymuth23a.html</a></td>
            </tr>
        
            <tr>
                <td>Generative Category-Level Shape and Pose Estimation with Semantic Primitives</td>
                <td>G, u, a, n, g, l, i, n,  , L, i, ,,  , Y, i, f, e, n, g,  , L, i, ,,  , Z, h, i, c, h, a, o,  , Y, e, ,,  , Q, i, h, a, n, g,  , Z, h, a, n, g, ,,  , T, a, o,  , K, o, n, g, ,,  , Z, h, a, o, p, e, n, g,  , C, u, i, ,,  , G, u, o, f, e, n, g,  , Z, h, a, n, g</td>
                <td>corl2022</td>
                <td>Empowering autonomous agents with 3D understanding for daily objects is a grand challenge in robotics applications. When exploring in an unknown environment, existing methods for object pose estimation are still not satisfactory due to the diversity of object shapes. In this paper, we propose a novel framework for category-level object shape and pose estimation from a single RGB-D image. To handle the intra-category variation, we adopt a semantic primitive representation that encodes diverse shapes into a unified latent space, which is the key to establish reliable correspondences between observed point clouds and estimated shapes. Then, by using a SIM(3)-invariant shape descriptor, we gracefully decouple the shape and pose of an object, thus supporting latent shape optimization of target objects in arbitrary poses. Extensive experiments show that the proposed method achieves SOTA pose estimation performance and better generalization in the real-world dataset. Code and video are available at https://zju3dv.github.io/gCasp.</td>
                <td><a href="https://proceedings.mlr.press/v205/li23d.html">https://proceedings.mlr.press/v205/li23d.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Goal-Conditioned Policies Offline with Self-Supervised Reward Shaping</td>
                <td>L, i, n, a,  , M, e, z, g, h, a, n, i, ,,  , S, a, i, n, b, a, y, a, r,  , S, u, k, h, b, a, a, t, a, r, ,,  , P, i, o, t, r,  , B, o, j, a, n, o, w, s, k, i, ,,  , A, l, e, s, s, a, n, d, r, o,  , L, a, z, a, r, i, c, ,,  , K, a, r, t, e, e, k,  , A, l, a, h, a, r, i</td>
                <td>corl2022</td>
                <td>Developing agents that can execute multiple skills by learning from pre-collected datasets is an important problem in robotics, where online interaction with the environment is extremely time-consuming. Moreover, manually designing reward functions for every single desired skill is prohibitive. Prior works targeted these challenges by learning goal-conditioned policies from offline datasets without manually specified rewards, through hindsight relabeling. These methods suffer from the issue of sparsity of rewards, and fail at long-horizon tasks. In this work, we propose a novel self-supervised learning phase on the pre-collected dataset to understand the structure and the dynamics of the model, and shape a dense reward function for learning policies offline. We evaluate our method on three continuous control tasks, and show that our model significantly outperforms existing approaches, especially on tasks that involve long-term planning.</td>
                <td><a href="https://proceedings.mlr.press/v205/mezghani23a.html">https://proceedings.mlr.press/v205/mezghani23a.html</a></td>
            </tr>
        
            <tr>
                <td>ROS-PyBullet Interface: A Framework for Reliable Contact Simulation and Human-Robot Interaction</td>
                <td>C, h, r, i, s, t, o, p, h, e, r,  , M, o, w, e, r, ,,  , T, h, e, o, d, o, r, o, s,  , S, t, o, u, r, a, i, t, i, s, ,,  , J, o, ã, o,  , M, o, u, r, a, ,,  , C, h, r, i, s, t, i, a, n,  , R, a, u, c, h, ,,  , L, e, i,  , Y, a, n, ,,  , N, a, z, a, n, i, n,  , Z, a, m, a, n, i,  , B, e, h, a, b, a, d, i, ,,  , M, i, c, h, a, e, l,  , G, i, e, n, g, e, r, ,,  , T, o, m,  , V, e, r, c, a, u, t, e, r, e, n, ,,  , C, h, r, i, s, t, o, s,  , B, e, r, g, e, l, e, s, ,,  , S, e, t, h, u,  , V, i, j, a, y, a, k, u, m, a, r</td>
                <td>corl2022</td>
                <td>Reliable contact simulation plays a key role in the development of (semi-)autonomous robots, especially when dealing with contact-rich manipulation scenarios, an active robotics research topic. Besides simulation, components such as sensing, perception, data collection, robot hardware control, human interfaces, etc. are all key enablers towards applying machine learning algorithms or model-based approaches in real world systems. However, there is a lack of software connecting reliable contact simulation with the larger robotics ecosystem (i.e. ROS, Orocos), for a more seamless application of novel approaches, found in the literature, to existing robotic hardware. In this paper, we present the ROS-PyBullet Interface, a framework that provides a bridge between the reliable contact/impact simulator PyBullet and the Robot Operating System (ROS). Furthermore, we provide additional utilities for facilitating Human-Robot Interaction (HRI) in the simulated environment. We also present several use-cases that highlight the capabilities and usefulness of our framework. Our code base is open source and can be found at https://github.com/ros-pybullet/ros_pybullet_interface.</td>
                <td><a href="https://proceedings.mlr.press/v205/mower23a.html">https://proceedings.mlr.press/v205/mower23a.html</a></td>
            </tr>
        
            <tr>
                <td>PLATO: Predicting Latent Affordances Through Object-Centric Play</td>
                <td>S, u, n, e, e, l,  , B, e, l, k, h, a, l, e, ,,  , D, o, r, s, a,  , S, a, d, i, g, h</td>
                <td>corl2022</td>
                <td>Constructing a diverse repertoire of manipulation skills in a scalable fashion remains an unsolved challenge in robotics. One way to address this challenge is with unstructured human play, where humans operate freely in an environment to reach unspecified goals. Play is a simple and cheap method for collecting diverse user demonstrations with broad state and goal coverage over an environment. Due to this diverse coverage, existing approaches for learning from play are more robust to online policy deviations from the offline data distribution. However, these methods often struggle to learn under scene variation and on challenging manipulation primitives, due in part to improperly associating complex behaviors to the scene changes they induce. Our insight is that an object-centric view of play data can help link human behaviors and the resulting changes in the environment, and thus improve multi-task policy learning. In this work, we construct a latent space to model object \textit{affordances} – properties of an object that define its uses – in the environment, and then learn a policy to achieve the desired affordances. By modeling and predicting the desired affordance across variable horizon tasks, our method, Predicting Latent Affordances Through Object-Centric Play (PLATO), outperforms existing methods on complex manipulation tasks in both 2D and 3D object manipulation simulation and real world environments for diverse types of interactions. Videos can be found on our website: https://sites.google.com/view/plato-corl22/home.</td>
                <td><a href="https://proceedings.mlr.press/v205/belkhale23a.html">https://proceedings.mlr.press/v205/belkhale23a.html</a></td>
            </tr>
        
            <tr>
                <td>You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example</td>
                <td>W, a, l, t, e, r,  , G, o, o, d, w, i, n, ,,  , I, o, a, n, n, i, s,  , H, a, v, o, u, t, i, s, ,,  , I, n, g, m, a, r,  , P, o, s, n, e, r</td>
                <td>corl2022</td>
                <td>In order to meaningfully interact with the world, robot manipulators must be able to interpret objects they encounter. A critical aspect of this interpretation is pose estimation: inferring quantities that describe the position and orientation of an object in 3D space. Most existing approaches to pose estimation make limiting assumptions, often working only for specific, known object instances, or at best generalising to an object category using large pose-labelled datasets. In this work, we present a method for achieving category-level pose estimation by inspection of just a single object from a desired category. We show that we can subsequently perform accurate pose estimation for unseen objects from an inspected category, and considerably outperform prior work by exploiting multi-view correspondences. We demonstrate that our method runs in real-time, enabling a robot manipulator to rearrange previously unseen objects faithfully in terms of placement and orientation. Finally, we showcase our method in a continual learning setting, with a robot able to determine whether objects belong to known categories, and if not, use active perception to produce a one-shot category representation for subsequent pose estimation</td>
                <td><a href="https://proceedings.mlr.press/v205/goodwin23a.html">https://proceedings.mlr.press/v205/goodwin23a.html</a></td>
            </tr>
        
            <tr>
                <td>ARC - Actor Residual Critic for Adversarial Imitation Learning</td>
                <td>A, n, k, u, r,  , D, e, k, a, ,,  , C, h, a, n, g, l, i, u,  , L, i, u, ,,  , K, a, t, i, a,  , P, .,  , S, y, c, a, r, a</td>
                <td>corl2022</td>
                <td>Adversarial Imitation Learning (AIL) is a class  of popular state-of-the-art Imitation Learning algorithms commonly used in robotics. In AIL, an artificial adversary’s misclassification is used as a reward signal that is optimized by any standard Reinforcement Learning (RL) algorithm. Unlike most RL settings, the reward in AIL is $differentiable$ but current model-free RL algorithms do not make use of this property to train a policy. The reward is AIL is also $shaped$ since it comes from an adversary. We leverage the differentiability property of the shaped AIL reward function and formulate a class of Actor Residual Critic (ARC) RL algorithms. ARC algorithms draw a parallel to the standard Actor-Critic (AC) algorithms in RL literature and uses a residual critic, $C$ function (instead of the standard $Q$ function) to approximate only the discounted future return (excluding the immediate reward). ARC algorithms have similar convergence properties as the standard AC algorithms with the additional advantage that the gradient through the immediate reward is exact. For the discrete (tabular) case with finite states, actions, and known dynamics, we prove that policy iteration with $C$ function converges to an optimal policy. In the continuous case with function approximation and unknown dynamics, we experimentally show that ARC aided AIL outperforms standard AIL in simulated continuous-control and real robotic manipulation tasks. ARC algorithms are simple to implement and can be incorporated into any existing AIL implementation with an AC algorithm.</td>
                <td><a href="https://proceedings.mlr.press/v205/deka23a.html">https://proceedings.mlr.press/v205/deka23a.html</a></td>
            </tr>
        
            <tr>
                <td>JFP: Joint Future Prediction with Interactive Multi-Agent Modeling for Autonomous Driving</td>
                <td>W, e, n, j, i, e,  , L, u, o, ,,  , C, h, e, o, l,  , P, a, r, k, ,,  , A, n, d, r, e,  , C, o, r, n, m, a, n, ,,  , B, e, n, j, a, m, i, n,  , S, a, p, p, ,,  , D, r, a, g, o, m, i, r,  , A, n, g, u, e, l, o, v</td>
                <td>corl2022</td>
                <td>We propose \textit{JFP}, a Joint Future Prediction model that can learn to generate accurate and consistent multi-agent future trajectories. For this task, many different methods have been proposed to capture social interactions in the encoding part of the model, however, considerably less focus has been placed on representing interactions in the decoder and output stages. As a result, the predicted trajectories are not necessarily consistent with each other, and often result in unrealistic trajectory overlaps. In contrast, we propose an end-to-end trainable model that learns directly the interaction between pairs of agents in a structured, graphical model formulation in order to generate consistent future trajectories. It sets new state-of-the-art results on Waymo Open Motion Dataset (WOMD) for the interactive setting. We also investigate a more complex multi-agent setting for both WOMD and a larger internal dataset, where our approach improves significantly on the trajectory overlap metrics while obtaining on-par or better performance on single-agent trajectory metrics.</td>
                <td><a href="https://proceedings.mlr.press/v205/luo23a.html">https://proceedings.mlr.press/v205/luo23a.html</a></td>
            </tr>
        
            <tr>
                <td>Online Inverse Reinforcement Learning with Learned Observation Model</td>
                <td>S, a, u, r, a, b, h,  , A, r, o, r, a, ,,  , P, r, a, s, h, a, n, t,  , D, o, s, h, i, ,,  , B, i, k, r, a, m, j, i, t,  , B, a, n, e, r, j, e, e</td>
                <td>corl2022</td>
                <td>With the motivation of extending incremental inverse reinforcement learning (I2RL) to real-world robotics applications with noisy observations as well as an unknown observation model, we introduce a new method (RIMEO) that approximates the observation model in order to best estimate the noise-free ground truth underlying the observations. It learns a maximum entropy distribution over the observation features governing the perception process, and then uses the inferred observation model to learn the reward function. Experimental evaluation is performed in two robotics tasks: (1) post-harvest vegetable sorting with a Sawyer arm based on human demonstration, and (2) breaching a perimeter patrol by two Turtlebots. Our experiments reveal that RIMEO learns a more accurate policy compared to (a) a state-of-the-art IRL method that does not directly learn an observation model, and (b) a custom baseline that learns a less sophisticated observation model. Furthermore, we show that RIMEO admits formal guarantees of monotonic convergence and a sample complexity bound.</td>
                <td><a href="https://proceedings.mlr.press/v205/arora23a.html">https://proceedings.mlr.press/v205/arora23a.html</a></td>
            </tr>
        
            <tr>
                <td>Hypernetworks in Meta-Reinforcement Learning</td>
                <td>J, a, c, o, b,  , B, e, c, k, ,,  , M, a, t, t, h, e, w,  , T, h, o, m, a, s,  , J, a, c, k, s, o, n, ,,  , R, i, s, t, o,  , V, u, o, r, i, o, ,,  , S, h, i, m, o, n,  , W, h, i, t, e, s, o, n</td>
                <td>corl2022</td>
                <td>Training a reinforcement learning (RL) agent on a real-world robotics task remains generally impractical due to sample inefficiency. Multi-task RL and meta-RL aim to improve sample efficiency by generalizing over a distribution of related tasks. However, doing so is difficult in practice: In multi-task RL, state of the art methods often fail to outperform a degenerate solution that simply learns each task separately. Hypernetworks are a promising path forward since they replicate the separate policies of the degenerate solution while also allowing for generalization across tasks, and are applicable to meta-RL. However, evidence from supervised learning suggests hypernetwork performance is highly sensitive to the initialization. In this paper, we 1) show that hypernetwork initialization is also a critical factor in meta-RL, and that naive initializations yield poor performance; 2) propose a novel hypernetwork initialization scheme that matches or exceeds the performance of a state-of-the-art approach proposed for supervised settings, as well as being simpler and more general; and 3) use this method to show that hypernetworks can improve performance in meta-RL by evaluating on multiple simulated robotics benchmarks.</td>
                <td><a href="https://proceedings.mlr.press/v205/beck23a.html">https://proceedings.mlr.press/v205/beck23a.html</a></td>
            </tr>
        
            <tr>
                <td>Efficient Tactile Simulation with Differentiability for Robotic Manipulation</td>
                <td>J, i, e,  , X, u, ,,  , S, a, n, g, w, o, o, n,  , K, i, m, ,,  , T, a, o,  , C, h, e, n, ,,  , A, l, b, e, r, t, o,  , R, o, d, r, i, g, u, e, z,  , G, a, r, c, i, a, ,,  , P, u, l, k, i, t,  , A, g, r, a, w, a, l, ,,  , W, o, j, c, i, e, c, h,  , M, a, t, u, s, i, k, ,,  , S, h, i, n, j, i, r, o,  , S, u, e, d, a</td>
                <td>corl2022</td>
                <td>Efficient simulation of tactile sensors can unlock new opportunities for learning tactile-based manipulation policies in simulation and then transferring the learned policy to real systems, but fast and reliable simulators for dense tactile normal and shear force fields are still under-explored. We present a novel approach for efficiently simulating both the normal and shear tactile force field covering the entire contact surface with an arbitrary tactile sensor spatial layout. Our simulator also provides analytical gradients of the tactile forces to accelerate policy learning. We conduct extensive simulation experiments to showcase our approach and demonstrate successful zero-shot sim-to-real transfer for a high-precision peg-insertion task with high-resolution vision-based GelSlim tactile sensors.</td>
                <td><a href="https://proceedings.mlr.press/v205/xu23b.html">https://proceedings.mlr.press/v205/xu23b.html</a></td>
            </tr>
        
            <tr>
                <td>Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates</td>
                <td>S, o, u, r, a, d, e, e, p,  , D, u, t, t, a, ,,  , K, a, u, s, t, u, b, h,  , S, r, i, d, h, a, r, ,,  , O, s, b, e, r, t,  , B, a, s, t, a, n, i, ,,  , E, d, g, a, r,  , D, o, b, r, i, b, a, n, ,,  , J, a, m, e, s,  , W, e, i, m, e, r, ,,  , I, n, s, u, p,  , L, e, e, ,,  , J, u, l, i, a,  , P, a, r, i, s, h, -, M, o, r, r, i, s</td>
                <td>corl2022</td>
                <td>Long horizon robot learning tasks with sparse rewards pose a significant challenge for current reinforcement learning algorithms. A key feature enabling humans to learn challenging control tasks is that they often receive expert intervention that enables them to understand the high-level structure of the task before mastering low-level control actions. We propose a framework for leveraging expert intervention to solve long-horizon reinforcement learning tasks. We consider \emph{option templates}, which are specifications encoding a potential option that can be trained using reinforcement learning. We formulate expert intervention as allowing the agent to execute option templates  before learning an implementation. This enables them to use an option, before committing costly resources to learning it. We evaluate our approach on three challenging reinforcement learning problems, showing that it outperforms state-of-the-art approaches by two orders of magnitude. Videos of trained agents and our code can be found at: https://sites.google.com/view/stickymittens</td>
                <td><a href="https://proceedings.mlr.press/v205/dutta23a.html">https://proceedings.mlr.press/v205/dutta23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Bimanual Scooping Policies for Food Acquisition</td>
                <td>J, e, n, n, i, f, e, r,  , G, r, a, n, n, e, n, ,,  , Y, i, l, i, n,  , W, u, ,,  , S, u, n, e, e, l,  , B, e, l, k, h, a, l, e, ,,  , D, o, r, s, a,  , S, a, d, i, g, h</td>
                <td>corl2022</td>
                <td>A robotic feeding system must be able to acquire a variety of foods. Prior bite acquisition works consider single-arm spoon scooping or fork skewering, which do not generalize to foods with complex geometries and deformabilities. For example, when acquiring a group of peas, skewering could smoosh the peas while scooping without a barrier could result in chasing the peas on the plate. In order to acquire foods with such diverse properties, we propose stabilizing food items during scooping using a second arm, for example, by pushing peas against the spoon with a flat surface to prevent dispersion. The addition of this second stabilizing arm can lead to a new set of challenges. Critically, these strategies should stabilize the food scene without interfering with the acquisition motion, which is especially difficult for easily breakable high-risk food items, such as tofu. These high-risk foods can break between the pusher and spoon during scooping, which can lead to food waste falling onto the plate or out of the workspace. We propose a general bimanual scooping primitive and an adaptive stabilization strategy that enables successful acquisition of a diverse set of food geometries and physical properties. Our approach, CARBS: Coordinated Acquisition with Reactive Bimanual Scooping, learns to stabilize without impeding task progress by identifying high-risk foods and robustly scooping them using closed-loop visual feedback. We find that CARBS is able to generalize across food shape, size, and deformability and is additionally able to manipulate multiple food items simultaneously. CARBS achieves 87.0% success on scooping rigid foods, which is 25.8% more successful than a single-arm baseline, and reduces food breakage by 16.2% compared to an analytical baseline. Videos can be found on our website at https://sites.google.com/view/bimanualscoop-corl22/home.</td>
                <td><a href="https://proceedings.mlr.press/v205/grannen23a.html">https://proceedings.mlr.press/v205/grannen23a.html</a></td>
            </tr>
        
            <tr>
                <td>SE(3)-Equivariant Point Cloud-Based Place Recognition</td>
                <td>C, h, i, e, n,  , E, r, h,  , L, i, n, ,,  , J, i, n, g, w, e, i,  , S, o, n, g, ,,  , R, a, y,  , Z, h, a, n, g, ,,  , M, i, n, g, h, a, n,  , Z, h, u, ,,  , M, a, a, n, i,  , G, h, a, f, f, a, r, i</td>
                <td>corl2022</td>
                <td>This paper reports on a new 3D point cloud-based place recognition framework that uses SE(3)-equivariant networks to learn SE(3)-invariant global descriptors. We discover that, unlike existing methods, learned SE(3)-invariant global descriptors are more robust to matching inaccuracy and failure in severe rotation and translation configurations. Mobile robots undergo arbitrary rotational and translational movements. The SE(3)-invariant property ensures that the learned descriptors are robust to the rotation and translation changes in the robot pose and can represent the intrinsic geometric information of the scene. Furthermore, we have discovered that the attention module aids in the enhancement of performance while allowing significant downsampling. We evaluate the performance of the proposed framework on real-world data sets. The experimental results show that the proposed framework outperforms state-of-the-art baselines in various metrics, leading to a reliable point cloud-based place recognition network. We have open-sourced our code at: https://github.com/UMich-CURLY/se3_equivariant_place_recognition.</td>
                <td><a href="https://proceedings.mlr.press/v205/lin23a.html">https://proceedings.mlr.press/v205/lin23a.html</a></td>
            </tr>
        
            <tr>
                <td>Leveraging Language for Accelerated Learning of Tool Manipulation</td>
                <td>A, l, l, e, n,  , Z, .,  , R, e, n, ,,  , B, h, a, r, a, t,  , G, o, v, i, l, ,,  , T, s, u, n, g, -, Y, e, n,  , Y, a, n, g, ,,  , K, a, r, t, h, i, k,  , R,  , N, a, r, a, s, i, m, h, a, n, ,,  , A, n, i, r, u, d, h, a,  , M, a, j, u, m, d, a, r</td>
                <td>corl2022</td>
                <td>Robust and generalized tool manipulation requires an understanding of the properties and affordances of different tools. We investigate whether linguistic information about a tool (e.g., its geometry, common uses) can help control policies adapt faster to new tools for a given task. We obtain diverse descriptions of various tools in natural language and use pre-trained language models to generate their feature representations. We then perform language-conditioned meta-learning to learn policies that can efficiently adapt to new tools given their corresponding text descriptions. Our results demonstrate that combining linguistic information and meta-learning significantly accelerates tool learning in several manipulation tasks including pushing, lifting, sweeping, and hammering.</td>
                <td><a href="https://proceedings.mlr.press/v205/ren23a.html">https://proceedings.mlr.press/v205/ren23a.html</a></td>
            </tr>
        
            <tr>
                <td>Adapting Neural Models with Sequential Monte Carlo Dropout</td>
                <td>P, a, m, e, l, a,  , C, a, r, r, e, n, o, ,,  , D, a, n, a,  , K, u, l, i, c, ,,  , M, i, c, h, a, e, l,  , B, u, r, k, e</td>
                <td>corl2022</td>
                <td>The ability to adapt to changing environments and settings is essential for robots acting in dynamic and unstructured environments or working alongside humans with varied abilities or preferences. This work introduces an extremely simple and effective approach to adapting neural models in response to changing settings, without requiring any specialised meta-learning strategies. We first train a standard network using dropout, which is analogous to learning an ensemble of predictive models or distribution over predictions. At run-time, we use a particle filter to maintain a distribution over dropout masks to adapt the neural model to changing settings in an online manner. Experimental results show improved performance in control problems requiring both online and look-ahead prediction, and showcase the interpretability of the inferred masks in a human behaviour modelling task for drone tele-operation. </td>
                <td><a href="https://proceedings.mlr.press/v205/carreno23a.html">https://proceedings.mlr.press/v205/carreno23a.html</a></td>
            </tr>
        
            <tr>
                <td>Do we use the Right Measure? Challenges in Evaluating Reward Learning Algorithms</td>
                <td>N, i, l, s,  , W, i, l, d, e, ,,  , J, a, v, i, e, r,  , A, l, o, n, s, o, -, M, o, r, a</td>
                <td>corl2022</td>
                <td>Reward learning is a highly active area of research in human-robot interaction (HRI), allowing a broad range of users to specify complex robot behaviour. Experiments with simulated user input play a major role in the development and evaluation of reward learning algorithms due to the availability of a ground truth. In this paper, we review measures for evaluating reward learning algorithms used in HRI, most of which fall into two classes. In a theoretical worst case analysis and several examples, we show that both classes of measures can fail to effectively indicate how good the learned robot behaviour is. Thus, our work contributes to the characterization of sim-to-real gaps of reward learning in HRI.</td>
                <td><a href="https://proceedings.mlr.press/v205/wilde23a.html">https://proceedings.mlr.press/v205/wilde23a.html</a></td>
            </tr>
        
            <tr>
                <td>Bayesian Object Models for Robotic Interaction with Differentiable Probabilistic Programming</td>
                <td>K, r, i, s, h, n, a,  , M, u, r, t, h, y,  , J, a, t, a, v, a, l, l, a, b, h, u, l, a, ,,  , M, i, l, e, s,  , M, a, c, k, l, i, n, ,,  , D, i, e, t, e, r,  , F, o, x, ,,  , A, n, i, m, e, s, h,  , G, a, r, g, ,,  , F, a, b, i, o,  , R, a, m, o, s</td>
                <td>corl2022</td>
                <td>A hallmark of human intelligence is the ability to build rich mental models of previously unseen objects from very few interactions. To achieve true, continuous autonomy, robots too must possess this ability. Importantly, to integrate with the probabilistic robotics software stack, such models must encapsulate the uncertainty (resulting from noisy dynamics and observation models) in a prescriptive manner. We present Bayesian Object Models (BOMs): generative (probabilistic) models that encode both the structural and kinodynamic attributes of an object. BOMs are implemented in the form of a differentiable probabilistic program that models latent scene structure, object dynamics, and observation models. This allows for efficient and automated Bayesian inference – samples (object trajectories) drawn from the BOM are compared with a small set of real-world observations and used to compute a likelihood function. Our model comprises a differentiable tree structure sampler and a differentiable physics engine, enabling gradient computation through this likelihood function. This enables gradient-based Bayesian inference to efficiently update the distributional parameters of our model. BOMs outperform several recent approaches, including differentiable physics-based, gradient-free, and neural inference schemes. Further information at: https://bayesianobjects.github.io/</td>
                <td><a href="https://proceedings.mlr.press/v205/jatavallabhula23a.html">https://proceedings.mlr.press/v205/jatavallabhula23a.html</a></td>
            </tr>
        
            <tr>
                <td>Deep Projective Rotation Estimation through Relative Supervision</td>
                <td>B, r, i, a, n,  , O, k, o, r, n, ,,  , C, h, u, e, r,  , P, a, n, ,,  , M, a, r, t, i, a, l,  , H, e, b, e, r, t, ,,  , D, a, v, i, d,  , H, e, l, d</td>
                <td>corl2022</td>
                <td>Orientation estimation is the core to a variety of vision and robotics tasks such as camera and object pose estimation. Deep learning has offered a way to develop image-based orientation estimators; however, such estimators often require training on a large labeled dataset, which can be time-intensive to collect. In this work, we explore whether self-supervised learning from unlabeled data can be used to alleviate this issue.  Specifically, we assume access to estimates of the relative orientation between neighboring poses, such that can be obtained via a local alignment method. While self-supervised learning has been used successfully for translational object keypoints, in this work, we show that naively applying relative supervision to the rotational group $SO(3)$ will often fail to converge due to the non-convexity of the rotational space. To tackle this challenge, we propose a new algorithm for self-supervised orientation estimation which utilizes Modified Rodrigues Parameters to stereographically project the closed manifold of $SO(3)$ to the open manifold of $\mathbb{R}^{3}$, allowing the optimization to be done in an open Euclidean space. We empirically validate the benefits of the proposed algorithm for rotational averaging problem in two settings: (1) direct optimization on rotation parameters, and (2) optimization of parameters of a convolutional neural network that predicts object orientations from images. In both settings, we demonstrate that our proposed algorithm is able to converge to a consistent relative orientation frame much faster than algorithms that purely operate in the $SO(3)$ space. Additional information can be found at https://sites.google.com/view/deep-projective-rotation.</td>
                <td><a href="https://proceedings.mlr.press/v205/okorn23a.html">https://proceedings.mlr.press/v205/okorn23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Markerless Robot-Depth Camera Calibration and End-Effector Pose Estimation</td>
                <td>B, u, g, r, a,  , C, a, n,  , S, e, f, e, r, c, i, k, ,,  , B, a, r, i, s,  , A, k, g, u, n</td>
                <td>corl2022</td>
                <td>Traditional approaches to extrinsic calibration use fiducial markers and learning-based approaches rely heavily on simulation data. In this work, we present a learning-based markerless extrinsic calibration system that uses a depth camera and does not rely on simulation data. We learn models for end-effector (EE) segmentation, single-frame rotation prediction and keypoint detection, from automatically generated real-world data. We use a transformation trick to get EE pose estimates from rotation predictions and a matching algorithm to get EE pose estimates from keypoint predictions. We further utilize the iterative closest point algorithm, multiple-frames, filtering and outlier detection to increase calibration robustness. Our evaluations with training data from multiple camera poses and test data from previously unseen poses give sub-centimeter and sub-deciradian average calibration and pose estimation errors. We also show that a carefully selected single training pose gives comparable results.</td>
                <td><a href="https://proceedings.mlr.press/v205/sefercik23a.html">https://proceedings.mlr.press/v205/sefercik23a.html</a></td>
            </tr>
        
            <tr>
                <td>Visuotactile Affordances for Cloth Manipulation with Local Control</td>
                <td>N, e, h, a,  , S, u, n, i, l, ,,  , S, h, a, o, x, i, o, n, g,  , W, a, n, g, ,,  , Y, u,  , S, h, e, ,,  , E, d, w, a, r, d,  , A, d, e, l, s, o, n, ,,  , A, l, b, e, r, t, o,  , R, o, d, r, i, g, u, e, z,  , G, a, r, c, i, a</td>
                <td>corl2022</td>
                <td>Cloth in the real world is often crumpled, self-occluded, or folded in on itself such that key regions, such as corners, are not directly graspable, making manipulation difficult. We propose a system that leverages visual and tactile perception to unfold the cloth via grasping and sliding on edges. Doing so, the robot is able to grasp two adjacent corners, enabling subsequent manipulation tasks like folding or hanging. We develop tactile perception networks that classify whether an edge is grasped and estimate the pose of the edge. We use the edge classification network to supervise a visuotactile edge grasp affordance network that can grasp edges with a 90% success rate. Once an edge is grasped, we demonstrate that the robot can slide along the cloth to the adjacent corner using tactile pose estimation/control in real time.</td>
                <td><a href="https://proceedings.mlr.press/v205/sunil23a.html">https://proceedings.mlr.press/v205/sunil23a.html</a></td>
            </tr>
        
            <tr>
                <td>Is Anyone There? Learning a Planner Contingent on Perceptual Uncertainty</td>
                <td>C, h, a, r, l, e, s,  , P, a, c, k, e, r, ,,  , N, i, c, h, o, l, a, s,  , R, h, i, n, e, h, a, r, t, ,,  , R, o, w, a, n,  , T, h, o, m, a, s,  , M, c, A, l, l, i, s, t, e, r, ,,  , M, a, t, t, h, e, w,  , A, .,  , W, r, i, g, h, t, ,,  , X, i, n,  , W, a, n, g, ,,  , J, e, f, f,  , H, e, ,,  , S, e, r, g, e, y,  , L, e, v, i, n, e, ,,  , J, o, s, e, p, h,  , E, .,  , G, o, n, z, a, l, e, z</td>
                <td>corl2022</td>
                <td>Robots in complex multi-agent environments should reason about the intentions of observed and currently unobserved agents. In this paper, we present a new learning-based method for prediction and planning in complex multi-agent environments where the states of the other agents are partially-observed. Our approach, Active Visual Planning (AVP), uses high-dimensional observations to learn a flow-based generative model of multi-agent joint trajectories, including unobserved agents that may be revealed in the near future, depending on the robot’s actions. Our predictive model is implemented using deep neural networks that map raw observations to future detection and pose trajectories and is learned entirely offline using a dataset of recorded observations (not ground-truth states). Once learned, our predictive model can be used for contingency planning over the potential existence, intentions, and positions of unobserved agents. We demonstrate the effectiveness of AVP on a set of autonomous driving environments inspired by real-world scenarios that require reasoning about the existence of other unobserved agents for safe and efficient driving. In these environments, AVP achieves optimal closed-loop performance, while methods that do not reason about potential unobserved agents exhibit either overconfident or underconfident behavior.</td>
                <td><a href="https://proceedings.mlr.press/v205/packer23a.html">https://proceedings.mlr.press/v205/packer23a.html</a></td>
            </tr>
        
            <tr>
                <td>Touching a NeRF: Leveraging Neural Radiance Fields for Tactile Sensory Data Generation</td>
                <td>S, h, a, o, h, o, n, g,  , Z, h, o, n, g, ,,  , A, l, e, s, s, a, n, d, r, o,  , A, l, b, i, n, i, ,,  , O, i, w, i,  , P, a, r, k, e, r,  , J, o, n, e, s, ,,  , P, e, r, l, a,  , M, a, i, o, l, i, n, o, ,,  , I, n, g, m, a, r,  , P, o, s, n, e, r</td>
                <td>corl2022</td>
                <td>Tactile perception is key for robotics applications such as manipulation. However, tactile data collection is time-consuming, especially when compared to vision. This limits the use of the tactile modality in machine learning solutions in robotics. In this paper, we propose a generative model to simulate realistic tactile sensory data for use in downstream tasks. Starting with easily-obtained camera images, we train Neural Radiance Fields (NeRF) for objects of interest. We then use NeRF-rendered RGB-D images as inputs to a conditional Generative Adversarial Network model (cGAN) to generate tactile images from desired orientations. We evaluate the generated data quantitatively using the Structural Similarity Index and Mean Squared Error metrics, and also using a tactile classification task both in simulation and in the real world. Results show that by augmenting a manually collected dataset, the generated data is able to increase classification accuracy by around 10%. In addition, we demonstrate that our model is able to transfer from one tactile sensor to another with a small fine-tuning dataset.</td>
                <td><a href="https://proceedings.mlr.press/v205/zhong23a.html">https://proceedings.mlr.press/v205/zhong23a.html</a></td>
            </tr>
        
            <tr>
                <td>HTRON: Efficient Outdoor Navigation with Sparse Rewards via  Heavy Tailed Adaptive Reinforce Algorithm</td>
                <td>K, a, s, u, n,  , W, e, e, r, a, k, o, o, n, ,,  , S, o, u, r, a, d, i, p,  , C, h, a, k, r, a, b, o, r, t, y, ,,  , N, a, r, e,  , K, a, r, a, p, e, t, y, a, n, ,,  , A, d, a, r, s, h,  , J, a, g, a, n,  , S, a, t, h, y, a, m, o, o, r, t, h, y, ,,  , A, m, r, i, t,  , B, e, d, i, ,,  , D, i, n, e, s, h,  , M, a, n, o, c, h, a</td>
                <td>corl2022</td>
                <td>We present a novel approach to improve the performance of deep reinforcement learning (DRL) based outdoor robot navigation systems. Most, existing DRL methods are based on carefully designed dense reward functions that learn the efficient behavior in an environment. We circumvent this issue by working only with sparse rewards (which are easy to design) and propose a novel adaptive Heavy-Tailed Reinforce algorithm for Outdoor Navigation called HTRON. Our main idea is to utilize heavy-tailed policy parametrizations which implicitly induce exploration in sparse reward settings. We evaluate the performance of HTRON against Reinforce, PPO, and TRPO algorithms in three different outdoor scenarios: goal-reaching, obstacle avoidance, and uneven terrain navigation. We observe average an increase of 34.41% in terms of success rate, a 15.15% decrease in the average time steps taken to reach the goal, and a 24.9% decrease in the elevation cost compared to the navigation policies obtained by the other methods. Further, we demonstrate that our algorithm can be transferred directly into a Clearpath Husky robot to perform outdoor terrain navigation in real-world scenarios.</td>
                <td><a href="https://proceedings.mlr.press/v205/weerakoon23a.html">https://proceedings.mlr.press/v205/weerakoon23a.html</a></td>
            </tr>
        
            <tr>
                <td>Planning with Spatial-Temporal Abstraction from Point Clouds for Deformable Object Manipulation</td>
                <td>X, i, n, g, y, u,  , L, i, n, ,,  , C, a, r, l,  , Q, i, ,,  , Y, u, n, c, h, u,  , Z, h, a, n, g, ,,  , Z, h, i, a, o,  , H, u, a, n, g, ,,  , K, a, t, e, r, i, n, a,  , F, r, a, g, k, i, a, d, a, k, i, ,,  , Y, u, n, z, h, u,  , L, i, ,,  , C, h, u, a, n, g,  , G, a, n, ,,  , D, a, v, i, d,  , H, e, l, d</td>
                <td>corl2022</td>
                <td>Effective planning of long-horizon deformable object manipulation requires suitable abstractions at both the spatial and temporal levels. Previous methods typically either focus on short-horizon tasks or make strong assumptions that full-state information is available, which prevents their use on deformable objects. In this paper, we propose PlAnning with Spatial-Temporal Abstraction (PASTA), which incorporates both spatial abstraction (reasoning about objects and their relations to each other) and temporal abstraction (reasoning over skills instead of low-level actions). Our framework maps high-dimension 3D observations such as point clouds into a set of latent vectors and plans over skill sequences on top of the latent set representation. We show that our method can effectively perform  challenging sequential deformable object manipulation tasks in the real world, which require combining multiple tool-use skills such as cutting with a knife, pushing with a pusher, and spreading dough with a roller. Additional materials can be found at our project website: https://sites.google.com/view/pasta-plan.</td>
                <td><a href="https://proceedings.mlr.press/v205/lin23b.html">https://proceedings.mlr.press/v205/lin23b.html</a></td>
            </tr>
        
            <tr>
                <td>Don’t Start From Scratch: Leveraging Prior Data to Automate Robotic Reinforcement Learning</td>
                <td>H, o, m, e, r,  , R, i, c, h,  , W, a, l, k, e, ,,  , J, o, n, a, t, h, a, n,  , H, e, e, w, o, n,  , Y, a, n, g, ,,  , A, l, b, e, r, t,  , Y, u, ,,  , A, v, i, r, a, l,  , K, u, m, a, r, ,,  , J, ę, d, r, z, e, j,  , O, r, b, i, k, ,,  , A, v, i,  , S, i, n, g, h, ,,  , S, e, r, g, e, y,  , L, e, v, i, n, e</td>
                <td>corl2022</td>
                <td>Reinforcement learning (RL) algorithms hold the promise of enabling autonomous skill acquisition for robotic systems. However, in practice, real-world robotic RL typically requires time consuming data collection and frequent human intervention to reset the environment. Moreover, robotic policies learned with RL often fail when deployed beyond the carefully controlled setting in which they were learned. In this work, we study how these challenges of real-world robotic learning can all be tackled by effective utilization of diverse offline datasets collected from previously seen tasks. When faced with a new task, our system adapts previously learned skills to quickly learn to both perform the new task and return the environment to an initial state, effectively performing its own environment reset. Our empirical results demonstrate that incorporating prior data into robotic reinforcement learning enables autonomous learning, substantially improves sample-efficiency of learning, and enables better generalization.</td>
                <td><a href="https://proceedings.mlr.press/v205/walke23a.html">https://proceedings.mlr.press/v205/walke23a.html</a></td>
            </tr>
        
            <tr>
                <td>LaRa: Latents and Rays for Multi-Camera Bird’s-Eye-View Semantic Segmentation</td>
                <td>F, l, o, r, e, n, t,  , B, a, r, t, o, c, c, i, o, n, i, ,,  , E, l, o, i,  , Z, a, b, l, o, c, k, i, ,,  , A, n, d, r, e, i,  , B, u, r, s, u, c, ,,  , P, a, t, r, i, c, k,  , P, e, r, e, z, ,,  , M, a, t, t, h, i, e, u,  , C, o, r, d, ,,  , K, a, r, t, e, e, k,  , A, l, a, h, a, r, i</td>
                <td>corl2022</td>
                <td>Recent works in autonomous driving have widely adopted the bird’seye-view (BEV) semantic map as an intermediate representation of the world. Online prediction of these BEV maps involves non-trivial operations such as multi-camera data extraction as well as fusion and projection into a common topview grid. This is usually done with error-prone geometric operations (e.g., homography or back-projection from monocular depth estimation) or expensive direct dense mapping between image pixels and pixels in BEV (e.g., with MLP or attention). In this work, we present ‘LaRa’, an efficient encoder-decoder, transformer-based model for vehicle semantic segmentation from multiple cameras. Our approach uses a system of cross-attention to aggregate information over multiple sensors into a compact, yet rich, collection of latent representations. These latent representations, after being processed by a series of selfattention blocks, are then reprojected with a second cross-attention in the BEV space. We demonstrate that our model outperforms the best previous works using transformers on nuScenes. The code and trained models are available at https://github.com/valeoai/LaRa.</td>
                <td><a href="https://proceedings.mlr.press/v205/bartoccioni23a.html">https://proceedings.mlr.press/v205/bartoccioni23a.html</a></td>
            </tr>
        
            <tr>
                <td>Leveraging Fully Observable Policies for Learning under Partial Observability</td>
                <td>H, a, i,  , H, u, u,  , N, g, u, y, e, n, ,,  , A, n, d, r, e, a,  , B, a, i, s, e, r, o, ,,  , D, i, a, n,  , W, a, n, g, ,,  , C, h, r, i, s, t, o, p, h, e, r,  , A, m, a, t, o, ,,  , R, o, b, e, r, t,  , P, l, a, t, t</td>
                <td>corl2022</td>
                <td>Reinforcement learning in partially observable domains is challenging due to the lack of observable state information. Thankfully, learning offline in a simulator with such state information is often possible. In particular, we propose a method for partially observable reinforcement learning that uses a fully observable policy (which we call a \emph{state expert}) during training to improve performance. Based on Soft Actor-Critic (SAC), our agent balances performing actions similar to the state expert and getting high returns under partial observability. Our approach can leverage the fully-observable policy for exploration and parts of the domain that are fully observable while still being able to learn under partial observability. On six robotics domains, our method outperforms pure imitation, pure reinforcement learning, the sequential or parallel combination of both types, and a recent state-of-the-art method in the same setting. A successful policy transfer to a physical robot in a manipulation task from pixels shows our approach’s practicality in learning interesting policies under partial observability.</td>
                <td><a href="https://proceedings.mlr.press/v205/nguyen23a.html">https://proceedings.mlr.press/v205/nguyen23a.html</a></td>
            </tr>
        
            <tr>
                <td>Modularity through Attention: Efficient Training and Transfer of Language-Conditioned Policies for Robot Manipulation</td>
                <td>Y, i, f, a, n,  , Z, h, o, u, ,,  , S, h, u, b, h, a, m,  , S, o, n, a, w, a, n, i, ,,  , M, a, r, i, a, n, o,  , P, h, i, e, l, i, p, p, ,,  , S, i, m, o, n,  , S, t, e, p, p, u, t, t, i, s, ,,  , H, e, n, i,  , A, m, o, r</td>
                <td>corl2022</td>
                <td>Language-conditioned policies allow robots to interpret and execute human instructions. Learning such policies requires a substantial investment with regards to time and compute resources. Still, the resulting controllers are highly device-specific and cannot easily be transferred to a robot with different morphology, capability, appearance or dynamics. In this paper, we propose a sample-efficient approach for training language-conditioned manipulation policies that allows for rapid transfer across different types of robots. By introducing a novel method, namely Hierarchical Modularity, and adopting supervised attention across multiple sub-modules, we bridge the divide between modular and end-to-end learning and enable the reuse of functional building blocks. In both simulated and real world robot manipulation experiments, we demonstrate that our method outperforms the current state-of-the-art methods and can transfer policies across 4 different robots in a sample-efficient manner. Finally, we show that the functionality of learned sub-modules is maintained beyond the training process and can be used to introspect the robot decision-making process.</td>
                <td><a href="https://proceedings.mlr.press/v205/zhou23b.html">https://proceedings.mlr.press/v205/zhou23b.html</a></td>
            </tr>
        
            <tr>
                <td>PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale</td>
                <td>K, u, a, n, g, -, H, u, e, i,  , L, e, e, ,,  , T, e, d,  , X, i, a, o, ,,  , A, d, r, i, a, n,  , L, i, ,,  , P, a, u, l,  , W, o, h, l, h, a, r, t, ,,  , I, a, n,  , F, i, s, c, h, e, r, ,,  , Y, a, o,  , L, u</td>
                <td>corl2022</td>
                <td>The predictive information, the mutual information between the past and future, has been shown to be a useful representation learning auxiliary loss for training reinforcement learning agents, as the ability to model what will happen next is critical to success on many control tasks. While existing studies are largely restricted to training specialist agents on single-task settings in simulation, in this work, we study modeling the predictive information for robotic agents and its importance for general-purpose agents that are trained to master a large repertoire of diverse skills from large amounts of data. Specifically, we introduce Predictive Information QT-Opt (PI-QT-Opt), a QT-Opt agent augmented with an auxiliary loss that learns representations of the predictive information to solve up to 297 vision-based robot manipulation tasks in simulation and the real world with a single set of parameters. We demonstrate that modeling the predictive information significantly improves success rates on the training tasks and leads to better zero-shot transfer to unseen novel tasks. Finally, we evaluate PI-QT-Opt on real robots, achieving substantial and consistent improvement over QT-Opt in multiple experimental settings of varying environments, skills, and multi-task configurations.</td>
                <td><a href="https://proceedings.mlr.press/v205/lee23a.html">https://proceedings.mlr.press/v205/lee23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Model Predictive Controllers with Real-Time Attention for Real-World Navigation</td>
                <td>X, u, e, s, u,  , X, i, a, o, ,,  , T, i, n, g, n, a, n,  , Z, h, a, n, g, ,,  , K, r, z, y, s, z, t, o, f,  , M, a, r, c, i, n,  , C, h, o, r, o, m, a, n, s, k, i, ,,  , T, s, a, n, g, -, W, e, i,  , E, d, w, a, r, d,  , L, e, e, ,,  , A, n, t, h, o, n, y,  , F, r, a, n, c, i, s, ,,  , J, a, k, e,  , V, a, r, l, e, y, ,,  , S, t, e, p, h, e, n,  , T, u, ,,  , S, u, m, e, e, t,  , S, i, n, g, h, ,,  , P, e, n, g,  , X, u, ,,  , F, e, i,  , X, i, a, ,,  , S, v, e, n,  , M, i, k, a, e, l,  , P, e, r, s, s, o, n, ,,  , D, m, i, t, r, y,  , K, a, l, a, s, h, n, i, k, o, v, ,,  , L, e, i, l, a,  , T, a, k, a, y, a, m, a, ,,  , R, o, y,  , F, r, o, s, t, i, g, ,,  , J, i, e,  , T, a, n, ,,  , C, a, r, o, l, i, n, a,  , P, a, r, a, d, a, ,,  , V, i, k, a, s,  , S, i, n, d, h, w, a, n, i</td>
                <td>corl2022</td>
                <td>Despite decades of research, existing navigation systems still face real-world challenges when deployed in the wild, e.g., in cluttered home environments or in human-occupied public spaces. To address this, we present a new class of implicit control policies combining the benefits of imitation learning with the robust handling of system constraints from Model Predictive Control (MPC). Our approach, called Performer-MPC, uses a learned cost function parameterized by vision context embeddings provided by Performers—a low-rank implicit-attention Transformer. We jointly train the cost function and construct the controller relying on it, effectively solving end-to-end the corresponding bi-level optimization problem. We show that the resulting policy improves standard MPC performance by leveraging a few expert demonstrations of the desired navigation behavior in different challenging real-world scenarios. Compared with a standard MPC policy, Performer-MPC achieves >40% better goal reached in cluttered environments and >65% better on social metrics when navigating around humans. </td>
                <td><a href="https://proceedings.mlr.press/v205/xiao23a.html">https://proceedings.mlr.press/v205/xiao23a.html</a></td>
            </tr>
        
            <tr>
                <td>In-Hand Object Rotation via Rapid Motor Adaptation</td>
                <td>H, a, o, z, h, i,  , Q, i, ,,  , A, s, h, i, s, h,  , K, u, m, a, r, ,,  , R, o, b, e, r, t, o,  , C, a, l, a, n, d, r, a, ,,  , Y, i,  , M, a, ,,  , J, i, t, e, n, d, r, a,  , M, a, l, i, k</td>
                <td>corl2022</td>
                <td>Generalized in-hand manipulation has long been an unsolved challenge of robotics. As a small step towards this grand goal, we demonstrate how to design and learn a simple adaptive controller to achieve in-hand object rotation using only fingertips. The controller is trained entirely in simulation on only cylindrical objects, which then – without any fine-tuning – can be directly deployed to a real robot hand to rotate dozens of objects with diverse sizes, shapes, and weights over the z-axis. This is achieved via rapid online adaptation of the robot’s controller to the object properties using only proprioception history. Furthermore, natural and stable finger gaits automatically emerge from training the control policy via reinforcement learning. Code and more videos are available at https://github.com/HaozhiQi/Hora .</td>
                <td><a href="https://proceedings.mlr.press/v205/qi23a.html">https://proceedings.mlr.press/v205/qi23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Sampling Distributions for Model Predictive Control</td>
                <td>J, a, c, o, b,  , S, a, c, k, s, ,,  , B, y, r, o, n,  , B, o, o, t, s</td>
                <td>corl2022</td>
                <td>Sampling-based methods have become a cornerstone of contemporary approaches to Model Predictive Control (MPC), as they make no restrictions on the differentiability of the dynamics or cost function and are straightforward to parallelize. However, their efficacy is highly dependent on the quality of the sampling distribution itself, which is often assumed to be simple, like a Gaussian. This restriction can result in samples which are far from optimal, leading to poor performance. Recent work has explored improving the performance of MPC by sampling in a learned latent space of controls. However, these methods ultimately perform all MPC parameter updates and warm-starting between time steps in the control space. This requires us to rely on a number of heuristics for generating samples and updating the distribution and may lead to sub-optimal performance. Instead, we propose to carry out all operations in the latent space, allowing us to take full advantage of the learned distribution. Specifically, we frame the learning problem as bi-level optimization and show how to train the controller with backpropagation-through-time. By using a normalizing flow parameterization of the distribution, we can leverage its tractable density to avoid requiring differentiability of the dynamics and cost function. Finally, we evaluate the proposed approach on simulated robotics tasks and demonstrate its ability to surpass the performance of prior methods and scale better with a reduced number of samples.</td>
                <td><a href="https://proceedings.mlr.press/v205/sacks23a.html">https://proceedings.mlr.press/v205/sacks23a.html</a></td>
            </tr>
        
            <tr>
                <td>Embodied Concept Learner: Self-supervised Learning of Concepts and Mapping through Instruction Following</td>
                <td>M, i, n, g, y, u,  , D, i, n, g, ,,  , Y, a, n,  , X, u, ,,  , Z, h, e, n, f, a, n, g,  , C, h, e, n, ,,  , D, a, v, i, d,  , D, a, n, i, e, l,  , C, o, x, ,,  , P, i, n, g,  , L, u, o, ,,  , J, o, s, h, u, a,  , B, .,  , T, e, n, e, n, b, a, u, m, ,,  , C, h, u, a, n, g,  , G, a, n</td>
                <td>corl2022</td>
                <td>Humans, even at a very early age, can learn visual concepts and understand geometry and layout through active interaction with the environment, and generalize their compositions to complete tasks described by natural languages in novel scenes. To mimic such capability, we propose Embodied Concept Learner (ECL) in an interactive 3D environment. Specifically, a robot agent can ground visual concepts, build semantic maps and plan actions to complete tasks by learning purely from human demonstrations and language instructions, without access to ground-truth semantic and depth supervision from simulations. ECL consists of: (i) an instruction parser that translates the natural languages into executable programs; (ii) an embodied concept learner that grounds visual concepts based on language descriptions; (iii) a map constructor that estimates depth and constructs semantic maps by leveraging the learned concepts; and (iv) a program executor with deterministic policies to execute each program. ECL has several appealing benefits thanks to its modularized design. Firstly, it enables the robotic agent to learn semantics and depth unsupervisedly acting like babies, e.g., ground concepts through active interaction and perceive depth by disparities when moving forward. Secondly, ECL is fully transparent and step-by-step interpretable in long-term planning. Thirdly, ECL could be beneficial for the embodied instruction following (EIF), outperforming previous works on the ALFRED benchmark when the semantic label is not provided. Also, the learned concept can be reused for other downstream tasks, such as reasoning of object states.</td>
                <td><a href="https://proceedings.mlr.press/v205/ding23b.html">https://proceedings.mlr.press/v205/ding23b.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Multi-Object Dynamics with Compositional Neural Radiance Fields</td>
                <td>D, a, n, n, y,  , D, r, i, e, s, s, ,,  , Z, h, i, a, o,  , H, u, a, n, g, ,,  , Y, u, n, z, h, u,  , L, i, ,,  , R, u, s, s,  , T, e, d, r, a, k, e, ,,  , M, a, r, c,  , T, o, u, s, s, a, i, n, t</td>
                <td>corl2022</td>
                <td>We present a method to learn compositional multi-object dynamics models from image observations based on implicit object encoders, Neural Radiance Fields (NeRFs), and graph neural networks. NeRFs have become a popular choice for representing scenes due to their strong 3D prior. However, most NeRF approaches are trained on a single scene, representing the whole scene with a global model, making generalization to novel scenes, containing different numbers of objects, challenging. Instead, we present a compositional, object-centric auto-encoder framework that maps multiple views of the scene to a set of latent vectors representing each object separately. The latent vectors parameterize individual NeRFs from which the scene can be reconstructed. Based on those latent vectors, we train a graph neural network dynamics model in the latent space to achieve compositionality for dynamics prediction. A key feature of our approach is that the latent vectors are forced to encode 3D information through the NeRF decoder, which enables us to incorporate structural priors in learning the dynamics models, making long-term predictions more stable compared to several baselines. Simulated and real world experiments show that our method can model and learn the dynamics of compositional scenes including rigid and deformable objects. Video: https://dannydriess.github.io/compnerfdyn/</td>
                <td><a href="https://proceedings.mlr.press/v205/driess23a.html">https://proceedings.mlr.press/v205/driess23a.html</a></td>
            </tr>
        
            <tr>
                <td>Inner Monologue: Embodied Reasoning through Planning with Language Models</td>
                <td>W, e, n, l, o, n, g,  , H, u, a, n, g, ,,  , F, e, i,  , X, i, a, ,,  , T, e, d,  , X, i, a, o, ,,  , H, a, r, r, i, s,  , C, h, a, n, ,,  , J, a, c, k, y,  , L, i, a, n, g, ,,  , P, e, t, e,  , F, l, o, r, e, n, c, e, ,,  , A, n, d, y,  , Z, e, n, g, ,,  , J, o, n, a, t, h, a, n,  , T, o, m, p, s, o, n, ,,  , I, g, o, r,  , M, o, r, d, a, t, c, h, ,,  , Y, e, v, g, e, n,  , C, h, e, b, o, t, a, r, ,,  , P, i, e, r, r, e,  , S, e, r, m, a, n, e, t, ,,  , T, o, m, a, s,  , J, a, c, k, s, o, n, ,,  , N, o, a, h,  , B, r, o, w, n, ,,  , L, i, n, d, a,  , L, u, u, ,,  , S, e, r, g, e, y,  , L, e, v, i, n, e, ,,  , K, a, r, o, l,  , H, a, u, s, m, a, n, ,,  , b, r, i, a, n,  , i, c, h, t, e, r</td>
                <td>corl2022</td>
                <td>Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.</td>
                <td><a href="https://proceedings.mlr.press/v205/huang23c.html">https://proceedings.mlr.press/v205/huang23c.html</a></td>
            </tr>
        
            <tr>
                <td>TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation</td>
                <td>C, h, u, e, r,  , P, a, n, ,,  , B, r, i, a, n,  , O, k, o, r, n, ,,  , H, a, r, r, y,  , Z, h, a, n, g, ,,  , B, e, n,  , E, i, s, n, e, r, ,,  , D, a, v, i, d,  , H, e, l, d</td>
                <td>corl2022</td>
                <td>How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship “cross-pose” and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired pose relationship (placing a pan into the oven or the mug onto the mug rack). We demonstrate our method’s capability to generalize to unseen objects, in some cases after training on only 10 demonstrations in the real world. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments across a number of tasks. Supplementary information and videos can be found at https://sites.google.com/view/tax-pose/home.</td>
                <td><a href="https://proceedings.mlr.press/v205/pan23a.html">https://proceedings.mlr.press/v205/pan23a.html</a></td>
            </tr>
        
            <tr>
                <td>SSL-Lanes: Self-Supervised Learning for Motion Forecasting in Autonomous Driving</td>
                <td>P, r, a, r, t, h, a, n, a,  , B, h, a, t, t, a, c, h, a, r, y, y, a, ,,  , C, h, e, n, g, j, i, e,  , H, u, a, n, g, ,,  , K, r, z, y, s, z, t, o, f,  , C, z, a, r, n, e, c, k, i</td>
                <td>corl2022</td>
                <td>Self-supervised learning (SSL) is an emerging technique that has been successfully employed to train convolutional neural networks (CNNs) and graph neural networks (GNNs) for more transferable, generalizable, and robust representation learning. However its potential in motion forecasting for autonomous driving has rarely been explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into motion forecasting. We first propose to investigate four novel self-supervised learning tasks for motion forecasting with theoretical rationale and quantitative and qualitative comparisons on the challenging large-scale Argoverse dataset. Secondly, we point out that our auxiliary SSL-based learning setup not only outperforms forecasting methods which use transformers, complicated fusion mechanisms and sophisticated online dense goal candidate optimization algorithms in terms of performance accuracy, but also has low inference time and architectural complexity. Lastly, we conduct several experiments to understand why SSL improves motion forecasting.</td>
                <td><a href="https://proceedings.mlr.press/v205/bhattacharyya23a.html">https://proceedings.mlr.press/v205/bhattacharyya23a.html</a></td>
            </tr>
        
            <tr>
                <td>VIRDO++: Real-World, Visuo-tactile Dynamics and Perception of Deformable Objects</td>
                <td>Y, o, u, n, g, s, u, n,  , W, i, ,,  , A, n, d, y,  , Z, e, n, g, ,,  , P, e, t, e,  , F, l, o, r, e, n, c, e, ,,  , N, i, m, a,  , F, a, z, e, l, i</td>
                <td>corl2022</td>
                <td>Deformable objects manipulation can benefit from representations that seamlessly integrate vision and touch while handling occlusions. In this work, we present a novel approach for, and real-world demonstration of, multimodal visuo-tactile state-estimation and dynamics prediction for deformable objects. Our approach, VIRDO++, builds on recent progress in multimodal neural implicit representations for deformable object state-estimation (VIRDO) via a new formulation for deformation dynamics and a complementary state-estimation algorithm that (i) maintains a belief over deformations, and (ii) enables practical real-world application by removing the need for privileged contact information. In the context of two real-world robotic tasks, we show: (i) high-fidelity cross-modal state-estimation and prediction of deformable objects from partial visuo-tactile feedback, and (ii) generalization to unseen objects and contact formations. </td>
                <td><a href="https://proceedings.mlr.press/v205/wi23a.html">https://proceedings.mlr.press/v205/wi23a.html</a></td>
            </tr>
        
            <tr>
                <td>Detecting Incorrect Visual Demonstrations for Improved Policy Learning</td>
                <td>M, o, s, t, a, f, a,  , H, u, s, s, e, i, n, ,,  , M, o, m, o, t, a, z,  , B, e, g, u, m</td>
                <td>corl2022</td>
                <td>Learning tasks only from raw video demonstrations is the current state of the art in robotics visual imitation learning research. The implicit assumption here is that all video demonstrations show an optimal/sub-optimal way of performing the task. What if that is not true? What if one or more videos show a wrong way of executing the task? A task policy learned from such incorrect demonstrations can be potentially unsafe for robots and humans. It is therefore important to analyze the video demonstrations for correctness before handing them over to the policy learning algorithm. This is a challenging task, especially due to the very large state space. This paper proposes a framework to autonomously detect incorrect video demonstrations of sequential tasks consisting of several sub-tasks. We analyze the demonstration pool to identify video(s) for which task-features follow a ‘disruptive’ sequence. We analyze entropy to measure this disruption and – through solving a minmax problem – assign poor weights to incorrect videos. We evaluated the framework with two real-world video datasets: our custom-designed Tea-Making with a YuMi robot and the publicly available 50-Salads. Experimental results show the effectiveness of the proposed framework in detecting incorrect video demonstrations even when they make up 40% of the demonstration set. We also show that various state-of-the-art imitation learning algorithms learn a better policy when incorrect demonstrations are discarded from the training pool.</td>
                <td><a href="https://proceedings.mlr.press/v205/hussein23a.html">https://proceedings.mlr.press/v205/hussein23a.html</a></td>
            </tr>
        
            <tr>
                <td>Concept Learning for Interpretable Multi-Agent Reinforcement Learning</td>
                <td>R, e, n, o, s,  , Z, a, b, o, u, n, i, d, i, s, ,,  , J, o, s, e, p, h,  , C, a, m, p, b, e, l, l, ,,  , S, i, m, o, n,  , S, t, e, p, p, u, t, t, i, s, ,,  , D, a, n, a,  , H, u, g, h, e, s, ,,  , K, a, t, i, a,  , P, .,  , S, y, c, a, r, a</td>
                <td>corl2022</td>
                <td>Multi-agent robotic systems are increasingly operating in real-world environments in close proximity to humans, yet are largely controlled by policy models with inscrutable deep neural network representations. We introduce a method for incorporating interpretable concepts from a domain expert into models trained through multi-agent reinforcement learning, by requiring the model to first predict such concepts then utilize them for decision making. This allows an expert to both reason about the resulting concept policy models in terms of these high-level concepts at run-time, as well as intervene and correct mispredictions to improve performance. We show that this yields improved interpretability and training stability, with benefits to policy performance and sample efficiency in a simulated and real-world cooperative-competitive multi-agent game.</td>
                <td><a href="https://proceedings.mlr.press/v205/zabounidis23a.html">https://proceedings.mlr.press/v205/zabounidis23a.html</a></td>
            </tr>
        
            <tr>
                <td>Latent Plans for Task-Agnostic Offline Reinforcement Learning</td>
                <td>E, r, i, c, k,  , R, o, s, e, t, e, -, B, e, a, s, ,,  , O, i, e, r,  , M, e, e, s, ,,  , G, a, b, r, i, e, l,  , K, a, l, w, e, i, t, ,,  , J, o, s, c, h, k, a,  , B, o, e, d, e, c, k, e, r, ,,  , W, o, l, f, r, a, m,  , B, u, r, g, a, r, d</td>
                <td>corl2022</td>
                <td>Everyday tasks of long-horizon and comprising a sequence of multiple implicit subtasks still impose a major challenge in offline robot control. While a number of prior methods aimed to address this setting with variants of imitation and offline reinforcement learning, the learned behavior is typically narrow and often struggles to reach configurable long-horizon goals. As both paradigms have complementary strengths and weaknesses, we propose a novel hierarchical approach that combines the strengths of both methods to learn task-agnostic long-horizon policies from high-dimensional camera observations. Concretely, we combine a low-level policy that learns latent skills via imitation learning and a high-level policy learned from offline reinforcement learning for skill-chaining the latent behavior priors. Experiments in various simulated and real robot control tasks show that our formulation enables producing previously unseen combinations of skills to reach temporally extended goals by “stitching” together latent skills through goal chaining with an order-of-magnitude improvement in performance upon state-of-the-art baselines. We even learn one multi-task visuomotor policy for 25 distinct manipulation tasks in the real world which outperforms both imitation learning and offline reinforcement learning techniques.</td>
                <td><a href="https://proceedings.mlr.press/v205/rosete-beas23a.html">https://proceedings.mlr.press/v205/rosete-beas23a.html</a></td>
            </tr>
        
            <tr>
                <td>Manipulation via Membranes: High-Resolution and Highly Deformable Tactile Sensing and Control</td>
                <td>M, i, q, u, e, l,  , O, l, l, e, r, ,,  , M, i, r, e, i, a,  , P, l, a, n, a, s,  , i,  , L, i, s, b, o, n, a, ,,  , D, m, i, t, r, y,  , B, e, r, e, n, s, o, n, ,,  , N, i, m, a,  , F, a, z, e, l, i</td>
                <td>corl2022</td>
                <td>Collocated tactile sensing is a fundamental enabling technology for dexterous manipulation. However, deformable sensors introduce complex dynamics between the robot, grasped object, and environment that must be considered for fine manipulation. Here, we propose a method to learn soft tactile sensor membrane dynamics that accounts for sensor deformations caused by the physical interaction between the grasped object and environment. Our method combines the perceived 3D geometry of the membrane with proprioceptive reaction wrenches to predict future deformations conditioned on robot action. Grasped object poses are recovered from membrane geometry and reaction wrenches, decoupling interaction dynamics from the tactile observation model. We benchmark our approach on two real-world contact-rich tasks: drawing with a grasped marker and in-hand pivoting. Our results suggest that explicitly modeling membrane dynamics achieves better task performance and generalization to unseen objects than baselines.</td>
                <td><a href="https://proceedings.mlr.press/v205/oller23a.html">https://proceedings.mlr.press/v205/oller23a.html</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Generation of Time-Optimal Quadrotor Trajectories with Semi-Supervised Seq2Seq Learning</td>
                <td>G, i, l, h, y, u, n,  , R, y, o, u, ,,  , E, z, r, a,  , T, a, l, ,,  , S, e, r, t, a, c,  , K, a, r, a, m, a, n</td>
                <td>corl2022</td>
                <td>Generating time-optimal quadrotor trajectories is challenging due to the complex dynamics of high-speed, agile flight. In this paper, we propose a data-driven method for real-time time-optimal trajectory generation that is suitable for complicated system models. We utilize a temporal deep neural network with sequence-to-sequence learning to find the optimal trajectories for sequences of a variable number of waypoints. The model is efficiently trained in a semi-supervised manner by combining supervised pretraining using a minimum-snap baseline method with Bayesian optimization and reinforcement learning. Compared to the baseline method, the trained model generates up to 20 % faster trajectories at an order of magnitude less computational cost. The optimized trajectories are evaluated in simulation and real-world flight experiments, where the improvement is further demonstrated. </td>
                <td><a href="https://proceedings.mlr.press/v205/ryou23a.html">https://proceedings.mlr.press/v205/ryou23a.html</a></td>
            </tr>
        
            <tr>
                <td>Sim-to-Real via Sim-to-Seg: End-to-end Off-road Autonomous Driving Without Real Data</td>
                <td>J, o, h, n,  , S, o, ,,  , A, m, b, e, r,  , X, i, e, ,,  , S, u, n, g, g, o, o,  , J, u, n, g, ,,  , J, e, f, f, r, e, y,  , E, d, l, u, n, d, ,,  , R, o, h, a, n,  , T, h, a, k, k, e, r, ,,  , A, l, i, -, a, k, b, a, r,  , A, g, h, a, -, m, o, h, a, m, m, a, d, i, ,,  , P, i, e, t, e, r,  , A, b, b, e, e, l, ,,  , S, t, e, p, h, e, n,  , J, a, m, e, s</td>
                <td>corl2022</td>
                <td>Autonomous driving is complex, requiring sophisticated 3D scene understanding, localization, mapping, and control. Rather than explicitly modelling and fusing each of these components, we instead consider an end-to-end approach via reinforcement learning (RL). However, collecting exploration driving data in the real world is impractical and dangerous. While training in simulation and deploying visual sim-to-real techniques has worked well for robot manipulation, deploying beyond controlled workspace viewpoints remains a challenge. In this paper, we address this challenge by presenting Sim2Seg, a re-imagining of RCAN that crosses the visual reality gap for off-road autonomous driving, without using any real-world data. This is done by learning to translate randomized simulation images into simulated segmentation and depth maps, subsequently enabling real-world images to also be translated. This allows us to train an end-to-end RL policy in simulation, and directly deploy in the real-world. Our approach, which can be trained in 48 hours on 1 GPU, can perform equally as well as a classical perception and control stack that took thousands of engineering hours over several months to build. We hope this work motivates future end-to-end autonomous driving research.</td>
                <td><a href="https://proceedings.mlr.press/v205/so23a.html">https://proceedings.mlr.press/v205/so23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Road Scene-level Representations via Semantic Region Prediction</td>
                <td>Z, i, h, a, o,  , X, i, a, o, ,,  , A, l, a, n,  , Y, u, i, l, l, e, ,,  , Y, i, -, T, i, n, g,  , C, h, e, n</td>
                <td>corl2022</td>
                <td>In this work, we tackle two vital tasks in automated driving systems, i.e., driver intent prediction and risk object identification from egocentric images. Mainly, we investigate the question: what would be good road scene-level representations for these two tasks? We contend that a scene-level representation must capture higher-level semantic and geometric representations of traffic scenes around ego-vehicle while performing actions to their destinations. To this end, we introduce the representation of semantic regions, which are areas where ego-vehicles visit while taking an afforded action (e.g., left-turn at 4-way intersections). We propose to learn scene-level representations via a novel semantic region prediction task and an automatic semantic region labeling algorithm. Extensive evaluations are conducted on the HDD and nuScenes datasets, and the learned representations lead to state-of-the-art performance for driver intention prediction and risk object identification. </td>
                <td><a href="https://proceedings.mlr.press/v205/xiao23b.html">https://proceedings.mlr.press/v205/xiao23b.html</a></td>
            </tr>
        
            <tr>
                <td>GenLoco: Generalized Locomotion Controllers for Quadrupedal Robots</td>
                <td>G, i, l, b, e, r, t,  , F, e, n, g, ,,  , H, o, n, g, b, o,  , Z, h, a, n, g, ,,  , Z, h, o, n, g, y, u,  , L, i, ,,  , X, u, e,  , B, i, n,  , P, e, n, g, ,,  , B, h, u, v, a, n,  , B, a, s, i, r, e, d, d, y, ,,  , L, i, n, z, h, u,  , Y, u, e, ,,  , Z, H, I, T, A, O,  , S, O, N, G, ,,  , L, i, z, h, i,  , Y, a, n, g, ,,  , Y, u, n, h, u, i,  , L, i, u, ,,  , K, o, u, s, h, i, l,  , S, r, e, e, n, a, t, h, ,,  , S, e, r, g, e, y,  , L, e, v, i, n, e</td>
                <td>corl2022</td>
                <td>Recent years have seen a surge in commercially-available and affordable quadrupedal robots, with many of these platforms being actively used in research and industry. As the availability of legged robots grows, so does the need for controllers that enable these robots to perform useful skills. However, most learning-based frameworks for controller development focus on training robot-specific controllers, a process that needs to be repeated for every new robot. In this work, we introduce a framework for training generalized locomotion (GenLoco) controllers for quadrupedal robots. Our framework synthesizes general-purpose locomotion controllers that can be deployed on a large variety of quadrupedal robots with similar morphologies. We present a simple but effective morphology randomization method that procedurally generates a diverse set of simulated robots for training. We show that by training a controller on this large set of simulated robots, our models acquire more general control strategies that can be directly transferred to novel simulated and real-world robots with diverse morphologies, which were not observed during training.</td>
                <td><a href="https://proceedings.mlr.press/v205/feng23a.html">https://proceedings.mlr.press/v205/feng23a.html</a></td>
            </tr>
        
            <tr>
                <td>Towards Long-Tailed 3D Detection</td>
                <td>N, e, e, h, a, r,  , P, e, r, i, ,,  , A, c, h, a, l,  , D, a, v, e, ,,  , D, e, v, a,  , R, a, m, a, n, a, n, ,,  , S, h, u,  , K, o, n, g</td>
                <td>corl2022</td>
                <td>Contemporary autonomous vehicle (AV) benchmarks have advanced techniques for training 3D detectors, particularly on large-scale lidar data. Surprisingly, although semantic class labels naturally follow a long-tailed distribution, contemporary benchmarks focus on only a few common classes (e.g., pedestrian and car) and neglect many rare classes in-the-tail (e.g., debris and stroller). However, AVs must still detect rare classes to ensure safe operation. Moreover, semantic classes are often organized within a hierarchy, e.g., tail classes such as child and construction-worker are arguably subclasses of pedestrian. However, such hierarchical relationships are often ignored, which may lead to misleading estimates of performance and missed opportunities for algorithmic innovation. We address these challenges by formally studying the problem of Long-Tailed 3D Detection (LT3D), which evaluates on all classes, including those in-the-tail. We evaluate and innovate upon popular 3D detection codebases, such as CenterPoint and PointPillars, adapting them for LT3D. We develop hierarchical losses that promote feature sharing across common-vs-rare classes, as well as improved detection metrics that award partial credit to "reasonable" mistakes respecting the hierarchy (e.g., mistaking a child for an adult). Finally, we point out that fine-grained tail class accuracy is particularly improved via multimodal fusion of RGB images with LiDAR; simply put, small fine-grained classes are challenging to identify from sparse (lidar) geometry alone, suggesting that multimodal cues are crucial to long-tailed 3D detection. Our modifications improve accuracy by 5% AP on average for all classes, and dramatically improve AP for rare classes (e.g., stroller AP improves from 3.6 to 31.6).</td>
                <td><a href="https://proceedings.mlr.press/v205/peri23a.html">https://proceedings.mlr.press/v205/peri23a.html</a></td>
            </tr>
        
            <tr>
                <td>MIRA: Mental Imagery for Robotic Affordances</td>
                <td>Y, e, n, -, C, h, e, n,  , L, i, n, ,,  , P, e, t, e,  , F, l, o, r, e, n, c, e, ,,  , A, n, d, y,  , Z, e, n, g, ,,  , J, o, n, a, t, h, a, n,  , T, .,  , B, a, r, r, o, n, ,,  , Y, i, l, u, n,  , D, u, ,,  , W, e, i, -, C, h, i, u,  , M, a, ,,  , A, n, t, h, o, n, y,  , S, i, m, e, o, n, o, v, ,,  , A, l, b, e, r, t, o,  , R, o, d, r, i, g, u, e, z,  , G, a, r, c, i, a, ,,  , P, h, i, l, l, i, p,  , I, s, o, l, a</td>
                <td>corl2022</td>
                <td>Humans form mental images of 3D scenes to support counterfactual imagination, planning, and motor control. Our abilities to predict the appearance and affordance of the scene from previously unobserved viewpoints aid us in performing manipulation tasks (e.g., 6-DoF kitting) with a level of ease that is currently out of reach for existing robot learning frameworks. In this work, we aim to build artificial systems that can analogously plan actions on top of imagined images. To this end, we introduce Mental Imagery for Robotic Affordances (MIRA), an action reasoning framework that optimizes actions with novel-view synthesis and affordance prediction in the loop. Given a set of 2D RGB images, MIRA builds a consistent 3D scene representation, through which we synthesize novel orthographic views amenable to pixel-wise affordances prediction for action optimization. We illustrate how this optimization process enables us to generalize to unseen out-of-plane rotations for 6-DoF robotic manipulation tasks given a limited number of demonstrations, paving the way toward machines that autonomously learn to understand the world around them for planning actions.</td>
                <td><a href="https://proceedings.mlr.press/v205/lin23c.html">https://proceedings.mlr.press/v205/lin23c.html</a></td>
            </tr>
        
            <tr>
                <td>CAtNIPP: Context-Aware Attention-based Network for Informative Path Planning</td>
                <td>Y, u, h, o, n, g,  , C, a, o, ,,  , Y, i, z, h, u, o,  , W, a, n, g, ,,  , A, p, o, o, r, v, a,  , V, a, s, h, i, s, t, h, ,,  , H, a, o, l, i, n,  , F, a, n, ,,  , G, u, i, l, l, a, u, m, e,  , A, d, r, i, e, n,  , S, a, r, t, o, r, e, t, t, i</td>
                <td>corl2022</td>
                <td>Informative path planning (IPP) is an NP-hard problem, which aims at planning a path allowing an agent to build an accurate belief about a quantity of interest throughout a given search domain, within constraints on resource budget (e.g., path length for robots with limited battery life). IPP requires frequent online replanning as this belief is updated with every new measurement (i.e., adaptive IPP), while balancing short-term exploitation and longer-term exploration to avoid suboptimal, myopic behaviors. Encouraged by the recent developments in deep reinforcement learning, we introduce CAtNIPP, a fully reactive, neural approach to the adaptive IPP problem. CAtNIPP relies on self-attention for its powerful ability to capture dependencies in data at multiple spatial scales. Specifically, our agent learns to form a context of its belief over the entire domain, which it uses to sequence local movement decisions that optimize short- and longer-term search objectives. We experimentally demonstrate that CAtNIPP significantly outperforms state-of-the-art non-learning IPP solvers in terms of solution quality and computing time once trained, and present experimental results on hardware.</td>
                <td><a href="https://proceedings.mlr.press/v205/cao23b.html">https://proceedings.mlr.press/v205/cao23b.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Diverse and Physically Feasible Dexterous Grasps with Generative Model and Bilevel Optimization</td>
                <td>A, l, b, e, r, t,  , W, u, ,,  , M, i, c, h, e, l, l, e,  , G, u, o, ,,  , K, a, r, e, n,  , L, i, u</td>
                <td>corl2022</td>
                <td>To fully utilize the versatility of a multi-fingered dexterous robotic hand for executing diverse object grasps, one must consider the rich physical constraints introduced by hand-object interaction and object geometry. We propose an integrative approach of combining a generative model and a bilevel optimization (BO) to plan diverse grasp configurations on novel objects. First, a conditional variational autoencoder trained on merely six YCB objects predicts the finger placement directly from the object point cloud. The prediction is then used to seed a nonconvex BO that solves for a grasp configuration under collision, reachability, wrench closure, and friction constraints. Our method achieved an 86.7% success over 120 real world grasping trials on 20 household objects, including unseen and challenging geometries. Through quantitative empirical evaluations, we confirm that grasp configurations produced by our pipeline are indeed guaranteed to satisfy kinematic and dynamic constraints. A video summary of our results is available at youtu.be/9DTrImbN99I.</td>
                <td><a href="https://proceedings.mlr.press/v205/wu23b.html">https://proceedings.mlr.press/v205/wu23b.html</a></td>
            </tr>
        
            <tr>
                <td>Verified Path Following Using Neural Control Lyapunov Functions</td>
                <td>A, l, e, c,  , R, e, e, d, ,,  , G, u, i, l, l, a, u, m, e,  , O,  , B, e, r, g, e, r, ,,  , S, r, i, r, a, m,  , S, a, n, k, a, r, a, n, a, r, a, y, a, n, a, n, ,,  , C, h, r, i, s,  , H, e, c, k, m, a, n</td>
                <td>corl2022</td>
                <td>We present a framework that uses control Lyapunov functions (CLFs) to implement provably stable path-following controllers for autonomous mobile platforms. Our approach is based on learning a guaranteed CLF for path following by using recent approaches — combining machine learning with automated theorem proving — to train a neural network feedback law along with a CLF that guarantees stabilization for driving along low-curvature reference paths. We discuss how key properties of the CLF can be exploited to extend the range of  the curvatures for which the stability guarantees remain valid. We then demonstrate that our approach yields a controller that obeys theoretical guarantees in simulation, but also performs well in practice. We show our method is both a verified method of control and better than a common MPC implementation in computation time. Additionally, we implement the controller on-board on a $\frac18$-scale autonomous vehicle testing platform and present results for various robust path following scenarios.</td>
                <td><a href="https://proceedings.mlr.press/v205/reed23a.html">https://proceedings.mlr.press/v205/reed23a.html</a></td>
            </tr>
        
            <tr>
                <td>Task-Relevant Failure Detection for Trajectory Predictors in Autonomous Vehicles</td>
                <td>A, l, e, c,  , F, a, r, i, d, ,,  , S, u, s, h, a, n, t,  , V, e, e, r, ,,  , B, o, r, i, s,  , I, v, a, n, o, v, i, c, ,,  , K, a, r, e, n,  , L, e, u, n, g, ,,  , M, a, r, c, o,  , P, a, v, o, n, e</td>
                <td>corl2022</td>
                <td>In modern autonomy stacks, prediction modules are paramount to planning motions in the presence of other mobile agents. However, failures in prediction modules can mislead the downstream planner into making unsafe decisions. Indeed, the high uncertainty inherent to the task of trajectory forecasting ensures that such mispredictions occur frequently. Motivated by the need to improve safety of autonomous vehicles without compromising on their performance, we develop a probabilistic run-time monitor that detects when a "harmful" prediction failure occurs, i.e., a task-relevant failure detector. We achieve this by propagating trajectory prediction errors to the planning cost to reason about their impact on the AV. Furthermore, our detector comes equipped with performance measures on the false-positive and the false-negative rate and allows for data-free calibration. In our experiments we compared our detector with various others and found that our detector has the highest area under the receiver operator characteristic curve.</td>
                <td><a href="https://proceedings.mlr.press/v205/farid23a.html">https://proceedings.mlr.press/v205/farid23a.html</a></td>
            </tr>
        
            <tr>
                <td>Safe Control Under Input Limits with Neural Control Barrier Functions</td>
                <td>S, i, m, i, n,  , L, i, u, ,,  , C, h, a, n, g, l, i, u,  , L, i, u, ,,  , J, o, h, n,  , D, o, l, a, n</td>
                <td>corl2022</td>
                <td>We propose new methods to synthesize control barrier function (CBF) based safe controllers that avoid input saturation, which can cause safety violations. In particular, our method is created for high-dimensional, general nonlinear systems, for which such tools are scarce. We leverage techniques from machine learning, like neural networks and deep learning, to simplify this challenging problem in nonlinear control design. The method consists of a learner-critic architecture, in which the critic gives counterexamples of input saturation and the learner optimizes a neural CBF to eliminate those counterexamples. We provide empirical results on a 10D state, 4D input quadcopter-pendulum system. Our learned CBF avoids input saturation and maintains safety over nearly 100% of trials. </td>
                <td><a href="https://proceedings.mlr.press/v205/liu23e.html">https://proceedings.mlr.press/v205/liu23e.html</a></td>
            </tr>
        
            <tr>
                <td>Eliciting Compatible Demonstrations for Multi-Human Imitation Learning</td>
                <td>K, a, n, i, s, h, k,  , G, a, n, d, h, i, ,,  , S, i, d, d, h, a, r, t, h,  , K, a, r, a, m, c, h, e, t, i, ,,  , M, a, d, e, l, i, n, e,  , L, i, a, o, ,,  , D, o, r, s, a,  , S, a, d, i, g, h</td>
                <td>corl2022</td>
                <td>Imitation learning from human-provided demonstrations is a strong approach for learning policies for robot manipulation. While the ideal dataset for imitation learning is homogenous and low-variance - reflecting a single, optimal method for performing a task - natural human behavior has a great deal of heterogeneity, with several optimal ways to demonstrate a task. This multimodality is inconsequential to human users, with task variations manifesting as subconscious choices; for example, reaching down, then across to grasp an object, versus reaching across, then down. Yet, this mismatch presents a problem for interactive imitation learning, where sequences of users improve on a policy by iteratively collecting new, possibly conflicting demonstrations. To combat this problem of demonstrator incompatibility, this work designs an approach for 1) measuring the compatibility of a new demonstration given a base policy, and 2) actively eliciting more compatible demonstrations from new users. Across two simulation tasks requiring long-horizon, dexterous manipulation and a real-world “food plating” task with a Franka Emika Panda arm, we show that we can both identify incompatible demonstrations via post-hoc filtering, and apply our compatibility measure to actively elicit compatible demonstrations from new users, leading to improved task success rates across simulated and real environments.</td>
                <td><a href="https://proceedings.mlr.press/v205/gandhi23a.html">https://proceedings.mlr.press/v205/gandhi23a.html</a></td>
            </tr>
        
            <tr>
                <td>When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation</td>
                <td>M, a, d, h, u,  , V, a, n, k, a, d, a, r, i, ,,  , S, t, u, a, r, t,  , G, o, l, o, d, e, t, z, ,,  , S, o, u, r, a, v,  , G, a, r, g, ,,  , S, a, n, g, y, u, n,  , S, h, i, n, ,,  , A, n, d, r, e, w,  , M, a, r, k, h, a, m, ,,  , N, i, k, i,  , T, r, i, g, o, n, i</td>
                <td>corl2022</td>
                <td>Self-supervised deep learning methods for joint depth and ego-motion estimation can yield accurate trajectories without needing ground-truth training data. However, as they typically use photometric losses, their performance can degrade significantly when the assumptions these losses make (e.g. temporal illumination consistency, a static scene, and the absence of noise and occlusions) are violated. This limits their use for e.g. nighttime sequences, which tend to contain many point light sources (including on dynamic objects) and low signal-to-noise ratio (SNR) in darker image regions. In this paper, we show how to use a combination of three techniques to allow the existing photometric losses to work for both day and nighttime images. First, we introduce a per-pixel neural intensity transformation to compensate for the light changes that occur between successive frames. Second, we predict a per-pixel residual flow map that we use to correct the reprojection correspondences induced by the estimated ego-motion and depth from the networks. And third, we denoise the training images to improve the robustness and accuracy of our approach. These changes allow us to train a single model for both day and nighttime images without needing separate encoders or extra feature networks like existing methods. We perform extensive experiments and ablation studies on the challenging Oxford RobotCar dataset to demonstrate the efficacy of our approach for both day and nighttime sequences.</td>
                <td><a href="https://proceedings.mlr.press/v205/vankadari23a.html">https://proceedings.mlr.press/v205/vankadari23a.html</a></td>
            </tr>
        
            <tr>
                <td>Towards Scale Balanced 6-DoF Grasp Detection in Cluttered Scenes</td>
                <td>H, a, o, x, i, a, n, g,  , M, a, ,,  , D, i,  , H, u, a, n, g</td>
                <td>corl2022</td>
                <td>In this paper, we focus on the problem of feature learning in the presence of scale imbalance for 6-DoF grasp detection and propose a novel approach to especially address the difficulty in dealing with small-scale samples. A Multi-scale Cylinder Grouping (MsCG) module is presented to enhance local geometry representation by combining multi-scale cylinder features and global context. Moreover, a Scale Balanced Learning (SBL) loss and an Object Balanced Sampling (OBS) strategy are designed, where SBL enlarges the gradients of the samples whose scales are in low frequency by apriori weights while OBS captures more points on small-scale objects with the help of an auxiliary segmentation network. They alleviate the influence of the uneven distribution of grasp scales in training and inference respectively. In addition, Noisy-clean Mix (NcM) data augmentation is introduced to facilitate training, aiming to bridge the domain gap between synthetic and raw scenes in an efficient way by generating more data which mix them into single ones at instance-level. Extensive experiments are conducted on the GraspNet-1Billion benchmark and competitive results are reached with significant gains on small-scale cases. Besides, the performance of real-world grasping highlights its generalization ability.</td>
                <td><a href="https://proceedings.mlr.press/v205/ma23a.html">https://proceedings.mlr.press/v205/ma23a.html</a></td>
            </tr>
        
            <tr>
                <td>Few-Shot Preference Learning for Human-in-the-Loop RL</td>
                <td>D, o, n, a, l, d,  , J, o, s, e, p, h,  , H, e, j, n, a,  , I, I, I, ,,  , D, o, r, s, a,  , S, a, d, i, g, h</td>
                <td>corl2022</td>
                <td>While reinforcement learning (RL) has become a more popular approach for robotics, designing sufficiently informative reward functions for complex tasks has proven to be extremely difficult due their inability to capture human intent and policy exploitation. Preference based RL algorithms seek to overcome these challenges by directly learning reward functions from human feedback. Unfortunately, prior work either requires an unreasonable number of queries implausible for any human to answer or overly restricts the class of reward functions to guarantee the elicitation of the most informative queries, resulting in models that are insufficiently expressive for realistic robotics tasks. Contrary to most works that focus on query selection to \emph{minimize} the amount of data required for learning reward functions, we take an opposite approach: \emph{expanding} the pool of available data by viewing human-in-the-loop RL through the more flexible lens of multi-task learning. Motivated by the success of meta-learning, we pre-train preference models on prior task data and quickly adapt them for new tasks using only a handful of queries. Empirically, we reduce the amount of online feedback needed to train manipulation policies in Meta-World by 20$\times$, and demonstrate the effectiveness of our method on a real Franka Panda Robot. Moreover, this reduction in query-complexity allows us to train robot policies from actual human users. Videos of our results can be found at https://sites.google.com/view/few-shot-preference-rl/home.</td>
                <td><a href="https://proceedings.mlr.press/v205/iii23a.html">https://proceedings.mlr.press/v205/iii23a.html</a></td>
            </tr>
        
            <tr>
                <td>Visuo-Tactile Transformers for Manipulation</td>
                <td>Y, i, z, h, o, u,  , C, h, e, n, ,,  , M, a, r, k,  , V, a, n,  , d, e, r,  , M, e, r, w, e, ,,  , A, n, d, r, e, a,  , S, i, p, o, s, ,,  , N, i, m, a,  , F, a, z, e, l, i</td>
                <td>corl2022</td>
                <td>Learning representations in the joint domain of vision and touch can improve manipulation dexterity, robustness, and sample-complexity by exploiting mutual information and complementary cues. Here, we present Visuo-Tactile Transformers (VTTs), a novel multimodal representation learning approach suited for model-based reinforcement learning and planning. Our approach extends the Visual Transformer to handle visuo-tactile feedback. Specifically, VTT uses tactile feedback together with self and cross-modal attention to build latent heatmap representations that focus attention on important task features in the visual domain. We demonstrate the efficacy of VTT for representation learning with a comparative evaluation against baselines on four simulated robot tasks and one real world block pushing task. We conduct an ablation study over the components of VTT to highlight the importance of cross-modality in representation learning for robotic manipulation. </td>
                <td><a href="https://proceedings.mlr.press/v205/chen23d.html">https://proceedings.mlr.press/v205/chen23d.html</a></td>
            </tr>
        
            <tr>
                <td>Offline Reinforcement Learning at Multiple Frequencies</td>
                <td>K, a, y, l, e, e,  , B, u, r, n, s, ,,  , T, i, a, n, h, e,  , Y, u, ,,  , C, h, e, l, s, e, a,  , F, i, n, n, ,,  , K, a, r, o, l,  , H, a, u, s, m, a, n</td>
                <td>corl2022</td>
                <td>To leverage many sources of offline robot data, robots must grapple with the heterogeneity of such data. In this paper, we focus on one particular aspect of this challenge: learning from offline data collected at different control frequencies. Across labs, the discretization of controllers, sampling rates of sensors, and demands of a task of interest may differ, giving rise to a mixture of frequencies in an aggregated dataset. We study how well offline reinforcement learning (RL) algorithms can accommodate data with a mixture of frequencies during training. We observe that the $Q$-value propagates at different rates for different discretizations, leading to a number of learning challenges for off-the-shelf offline RL algorithms. We present a simple yet effective solution that enforces consistency in the rate of  $Q$-value updates to stabilize learning. By scaling the value of $N$ in $N$-step returns with the discretization size, we effectively balance $Q$-value propagation, leading to more stable convergence. On three simulated robotic control problems, we empirically find that this simple approach significantly outperforms naïve mixing both terms of absolute performance and training stability, while also improving over using only the data from a single control frequency.</td>
                <td><a href="https://proceedings.mlr.press/v205/burns23a.html">https://proceedings.mlr.press/v205/burns23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning the Dynamics of Compliant Tool-Environment Interaction for Visuo-Tactile Contact Servoing</td>
                <td>M, a, r, k,  , V, a, n,  , d, e, r,  , M, e, r, w, e, ,,  , D, m, i, t, r, y,  , B, e, r, e, n, s, o, n, ,,  , N, i, m, a,  , F, a, z, e, l, i</td>
                <td>corl2022</td>
                <td>Many manipulation tasks require the robot to control the contact between a grasped compliant tool and the environment, e.g. scraping a frying pan with a spatula. However, modeling tool-environment interaction is difficult, especially when the tool is compliant, and the robot cannot be expected to have the full geometry and physical properties (e.g., mass, stiffness, and friction) of all the tools it must use. We propose a framework that learns to predict the effects of a robot’s actions on the contact between the tool and the environment given visuo-tactile perception. Key to our framework is a novel contact feature representation that consists of a binary contact value, the line of contact, and an end-effector wrench. We propose a method to learn the dynamics of these contact features from real world data that does not require predicting the geometry of the compliant tool. We then propose a controller that uses this dynamics model for visuo-tactile contact servoing and show that it is effective at performing scraping tasks with a spatula, even in scenarios where precise contact needs to be made to avoid obstacles.</td>
                <td><a href="https://proceedings.mlr.press/v205/merwe23a.html">https://proceedings.mlr.press/v205/merwe23a.html</a></td>
            </tr>
        
            <tr>
                <td>Multi-Robot Scene Completion: Towards Task-Agnostic Collaborative Perception</td>
                <td>Y, i, m, i, n, g,  , L, i, ,,  , J, u, e, x, i, a, o,  , Z, h, a, n, g, ,,  , D, e, k, u, n,  , M, a, ,,  , Y, u, e,  , W, a, n, g, ,,  , C, h, e, n,  , F, e, n, g</td>
                <td>corl2022</td>
                <td>Collaborative perception learns how to share information among multiple robots to perceive the environment better than individually done. Past research on this has been task-specific, such as detection or segmentation. Yet this leads to different information sharing for different tasks, hindering the large-scale deployment of collaborative perception. We propose the first task-agnostic collaborative perception paradigm that learns a single collaboration module in a self-supervised manner for different downstream tasks. This is done by a novel task termed multi-robot scene completion, where each robot learns to effectively share information for reconstructing a complete scene viewed by all robots. Moreover, we propose a spatiotemporal autoencoder (STAR) that amortizes over time the communication cost by spatial sub-sampling and temporal mixing. Extensive experiments validate our method’s effectiveness on scene completion and collaborative perception in autonomous driving scenarios. Our code is available at https://coperception.github.io/star/.</td>
                <td><a href="https://proceedings.mlr.press/v205/li23e.html">https://proceedings.mlr.press/v205/li23e.html</a></td>
            </tr>
        
            <tr>
                <td>USHER: Unbiased Sampling for Hindsight Experience Replay</td>
                <td>L, i, a, m,  , S, c, h, r, a, m, m, ,,  , Y, u, n, f, u,  , D, e, n, g, ,,  , E, d, g, a, r,  , G, r, a, n, a, d, o, s, ,,  , A, b, d, e, s, l, a, m,  , B, o, u, l, a, r, i, a, s</td>
                <td>corl2022</td>
                <td> Dealing with sparse rewards is a long-standing challenge in reinforcement learning (RL). Hindsight Experience Replay (HER) addresses this problem by reusing failed trajectories for one goal as successful trajectories for another. This allows for both a minimum density of reward and for generalization across multiple goals. However, this strategy is known to result in a biased value function, as the update rule underestimates the likelihood of bad outcomes in a stochastic environment. We propose an asymptotically unbiased importance-sampling-based algorithm to address this problem without sacrificing performance on deterministic environments. We show its effectiveness on a range of robotic systems, including challenging high dimensional stochastic environments.</td>
                <td><a href="https://proceedings.mlr.press/v205/schramm23a.html">https://proceedings.mlr.press/v205/schramm23a.html</a></td>
            </tr>
        
            <tr>
                <td>Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations</td>
                <td>L, e, t, i, a, n,  , C, h, e, n, ,,  , S, r, a, v, a, n,  , J, a, y, a, n, t, h, i, ,,  , R, o, h, a, n,  , R,  , P, a, l, e, j, a, ,,  , D, a, n, i, e, l,  , M, a, r, t, i, n, ,,  , V, i, a, c, h, e, s, l, a, v,  , Z, a, k, h, a, r, o, v, ,,  , M, a, t, t, h, e, w,  , G, o, m, b, o, l, a, y</td>
                <td>corl2022</td>
                <td>Learning from Demonstration (LfD) approaches empower end-users to teach robots novel tasks via demonstrations of the desired behaviors, democratizing access to robotics. However, current LfD frameworks are not capable of fast adaptation to heterogeneous human demonstrations nor the large-scale deployment in ubiquitous robotics applications. In this paper, we propose a novel LfD framework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Our approach (1) leverages learned strategies to construct policy mixtures for fast adaptation to new demonstrations, allowing for quick end-user personalization, (2) distills common knowledge across demonstrations, achieving accurate task inference; and (3) expands its model only when needed in lifelong deployments, maintaining a concise set of prototypical strategies that can approximate all behaviors via policy mixtures. We empirically validate that FLAIR achieves adaptability (i.e., the robot adapts to heterogeneous, user-specific task preferences), efficiency (i.e., the robot achieves sample-efficient adaptation), and scalability (i.e., the model grows sublinearly with the number of demonstrations while maintaining high performance). FLAIR surpasses benchmarks across three control tasks with an average 57% improvement in policy returns and an average 78% fewer episodes required for demonstration modeling using policy mixtures. Finally, we demonstrate the success of FLAIR in a table tennis task and find users rate FLAIR as having higher task ($p<.05$) and personalization ($p<.05$) performance. </td>
                <td><a href="https://proceedings.mlr.press/v205/chen23e.html">https://proceedings.mlr.press/v205/chen23e.html</a></td>
            </tr>
        
            <tr>
                <td>Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics</td>
                <td>K, r, i, s, h, a, n,  , R, a, n, a, ,,  , M, i, n, g,  , X, u, ,,  , B, r, e, n, d, a, n,  , T, i, d, d, ,,  , M, i, c, h, a, e, l,  , M, i, l, f, o, r, d, ,,  , N, i, k, o,  , S, u, e, n, d, e, r, h, a, u, f</td>
                <td>corl2022</td>
                <td>Skill-based reinforcement learning (RL) has emerged as a promising strategy to leverage prior knowledge for accelerated robot learning. Skills are typically extracted from expert demonstrations and are embedded into a latent space from which they can be sampled as actions by a high-level RL agent. However, this \textit{skill space} is expansive, and not all skills are relevant for a given robot state, making exploration difficult. Furthermore, the downstream RL agent is limited to learning structurally similar tasks to those used to construct the skill space. We firstly propose accelerating exploration in the skill space using state-conditioned generative models to directly bias the high-level agent towards only \textit{sampling} skills relevant to a given state based on prior experience. Next, we propose a low-level residual policy for fine-grained \textit{skill adaptation} enabling downstream RL agents to adapt to unseen task variations. Finally, we validate our approach across four challenging manipulation tasks that differ from those used to build the skill space, demonstrating our ability to learn across task variations while significantly accelerating exploration, outperforming prior works.</td>
                <td><a href="https://proceedings.mlr.press/v205/rana23a.html">https://proceedings.mlr.press/v205/rana23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Representations that Enable Generalization in Assistive Tasks</td>
                <td>J, e, r, r, y,  , Z, h, i, -, Y, a, n, g,  , H, e, ,,  , Z, a, c, k, o, r, y,  , E, r, i, c, k, s, o, n, ,,  , D, a, n, i, e, l,  , S, .,  , B, r, o, w, n, ,,  , A, d, i, t, i,  , R, a, g, h, u, n, a, t, h, a, n, ,,  , A, n, c, a,  , D, r, a, g, a, n</td>
                <td>corl2022</td>
                <td>Recent work in sim2real has successfully enabled robots to act in physical environments by training in simulation with a diverse “population” of environments (i.e. domain randomization). In this work, we focus on enabling generalization in \emph{assistive tasks}: tasks in which the robot is acting to assist a user (e.g. helping someone with motor impairments with bathing or with scratching an itch). Such tasks are particularly interesting relative to prior sim2real successes because the environment now contains a \emph{human who is also acting}. This complicates the problem because the diversity of human users (instead of merely physical environment parameters) is more difficult to capture in a population, thus increasing the likelihood of encountering out-of-distribution (OOD) human policies at test time. We advocate that generalization to such OOD policies benefits from (1) learning a good latent representation for human policies that test-time humans can accurately be mapped to, and (2) making that representation adaptable with test-time interaction data, instead of relying on it to perfectly capture the space of human policies based on the simulated population only. We study how to best learn such a representation by evaluating on purposefully constructed OOD test policies. We find that sim2real methods that encode environment (or population) parameters and work well in tasks that robots do in isolation, do not work well in \emph{assistance}.  In assistance, it seems crucial to train the representation based on the \emph{history of interaction} directly, because that is what the robot will have access to at test time. Further, training these representations to then \emph{predict human actions} not only gives them better structure, but also enables them to be fine-tuned at test-time, when the robot observes the partner act.</td>
                <td><a href="https://proceedings.mlr.press/v205/he23a.html">https://proceedings.mlr.press/v205/he23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning to Correct Mistakes: Backjumping in Long-Horizon Task and Motion Planning</td>
                <td>Y, o, o, n, c, h, a, n, g,  , S, u, n, g, ,,  , Z, i, z, h, a, o,  , W, a, n, g, ,,  , P, e, t, e, r,  , S, t, o, n, e</td>
                <td>corl2022</td>
                <td>As robots become increasingly capable of manipulation and long-term autonomy, long-horizon task and motion planning problems are becoming increasingly important.  A key challenge in such problems is that early actions in the plan may make future actions infeasible. When reaching a dead-end in the search, most existing planners use backtracking, which exhaustively reevaluates motion-level actions, often resulting in inefficient planning, especially when the search depth is large. In this paper, we propose to learn backjumping heuristics which identify the culprit action directly using supervised learning models to guide the task-level search. Based on evaluations of two different tasks, we find that our method significantly improves planning efficiency compared to backtracking and also generalizes to problems with novel numbers of objects.</td>
                <td><a href="https://proceedings.mlr.press/v205/sung23a.html">https://proceedings.mlr.press/v205/sung23a.html</a></td>
            </tr>
        
            <tr>
                <td>Lyapunov Design for Robust and Efficient Robotic Reinforcement Learning</td>
                <td>T, y, l, e, r,  , W, e, s, t, e, n, b, r, o, e, k, ,,  , F, e, r, n, a, n, d, o,  , C, a, s, t, a, n, e, d, a, ,,  , A, y, u, s, h,  , A, g, r, a, w, a, l, ,,  , S, h, a, n, k, a, r,  , S, a, s, t, r, y, ,,  , K, o, u, s, h, i, l,  , S, r, e, e, n, a, t, h</td>
                <td>corl2022</td>
                <td>Recent advances in the reinforcement learning (RL) literature have enabled roboticists to automatically train complex policies in simulated environments. However, due to the poor sample complexity of these methods, solving RL problems using real-world data remains a challenging problem. This paper introduces a novel cost-shaping method which aims to reduce the number of samples needed to learn a stabilizing controller. The method adds a term involving a Control Lyapunov Function (CLF) – an ‘energy-like’ function from the model-based control literature – to typical cost formulations. Theoretical results demonstrate the new costs lead to stabilizing controllers when smaller discount factors are used, which is well-known to reduce sample complexity. Moreover, the addition of the CLF term ‘robustifies’ the search for a stabilizing controller by ensuring that even highly sub-optimal polices will stabilize the system. We demonstrate our approach with two hardware examples where we learn stabilizing controllers for a cartpole and an A1 quadruped with only seconds and a few minutes of fine-tuning data, respectively. Furthermore, simulation benchmark studies show that obtaining stabilizing policies by optimizing our proposed costs requires orders of magnitude less data compared to standard cost designs.</td>
                <td><a href="https://proceedings.mlr.press/v205/westenbroek23a.html">https://proceedings.mlr.press/v205/westenbroek23a.html</a></td>
            </tr>
        
            <tr>
                <td>ROAD: Learning an Implicit Recursive Octree Auto-Decoder to Efficiently Encode 3D Shapes</td>
                <td>S, e, r, g, e, y,  , Z, a, k, h, a, r, o, v, ,,  , R, a, r, e, s,  , A, n, d, r, e, i,  , A, m, b, r, u, s, ,,  , K, a, t, h, e, r, i, n, e,  , L, i, u, ,,  , A, d, r, i, e, n,  , G, a, i, d, o, n</td>
                <td>corl2022</td>
                <td>Compact and accurate representations of 3D shapes are central to many perception and robotics tasks. State-of-the-art learning-based methods can reconstruct single objects but scale poorly to large datasets. We present a novel recursive implicit representation to efficiently and accurately encode large datasets of complex 3D shapes by recursively traversing an implicit octree in latent space. Our implicit Recursive Octree Auto-Decoder (ROAD) learns a hierarchically structured latent space enabling state-of-the-art reconstruction results at a compression ratio above 99%. We also propose an efficient curriculum learning scheme that naturally exploits the coarse-to-fine properties of the underlying octree spatial representation. We explore the scaling law relating latent space dimension, dataset size, and reconstruction accuracy, showing that increasing the latent space dimension is enough to scale to large shape datasets. Finally, we show that our learned latent space encodes a coarse-to-fine hierarchical structure yielding reusable latents across different levels of details, and we provide qualitative evidence of generalization to novel shapes outside the training set. </td>
                <td><a href="https://proceedings.mlr.press/v205/zakharov23a.html">https://proceedings.mlr.press/v205/zakharov23a.html</a></td>
            </tr>
        
            <tr>
                <td>Safe Robot Learning in Assistive Devices through Neural Network Repair</td>
                <td>K, e, y, v, a, n,  , M, a, j, d, ,,  , G, e, o, f, f, r, e, y,  , M, i, t, c, h, e, l, l,  , C, l, a, r, k, ,,  , T, a, n, m, a, y,  , K, h, a, n, d, a, i, t, ,,  , S, i, y, u,  , Z, h, o, u, ,,  , S, r, i, r, a, m,  , S, a, n, k, a, r, a, n, a, r, a, y, a, n, a, n, ,,  , G, e, o, r, g, i, o, s,  , F, a, i, n, e, k, o, s, ,,  , H, e, n, i,  , A, m, o, r</td>
                <td>corl2022</td>
                <td>Assistive robotic devices are a particularly promising field of application for neural networks (NN) due to the need for personalization and hard-to-model human-machine interaction dynamics. However, NN based estimators and controllers may produce potentially unsafe outputs over previously unseen data points. In this paper, we introduce an algorithm for updating NN control policies to satisfy a given set of formal safety constraints, while also optimizing the original loss function.  Given a set of mixed-integer linear constraints, we define the NN repair problem as a Mixed Integer Quadratic Program (MIQP). In extensive experiments, we demonstrate the efficacy of our repair method in generating safe policies for a lower-leg prosthesis.</td>
                <td><a href="https://proceedings.mlr.press/v205/majd23a.html">https://proceedings.mlr.press/v205/majd23a.html</a></td>
            </tr>
        
            <tr>
                <td>Contrastive Decision Transformers</td>
                <td>S, a, c, h, i, n,  , G, .,  , K, o, n, a, n, ,,  , E, s, m, a, e, i, l,  , S, e, r, a, j, ,,  , M, a, t, t, h, e, w,  , G, o, m, b, o, l, a, y</td>
                <td>corl2022</td>
                <td>Decision Transformers (DT) have drawn upon the success of Transformers by abstracting Reinforcement Learning as a target-return-conditioned, sequence modeling problem. In our work, we claim that the distribution of DT’s target-returns represents a series of different tasks that agents must learn to handle. Work in multi-task learning has shown that separating the representations of input data belonging to different tasks can improve performance. We draw from this approach to construct ConDT (Contrastive Decision Transformer). ConDT leverages an enhanced contrastive loss to train a return-dependent transformation of the input embeddings, which we empirically show clusters these embeddings by their return. We find that ConDT significantly outperforms DT in Open-AI Gym domains by 10% and 39% in visually challenging Atari domains.</td>
                <td><a href="https://proceedings.mlr.press/v205/konan23a.html">https://proceedings.mlr.press/v205/konan23a.html</a></td>
            </tr>
        
            <tr>
                <td>DiffStack: A Differentiable and Modular Control Stack for Autonomous Vehicles</td>
                <td>P, e, t, e, r,  , K, a, r, k, u, s, ,,  , B, o, r, i, s,  , I, v, a, n, o, v, i, c, ,,  , S, h, i, e,  , M, a, n, n, o, r, ,,  , M, a, r, c, o,  , P, a, v, o, n, e</td>
                <td>corl2022</td>
                <td>Autonomous vehicle (AV) stacks are typically built in a modular fashion, with explicit components performing detection, tracking, prediction, planning, control, etc. While modularity improves reusability, interpretability, and generalizability, it also suffers from compounding errors, information bottlenecks, and integration challenges. To overcome these challenges, a prominent approach is to convert the AV stack into an end-to-end neural network and train it with data. While such approaches have achieved impressive results, they typically lack interpretability and reusability, and they eschew principled analytical components, such as planning and control, in favor of deep neural networks. To enable the joint optimization of AV stacks while retaining modularity, we present DiffStack, a differentiable and modular stack for prediction, planning, and control. Crucially, our model-based planning and control algorithms leverage recent advancements in differentiable optimization to produce gradients, enabling optimization of upstream components, such as prediction, via backpropagation through planning and control. Our results on the nuScenes dataset indicate that end-to-end training with DiffStack yields substantial improvements in open-loop and closed-loop planning metrics by, e.g., learning to make fewer prediction errors that would affect planning. Beyond these immediate benefits, DiffStack opens up new opportunities for fully data-driven yet modular and interpretable AV architectures.</td>
                <td><a href="https://proceedings.mlr.press/v205/karkus23a.html">https://proceedings.mlr.press/v205/karkus23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning and Retrieval from Prior Data for Skill-based Imitation Learning</td>
                <td>S, o, r, o, u, s, h,  , N, a, s, i, r, i, a, n, y, ,,  , T, i, a, n,  , G, a, o, ,,  , A, j, a, y,  , M, a, n, d, l, e, k, a, r, ,,  , Y, u, k, e,  , Z, h, u</td>
                <td>corl2022</td>
                <td>Imitation learning offers a promising path for robots to learn general-purpose tasks, but traditionally has enjoyed limited scalability due to high data supervision requirements and brittle generalization. Inspired by recent work on skill-based imitation learning, we investigate whether leveraging prior data from previous related tasks can enable learning novel tasks in a more robust, data-efficient manner. To make effective use of the prior data, the agent must internalize knowledge from the prior data and contextualize this knowledge in novel tasks. To that end we propose a skill-based imitation learning framework that extracts temporally-extended sensorimotor skills from prior data and subsequently learns a policy for the target task with respect to these learned skills. We find a number of modeling choices significantly improve performance on novel tasks, namely representation learning objectives to enable more predictable and consistent skill representations and a retrieval-based data augmentation procedure to increase the scope of supervision for the policy. On a number of multi-task manipulation domains, we demonstrate that our method significantly outperforms existing imitation learning and offline reinforcement learning approaches. Videos and code are available at https://ut-austin-rpl.github.io/sailor</td>
                <td><a href="https://proceedings.mlr.press/v205/nasiriany23a.html">https://proceedings.mlr.press/v205/nasiriany23a.html</a></td>
            </tr>
        
            <tr>
                <td>Learning Semantics-Aware Locomotion Skills from Human Demonstration</td>
                <td>Y, u, x, i, a, n, g,  , Y, a, n, g, ,,  , X, i, a, n, g, y, u, n,  , M, e, n, g, ,,  , W, e, n, h, a, o,  , Y, u, ,,  , T, i, n, g, n, a, n,  , Z, h, a, n, g, ,,  , J, i, e,  , T, a, n, ,,  , B, y, r, o, n,  , B, o, o, t, s</td>
                <td>corl2022</td>
                <td>The semantics of the environment, such as the terrain type and property, reveals important information for legged robots to adjust their behaviors. In this work, we present a framework that learns semantics-aware locomotion skills from perception for quadrupedal robots, such that the robot can traverse through complex offroad terrains with appropriate speeds and gaits using perception information. Due to the lack of high-fidelity outdoor simulation, our framework needs to be trained directly in the real world, which brings unique challenges in data efficiency and safety. To ensure sample efficiency, we pre-train the perception model with an off-road driving dataset. To avoid the risks of real-world policy exploration, we leverage human demonstration to train a speed policy that selects a desired forward speed from camera image. For maximum traversability, we pair the speed policy with a gait selector, which selects a robust locomotion gait for each forward speed. Using only 40 minutes of human demonstration data, our framework learns to adjust the speed and gait of the robot based on perceived terrain semantics, and enables the robot to walk over 6km without failure at close-to-optimal speed</td>
                <td><a href="https://proceedings.mlr.press/v205/yang23a.html">https://proceedings.mlr.press/v205/yang23a.html</a></td>
            </tr>
        
            <tr>
                <td>TRITON: Neural Neural Textures for Better Sim2Real</td>
                <td>R, y, a, n,  , D, .,  , B, u, r, g, e, r, t, ,,  , J, i, n, g, h, u, a, n,  , S, h, a, n, g, ,,  , X, i, a, n, g,  , L, i, ,,  , M, i, c, h, a, e, l,  , S, .,  , R, y, o, o</td>
                <td>corl2022</td>
                <td>Unpaired image translation algorithms can be used for sim2real tasks, but many fail to generate temporally consistent results. We present a new approach that combines differentiable rendering with image translation to achieve temporal consistency over indefinite timescales, using surface consistency losses and neu- ral neural textures. We call this algorithm TRITON (Texture Recovering Image Translation Network): an unsupervised, end-to-end, stateless sim2real algorithm that leverages the underlying 3D geometry of input scenes by generating realistic- looking learnable neural textures. By settling on a particular texture for the objects in a scene, we ensure consistency between frames statelessly. TRITON is not lim- ited to camera movements — it can handle the movement and deformation of ob- jects as well, making it useful for downstream tasks such as robotic manipulation. We demonstrate the superiority of our approach both qualitatively and quantita- tively, using robotic experiments and comparisons to ground truth photographs. We show that TRITON generates more useful images than other algorithms do. Please see our project website: tritonpaper.github.io</td>
                <td><a href="https://proceedings.mlr.press/v205/burgert23a.html">https://proceedings.mlr.press/v205/burgert23a.html</a></td>
            </tr>
        
            <tr>
                <td>DayDreamer: World Models for Physical Robot Learning</td>
                <td>P, h, i, l, i, p, p,  , W, u, ,,  , A, l, e, j, a, n, d, r, o,  , E, s, c, o, n, t, r, e, l, a, ,,  , D, a, n, i, j, a, r,  , H, a, f, n, e, r, ,,  , P, i, e, t, e, r,  , A, b, b, e, e, l, ,,  , K, e, n,  , G, o, l, d, b, e, r, g</td>
                <td>corl2022</td>
                <td>To solve tasks in complex environments, robots need to learn from experience. Deep reinforcement learning is a common approach to robot learning but requires a large amount of trial and error to learn, limiting its deployment in the physical world. As a consequence, many advances in robot learning rely on simulators. On the other hand, learning inside of simulators fails to capture the complexity of the real world, is prone to simulator inaccuracies, and the resulting behaviors do not adapt to changes in the world. The Dreamer algorithm has recently shown great promise for learning from small amounts of interaction by planning within a learned world model, outperforming pure reinforcement learning in video games. Learning a world model to predict the outcomes of potential actions enables planning in imagination, reducing the amount of trial and error needed in the real environment. However, it is unknown whether Dreamer can facilitate faster learning on physical robots. In this paper, we apply Dreamer to 4 robots to learn online and directly in the real world, without any simulators. Dreamer trains a quadruped robot to roll off its back, stand up, and walk from scratch and without resets in only 1 hour. We then push the robot and find that Dreamer adapts within 10 minutes to withstand perturbations or quickly roll over and stand back up. On two different robotic arms, Dreamer learns to pick and place objects from camera images and sparse rewards, approaching human-level teleoperation performance. On a wheeled robot, Dreamer learns to navigate to a goal position purely from camera images, automatically resolving ambiguity about the robot orientation. Using the same hyperparameters across all experiments, we find that Dreamer is capable of online learning in the real world, which establishes a strong baseline. We release our infrastructure for future applications of world models to robot learning.</td>
                <td><a href="https://proceedings.mlr.press/v205/wu23c.html">https://proceedings.mlr.press/v205/wu23c.html</a></td>
            </tr>
        
            <tr>
                <td>INQUIRE: INteractive Querying for User-aware Informative REasoning</td>
                <td>T, e, s, c, a,  , F, i, t, z, g, e, r, a, l, d, ,,  , P, a, l, l, a, v, i,  , K, o, p, p, o, l, ,,  , P, a, t, r, i, c, k,  , C, a, l, l, a, g, h, a, n, ,,  , R, u, s, s, e, l, l,  , Q, u, i, n, l, a, n,  , J, u, n,  , H, e, i,  , W, o, n, g, ,,  , R, e, i, d,  , S, i, m, m, o, n, s, ,,  , O, l, i, v, e, r,  , K, r, o, e, m, e, r, ,,  , H, e, n, n, y,  , A, d, m, o, n, i</td>
                <td>corl2022</td>
                <td>Research on Interactive Robot Learning has yielded several modalities for querying a human for training data, including demonstrations, preferences, and corrections. While prior work in this space has focused on optimizing the robot’s queries within each interaction type, there has been little work on optimizing over the selection of the interaction type itself. We present INQUIRE, the first algorithm to implement and optimize over a generalized representation of information gain across multiple interaction types. Our evaluations show that INQUIRE can dynamically optimize its interaction type (and respective optimal query) based on its current learning status and the robot’s state in the world, resulting in more robust performance across tasks in comparison to state-of-the art baseline methods. Additionally, INQUIRE allows for customizable cost metrics to bias its selection of interaction types, enabling this algorithm to be tailored to a robot’s particular deployment domain and formulate cost-aware, informative queries.</td>
                <td><a href="https://proceedings.mlr.press/v205/fitzgerald23a.html">https://proceedings.mlr.press/v205/fitzgerald23a.html</a></td>
            </tr>
        
            <tr>
                <td>Online Dynamics Learning for Predictive Control with an Application to Aerial Robots</td>
                <td>T, o, m,  , Z, .,  , J, i, a, h, a, o, ,,  , K, o, n, g,  , Y, a, o,  , C, h, e, e, ,,  , M, .,  , A, n, i,  , H, s, i, e, h</td>
                <td>corl2022</td>
                <td>In this work, we consider the task of improving the accuracy of dynamic models for model predictive control (MPC) in an online setting. Although prediction models can be learned and applied to model-based controllers, these models are often learned offline. In this offline setting, training data is first collected and a prediction model is learned through an elaborated training procedure. However, since the model is learned offline, it does not adapt to disturbances or model errors observed during deployment. To improve the adaptiveness of the model and the controller, we propose an online dynamics learning framework that continually improves the accuracy of the dynamic model during deployment. We adopt knowledge-based neural ordinary differential equations (KNODE) as the dynamic models, and use techniques inspired by transfer learning to continually improve the model accuracy. We demonstrate the efficacy of our framework with a quadrotor, and verify the framework in both simulations and physical experiments. Results show that our approach can account for disturbances that are possibly time-varying, while maintaining good trajectory tracking performance.</td>
                <td><a href="https://proceedings.mlr.press/v205/jiahao23a.html">https://proceedings.mlr.press/v205/jiahao23a.html</a></td>
            </tr>
        
            <tr>
                <td>Skill-based Model-based Reinforcement Learning</td>
                <td>L, u, c, y,  , X, i, a, o, y, a, n, g,  , S, h, i, ,,  , J, o, s, e, p, h,  , J,  , L, i, m, ,,  , Y, o, u, n, g, w, o, o, n,  , L, e, e</td>
                <td>corl2022</td>
                <td>Model-based reinforcement learning (RL) is a sample-efficient way of learning complex behaviors by leveraging a learned single-step dynamics model to plan actions in imagination. However, planning every action for long-horizon tasks is not practical, akin to a human planning out every muscle movement. Instead, humans efficiently plan with high-level skills to solve complex tasks. From this intuition, we propose a Skill-based Model-based RL framework (SkiMo) that enables planning in the skill space using a skill dynamics model, which directly predicts the skill outcomes, rather than predicting all small details in the intermediate states, step by step. For accurate and efficient long-term planning, we jointly learn the skill dynamics model and a skill repertoire from prior experience. We then harness the learned skill dynamics model to accurately simulate and plan over long horizons in the skill space, which enables efficient downstream learning of long-horizon, sparse reward tasks. Experimental results in navigation and manipulation domains show that SkiMo extends the temporal horizon of model-based approaches and improves the sample efficiency for both model-based RL and skill-based RL. Code and videos are available at https://clvrai.com/skimo</td>
                <td><a href="https://proceedings.mlr.press/v205/shi23a.html">https://proceedings.mlr.press/v205/shi23a.html</a></td>
            </tr>
        
            <tr>
                <td>Data-Efficient Model Learning for Control with Jacobian-Regularized Dynamic-Mode Decomposition</td>
                <td>B, r, i, a, n,  , E, d, w, a, r, d,  , J, a, c, k, s, o, n, ,,  , J, e, o, n, g,  , H, u, n,  , L, e, e, ,,  , K, e, v, i, n,  , T, r, a, c, y, ,,  , Z, a, c, h, a, r, y,  , M, a, n, c, h, e, s, t, e, r</td>
                <td>corl2022</td>
                <td>We present a data-efficient algorithm for learning models for model-predictive control (MPC). Our approach, Jacobian-Regularized Dynamic-Mode Decomposition (JDMD), offers improved sample efficiency over traditional Koopman approaches based on Dynamic-Mode Decomposition (DMD) by leveraging Jacobian information from an approximate prior model of the system, and improved tracking performance over traditional model-based MPC. We demonstrate JDMD’s ability to quickly learn bilinear Koopman dynamics representations across several realistic examples in simulation, including a perching maneuver for a fixed-wing aircraft with an empirically derived high-fidelity physics model. In all cases, we show that the models learned by JDMD provide superior tracking and generalization performance within a model-predictive control framework, even in the presence of significant model mismatch, when compared to approximate prior models and models learned by standard Extended DMD (EDMD).</td>
                <td><a href="https://proceedings.mlr.press/v205/jackson23a.html">https://proceedings.mlr.press/v205/jackson23a.html</a></td>
            </tr>
        
            <tr>
                <td>In-Hand Gravitational Pivoting Using Tactile Sensing</td>
                <td>J, a, s, o, n,  , T, o, s, k, o, v, ,,  , R, h, y, s,  , N, e, w, b, u, r, y, ,,  , M, u, s, t, a, f, a,  , M, u, k, a, d, a, m, ,,  , D, a, n, a,  , K, u, l, i, c, ,,  , A, k, a, n, s, e, l,  , C, o, s, g, u, n</td>
                <td>corl2022</td>
                <td>We study gravitational pivoting, a constrained version of in-hand manipulation, where we aim to control the rotation of an object around the grip point of a parallel gripper. To achieve this, instead of controlling the gripper to avoid slip, we \emph{embrace slip} to allow the object to rotate in-hand. We collect two real-world datasets, a static tracking dataset and a controller-in-the-loop dataset, both annotated with object angle and angular velocity labels. Both datasets contain force-based tactile information on ten different household objects. We train an LSTM model to predict the angular position and velocity of the held object from purely tactile data. We integrate this model with a controller that opens and closes the gripper allowing the object to rotate to desired relative angles. We conduct real-world experiments where the robot is tasked to achieve a relative target angle. We show that our approach outperforms a sliding-window based MLP in a zero-shot generalization setting with unseen objects. Furthermore, we show a 16.6% improvement in performance when the LSTM model is fine-tuned on a small set of data collected with both the LSTM model and the controller in-the-loop. Code and videos are available at https://rhys-newbury.github.io/projects/pivoting/.</td>
                <td><a href="https://proceedings.mlr.press/v205/toskov23a.html">https://proceedings.mlr.press/v205/toskov23a.html</a></td>
            </tr>
        
            <tr>
                <td>CC-3DT: Panoramic 3D Object Tracking via Cross-Camera Fusion</td>
                <td>T, o, b, i, a, s,  , F, i, s, c, h, e, r, ,,  , Y, u, n, g, -, H, s, u,  , Y, a, n, g, ,,  , S, u, r, y, a, n, s, h,  , K, u, m, a, r, ,,  , M, i, n,  , S, u, n, ,,  , F, i, s, h, e, r,  , Y, u</td>
                <td>corl2022</td>
                <td>To track the 3D locations and trajectories of the other traffic participants at any given time, modern autonomous vehicles are equipped with multiple cameras that cover the vehicle’s full surroundings. Yet, camera-based 3D object tracking methods prioritize optimizing the single-camera setup and resort to post-hoc fusion in a multi-camera setup. In this paper, we propose a method for panoramic 3D object tracking, called CC-3DT, that associates and models object trajectories both temporally and across views, and improves the overall tracking consistency. In particular, our method fuses 3D detections from multiple cameras before association, reducing identity switches significantly and improving motion modeling. Our experiments on large-scale driving datasets show that fusion before association leads to a large margin of improvement over post-hoc fusion. We set a new state-of-the-art with 12.6% improvement in average multi-object tracking accuracy (AMOTA) among all camera-based methods on the competitive NuScenes 3D tracking benchmark, outperforming previously published methods by 6.5% in AMOTA with the same 3D detector. </td>
                <td><a href="https://proceedings.mlr.press/v205/fischer23a.html">https://proceedings.mlr.press/v205/fischer23a.html</a></td>
            </tr>
        
            <tr>
                <td>QuaDUE-CCM: Interpretable Distributional Reinforcement Learning using Uncertain Contraction Metrics for Precise Quadrotor Trajectory Tracking</td>
                <td>Y, a, n, r, a, n,  , W, a, n, g, ,,  , J, a, m, e, s,  , O, ’, K, e, e, f, f, e, ,,  , Q, i, u, c, h, e, n,  , Q, i, a, n, ,,  , D, a, v, i, d,  , B, o, y, l, e</td>
                <td>corl2022</td>
                <td>Accuracy and stability are common requirements for Quadrotor trajectory tracking systems. Designing an accurate and stable tracking controller remains challenging, particularly in unknown and dynamic environments with complex aerodynamic disturbances. We propose a Quantile-approximation-based Distributional-reinforced Uncertainty Estimator (QuaDUE) to accurately identify the effects of aerodynamic disturbances, i.e., the uncertainties between the true and estimated Control Contraction Metrics (CCMs). Taking inspiration from contraction theory and integrating the QuaDUE for uncertainties, our novel CCM-based trajectory tracking framework tracks any feasible reference trajectory precisely whilst guaranteeing exponential convergence. More importantly, the convergence and training acceleration of the distributional RL are guaranteed and analyzed, respectively, from theoretical perspectives. We also demonstrate our system under unknown and diverse aerodynamic forces. Under large aerodynamic forces (>2  m/s^2), compared with the classic data-driven approach, our QuaDUE-CCM achieves at least a 56.6% improvement in tracking error. Compared with QuaDRED-MPC, a distributional RL-based approach, QuaDUE-CCM achieves at least a 3 times improvement in contraction rate.</td>
                <td><a href="https://proceedings.mlr.press/v205/wang23d.html">https://proceedings.mlr.press/v205/wang23d.html</a></td>
            </tr>
        
        </table>
    </body>
    </html>
    