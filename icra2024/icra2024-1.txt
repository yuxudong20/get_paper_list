"ICRA 2024 Cover Page," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. c1-c1, doi: 10.1109/ICRA57147.2024.10611434.Abstract: URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611434&isnumber=10609862

"ICRA 2024 Welcome," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. i-ii, doi: 10.1109/ICRA57147.2024.10610403.Abstract: URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610403&isnumber=10609862

"ICRA 2024 Organizing Committee," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4-49, doi: 10.1109/ICRA57147.2024.10610680.Abstract: URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610680&isnumber=10609862

"ICRA 2024 Contributor Page," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. i-cccxv, doi: 10.1109/ICRA57147.2024.10611681.Abstract: URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611681&isnumber=10609862

"ICRA 2024 Program," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. i-iv, doi: 10.1109/ICRA57147.2024.10610032.Abstract: URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610032&isnumber=10609862

"ICRA 2024 Author Index," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. i-lxxxvii, doi: 10.1109/ICRA57147.2024.10610523.Abstract: URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610523&isnumber=10609862

K. Nguyen, S. Schoedel, A. Alavilli, B. Plancher and Z. Manchester, "TinyMPC: Model-Predictive Control on Resource-Constrained Microcontrollers," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1-7, doi: 10.1109/ICRA57147.2024.10610987.Abstract: Model-predictive control (MPC) is a powerful tool for controlling highly dynamic robotic systems subject to complex constraints. However, MPC is computationally demanding, and is often impractical to implement on small, resource-constrained robotic platforms. We present TinyMPC, a high-speed MPC solver with a low memory footprint targeting the microcontrollers common on small robots. Our approach is based on the alternating direction method of multipliers (ADMM) and leverages the structure of the MPC problem for efficiency. We demonstrate TinyMPC’s effectiveness by bench-marking against the state-of-the-art solver OSQP, achieving nearly an order of magnitude speed increase, as well as through hardware experiments on a 27 gram quadrotor, demonstrating high-speed trajectory tracking and dynamic obstacle avoidance. TinyMPC is publicly available at https://tinympc.org. keywords: {Microcontrollers;Trajectory tracking;Hardware;Convex functions;Collision avoidance;Robots;Predictive control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610987&isnumber=10609862

S. Liang, S. Amaya, H. Sugiura, H. Mo, Y. Dai and F. Arai, "A Movable Microfluidic Chip with Gap Effect for Manipulation of Oocytes," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8-13, doi: 10.1109/ICRA57147.2024.10610409.Abstract: This study proposes a novel movable microfluidic chip in which a microfluidic chip is integrated into a robotic manipulator for manipulating oocytes. The microfluidic device has the ability to release a single oocyte with a gap effect. The robotic manipulator can control the position of the microfluidic chip. The microfluidic chip with a pipette tip is directly fabricated using 3D printing. Xenopus oocyte was used in the experiment. When oocytes move from the back side of the channel to the front side, they generate gaps between each other. The gap distance can reach about 16 times the diameter of the oocyte. In addition, a capacitive sensor was used to detect oocytes in the manipulation processes. The results showed that oocytes were successfully released one by one with no deformation in shape using the movable microfluidic chip. The method has significant advantages in biomedicine engineering and micro-nano-manipulation. keywords: {Shape;Deformation;Manipulators;Three-dimensional printing;Capacitive sensors;Microfluidics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610409&isnumber=10609862

M. Reitsma, J. Keller, K. Blomqvist and R. Siegwart, "Under pressure: learning-based analog gauge reading in the wild," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14-20, doi: 10.1109/ICRA57147.2024.10610793.Abstract: We propose an interpretable framework for reading analog gauges that is deployable on real world robotic systems. Our framework splits the reading task into distinct steps, such that we can detect potential failures at each step. Our system needs no prior knowledge of the type of gauge or the range of the scale and is able to extract the units used. We show that our gauge reading algorithm is able to extract readings with a relative reading error of less than 2%. keywords: {Optical character recognition;Buildings;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610793&isnumber=10609862

T. Shi, W. Li, H. Yu and Y. Pan, "Efficient Composite Learning Robot Control Under Partial Interval Excitation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 21-26, doi: 10.1109/ICRA57147.2024.10610877.Abstract: Parameter convergence in adaptive control is crucial for improving the stability and robustness of robotic systems. Nevertheless, a stringent condition named persistent excitation (PE) needs to be satisfied to ensure parameter convergence in the conventional adaptive robot control. Composite learning robot control (CLRC) is an innovative methodology that guarantees parameter convergence under a condition of interval excitation (IE) that is strictly weaker than PE. This paper puts forward a time-division multi-channel (TDMC) CLRC strategy such that parameter convergence is achieved even without the IE condition. In the TDMC mechanism, a filtered regressor is integrated with multiple time intervals to generate a generalized prediction error for parameter update, such that excitation information of regressor channels at different instants is exploited more effectively and efficiently to achieve fast and accurate parameter estimation. Global exponential stability with parameter convergence of the closed-loop system is achieved under a partial IE condition that is much weaker than IE. Experiments on a collaborative robot with 7 degrees of freedom have demonstrated the superiority of the proposed approach in both parameter estimation and trajectory tracking compared to start-of-the-art approaches. keywords: {Parameter estimation;Trajectory tracking;Robot control;Collaborative robots;Information filters;Stability analysis;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610877&isnumber=10609862

E. R. Vieira, A. Sivaramakrishnan, S. Tangirala, E. Granados, K. Mischaikow and K. E. Bekris, "MORALS: Analysis of High-Dimensional Robot Controllers via Topological Tools in a Latent Space," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 27-33, doi: 10.1109/ICRA57147.2024.10610383.Abstract: Estimating the region of attraction (RoA) for a robot controller is essential for safe application and controller composition. Many existing methods require a closed-form expression that limit applicability to data-driven controllers. Methods that operate only over trajectory rollouts tend to be data-hungry. In prior work, we have demonstrated that topological tools based on Morse Graphs (directed acyclic graphs that combinatorially represent the underlying nonlinear dynamics) offer data-efficient RoA estimation without needing an analytical model. They struggle, however, with high-dimensional systems as they operate over a state-space discretization. This paper presents Morse Graph-aided discovery of Regions of Attraction in a learned Latent Space (MORALS)**. The approach combines auto-encoding neural networks with Morse Graphs. MORALS shows promising predictive capabilities in estimating attractors and their RoAs for data-driven controllers operating over high-dimensional systems, including a 67-dim humanoid robot and a 96-dim 3-fingered manipulator. It first projects the dynamics of the controlled system into a learned latent space. Then, it constructs a reduced form of Morse Graphs representing the bistability of the underlying dynamics, i.e., detecting when the controller results in a desired versus an undesired behavior. The evaluation on high-dimensional robotic datasets indicates data efficiency in RoA estimation. keywords: {Ethics;Directed acyclic graph;Neural networks;Estimation;Humanoid robots;Aerospace electronics;Control systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610383&isnumber=10609862

C. Zhang et al., "Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 34-41, doi: 10.1109/ICRA57147.2024.10611254.Abstract: Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures. Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization. In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot. Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end. To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions. We validate our approach in simulation and on the real quadruped robot ANYmal running in real-time (<10ms CPU inference). In a quantitative comparison with existing heuristic-based locally reactive planners, our policy increases the success rate over 30% when facing perception failures. Project Page: https://bit.ly/45NBTuh. keywords: {Legged locomotion;Navigation;Reinforcement learning;Robot sensing systems;Real-time systems;Sensors;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611254&isnumber=10609862

N. Yokoyama, S. Ha, D. Batra, J. Wang and B. Bucher, "VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 42-48, doi: 10.1109/ICRA57147.2024.10610712.Abstract: Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM’s zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real world deployment can be viewed at naoki.io/vlfm. keywords: {Weight measurement;Three-dimensional displays;Navigation;Semantics;Robot vision systems;Cameras;Cognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610712&isnumber=10609862

S. Yan, B. Zhang, Y. Zhang, J. Boedecker and W. Burgard, "Learning Continuous Control with Geometric Regularity from Robot Intrinsic Symmetry," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 49-55, doi: 10.1109/ICRA57147.2024.10610949.Abstract: Geometric regularity, which leverages data symmetry, has been successfully incorporated into deep learning architectures such as CNNs, RNNs, GNNs, and Transformers. While this concept has been widely applied in robotics to address the curse of dimensionality when learning from high-dimensional data, the inherent reflectional and rotational symmetry of robot structures has not been adequately explored. Drawing inspiration from cooperative multi-agent reinforcement learning, we introduce novel network structures for single-agent control learning that explicitly capture these symmetries. Moreover, we investigate the relationship between the geometric prior and the concept of Parameter Sharing in multi-agent reinforcement learning. Last but not the least, we implement the proposed framework in online and offline learning methods to demonstrate its ease of use. Through experiments conducted on various challenging continuous control tasks on simulators and real robots, we highlight the significant potential of the proposed geometric regularity in enhancing robot learning capabilities. keywords: {Learning systems;Deep architecture;Reinforcement learning;Transformers;Robot learning;Task analysis;Periodic structures},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610949&isnumber=10609862

H. Duan et al., "Learning Vision-Based Bipedal Locomotion for Challenging Terrain," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 56-62, doi: 10.1109/ICRA57147.2024.10611621.Abstract: Reinforcement learning (RL) for bipedal locomotion has recently demonstrated robust gaits over moderate terrains using only proprioceptive sensing. However, such blind controllers will fail in environments where robots must anticipate and adapt to local terrain, which requires visual perception. In this paper, we propose a fully-learned system that allows bipedal robots to react to local terrain while maintaining commanded travel speed and direction. Our approach first trains a controller in simulation using a heightmap expressed in the robot’s local frame. Next, data is collected in simulation to train a heightmap predictor, whose input is the history of depth images and robot states. We demonstrate that with appropriate domain randomization, this approach allows for successful sim-to-real transfer with no explicit pose estimation and no fine-tuning using real-world data. To the best of our knowledge, this is the first example of sim-to-real learning for vision-based bipedal locomotion over challenging terrains. keywords: {Legged locomotion;Robust control;Robot vision systems;Pose estimation;Neural networks;Propioception;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611621&isnumber=10609862

A. Sridhar, D. Shah, C. Glossop and S. Levine, "NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 63-70, doi: 10.1109/ICRA57147.2024.10610665.Abstract: Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer-based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal-conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches. keywords: {Navigation;Transformers;Diffusion models;Planning;Decoding;Proposals;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610665&isnumber=10609862

S. Xu, H. Ruan, W. Zhang, Y. Wang, L. Zhu and C. P. Ho, "Distributionally Robust Chance Constrained Trajectory Optimization for Mobile Robots within Uncertain Safe Corridor," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 88-94, doi: 10.1109/ICRA57147.2024.10611252.Abstract: Safe corridor-based Trajectory Optimization (TO) presents an appealing approach for collision-free path planning of autonomous robots, because its convex formulation can guarantee global optimality. The safe corridor is constructed based on the obstacle map, however, the non-ideal perception induces uncertainty, which is rarely considered in the context of trajectory generation. In this paper, we propose Distributionally Robust Safe Corridor Constraints (DRSCCs) to consider the uncertainty of the safe corridor. Then, we integrate DRSCCs into the trajectory optimization framework using Bernstein basis polynomials. Theoretically, we rigorously prove that the proposed trajectory optimization problem is equivalent to a convex quadratic program, which is computationally efficient to deploy onto real robots. The simulation results show that our method enhances navigation safety by significantly reducing the infeasible motions compared to the baseline. Moreover, the proposed approach is validated through two robotic applications, a micro Unmanned Aerial Vehicle (UAV) and a quadruped robot Unitree A1. keywords: {Uncertainty;Navigation;Simulation;Autonomous aerial vehicles;Numerical simulation;Polynomials;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611252&isnumber=10609862

S. Safaoui and T. H. Summers, "Distributionally Robust CVaR-Based Safety Filtering for Motion Planning in Uncertain Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 103-109, doi: 10.1109/ICRA57147.2024.10611276.Abstract: Safety is a core challenge of autonomous robot motion planning, especially in the presence of dynamic and uncertain obstacles. Many recent results use learning and deep learning-based motion planners and prediction modules to predict multiple possible obstacle trajectories and generate obstacle-aware ego robot plans. However, planners that ignore the inherent uncertainties in such predictions incur collision risks and lack formal safety guarantees. In this paper, we present a computationally efficient safety filtering solution to reduce the collision risk of ego robot motion plans using multiple samples of obstacle trajectory predictions. The proposed approach reformulates the collision avoidance problem by computing safe halfspaces based on obstacle sample trajectories using distributionally robust optimization (DRO) techniques. The safe halfspaces are used in a model predictive control (MPC)-like safety filter to apply corrections to the reference ego trajectory thereby promoting safer planning. The efficacy and computational efficiency of our approach are demonstrated through numerical simulations. keywords: {Maximum likelihood detection;Uncertainty;Filtering;Dynamics;Nonlinear filters;Safety;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611276&isnumber=10609862

S. Sheng, D. Parker and L. Feng, "Safe POMDP Online Planning via Shielding," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 126-132, doi: 10.1109/ICRA57147.2024.10610195.Abstract: Partially observable Markov decision processes (POMDPs) have been widely used in many robotic applications for sequential decision-making under uncertainty. POMDP online planning algorithms such as Partially Observable Monte-Carlo Planning (POMCP) can solve very large POMDPs with the goal of maximizing the expected return. But the resulting policies cannot provide safety guarantees which are imperative for real-world safety-critical tasks (e.g., autonomous driving). In this work, we consider safety requirements represented as almost-sure reach-avoid specifications (i.e., the probability to reach a set of goal states is one and the probability to reach a set of unsafe states is zero). We compute shields that restrict unsafe actions which would violate the almost-sure reach-avoid specifications. We then integrate these shields into the POMCP algorithm for safe POMDP online planning. We propose four distinct shielding methods, differing in how the shields are computed and integrated, including factored variants designed to improve scalability. Experimental results on a set of benchmark domains demonstrate that the proposed shielding methods successfully guarantee safety (unlike the baseline POMCP without shielding) on large POMDPs, with negligible impact on the runtime for online planning. keywords: {Runtime;Uncertainty;Monte Carlo methods;Scalability;Benchmark testing;Planning;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610195&isnumber=10609862

Y. Veys, M. S. Kurtz and N. Roy, "Generating Sparse Probabilistic Graphs for Efficient Planning in Uncertain Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 133-139, doi: 10.1109/ICRA57147.2024.10610493.Abstract: Environments with regions of uncertain traversability can be modeled as roadmaps with probabilistic edges for efficient planning under uncertainty. We would like to generate roadmaps that enable planners to efficiently find paths with expected low costs through uncertain environments. The roadmap must be sparse so that the planning problem is tractable, but still contain edges that are likely to contribute to low-cost plans under various realizations of the environmental uncertainty. Determining the optimal set of edges to add to the roadmap without considering an exponential number of traversability scenarios is challenging. We propose the use of a heuristic that bounds the ratio between the expected path cost in our graph and the expected path cost in an optimal graph to determine whether a given edge should be added to the roadmap. We test our approach in several environments, demonstrating that our uncertainty-aware roadmaps effectively trade off between plan quality and planning efficiency for uncertainty-aware agents navigating in the graph. keywords: {Measurement;Costs;Uncertainty;Three-dimensional displays;Probabilistic logic;Satellite navigation systems;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610493&isnumber=10609862

T. E. Amish, J. T. Auletta, C. C. Kessens, J. R. Smith and J. I. Lipton, "Johnsen-Rahbek Capstan Clutch: A High Torque Electrostatic Clutch," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 148-154, doi: 10.1109/ICRA57147.2024.10611283.Abstract: In many robotic systems, the holding state consumes power, limits operating time, and increases operating costs. Electrostatic clutches have the potential to improve robotic performance by generating holding torques with low power consumption. A key limitation of electrostatic clutches has been their low specific shear stresses which restrict generated holding torque, limiting many applications. Here we show how combining the Johnsen-Rahbek (JR) effect with the exponential tension scaling capstan effect can produce clutches with the highest specific shear stress in the literature. Our system generated 31.3 N/cm2 sheer stress and a total holding torque of 7.1 N•m while consuming only 2.5 mW/cm2 at 500 V. We demonstrate a theoretical model of an electrostatic adhesive capstan clutch and demonstrate how large angle (θ > 2π) designs increase efficiency over planar or small angle (θ < π) clutch designs. We also report the first unfilled polymeric material, polybenzimidazole (PBI), to exhibit the JR-effect. keywords: {Electric potential;Torque;Power demand;Limiting;Costs;Polymers;Electrostatics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611283&isnumber=10609862

S. Xiao et al., "Research on bionic foldable wing for flapping wing micro air vehicle," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 155-160, doi: 10.1109/ICRA57147.2024.10610536.Abstract: This paper presents a bionic foldable wing that imitates the hind wing of ladybirds. Based on the folding mechanism of the hind wing of ladybirds and the theory of origami, the motion model of the bionic foldable wing is established, yield the motion law of the crease angles and the variation relationship between the panels are obtained. Bionic foldable wings utilise shape memory alloy to drive wings to fold, and embedded torsion springs to release energy to realize the function of wing unfolding. In the experiments of the vehicle equipped with foldable wings, the lift and attitude torque of bionic foldable wings are measured by the F/T sensor. The experimental results indicated that its aerodynamic performance is basically close to that of our optimized non-foldable wings. Moreover, the vehicle with foldable wings has been able to overcome gravity to achieve flight, which provides a novel concept for the research on flapping wing. keywords: {Torque;Shape memory alloys;Prototypes;Kinematics;Aerodynamics;Real-time systems;Torque measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610536&isnumber=10609862

P. Baisamy, A. Stokes and F. Giorgio-Serchi, "A scalable monolithic 3D printable variable stiffness mechanism," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 169-175, doi: 10.1109/ICRA57147.2024.10610379.Abstract: Variable Stiffness Mechanisms (VSM) are becoming ubiquitous in mechatronics given the benefit they provide in terms of safety and performance. Despite these assets, VSMs remain fairly complex mechanical devices lacking in compactness, ease of manufacturing and accessibility. In addition, the scarcity of commercially available VSMs requires that such systems are mostly designed in-house. We propose a new type of VSM that improves on the pre-existing Jack Spring concept by making it more compact and robust. The new concept, which we refer to as the Compact Modifier of Active Coils (C-MAC) mechanism, is specifically designed to be manufactured through a monolithic 3D print. This approach enables to modify a minimal set of design features, namely the spring diameter and the coil diameter, to achieve the desired range of stiffness variation. We test the proposed design on six configurations; these show hysteretic energy losses no larger than 35% over the stiffness variation and confirm stiffness to scale according to theory. Stiffness ranging from 0.15 N/mm to 1.02N/mm were measured for an overall device length of 140 mm, including a maximal stroke length of 22 mm. The results confirm excellent scalability and manufacturability of the proposed design, providing a versatile mechanism for fast prototyping and the development of entire 3D printed robotic systems embedding variable stiffness capabilities. keywords: {Coils;Three-dimensional displays;Scalability;Three-dimensional printing;Market research;Manufacturing;Resins},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610379&isnumber=10609862

D. Du, E. Del Dottore, A. Mondini, E. Sinibaldi and B. Mazzolai, "Modular Growing Mechanism with Multi-axis Deformation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 176-182, doi: 10.1109/ICRA57147.2024.10610637.Abstract: Plant cells expand and elongate. Their cumulative actuation defines organ morphing. Inspired by this modular transformability, this study proposes a modular concept for growing robots that will be able to grow by adding at their tip Transformable Modules (TMs). We provide a two-module implementation to evaluate the concept viability. We designed and characterized Shape-Retention Bellows (SRBs) that constitute the TM and are used to maintain the shape once the extension force is relaxed. We demonstrate module radial expansion and axial elongation in a straight and bent configuration (up to ~4°). This is the first concept of growing robots to enact the robot's modularity and transformability for future deployment in distributed growing systems capable of acting in various scenarios. keywords: {Bellows;Actuators;Shape;Force;Bending;Soft robotics;Robot sensing systems;Growing robots;Shape-retention bellows;Transformable modules;Radial expansion;Axial elongation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610637&isnumber=10609862

C. A. Pérez-Díaz et al., "Design and Experimental Characterisation of a Novel Quasi-Direct Drive Actuator for Highly Dynamic Robotic Applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 183-189, doi: 10.1109/ICRA57147.2024.10611567.Abstract: This paper presents the design and experimental results of a proprioceptive, high-bandwidth quasi-direct drive (QDD) actuator for highly dynamic robotic applications. A comprehensive review of the mechanical design of the PULSE115-60 actuator is presented, with particular focus on the design parameters affecting the dynamic performance of the actuator and a full specification is provided. Fundamental parameters to describe the dynamic behaviour of an actuator are discussed, and an experimental method to determine speed and torque bandwidth of the actuator is presented. A rigorous method to determine backdrive torque is also explained. Finally, experimental results quantifying the dynamic performance of the PULSE115-60 actuator are discussed. The PULSE115-60 actuator has a highly dynamic response, surpassing the torque bandwidth at low torque amplitudes showcased in state-of-the-art literature. The differences between current and torque bandwidth, two concepts often conflated in literature, are elucidated. Experimental procedures detailed in previous work are discussed and a novel standardised procedure is proposed for robust characterisation and fair comparison of different actuation systems. Finally, performance results for PULSE115-60 are presented, demonstrating a torque bandwidth of 66.3 Hz at an amplitude of 6 N•m, ±0.11° of backlash and 0.37 N•m of backdrive torque. keywords: {Actuators;Torque;Reviews;Pulse measurements;Propioception;Human-robot interaction;Bandwidth;Actuation and Joint Mechanisms;Dynamics;Force Control;Bandwidth;Backdrivability;Transparency;Proprioception;Quasi-Direct Drive;Development and Prototyping;Physical Human-Robot Interaction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611567&isnumber=10609862

L. Zheng, Q. Wu, Y. Zhu and Q. Zhang, "Design and Evaluation of a Reconfigurable 7-DOF Upper Limb Rehabilitation Exoskeleton with Gravity Compensation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 190-196, doi: 10.1109/ICRA57147.2024.10610011.Abstract: With the development of society, aging population and the number of stroke patients is increasing year by year. Rehabilitation exoskeleton can help patients to carry out rehabilitation training and improve their activities of daily living (ADL). First of all, a reconfigurable exoskeleton for upper limb rehabilitation is designed in this paper. The exoskeleton combines gravity compensation with left-right arm switching function through its reconfigurability. Secondly, the motion space and singular configuration of the exoskeleton are analyzed. By changing the working mode of the gravity compensation device, the control experiment of the motor is carried out. The influence of gravity compensation device on motor driving torque and energy consumption is analyzed. Finally, the results of experiment show that, in the best case, the gravity compensation device can reduce the energy consumption by 41.15% and the maximum motor current by 33.56% of the driving element. keywords: {Training;Performance evaluation;Energy consumption;Torque;Exoskeletons;Switches;Kinematics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610011&isnumber=10609862

M. G. Selvamuthu, K. Abe, K. Tadakuma and R. Tadakuma, "Flexible Omnidirectional Driving Gear Mechanism with Adaptation over Arbitrary Curvatures," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 197-203, doi: 10.1109/ICRA57147.2024.10609984.Abstract: A support structure for flexible displays such as OLED or flexible LEDs was developed using the flexible omnidirectional driving gear mechanism. It is a gear mechanism having two degrees of freedom on one surface. This flexible display mechanism is expected to be placed inside a car dashboard as a human interface and for workspace optimization. In this study, we propose a novel flexible omnidirectional driving gear for supporting flexible displays discussing its design, motion range, repeatability, positional accuracy, and adaptability to any guide surface through magnetic coupling. The experiments showed satisfactory results for positional accuracy and repeatability with adaptability over a wide range of curvatures. keywords: {Couplings;Accuracy;Gears;Tracking;2-DOF;Prototypes;Organic light emitting diodes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609984&isnumber=10609862

K. Karacan, R. J. Kirschner, H. Sadeghian, F. Wu and S. Haddadin, "Tactile Robot Programming: Transferring Task Constraints into Constraint-Based Unified Force-Impedance Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 204-210, doi: 10.1109/ICRA57147.2024.10610054.Abstract: Flexible manufacturing lines are required to meet the demand for customized and small batch-size products. Even though state-of-the-art tactile robots may provide the versatility for increased adaptability and flexibility, their potential is yet to be fully exploited. To support robotics deployment in manufacturing, we propose a task-based tactile robot programming paradigm that uses an object-centric tactile skill definition that directly links identified object constraints of the task to the definition of constraint-based unified force-impedance control. In this study, we first explain the basic concept of abstracting the task constraints experienced by the object and transferring them to the robot’s operational space frame. Second, using the object-centric tactile skill definition, we synthesize unified force-impedance control and formalized holonomic constraints to enable flexible task execution. Later, we propose the quantified analysis metrics for the process by analyzing them as a typical example of flexible manipulation disassembly skills, e.g., levering and unscrew-driving regarding their object requirements. Supported by realistic experimental evaluation using a Franka Emika robot, our tactile robot programming approach for the direct translation between task-level constraints and robot control parameter design is shown to be a viable solution for increased robotic deployment in flexible manufacturing lines. keywords: {Measurement;Robot control;Force;Manipulators;Manufacturing;Object recognition;Impedance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610054&isnumber=10609862

L. Gupta, J. Chowdhury Choton and P. Prabhakar, "Safety Verification of Closed-loop Control System with Anytime Perception," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 227-233, doi: 10.1109/ICRA57147.2024.10611044.Abstract: In this paper, we consider the problem of safety analysis of a closed-loop control system with anytime perception sensor. We formalize the framework and present a general procedure for safety analysis using reachable set computation. We instantiate the procedure for two concrete classes, namely, the classical discrete-time linear system with linear state feedback controller and an extension with variable update rates. We present an exact computational method based on polyhedral manipulations for the first class and an overapproximate method for the second class. Our experimental results demonstrate the feasibility of the approach. keywords: {Linear systems;State feedback;Scalability;Control systems;Robot sensing systems;Safety;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611044&isnumber=10609862

N. Jaquier, L. Rozo and T. Asfour, "Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 242-249, doi: 10.1109/ICRA57147.2024.10611701.Abstract: In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the "single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algorithm is applied. This paper provides a theoretical elucidation of various misconceptions surrounding this approach and offers experimental evidence of its shortcomings. Finally, it presents valuable insights to promote best practices when employing Riemannian geometry within robot learning applications. keywords: {Geometry;Manifolds;Quaternions;Robot learning;Data models;Task analysis;Ellipsoids},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611701&isnumber=10609862

D. Kamale and C. -I. Vasile, "Optimal Control Synthesis with Relaxed Global Temporal Logic Specifications for Homogeneous Multi-robot Teams," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 250-256, doi: 10.1109/ICRA57147.2024.10610142.Abstract: In this work, we address the problem of control synthesis for a homogeneous team of robots given a global temporal logic specification and formal user preferences for relaxation in case of infeasibility. The relaxation preferences are represented as a Weighted Finite-state Edit System and are used to compute a relaxed specification automaton that captures all allowable relaxations of the mission specification and their costs. For synthesis, we introduce a Mixed Integer Linear Programming (MILP) formulation that combines the motion of the team of robots with the relaxed specification automaton. Our approach combines automata-based and MILP-based methods and leverages the strengths of both approaches, while avoiding their shortcomings. Specifically, the relaxed specification automaton explicitly accounts for the progress towards satisfaction, and the MILP-based optimization approach avoids the state-space explosion associated with explicit product-automata construction, thereby efficiently solving the problem. The case studies highlight the efficiency of the proposed approach. keywords: {Runtime;Costs;Automata;Optimal control;Explosions;Mixed integer linear programming;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610142&isnumber=10609862

K. Liang, G. A. Cardona and C. -I. Vasile, "An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Temporal Logic Goals and Travel Duration Uncertainty," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 257-263, doi: 10.1109/ICRA57147.2024.10611519.Abstract: This paper introduces an iterative approach to multi-agent route planning under chance constraints. A heterogeneous team of agents with various capabilities is tasked with a Capability Temporal Logic (CaTL) mission, a fragment of Signal Temporal Logic. The agents’ motion is modeled as a finite weighted graph, where the weights represent travel durations. Given the probability distribution over the durations of each edge’s traversal, we want to find paths for all agents such that (a) the specification robustness is maximized, (b) travel time is minimized, and (c) the success probability is maximized. We tackle the problem using an iterative approach. In each stage, it selects edges’ traversal duration and success probabilities and then solves a multi-agent route planning problem. We use an efficient Mixed-Integer Linear Programming (MILP) encoding for the latter. Our method provides a framework for agents to make informed decisions in choosing the most suitable edge attributes (travel durations and success probabilities) that consider agents’ capabilities to perform tasks in the environment. The proposed iterative method leverages graph structure to generate a more efficient search space. The effectiveness of our method is demonstrated through simulated case studies where obtaining the optimal solution would otherwise be computationally expensive. Our approach efficiently explores the solution space, generating better solutions and improving the performance of multi-agent route planning with uncertain travel durations. keywords: {Uncertainty;Stochastic processes;Robustness;Planning;Space exploration;Iterative methods;Logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611519&isnumber=10609862

J. Jiang et al., "Phasic Diversity Optimization for Population-Based Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 272-278, doi: 10.1109/ICRA57147.2024.10610814.Abstract: Reviewing the previous work of diversity Reinforcement Learning, diversity is often obtained via an augmented loss function, which requires a balance between reward and diversity. Generally, diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degradation. Furthermore, we construct a dogfight scenario for aerial agents to demonstrate the practicality of the PDO algorithm. We introduce two implementations of PDO archive and conduct tests in the newly proposed adversarial dogfight and MuJoCo simulations. The results show that our proposed algorithm achieves better performance than baselines. keywords: {Training;Degradation;DVD;Heuristic algorithms;Reinforcement learning;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610814&isnumber=10609862

F. Lin, C. Wei, R. Grech and Z. Ji, "VO-Safe Reinforcement Learning for Drone Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 279-285, doi: 10.1109/ICRA57147.2024.10611487.Abstract: This work is focused on reinforcement learning (RL)-based navigation for drones, whose localisation is based on visual odometry (VO). Such drones should avoid flying into areas with poor visual features, as this can lead to deteriorated localization or complete loss of tracking. To achieve this, we propose a hierarchical control scheme, which uses an RL-trained policy as the high-level controller to generate waypoints for the next control step and a low-level controller to guide the drone to reach subsequent waypoints. For the high-level policy training, unlike other RL-based navigation approaches, we incorporate awareness of VO performance into our policy by introducing pose estimation-related punishment. To aid robots in distinguishing between perception-friendly areas and unfavoured zones, we instead provide semantic scenes, as input for decision-making instead of raw images. This approach also helps minimise the sim-to-real application gap. keywords: {Training;Motion planning;Visualization;Navigation;Semantics;Reinforcement learning;Reliability theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611487&isnumber=10609862

Z. Mandi, S. Jain and S. Song, "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 286-299, doi: 10.1109/ICRA57147.2024.10610855.Abstract: We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMs’ agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach — it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility — in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. keywords: {Trajectory planning;Robot kinematics;Large language models;Semantics;Collaboration;Benchmark testing;Human in the loop},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610855&isnumber=10609862

Z. Huang, Z. Yang, R. Krupani, B. Şenbaşlar, S. Batra and G. S. Sukhatme, "Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 300-306, doi: 10.1109/ICRA57147.2024.10611499.Abstract: End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits – easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our work is the first work that demonstrates the possibility of learning neighbor-avoiding and obstacle-avoiding control policies trained with end-to-end DRL that transfers zero-shot to real quadrotors. Our approach scales to 32 robots with 80% obstacle density in simulation and 8 robots with 20% obstacle density in physical deployment. Website: https://sites.google.com/view/obst-avoid-swarm-rl keywords: {Training;Navigation;Deep reinforcement learning;Real-time systems;Hardware;Planning;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611499&isnumber=10609862

A. Henshall and S. Karaman, "Multi-Level Action Tree Rollout (MLAT-R): Efficient and Accurate Online Multiagent Policy Improvement," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 315-321, doi: 10.1109/ICRA57147.2024.10610888.Abstract: Rollout algorithms are renowned for their abilities to correct for the suboptimalities of offline-trained base policies. In the multiagent setting, performing online rollout can require an exponentially large number of optimizations with respect to the number of agents. One-agent-at-a-time algorithms offer computationally efficient approaches to guaranteed policy improvement; however, this improvement is with respect to a state value estimate derived from a potentially poor base policy. Monte Carlo tree search (MCTS) provably converges to the true state value estimates; however, the exponentially large search space often makes its online use limited. Here, we present the Multi-Level Action Tree Rollout (MLAT-R) algorithm. MLAT-R provides 1) provable improvement over a base policy, 2) policy improvement with respect to the true state value, 3) applicability to any number of agents, and 4) an action space that grows linearly with the number of agents rather than exponentially. In this paper, we outline the algorithm, sketch a proof of its improvement over a base policy, and evaluate its performance on a challenging problem for which the base policy cannot reach a terminal state. Despite the challenging experimental setup, our algorithm reached a terminal state in 86% of all experiments, compared to 31% for state-of-the-art one-agent-at-a-time algorithms. In experiments involving MCTS, MLAT-R reached a terminal state in 99% of experiments compared to 92% for MCTS. MLAT-R achieved these results while considering an exponentially smaller action space than MCTS. keywords: {Monte Carlo methods;Accuracy;Computational efficiency;Robotics and automation;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610888&isnumber=10609862

K. Huang, D. Guo, X. Zhang, X. Ji and H. Liu, "Stimulate the Potential of Robots via Competition," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 322-328, doi: 10.1109/ICRA57147.2024.10611581.Abstract: It is common for us to feel pressure in a competition environment, which arises from the desire to obtain success comparing with other individuals or opponents. Although we might get anxious under the pressure, it could also be a drive for us to stimulate our potentials to the best in order to keep up with others. Inspired by this, we propose a competitive learning framework which is able to help individual robot to acquire knowledge from the competition, fully stimulating its dynamics potential in the race. Specifically, the competition information among competitors is introduced as the additional auxiliary signal to learn advantaged actions. We further build a Multiagent-Race environment, and extensive experiments are conducted, demonstrating that robots trained in competitive environments outperform ones that are trained with SoTA algorithms in single robot environment. keywords: {Training;Competitive learning;Heuristic algorithms;Propioception;Benchmark testing;Robots;Signal to noise ratio},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611581&isnumber=10609862

S. Xin et al., "Multi-modal 3D Human Tracking for Robots in Complex Environment with Siamese Point-Video Transformer," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 337-344, doi: 10.1109/ICRA57147.2024.10610979.Abstract: Tracking a specific person in 3D scene is gaining momentum due to its numerous applications in robotics. Currently, most 3D trackers focus on driving scenarios with neglected jitter and uncomplicated surroundings, which results in their severe degeneration in complex environments, especially on jolting robot platforms (only 20-60% success rate). To improve the accuracy, a Point-Video-based Transformer Tracking model (PVTrack) is presented for robots. It is the first multi-modal 3D human tracking work that incorporates point clouds together with RGB videos to achieve information complementarity. Moreover, PVTrack proposes the Siamese Point-Video Transformer for feature aggregation to overcome dynamic environments, which captures more target-aware information through the hierarchical attention mechanism adaptively. Considering the violent shaking on robots and rugged terrains, a lateral Human-ware Proposal Network is designed together with an Anti-shake Proposal Compensation module. It alleviates the disturbance caused by complex scenes as well as the particularity of the robot platform. Experiments show that our method achieves state-of-the-art performance on both KITTI/Waymo datasets and a quadruped robot for various indoor and outdoor scenes. keywords: {Point cloud compression;Three-dimensional displays;Target tracking;Laser radar;Jitter;Transformers;Proposals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610979&isnumber=10609862

L. Steffen, T. Trapp, A. Roennau and R. Dillmann, "Efficient Gesture Recognition on Spiking Convolutional Networks Through Sensor Fusion of Event-Based and Depth Data," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 345-352, doi: 10.1109/ICRA57147.2024.10610824.Abstract: As intelligent systems become increasingly important in our daily lives, new ways of interaction are needed. Classical user interfaces pose issues for the physically impaired and are partially not practical or convenient. Gesture recognition is an alternative, but often not reactive enough when conventional cameras are used. This work proposes a Spiking Convolutional Neural Network, processing event- and depth data for gesture recognition. The network is simulated using the open-source neuromorphic computing framework LAVA for offline training and evaluation on an embedded system. For the evaluation three open source data sets are used. Since these do not represent the applied bi-modality, a new data set with synchronized event- and depth data was recorded. The results show the viability of temporal encoding on depth information and modality fusion, even on differently encoded data, to be beneficial to network performance and generalization capabilities. keywords: {Training;Neuromorphic engineering;Lava;Gesture recognition;User interfaces;Sensor fusion;Convolutional neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610824&isnumber=10609862

M. De Sa, P. Kotaru and K. Sreenath, "Point Cloud-Based Control Barrier Function Regression for Safe and Efficient Vision-Based Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 366-372, doi: 10.1109/ICRA57147.2024.10610647.Abstract: Control barrier functions have become an increasingly popular framework for safe real-time control. In this work, we present a computationally low-cost framework for synthesizing barrier functions over point cloud data for safe vision-based control. We take advantage of surface geometry to locally define and synthesize a quadratic CBF over a point cloud. This CBF is used in a CBF-QP for control and verified in simulation on quadrotors and in hardware on quadrotors and the TurtleBot3. This technique enables safe navigation through unstructured and dynamically changing environments and is shown to be significantly more efficient than current methods. keywords: {Point cloud compression;Geometry;Navigation;Real-time systems;Hardware;Robotics and automation;Quadrotors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610647&isnumber=10609862

S. Zhang et al., "An Image Acquisition Scheme for Visual Odometry based on Image Bracketing and Online Attribute Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 381-387, doi: 10.1109/ICRA57147.2024.10611141.Abstract: Visual odometry (VO) system is challenged by complex illumination environments. Image quality and its consistency in the time domain directly determine feature detection and tracking performance, which further affect the robustness and accuracy of the entire system. In this paper, an image acquisition scheme with image bracketing patterns is proposed. Images with different exposure levels are continuously captured to sufficiently explore the scene under varying illumination. An attribute control method is designed to adjust image exposures within the brackets online. Gaussian process regression fits the relationship between image quality metric and exposure via image synthesis technique. The optimal exposures for the next bracket are obtained directly without attempts to ensure a quick response. Experiments show our acquisition system’s effectiveness and performance improvement for VO tasks in complex illumination scenes. keywords: {Image quality;Measurement;Image synthesis;Lighting;Gaussian processes;Robustness;Time-domain analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611141&isnumber=10609862

W. Fan, H. Li and D. Zhang, "MagicTac: A Novel High-Resolution 3D Multi-layer Grid-Based Tactile Sensor," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 388-394, doi: 10.1109/ICRA57147.2024.10610615.Abstract: Accurate robotic control over interactions with the environment is fundamentally grounded in understanding tactile contacts. In this paper, we introduce MagicTac, a novel high-resolution grid-based tactile sensor. This sensor employs a 3D multi-layer grid-based design, inspired by the Magic Cube structure. This structure can help increase the spatial resolution of MagicTac to perceive external interaction contacts. Moreover, the sensor is produced using the multi-material additive manufacturing technique, which simplifies the manufacturing process while ensuring repeatability of production. Compared to traditional vision-based tactile sensors, it offers the advantages of i) high spatial resolution, ii) significant affordability, and iii) fabrication-friendly construction that requires minimal assembly skills. We evaluated the proposed MagicTac in the tactile reconstruction task using the deformation field and optical flow. Results indicated that MagicTac could capture fine textures and is sensitive to dynamic contact information. Through the grid-based multi-material additive manufacturing technique, the affordability and productivity of MagicTac can be enhanced with a minimum manufacturing cost of £4.76 and a minimum manufacturing time of 24.6 minutes. keywords: {Productivity;Three-dimensional displays;Manufacturing processes;Costs;Deformation;Tactile sensors;Three-dimensional printing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610615&isnumber=10609862

S. Sontakke, P. Hegde, P. Bannulmath and D. K T, "Mobile Bot Rotation Using Sound Source Localization And Distant Speech Recognition," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 403-409, doi: 10.1109/ICRA57147.2024.10610539.Abstract: In the last few years, mobile robots such as floor cleaners, assistive robots, and home telepresence have become an essential part of our day-to-day activities. In human-robot interaction, speech is the preferred way of communication, especially in indoor environments. This paper proposes a speech module to rotate the mobile robot. It has two components, namely, a distant automatic speech recognizer and a sound source localizer. To build distant speech recognizer, far-field speech data is collected at 1, 3, and 5-meters distances. The model performs well even at a 5-meters distance with a Word Error Rate of 40.38% and a Character Error Rate of 28.85%. The direction of arrival of the speech signal is computed from the 4-mic circular array microphone. The speech module is integrated with the Robot Operating System and physically demonstrated on Turtlebot3 Waffle Pi. It is observed that the speech recognizer and sound source localizer work well in the reverberant indoor environment with a small single-board computer. keywords: {Location awareness;Telepresence;Error analysis;Operating systems;Human-robot interaction;Speech recognition;Microphone arrays},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610539&isnumber=10609862

A. Das, S. Biswas, U. Pal and J. Lladós, "Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 410-417, doi: 10.1109/ICRA57147.2024.10611120.Abstract: When used in a real-world noisy environment, the capacity to generalize to multiple domains is essential for any autonomous scene text spotting system. However, existing state-of-the-art methods employ pretraining and fine-tuning strategies on natural scene datasets, which do not exploit the feature interaction across other complex domains. In this work, we explore and investigate the problem of domain-agnostic scene text spotting, i.e., training a model on multi-domain source data such that it can directly generalize to target domains rather than being specialized for a specific domain or scenario. In this regard, we present the community a text spotting validation benchmark called Under-Water Text (UWT) for noisy underwater scenes to establish an important case study. Moreover, we also design an efficient super-resolution based end-to-end transformer baseline called DA-TextSpotter which achieves comparable or superior performance over existing text spotting architectures for both regular and arbitrary-shaped scene text spotting benchmarks in terms of both accuracy and model efficiency. The dataset, code and pre-trained models have been released in our Github. keywords: {Training;Codes;Accuracy;Superresolution;Benchmark testing;Transformers;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611120&isnumber=10609862

B. Xing, X. Ying and R. Wang, "Masked Local-Global Representation Learning for 3D Point Cloud Domain Adaptation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 418-424, doi: 10.1109/ICRA57147.2024.10611402.Abstract: Point cloud is a popular and widely used geometric representation, which has attracted significant attention in 3D vision. However, the geometric variability of point cloud representations across different datasets can cause domain discrepancies, which hinder knowledge transfer and model generalization, resulting in degraded performance in target domain. In this paper, we present a novel approach to improve point cloud domain adaptation by employing masked representation learning in a self-supervised manner. Specifically, our method combines masked feature prediction and masked sample consistency to encode both local structure and global semantic information for learning invariant point cloud representation across domains. Moreover, to learn domain-specific representation and transfer knowledge from source to target, we propose prototype-calibrated self-training. By exploiting class-wise prototypes in the shared feature space, the soft pseudo labels can be adaptively denoised, which benefits the decision boundary learning in target domain. We conduct experiments on PointDA-10 and PointSegDA for 3D point cloud shape classification and semantic segmentation, respectively. The results demonstrate the effectiveness of our method and show that we can achieve the new state-of-the-art performance on point cloud domain adaptation. keywords: {Point cloud compression;Representation learning;Adaptation models;Three-dimensional displays;Shape;Semantic segmentation;Semantics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611402&isnumber=10609862

F. Rollo, A. Zunino, N. Tsagarakis, E. M. Hoffman and A. Ajoudani, "Continuous Adaptation in Person Re-identification for Robotic Assistance," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 425-431, doi: 10.1109/ICRA57147.2024.10611226.Abstract: In scenarios of Human-Robot Interaction (HRI), it is often assumed that the robot should cooperate with the closest individual or that only one person is present. However, in real-life situations, such as shop floor operations, this assumption may not hold. Thus, it becomes necessary for a robot to recognize a specific target in a crowded environment. To address this problem, we propose a person re-identification module that uses continuous visual adaptation techniques. This module ensures that the robot can seamlessly cooperate with the appropriate individual despite its appearance changes or partial or total occlusions. We used both a laboratory environment and an HRI scenario where the robot followed a person to test our framework. During the test, the targets were asked to change their appearance and disappear from the camera’s field of view to test the module’s ability to handle challenging cases of occlusion and outfit variations. We compared our framework with a state-of-the-art Multi-Object Tracking (MOT) method, and the results showed that our module, shortly named CARPE-ID, accurately tracked each selected target throughout the experiments in all cases except for two cases. In contrast, the MOT had an average of 4 tracking errors for each video. keywords: {Training;Visualization;Target tracking;Target recognition;Face recognition;Robot vision systems;Human-robot interaction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611226&isnumber=10609862

C. Wei and Z. Deng, "Incorporating Scene Graphs into Pre-trained Vision-Language Models for Multimodal Open-vocabulary Action Recognition," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 440-447, doi: 10.1109/ICRA57147.2024.10611454.Abstract: This paper presents Action-SGFA, a novel action feature alignment approach to learn unified joint embeddings across four action modalities incorporating scene graph (SG) comprehension. A new training paradigm for Action-SGFA is also devised to improve pre-trained VL models using datasets with SG annotation. When learning from image-SG pairs, it captures structure-associated action knowledge for visual and textual encoders. SG supervision generates fine-grained captions based on various graph augmentations highlighting different compositional aspects of action scenes. Furthermore, our research reveals that all combinations of paired data are unnecessary to train such unified embeddings, and only image-paired data is sufficient to bind all action modalities together. Our Action-SGFA can leverage existing large VL models, enhancing their zero-shot capabilities of new modalities due to their natural pairings with images. The open-vocabulary zero-shot performance improves with the strength of the pre-trained VL model and the SG comprehension. We establish a new state-of-the-art in several zero-shot action recognition tasks across modalities, significantly surpassing the vanilla skeleton zero-shot method by 27.0% and 19.7% on NTU-60 and NTU-120, respectively. Additionally, in the context of RGB videos, we surpass the state-of-the-art method on Kinetics-400 by 2.1%. keywords: {Training;Visualization;Annotations;Skeleton;Task analysis;Robotics and automation;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611454&isnumber=10609862

C. Liu, G. Chen and R. Song, "LPS-Net: Lightweight Parameter-shared Network for Point Cloud-based Place Recognition," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 448-454, doi: 10.1109/ICRA57147.2024.10610758.Abstract: With innovation in fields such as autonomous driving and augmented reality, point cloud-based place recognition has gained significant attention. Many methods try to address this problem by extracting and matching global descriptors in a database, but they often must balance the extraction of comprehensive contextual information and large model sizes. To overcome this challenge, we propose a lightweight parameter-shared network (LPS-Net), which includes multiple bidirectional perception units (BPUs) to extract multiscale long-range contextual information and parameter-shared NetVLADs (PS-VLADs) to aggregate descriptors. A BPU includes a parameter-shared convolution module (SharedConv) that significantly compresses the model and enhances its ability to capture informative features. In PS-VLADs, we replace half the parameters used in the original NetVLAD with trainable scalars, which further reduces the model size, and theoretically prove their equivalence. Experimental results demonstrate that LPS-Net achieves state-of-the-art performance at the task of point cloud-based place recognition while maintaining a small model size. Code and supplementary materials can be found at https://github.com/Yavinr/LPS-Net. keywords: {Technological innovation;Codes;Databases;Convolution;Feature extraction;Data mining;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610758&isnumber=10609862

B. Wang, W. Li, B. Zhang and Y. Liu, "Joint Response and Background Learning for UAV Visual Tracking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 455-462, doi: 10.1109/ICRA57147.2024.10611308.Abstract: Correlation filter (CF)-based approaches have gained widespread attention in the field of unmanned aerial vehicle (UAV) visual tracking due to their light-weight characteristics. However, CFs are prone to generating low-quality response in challenging UAV scenarios, e.g., fast motion and background clutter. In this paper, in order to model the tracker more robustly, we first conduct an effective regularization analysis from the perspectives of response- and background-learning. Specifically, to address response degradation, we propose a module for learning temporal consistency and reversibility of response, supplemented by a novel background-aware module to enhance the ability to learn from negative samples. In addition, we propose a fast coarse-to-fine scale search strategy, which alleviates the challenges in estimating bounding boxes under non-uniform aspect ratios. We have developed two tracker versions, namely RBLT and DeepRBLT, based on the depth of the features. Comprehensive experiments on four UAV benchmarks and one generic benchmark have indicated the superiority of our trackers compared to other state-of-the-art trackers, with enough speed for real-time applications. keywords: {Degradation;Visualization;Correlation;Tracking;Benchmark testing;Autonomous aerial vehicles;Search problems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611308&isnumber=10609862

P. Ausserlechner, D. Haberger, S. Thalhammer, J. -B. Weibel and M. Vincze, "ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 463-469, doi: 10.1109/ICRA57147.2024.10611464.Abstract: As robotic systems increasingly encounter complex and unconstrained real-world scenarios, there is a demand to recognize diverse objects. The state-of-the-art 6D object pose estimation methods rely on object-specific training and therefore do not generalize to unseen objects. Recent novel object pose estimation methods are solving this issue using task-specific fine-tuned CNNs for deep template matching. This adaptation for pose estimation still requires expensive data rendering and training procedures. MegaPose for example is trained on a dataset consisting of two million images showing 20,000 different objects to reach such generalization capabilities. To overcome this shortcoming we introduce ZS6D, for zero-shot novel object 6D pose estimation. Visual descriptors, extracted using pre-trained Vision Transformers (ViT), are used for matching rendered templates against query images of objects and for establishing local correspondences. These local correspondences enable deriving geometric correspondences and are used for estimating the object's 6D pose with RANSAC- based PnP. This approach showcases that the image descriptors extracted by pre-trained ViTs are well-suited to achieve a notable improvement over two state-of-the-art novel object 6D pose estimation methods, without the need for task-specific fine-tuning. Experiments are performed on LMO, YCBV, and TLESS. In comparison to MegaPose, we improve the Average Recall on all three datasets and compared to OSOP we improve on two datasets. The code is available at https://github.com/PhilippAuss/ZS6D. keywords: {Training;Computer vision;Visualization;Codes;Computational modeling;Pose estimation;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611464&isnumber=10609862

S. Banerjee, V. K. Verma, A. Mukherjee, D. Gupta, V. P. Namboodiri and P. Rai, "VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 493-500, doi: 10.1109/ICRA57147.2024.10610702.Abstract: Lifelong learning or continual learning is the problem of training an AI agent continuously while also preventing it from forgetting its previously acquired knowledge. Streaming lifelong learning is a challenging setting of lifelong learning with the goal of continuous learning in a dynamic non-stationary environment without forgetting. We introduce a novel approach to lifelong learning, which is streaming (observes each training example only once), requires a single pass over the data, can learn in a class-incremental manner, and can be evaluated on-the-fly (anytime inference). To accomplish these, we propose a novel virtual gradients based approach for continual representation learning which adapts to each new example while also generalizing well on past data to prevent catastrophic forgetting. Our approach also leverages an exponential-moving-average-based semantic memory to further enhance performance. Experiments on diverse datasets with temporally correlated observations demonstrate our method’s efficacy and superior performance over existing methods. keywords: {Training;Representation learning;Continuing education;Semantics;Task analysis;Robotics and automation;Artificial intelligence},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610702&isnumber=10609862

C. Zhao, J. Xu, R. Peng, X. Chen, K. Mei and X. Lan, "Experience Consistency Distillation Continual Reinforcement Learning for Robotic Manipulation Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 501-507, doi: 10.1109/ICRA57147.2024.10611494.Abstract: Continual reinforcement learning, which aims to help robots acquire skills without catastrophic forgetting, obviating the need to re-learn all tasks from scratch. In order to enable lifelong acquisition of skills in robots, replay-based continual reinforcement learning has emerged as a promising research direction. These techniques replay data from previous tasks to mitigate forgetting when learning new skills. However, existing replay-based methods store poor representative experience, and the experience utilization of old tasks is inefficient. To address these issues, we propose an experience consistency distillation method for robot continual reinforcement learning to improve the data efficiency of the experience. Specifically, the experience of old tasks are distilled to obtain Markov Decision Process (MDP) data with high compression ratio and information content. To ensure consistent data distributions before and after distillation, we further utilize a Fréchet Inception Distance (FID) loss as a regularization constraint. In order to improve experience utilization efficiency, the policy is then trained using both the distilled data and current task data, with policy distillation performed based on uncertainty metrics. Our method is validated in the continual reinforcement learning simulation platform and real scene with a UR5e robot arm. Experimental results indicate that our method achieves higher success and lower buffer size requirement compared to other methods. keywords: {Measurement;Uncertainty;Markov decision processes;Reinforcement learning;Manipulators;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611494&isnumber=10609862

P. Lorang, H. Horvath, T. Kietreiber, P. Zips, C. Heitzinger and M. Scheutz, "Adapting to the “Open World”: The Utility of Hybrid Hierarchical Reinforcement Learning and Symbolic Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 508-514, doi: 10.1109/ICRA57147.2024.10611594.Abstract: Open-world robotic tasks such as autonomous driving pose significant challenges to robot control due to unknown and unpredictable events that disrupt task performance. Neural network-based reinforcement learning (RL) techniques (like DQN, PPO, SAC, etc.) struggle to adapt in large domains and suffer from catastrophic forgetting. Hybrid planning and RL approaches have shown some promise in handling environmental changes but lack efficiency in accommodation speed. To address this limitation, we propose an enhanced hybrid system with a nested hierarchical action abstraction that can utilize previously acquired skills to effectively tackle unexpected novelties. We show that it can adapt faster and generalize better compared to state-of-the-art RL and hybrid approaches, significantly improving robustness when multiple environmental changes occur at the same time. keywords: {Training;Robot control;Reinforcement learning;Robustness;Planning;Task analysis;Autonomous vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611594&isnumber=10609862

G. Tziafas and H. Kasaei, "Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 515-522, doi: 10.1109/ICRA57147.2024.10611448.Abstract: Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills library and steer the LLM with carefully handcrafted prompt engineering, limiting the agent to a stationary range of addressable tasks. In this work, we introduce LRLL, an LLM-based lifelong learning agent that continuously grows the robot skill library to tackle manipulation tasks of ever-growing complexity. LRLL achieves this with four novel contributions: 1) a soft memory module that allows dynamic storage and retrieval of past experiences to serve as context, 2) a self-guided exploration policy that proposes new tasks in simulation, 3) a skill abstractor that distills recent experiences into new library skills, and 4) a lifelong learning algorithm for enabling human users to bootstrap new skills with minimal online interaction. LRLL continuously transfers knowledge from the memory to the library, building composable, general and interpretable policies, while bypassing gradient-based optimization, thus relieving the learner from catastrophic forgetting. Empirical evaluation in a simulated tabletop environment shows that LRLL outperforms end-to-end and vanilla LLM approaches in the lifelong setup while learning skills that are transferable to the real world. Project material will become available at the webpage https://gtziafas.github.io/LRLL_project/. keywords: {Limiting;Heuristic algorithms;Large language models;Memory modules;Libraries;Complexity theory;Prompt engineering},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611448&isnumber=10609862

M. Parakh, A. Fong, A. Simeonov, T. Chen, A. Gupta and P. Agrawal, "Lifelong Robot Learning with Human Assisted Language Planners," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 523-529, doi: 10.1109/ICRA57147.2024.10610225.Abstract: Large Language Models (LLMs) have been shown to act like planners that can decompose high-level instructions into a sequence of executable instructions. However, current LLM-based planners are only able to operate with a fixed set of skills. We overcome this critical limitation and present a method for using LLM-based planners to query new skills and teach robots these skills in a data and time-efficient manner for rigid object manipulation. Our system can re-use newly acquired skills for future tasks, demonstrating the potential of open world and lifelong learning. We evaluate the proposed framework on multiple tasks in simulation and the real world. Videos are available at: https://sites.google.com/mit.edu/halp-robot-learning keywords: {Large language models;Natural languages;Robot learning;Cognition;6-DOF;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610225&isnumber=10609862

S. Fang et al., "Probabilistic Spiking Neural Network for Robotic Tactile Continual Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 530-536, doi: 10.1109/ICRA57147.2024.10610553.Abstract: The sense of touch is essential for robots to perform various daily tasks. Artificial Neural Networks have shown significant promise in advancing robotic tactile learning. However, due to the changing of tactile data distribution as robots encounter new tasks, ANN-based robotic tactile learning suffers from catastrophic forgetting. To solve this problem, we introduce a novel continual learning (CL) framework called the Probabilistic Spiking Neural Network with Variational Continual Learning (PSNN-VCL). In this framework, PSNN introduces uncertainty during spike emission and can apply fast Variational Inference by optimizing the uncertainty through backpropagation, which significantly reduces the required model parameters for VCL. We establish a robotic tactile CL benchmark using publicly available datasets to evaluate our method. Experimental results demonstrated that, compared to other CL methods, PSNN-VCL not only achieves superior performance in terms of widely used CL metrics but also achieves at least a 50% reduction in model parameters on the robotic tactile CL benchmark. keywords: {Continuing education;Measurement;Uncertainty;Spiking neural networks;Learning (artificial intelligence);Benchmark testing;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610553&isnumber=10609862

W. Wan, Y. Zhu, R. Shah and Y. Zhu, "LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 537-544, doi: 10.1109/ICRA57147.2024.10611129.Abstract: We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More results and videos can be found on the project website: https://ut-austin-rpl.github.io/Lotus/. keywords: {Imitation learning;Clustering methods;Memory management;Buildings;Libraries;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611129&isnumber=10609862

H. Zheng, H. Ma, S. Zheng, S. E. Li and J. Wang, "Synthesize Efficient Safety Certificates for Learning-Based Safe Control using Magnitude Regularization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 545-551, doi: 10.1109/ICRA57147.2024.10610959.Abstract: Safety certificates based on energy functions can provide demonstrable safety for complex robotic systems. However, all recent studies on learning-based energy function synthesis only consider the feasibility of the control policy, which might cause over-conservativeness and even fail to achieve the control goal. To solve the problem of over-conservative controllers, we proposed the magnitude regularization technique to improve the controller performance of safe controllers by reducing the conservativeness inside the energy function, while keeping the promising provable safety guarantees. Specifically, we quantify the conservativeness by the magnitude of the energy function, and we reduce the conservativeness by adding a magnitude regularization term to the synthesis loss. We propose an algorithm using reinforcement learning (RL) for synthesis to unify the learning process of safe controllers and energy functions. We conducted simulation experiments on Safety Gym and real-robot experiments using small quadrotors. Simulation results show that the proposed algorithm does reduce the conservativeness of the energy function and outperforms baselines in terms of controller performance while maintaining safety. Real-robot experiments have shown that the proposed algorithm indeed reduce conservativeness on the small quadrotors. keywords: {Simulation;Process control;Artificial neural networks;Reinforcement learning;Safety;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610959&isnumber=10609862

M. A. Graule and V. Isler, "GG-LLM: Geometrically Grounding Large Language Models for Zero-shot Human Activity Forecasting in Human-Aware Task Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 568-574, doi: 10.1109/ICRA57147.2024.10611090.Abstract: A robot in a human-centric environment needs to account for the human’s intent and future motion in its task and motion planning to ensure safe and effective operation. This requires symbolic reasoning about probable future actions and the ability to tie these actions to specific locations in the physical environment. While one can train behavioral models capable of predicting human motion from past activities, this approach requires large amounts of data to achieve acceptable long-horizon predictions. More importantly, the resulting models are constrained to specific data formats and modalities. Moreover, connecting predictions from such models to the environment at hand to ensure the applicability of these predictions is an unsolved problem. We present a system that utilizes a Large Language Model (LLM) to infer a human’s next actions from a range of modalities without fine-tuning. A novel aspect of our system that is critical to robotics applications is that it links the predicted actions to specific locations in a semantic map of the environment. Our method leverages the fact that LLMs, trained on a vast corpus of text describing typical human behaviors, encode substantial world knowledge, including probable sequences of human actions and activities. We demonstrate how these localized activity predictions can be incorporated in a human-aware task planner for an assistive robot to reduce the occurrences of undesirable human-robot interactions by 29.2% on average. keywords: {Grounding;Large language models;Semantics;Human-robot interaction;Predictive models;Data models;Cognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611090&isnumber=10609862

K. Kawaharazuka, K. Okada and M. Inaba, "Adaptive Whole-body Robotic Tool-use Learning on Low-rigidity Plastic-made Humanoids Using Vision and Tactile Sensors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 583-589, doi: 10.1109/ICRA57147.2024.10610913.Abstract: Various robots have been developed so far; however, we face challenges in modeling the low-rigidity bodies of some robots. In particular, the deflection of the body changes during tool-use due to object grasping, resulting in significant shifts in the tool-tip position and the body’s center of gravity. Moreover, this deflection varies depending on the weight and length of the tool, making these models exceptionally complex. However, there is currently no control or learning method that takes all of these effects into account. In this study, we propose a method for constructing a neural network that describes the mutual relationship among joint angle, visual information, and tactile information from the feet. We aim to train this network using the actual robot data and utilize it for tool-tip control. Additionally, we employ Parametric Bias to capture changes in this mutual relationship caused by variations in the weight and length of tools, enabling us to understand the characteristics of the grasped tool from the current sensor information. We apply this approach to the whole-body tool-use on KXR, a low-rigidity plastic-made humanoid robot, to validate its effectiveness. keywords: {Learning systems;Visualization;Robot kinematics;Neural networks;Humanoid robots;Tactile sensors;Position control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610913&isnumber=10609862

K. Kawaharazuka, K. Okada and M. Inaba, "Robotic Constrained Imitation Learning for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 606-612, doi: 10.1109/ICRA57147.2024.10610059.Abstract: In this study, we present an implementation strategy for a robot that performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via imitation learning, aimed at the development of an autonomous robot for laparoscopic surgery. Robotic laparoscopic surgery presents two main challenges: (1) the need to manipulate forceps using ports established on the body surface as fulcrums, and (2) difficulty in perceiving depth information when working with a monocular camera that displays its images on a monitor. Especially, regarding issue (2), most prior research has assumed the availability of depth images or models of a target to be operated on. Therefore, in this study, we achieve more accurate imitation learning with only monocular images by extracting motion constraints from one exemplary motion of skilled operators, collecting data based on these constraints, and conducting imitation learning based on the collected data. We implemented an overall system using two Franka Emika Panda Robot Arms and validated its effectiveness. keywords: {Performance evaluation;Minimally invasive surgery;Accuracy;Imitation learning;Robot vision systems;Kinematics;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610059&isnumber=10609862

Y. -F. Tang et al., "Mobile Robot Oriented Large-Scale Indoor Dataset for Dynamic Scene Understanding," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 613-620, doi: 10.1109/ICRA57147.2024.10611489.Abstract: Most existing robotic datasets capture static scene data and thus are limited in evaluating robots’ dynamic performance. To address this, we present a mobile robot oriented large-scale indoor dataset, denoted as THUD (Tsinghua University Dynamic) robotic dataset, for training and evaluating their dynamic scene understanding algorithms. Specifically, the THUD dataset construction is first detailed, including organization, acquisition, and annotation methods. It comprises both real-world and synthetic data, collected with a real robot platform and a physical simulation platform, respectively. Our current dataset includes 13 larges-scale dynamic scenarios, 90K image frames, 20M 2D/3D bounding boxes of static and dynamic objects, camera poses, and IMU. The dataset is still continuously expanding. Then, the performance of mainstream indoor scene understanding tasks, e.g. 3D object detection, semantic segmentation, and robot relocalization, is evaluated on our THUD dataset. These experiments reveal serious challenges for some robot scene understanding tasks in dynamic scenes. By sharing this dataset, we aim to foster and iterate new mobile robot algorithms quickly for robot actual working dynamic environment, i.e. complex crowded dynamic scenes. keywords: {Training;Three-dimensional displays;Heuristic algorithms;Semantic segmentation;Robot vision systems;Organizations;Trajectory;mobile robot;RGB-D dataset;dynamic indoor scenes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611489&isnumber=10609862

K. Kedia, A. Bhardwaj, P. Dan and S. Choudhury, "InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 621-628, doi: 10.1109/ICRA57147.2024.10610681.Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human’s intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets.Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collaborative human-robot manipulation tasks and show that our conditional model improves over various marginal baselines. We also introduce new techniques to tele-operate a 7-DoF robot arm and collect a diverse range of human-robot collaborative manipulation data which we open-source. We release our code and datasets at https://portal-cornell.github.io/InteRACT/. keywords: {Training;Adaptation models;Transfer learning;Collaboration;Human-robot interaction;Predictive models;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610681&isnumber=10609862

N. Karnchanachari et al., "Towards learning-based planning: The nuPlan benchmark for real-world autonomous driving," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 629-636, doi: 10.1109/ICRA57147.2024.10610077.Abstract: Machine Learning (ML) has replaced handcrafted methods for perception and prediction in autonomous vehicles. Yet for the equally important planning task, the adoption of ML-based techniques is slow. We present nuPlan, the world’s first real-world autonomous driving dataset and benchmark. The benchmark is designed to test the ability of ML-based planners to handle diverse driving situations and to make safe and efficient decisions. We introduce a new large-scale dataset that consists of 1282 hours of diverse driving scenarios from 4 cities (Las Vegas, Boston, Pittsburgh, and Singapore) and includes high-quality auto-labeled object tracks and traffic light data. We mine and taxonomize common & rare driving scenarios which are used during evaluation to get fine-grained insights into the performance and characteristics of a planner. Beyond the dataset, we provide a simulation and evaluation framework that enables a planner’s actions to be simulated in closed-loop to account for interactions with other traffic participants. We present a detailed analysis of numerous baselines and investigate gaps between ML-based and traditional methods. Find the nuPlan dataset and code at nuplan.org. keywords: {Training;Measurement;Urban areas;Machine learning;Benchmark testing;Robot sensing systems;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610077&isnumber=10609862

A. Wang, D. Sato, Y. Corzo, S. Simkin, A. Biswas and A. Steinfeld, "TBD Pedestrian Data Collection: Towards Rich, Portable, and Large-Scale Natural Pedestrian Data," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 637-644, doi: 10.1109/ICRA57147.2024.10610335.Abstract: Social navigation and pedestrian behavior research has shifted towards machine learning-based methods and converged on the topic of modeling inter-pedestrian interactions and pedestrian-robot interactions. For this, large-scale datasets that contain rich information are needed. We describe a portable data collection system, coupled with a semi-autonomous labeling pipeline. As part of the pipeline, we designed a label correction web application that facilitates human verification of automated pedestrian tracking outcomes. Our system enables large-scale data collection in diverse environments and fast trajectory label production. Compared with existing pedestrian data collection methods, our system contains three components: a combination of top-down and ego-centric views, natural human behavior in the presence of a socially appropriate "robot", and human-verified labels grounded in the metric space. To the best of our knowledge, no prior data collection system has a combination of all three components. We further introduce our ever-expanding dataset from the ongoing data collection effort – the TBD Pedestrian Dataset and show that our collected data is larger in scale, contains richer information when compared to prior datasets with human-verified labels, and supports new research opportunities. keywords: {Pedestrians;Pipelines;Robot vision systems;Production;Data collection;Trajectory;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610335&isnumber=10609862

P. Sermanet et al., "RoboVQA: Multimodal Long-Horizon Reasoning for Robotics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 645-652, doi: 10.1109/ICRA57147.2024.10610216.Abstract: We present a scalable, bottom-up and intrinsically diverse data collection scheme that can be used for high-level reasoning with long and medium horizons and that has 2.2x higher throughput compared to traditional narrow top-down step-by-step collection. We collect realistic data by performing any user requests within the entirety of 3 office buildings and using multiple embodiments (robot, human, human with grasping tool). With this data, we show that models trained on all embodiments perform better than ones trained on the robot data only, even when evaluated solely on robot episodes. We explore the economics of collection costs and find that for a fixed budget it is beneficial to take advantage of the cheaper human collection along with robot collection. We release a large and highly diverse (29,520 unique instructions) dataset dubbed RoboVQA containing 829,502 (video, text) pairs for robotics-focused visual question answering. We also demonstrate how evaluating real robot experiments with an intervention mechanism enables performing tasks to completion, making it deployable with human oversight even if imperfect while also providing a single performance metric. We demonstrate a single video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is capable of performing a variety of grounded high-level reasoning tasks in broad realistic settings with a cognitive intervention rate 46% lower than the zeroshot state of the art visual language model (VLM) baseline and is able to guide real robots through long-horizon tasks. The performance gap with zero-shot state-of-the-art models indicates that a lot of grounded data remains to be collected for real-world deployment, emphasizing the critical need for scalable data collection approaches. Finally, we show that video VLMs significantly outperform single-image VLMs with an average error rate reduction of 19% across all VQA tasks. Thanks to video conditioning and dataset diversity, the model can be used as general video value functions (e.g. success and affordance) in situations where actions needs to be recognized rather than states, expanding capabilities and environment understanding for robots. Data and videos are available at robovqa.github.io keywords: {Measurement;Visualization;Biological system modeling;Data collection;Throughput;Cognition;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610216&isnumber=10609862

H. -S. Fang et al., "RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 653-660, doi: 10.1109/ICRA57147.2024.10611615.Abstract: A key challenge for robotic manipulation in open domains is how to acquire diverse and generalizable skills for robots. Recent progress in one-shot imitation learning and robotic foundation models have shown promise in transferring trained policies to new tasks based on demonstrations. This feature is attractive for enabling robots to acquire new skills and improve their manipulative ability. However, due to limitations in the training dataset, the current focus of the community has mainly been on simple cases, such as push or pick-place tasks, relying solely on visual guidance. In reality, there are many complex skills, some of which may even require both visual and tactile perception to solve. This paper aims to unlock the potential for an agent to generalize to hundreds of real-world skills with multi-modal perception. To achieve this, we have collected a dataset comprising over 110,000 contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, all collected in the real world. Each sequence in the dataset includes visual, force, audio, and action information. Moreover, we also provide a corresponding human demonstration video and a language description for each robot sequence. We have invested significant efforts in calibrating all the sensors and ensuring a high-quality dataset. The dataset is made publicly available on our website: rh20t.github.io. keywords: {Training;Visualization;Imitation learning;Robot vision systems;Force;Cameras;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611615&isnumber=10609862

X. Wang, X. Pei, X. Wang and T. Hou, "Lightweight Untethered Soft Robotic Fish," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 669-675, doi: 10.1109/ICRA57147.2024.10610533.Abstract: Aquatic organisms, due to soft body structure and high agility, have inspired many biomimetic robots. However, considering the issues of insulation and waterproofing, as well as the driving module of soft materials, their control systems are usually larger and heavier. Therefore, small underwater robots often tethered, i.e., it cannot integrate energy and control systems onto the body, which greatly limited in its working range and activity mode. This paper presents a small untethered bionic manta ray. The robotic fish is driven by dielectric elastomer actuators (DEA), which controls the double wing structure on both sides by the central muscle part to simulate the process of the manta ray’s lateral fins fanning to propel itself forward. And the flexible printed circuit board (FPC) constitutes the body of the fish and is also an independent energy control system. The electronic components are evenly distributed on the double-wing structure of the robotic fish to realize the integration of the energy control system. This circuit system can be powered by a small lithium battery and output a periodic voltage to drive the motion of the robotic fish. The masses of our tethered and untethered fish are 1.9g and 5.1g respectively. The swimming speed of these two types of fish can reach 42.5mm/s and 17.0 mm/s. And this design principle can be extended to the research and design of various flexible devices and soft robots. keywords: {Shape;Wires;Soft robotics;Fish;Transformers;Circuit stability;Voltage control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610533&isnumber=10609862

S. Paul, M. R. Devlin and E. W. Hawkes, "A scalable, light-controlled, individually addressable, non-metal actuator array," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 684-690, doi: 10.1109/ICRA57147.2024.10610023.Abstract: Research in the area of photo-actuation is growing rapidly, yet there are few examples of photo-actuators with practical use cases. One potential application is for the control of intelligent electromagnetic surfaces, or two-dimensional arrays that could shape and control an incident electromagnetic field in ideally any manner. A promising concept to realize such a surface leverages signal refraction via antenna edges, but requires non-metal actuation, large antenna rotations, and high antenna angular accuracy for long periods of time. Here, we present a nonmetal, light-controlled, multi-position inchworm actuator array that can rotate an antenna 88 degrees in incremental steps of less than 3.4 degrees with zero-power shape-persistence. The design is modular and rapidly manufacturable via a layered laser-cutting technique, such that the actuator can be tiled into an array to control the rotation of many antennas. We control the array with a single focused IR light that rasters across the actuators to precisely control all antenna positions. We characterize the response time, accuracy, and repeatability of a single actuator, and demonstrate the array achieving diverse antenna configurations. This work advances the precision and scalability of photothermal actuation not only for use in intelligent electromagnetic surfaces but for any application benefitting from light-controlled actuation. keywords: {Actuators;Electric potential;Accuracy;Shape;Scalability;Time factors;Electromagnetic fields},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610023&isnumber=10609862

S. Q. Liu and E. H. Adelson, "A Passively Bendable, Compliant Tactile Palm with RObotic Modular Endoskeleton Optical (ROMEO) Fingers," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 691-697, doi: 10.1109/ICRA57147.2024.10611043.Abstract: Many robotic hands currently rely on extremely dexterous robotic fingers and a thumb joint to envelop themselves around an object. Few hands focus on the palm even though human hands greatly benefit from their central fold and soft surface. As such, we develop a novel structurally compliant soft palm, which enables more surface area contact for the objects that are pressed into it. Moreover, this design, along with the development of a new low-cost, flexible illumination system, is able to incorporate a high-resolution tactile sensing system inspired by the GelSight sensors. Concurrently, we design RObotic Modular Endoskeleton Optical (ROMEO) fingers, which are underactuated two-segment soft fingers that are able to house the new illumination system, and we integrate them into these various palm configurations. The resulting robotic hand is slightly bigger than a baseball and represents one of the first soft robotic hands with actuated fingers and a passively compliant palm, all of which have high-resolution tactile sensing. This design also potentially helps researchers discover and explore more soft-rigid tactile robotic hand designs with greater capabilities in the future. keywords: {Integrated optics;Thumb;Lighting;Soft robotics;Robot sensing systems;Space exploration;Optical sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611043&isnumber=10609862

A. Keller, T. Yue, Q. Qi, A. T. Conn and J. Rossiter, "A Phase-Change Emulsion Jamming Gripper for Manipulation of Micro-Scale Textured Surfaces," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 706-712, doi: 10.1109/ICRA57147.2024.10611273.Abstract: The inherent elasticity of soft materials can be used to create robotic grippers that deform and comply to a variety of irregular shapes. To date, several soft adaptive grasping strategies have been reported, however, most of them focus on adapting to the overall shape of the structure, while the adaptive grasping of small surface asperities is overlooked. In this paper, we propose a novel method to achieve adaptive grasping on surface asperities with a smart shape-memory silicone sponge. Heating above 60°C makes the sponge soft and deformable to allow it to penetrate within surface asperities via a pressure normal to the surface. Cooling down below 60°C makes the sponge "jam" to retain its deformed shape. The interlocking force between the jammed sponge and the asperities, and the increased area of contact, allows for adaptive grasping on asperities down to 0.4 mm with an adhesive force of up to 27.7 N in a 40 × 40 mm contacting area. We introduce the design, working principle, fabrication, and optimization of a robotic gripper based on this shape-memory silicone sponge. This sponge-jamming gripper shows great potential for developing next-generation robotic grippers for the manipulation of textured and discontinuous surfaces. keywords: {Shape;Service robots;Force;Grasping;Pneumatic systems;Surface texture;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611273&isnumber=10609862

P. Yang and S. Li, "Design and Fabrication of String-driven Origami Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 713-719, doi: 10.1109/ICRA57147.2024.10610989.Abstract: Origami designs and structures have been widely used in many fields, such as morphing structures, robotics, and metamaterials. However, the design and fabrication of origami structures rely on human experiences and skills, which are both time and labor-consuming. In this paper, we present a rapid design and fabrication method for string-driven origami structures and robots. We developed an origami design software to generate desired crease patterns based on analytical models and Evolution Strategies (ES). Additionally, the software can automatically produce 3D models of origami designs. We then used a dual-material 3D printer to fabricate those wrapping-based origami structures with the required mechanical properties. We utilized Twisted String Actuators (TSAs) to fold the target 3D structures from flat plates. To demonstrate the capability of these techniques, we built and tested an origami crawling robot and an origami robotic arm using 3D-printed origami structures driven by TSAs. keywords: {Fabrication;Solid modeling;Actuators;Three-dimensional displays;Design methodology;Routing;Manipulators;origami;origami robot;self-folding origami;soft robot;twisted string actuator},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610989&isnumber=10609862

S. Wang, Y. Tong, X. Shang and Z. Zhang, "Multi-Confidence Guided Source-Free Domain Adaption Method for Point Cloud Primitive Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 737-743, doi: 10.1109/ICRA57147.2024.10611600.Abstract: Point cloud primitive segmentation aims to segment the surface point cloud into various geometric types of primitives, which plays a vital role in robot operation and industrial automation. However, differences in object structures and shapes across industrial datasets create domain shift issues, compounded by privacy concerns preventing dataset sharing. To address these challenges, we propose a novel source-free domain adaptation method for point cloud primitive segmentation, which follows the popular pseudo-label based self-training framework. Unlike previous works using single-model uncertainty to refine pseudo labels, our method leverages multi-confidence, including transformation consistency, task confidence, and geometric saliency to provide more informative guidance. Specifically, the transformation consistency is first utilized to vote pseudo-labels and task confidences. Furthermore, to filter out high-confident noises and obtain more reliable pseudo-labels, we investigate the geometric curvature properties of primitives and propose a geometric saliency guided dynamic prototype matching and label graph aggregation strategies for pseudo-label reassignment with different task confidence. For this novel task, we construct several datasets and verify the effectiveness of the proposed methods through a series of experiments. keywords: {Point cloud compression;Privacy;Automation;Uncertainty;Shape;Service robots;Noise},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611600&isnumber=10609862

N. Ma, M. Wang, Y. Han and Y. -J. Liu, "FF-LOGO: Cross-Modality Point Cloud Registration with Feature Filtering and Local to Global Optimization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 744-750, doi: 10.1109/ICRA57147.2024.10610549.Abstract: Cross-modality point cloud registration is confronted with significant challenges due to inherent differences in modalities between sensors. To deal with this problem, we propose FF-LOGO: a cross-modality point cloud registration framework with Feature Filtering and LOcal-Global Optimization. The cross-modality feature correlation filtering module extracts geometric transformation-invariant features from cross-modality point clouds and achieves point selection by feature matching. We also introduce a cross-modality optimization process, including a local adaptive key region aggregation module and a global modality consistency fusion optimization module. Experimental results demonstrate that our two-stage optimization significantly improves the registration accuracy of the feature association and selection module. Our method achieves a substantial increase in recall rate compared to the current state-of-the-art methods on the 3DCSR dataset, improving from 40.59% to 75.74%. Our code will be available at https://github.com/wangmohan17/FFLOGO. keywords: {Point cloud compression;Deep learning;Accuracy;Correlation;Codes;Filtering;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610549&isnumber=10609862

L. Fu, R. Ishikawa, Y. Sato and T. Oishi, "CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 751-757, doi: 10.1109/ICRA57147.2024.10611073.Abstract: The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer. CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research provides a promising solution for applying Transformer-based architectures in articulated object analysis. keywords: {Point cloud compression;Ovens;Dynamics;Estimation;Computer architecture;Transformers;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611073&isnumber=10609862

H. Yao, N. Hao, C. Xie and F. He, "EdgePoint: Efficient Point Detection and Compact Description via Distillation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 766-772, doi: 10.1109/ICRA57147.2024.10611607.Abstract: Efficient interest point detection and description in images play a crucial role in many tasks such as multi-robot SLAM and collaborative localization. To facilitate fast detection and generate compact descriptions on edge devices, we introduce EdgePoint, a lightweight neural network. We design a new detection loss UnfoldSoftmax to improve inference speed. Futhermore, we propose Ortho-Alignment loss combined with LocalPCA compression to learn compact 32-dimensional descriptors. To enable efficient storage or communication, we also quantize the generated descriptors into integral values. We perform EdgePoint on various datasets, and show that it surpasses SuperPoint in performance while utilizing only 1% of the parameters and achieving up to more than 10 times faster inference speed. By applying descriptor quantization, the requirements for storage and communication can be reduced by up to 97% without performance decreasing. keywords: {Performance evaluation;Location awareness;Quantization (signal);Simultaneous localization and mapping;Interest point detection;Image edge detection;Neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611607&isnumber=10609862

G. Chen, M. Wang, Y. Yang, L. Yuan and Y. Yue, "Fast and Robust Point Cloud Registration with Tree-based Transformer," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 773-780, doi: 10.1109/ICRA57147.2024.10610004.Abstract: Point cloud registration is essential in computer vision and robotics. Recently, transformer-based methods have achieved advanced point cloud registration performance. However, the standard attention mechanism utilized in these methods considers many low-relevance points, and it has difficulty focusing its attention weights on sparse and meaningful points, leading to limited local structure modeling capabilities and quadratic computational complexity. To address these limitations, we present the Tree-based Transformer (TrT), which is able to extract abundant local and global features with linear computational complexity. Specifically, the TrT builds coarse-to-dense feature trees, and a novel Tree-based Attention (TrA) is proposed to guide the progressive convergence of the attended regions toward meaningful points and to structurize point clouds following tree structures. In each layer, the top ${\mathcal{S}}$ key points with the highest attention scores are selected, such that in the next layer, attention is evaluated only within the specified high-relevance regions, corresponding to the child points of these selected ${\mathcal{S}}$ points. Additionally, coarse features containing high-level semantic information are incorporated into the child points to guide the feature extraction process, facilitating local structure modeling and multiscale information integration. Consequently, TrA enables the model to focus on critical local structures and extract rich local information with linear computational complexity. Experiments demonstrate that our method achieves state-of-the-art performance on 3DMatch and KITTI benchmarks. The code for our method is publicly available at https://github.com/CGuangyan-BIT/TrT. keywords: {Point cloud compression;Computational modeling;Semantics;Focusing;Benchmark testing;Feature extraction;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610004&isnumber=10609862

Y. Shi et al., "Uncertainty-driven Exploration Strategies for Online Grasp Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 781-787, doi: 10.1109/ICRA57147.2024.10610056.Abstract: Existing grasp prediction approaches are mostly based on offline learning, while, ignoring the exploratory grasp learning during online adaptation to new picking scenarios, i.e., objects that are unseen or out-of-domain (OOD), camera and bin settings, etc. In this paper, we present an uncertainty-based approach for online learning of grasp predictions for robotic bin picking. Specifically, the online learning algorithm with an effective exploration strategy can significantly improve its adaptation performance to unseen environment settings. To this end, we first propose to formulate online grasp learning as an RL problem that will allow us to adapt both grasp reward prediction and grasp poses. We propose various uncertainty estimation schemes based on Bayesian uncertainty quantification and distributional ensembles. We carry out evaluations on real-world bin picking scenes of varying difficulty. The objects in the bin have various challenging physical and perceptual characteristics that can be characterized by semi- or total transparency, and irregular or curved surfaces. The results of our experiments demonstrate a notable improvement of grasp performance in comparison to conventional online learning methods which incorporate only naive exploration strategies. Video: https://youtu.be/fPKOrjC2QrU keywords: {Training;Visualization;Uncertainty;Robot vision systems;Noise;Estimation;Propioception},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610056&isnumber=10609862

H. Le, P. Schillinger, M. Gabriel, A. Qualmann and N. A. Vien, "Pseudo Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 788-794, doi: 10.1109/ICRA57147.2024.10611348.Abstract: The prevailing grasp prediction methods predominantly rely on offline learning, overlooking the dynamic grasp learning that occurs during real-time adaptation to novel picking scenarios. These scenarios may involve previously unseen objects, variations in camera perspectives, and bin configurations, among other factors. In this paper, we introduce a novel approach, SSL-ConvSAC, that combines semi-supervised learning and reinforcement learning for online grasp learning. By treating pixels with reward feedback as labeled data and others as unlabeled, it efficiently exploits unlabeled data to enhance learning. In addition, we address the imbalance between labeled and unlabeled data by proposing a contextual curriculum-based method. We ablate the proposed approach on real-world evaluation data and demonstrate promise for improving online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika robot arm with a suction gripper. Video: https://youtu.be/OAro5pg8I9U keywords: {Reinforcement learning;Prediction methods;Grasping;Semisupervised learning;Manipulators;Cameras;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611348&isnumber=10609862

X. Han, S. Wang, X. Huang and Z. Kan, "PoseFusion: Multi-Scale Keypoint Correspondence for Monocular Camera-to-Robot Pose Estimation in Robotic Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 795-801, doi: 10.1109/ICRA57147.2024.10610844.Abstract: Visual-based robot pose estimation is a fundamental challenge, involving the determination of the camera’s pose with respect to a robot. Conventional methods for camera-to-robot pose calibration rely on fiducial markers to establish keypoint correspondences. However, these approaches exhibit significant variability in accuracy and robustness, particularly in 2D keypoint detection. In this work, we present an end-to-end pose estimation approach that achieves camera-to-robot calibration using monocular images and keypoint information. Our method employs a two-level nested U-shaped architecture, featuring a bottom-level residual U-block to extract richer contextual information from diverse receptive fields to enhance keypoint refinement. By incorporating the perspective-n-point (PnP) algorithm and leveraging 3D robot joint keypoints, we establish correspondence of 3D coordinate points between the robot’s coordinate system and the camera’s coordinate system, facilitating accurate pose estimation. Experimental evaluations encompass real-world and synthetic datasets, demonstrating competitive results across three distinct robot manipulators. keywords: {Three-dimensional displays;Accuracy;Robot kinematics;Pose estimation;Manipulators;Feature extraction;Fiducial markers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610844&isnumber=10609862

M. W. Lanighan and O. Youngquist, "Online Fault Detection in Manipulation Tasks via Generative Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 802-808, doi: 10.1109/ICRA57147.2024.10611418.Abstract: This paper introduces a method, Generative Adversarial Networks for Detecting Erroneous Results (GANDER), leveraging Generative Adversarial Networks to provide online error detection in manipulation tasks for autonomous robot systems. GANDER relies on mapping input images of a trained task to a learned manifold that contains only positive task executions and outcomes. When reconstructed through this manifold, the input images from successful task executions will remain largely unchanged, while the images from a failed task will change significantly. Using this insight, GANDER enables inspection and task outcome verification capabilities using a large number of positive examples but only a small set of negative examples, thus increasing the applicability of autonomous robot systems. We detail the design of GANDER and provide results of a proof-of-concept system, establishing its efficacy in an autonomous inspection, maintenance, and repair task. GANDER produces favorable results compared to baseline approaches and is capable of correctly identifying off-nominal behavior with 91.65% accuracy in our test task. Ablation studies were also performed to quantify the amount of data ultimately needed for this approach to succeed. keywords: {Training;Manifolds;Accuracy;Fault detection;Transforms;Inspection;Generative adversarial networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611418&isnumber=10609862

X. Zhu, D. K. Jha, D. Romeres, L. Sun, M. Tomizuka and A. Cherian, "Multi-level Reasoning for Robotic Assembly: From Sequence Inference to Contact Selection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 816-823, doi: 10.1109/ICRA57147.2024.10611259.Abstract: Automating the assembly of objects from their parts is a complex problem with innumerable applications in manufacturing, maintenance, and recycling. Unlike existing research, which is limited to target segmentation, pose regression, or using fixed target blueprints, our work presents a holistic multi-level framework for part assembly planning consisting of part assembly sequence inference, part motion planning, and robot contact optimization. We present the Part Assembly Sequence Transformer (PAST) – a sequence-to-sequence neural network – to infer assembly sequences recursively from a target blueprint. We then use a motion planner and optimization to generate part movements and contacts. To train PAST, we introduce D4PAS: a large-scale Dataset for Part Assembly Sequences consisting of physically valid sequences for industrial objects. Experimental results show that our approach generalizes better than prior methods while needing significantly less computational time for inference. Further details on our experiments and results are available in the video. keywords: {Robotic assembly;Visualization;Neural networks;Transformers;Robot sensing systems;Planning;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611259&isnumber=10609862

M. Guo, Z. Liu, S. Tian, Z. Xie, J. Wu and C. K. Liu, "Learning to Design 3D Printable Adaptations on Everyday Objects for Robot Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 824-830, doi: 10.1109/ICRA57147.2024.10610268.Abstract: Advancements in robot learning for object manipulation have shown promising results, yet certain everyday objects remain challenging for robots to effectively interact with. This discrepancy arises from the fact that human-designed objects are optimized for human use rather than robot manipulation. To address this gap, we propose a framework to automatically design 3D printable adaptations that can be attached to hard-to-use objects, thus improving "robot ergonomics". Our learning-based framework formulates the adaptation design and control as a dual Markov decision process and is able to improve robot-object interactions for various robot end effectors and objects. We further validate our designs in the real world with a Franka Panda robot. Please see the supplementary video and https://object-adaptation.github.io for additional visualizations. keywords: {Visualization;Three-dimensional displays;Ergonomics;Markov decision processes;Affordances;Collaboration;Robot learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610268&isnumber=10609862

C. Ku, C. Winge, R. Diaz, W. Yuan and K. Desingh, "Evaluating Robustness of Visual Representations for Object Assembly Task Requiring Spatio-Geometrical Reasoning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 831-837, doi: 10.1109/ICRA57147.2024.10610774.Abstract: This paper primarily focuses on evaluating and benchmarking the robustness of visual representations in the context of object assembly tasks. Specifically, it investigates the alignment and insertion of objects with geometrical extrusions, commonly referred to as a peg-in-hole task. The accuracy required to detect and orient the peg and the hole geometry in SE(3) space for successful assembly poses significant challenges. Addressing this, we employ a general framework in visuomotor policy learning that utilizes visual pretraining models as vision encoders. Our study investigates the robustness of this framework when applied to a dual-arm manipulation setup, specifically to the grasp variations. Our quantitative analysis shows that existing pretrained models fail to capture the essential visual features necessary for this task: a visual encoder trained from scratch consistently outperforms the frozen pre-trained models. Moreover, we discuss rotation representations and associated loss functions that substantially improve policy learning. We present a novel task scenario designed to evaluate the progress in visuomotor policy learning, with a specific focus on improving the robustness of intricate assembly tasks that require both geometrical and spatial reasoning. Videos, additional experiments, dataset, and code are available at https://sites.google.com/view/geometric-peg-in-hole. keywords: {Visualization;Analytical models;Statistical analysis;Imitation learning;Robustness;Cognition;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610774&isnumber=10609862

K. Bourahmoune, K. Ishac and M. Carmichael, "Towards Robot to Human Skill Coaching: A ML-powered IoT and HRI Platform for Martial Arts Training," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 853-859, doi: 10.1109/ICRA57147.2024.10610291.Abstract: Advances in human sensing and machine learning are paving the way for new applications of robotics in sports and fitness, making skill coaching smarter, easier and more accessible. Physical and social human robot interaction in particular has received special attention as a feedback mechanism for human performance augmentation. A core challenge in deploying robots that interact physically with humans in dynamic environments such as sports, relates to modeling human skills and designing appropriate interaction schemes. We present the first ML-based HRI platform for physical robot to human skill coaching in real-time in Martial Arts which can be extended to various sports. Our system comprises of the Sawyer robot, our specially developed IoT katana and a skill-training program for the Martial Art of Iaido. We built and deployed in real-time a ML-based Iaido strike recognition model trained on expert and beginner data, and achieved accuracies ranging between 94.8% and 99.97%. We assessed the system’s effectiveness in coaching skills through robot interaction in a sparring experiment and a survey involving 12 participants practicing key Iaido techniques with guided training from Sawyer. Our results demonstrated improvement in all participants’ Iaido strike skill after training with Sawyer, and they responded positively to robot-assisted skill coaching. keywords: {Training;Surveys;Art;Human-robot interaction;Machine learning;Streaming media;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610291&isnumber=10609862

C. Li, X. Wu, T. Teng, S. Calinon and F. Chen, "Towards Robo-Coach: Robot Interactive Stiffness/Position Adaptation for Human Strength and Conditioning Training," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 860-866, doi: 10.1109/ICRA57147.2024.10611028.Abstract: Traditional strength and conditioning training relies on the utilization of free weights, such as weighted implements, to elicit external stimuli. However, this approach poses a significant challenge when attempting to modify or adjust the loads within a single training set. This paper introduces an innovative method for achieving adjustable loads during resistance training by leveraging physical Human-Robot Interaction (pHRI). The primary objective is to regulate targeted muscle activation through the use of Robo-Coach (robotic coach system). We first utilize a Task-Parameterized Gaussian Mixture Model (TP-GMM) to learn the motion of coach demonstration, which can be generalized for the trainees. The 3D path extracted from the generated trajectory is then projected onto a 2D plane with respect to the direction of the load. Furthermore, we propose a hybrid stiffness/position generator for online task execution. This generator determines the desired positions in the 2D plane according to the contact point displacements in the stimuli direction and, simultaneously, sets the desired stiffness based on the muscle activation feedback. Finally, the Robo-Coach is implemented with a variable impedance controller to achieve load-adjustable resistance training with the trainee. The biceps curl exercises were conducted and the results showed favorable performance, indicating the effectiveness of this approach. keywords: {Training;Three-dimensional displays;Human-robot interaction;Muscles;Generators;Trajectory;Impedance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611028&isnumber=10609862

S. Kotsovolis and Y. Demiris, "Model Predictive Control with Graph Dynamics for Garment Opening Insertion during Robot-Assisted Dressing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 883-890, doi: 10.1109/ICRA57147.2024.10611478.Abstract: Robots have a great potential to help people with movement limitations in activities of daily living, such as dressing. A common problem in almost all dressing tasks is the insertion of a garment’s opening around a part of the human body. The rich contact environment and the deformations of the garment make the task a challenging problem for robots. In this paper, we propose a bi-manual control method for garment opening insertion during robot-assisted dressing. Specifically, we propose a model predictive controller that uses an Attention-based Relational Graph Convolutional Network (ARGCN) for modeling the dynamics of the opening in the presence of the body. We train the model entirely in simulation and validate our method in four real-world dressing scenarios of a medical training manikin. We show that our method generalizes well in the real-world opening insertion tasks achieving an overall success rate of 97.5%, even though the dynamics and the shapes vastly differ from the simulation setup. keywords: {Training;Shape;Graph convolutional networks;Deformation;Clothing;Predictive models;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611478&isnumber=10609862

Q. Wang, Z. Wang, M. G. Carmichael, D. Liu and C. -T. Lin, "Comparison of Rating Scale and Pairwise Comparison Methods for Measuring Human Co-worker Subjective Impression of Robot during Physical Human-Robot Collaboration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 907-913, doi: 10.1109/ICRA57147.2024.10611050.Abstract: The Rating Scale method has been long deemed the standard for measuring subjective perceptions. However, in the field of physical human-robot collaboration (pHRC), its aptness should be put under scrutiny due to inherent challenges such as response bias, between-subject variations, and the granularity nature.Individual variances can introduce significant bias in the rating scale results. A high granularity in the scale could overwhelm participants, leading to unclear and biased responses, while a low granularity may gloss over the fine nuances of human feelings. Additionally, there’s a notable risk of receiving careless responses, which compromise data reliability. Recognizing these challenges, this paper proposes the application of Pairwise Comparison (PC) in pHRC — an alternative survey technique that emphasizes direct comparisons between items on the defined criteria. By using the NASA Task Load Index (NASA-TLX) as a template, RS and PC questionnaires are designed and used in a series of pHRC experiments. Our preliminary findings suggest that PC is more precise and robust than the rating scale method. Compared to RS, PC fosters authentic participant interests in the experiment by intuitive question design and reducing the experimental duration. Besides, the accuracy and reliability of PC are also found to be consistent regardless of the variations in our experimental procedure design. keywords: {Surveys;Accuracy;NASA;Collaboration;Reliability engineering;Particle measurements;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611050&isnumber=10609862

D. Jung, S. Park and J. Choi, "A transtibial prosthesis using a parallel spring mechanism," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 914-920, doi: 10.1109/ICRA57147.2024.10610725.Abstract: Prosthetic legs have been used to restore function in the lower limbs lost due to amputation. Early designs including prosthetic legs with a passive joint or without any joint as well as the Energy Storing and Releasing (ESR) feet have shown deficiency in push-off torque, which results in asymmetric gait pattern, slower walking speed, and higher cost of transportation. Although powered prosthetic legs address the aforementioned problems, they suffer from lower energy efficiency, higher volume and weight. In this paper, a powered transtibial prosthesis using a Parallel Elastic Actuator (PEA) is proposed in order to generate the joint torque needed for walking with a lower-powered actuator for lighter and more compact design. A non-linear spring mechanism is proposed to generate the spring torque as needed. The implemented prosthetic leg is evaluated with three intact subjects. The experimental results shows that smaller torque is required for the motor with the spring mechanism. Therefore, less electrical power is consumed when the spring mechanism is used, which implies a lower-powered actuator is sufficient to generate the joint torque needed for walking. keywords: {Legged locomotion;Actuators;Torque;Pulleys;Transportation;Mechanical cables;Motors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610725&isnumber=10609862

B. Lan, M. Abayazid, N. Verdonschot, S. Stramigioli and K. Niu, "Deep Learning based acoustic measurement approach for robotic applications on orthopedics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 921-927, doi: 10.1109/ICRA57147.2024.10611713.Abstract: In Total Knee Replacement Arthroplasty (TKA), surgical robotics can provide image-guided navigation to fit implants with high precision. Its tracking approach highly relies on inserting bone pins into the bones tracked by the optical tracking system. This is normally done by invasive, radiative manners (implantable markers and CT scans), which introduce unnecessary trauma and prolong the preparation time for patients. To tackle this issue, ultrasound-based bone tracking could offer an alternative. In this study, we proposed a novel deep-learning structure to improve the accuracy of bone tracking by an A-mode ultrasound (US). We first obtained a set of ultrasound dataset from the cadaver experiment, where the ground truth locations of bones were calculated using bone pins. These data were used to train the proposed CasAtt-UNet to predict bone location automatically and robustly. The ground truth bone locations and those locations of US were recorded simultaneously. Therefore, we could label bone peaks in the raw US signals. As a result, our method achieved sub-millimeter precision across all eight bone areas with the only exception of one channel in the ankle. This method enables the robust measurement of lower extremity bone positions from 1D raw ultrasound signals. It shows great potential to apply A-mode ultrasound in orthopedic surgery from safe, convenient, and efficient perspectives. keywords: {Ultrasonic imaging;Accuracy;Ultrasonic variables measurement;Orthopedic surgery;Position measurement;Bones;Acoustic measurements},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611713&isnumber=10609862

A. J. Lee, C. A. Laubscher, T. K. Best and R. D. Gregg, "Towards a Unified Approach for Continuously-Variable Impedance Control of Powered Prosthetic Legs over Walking Speeds and Inclines," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 944-950, doi: 10.1109/ICRA57147.2024.10610071.Abstract: Research in powered prosthesis control has explored the use of impedance-based control algorithms due to their biomimetic capabilities and intuitive structure. Modern impedance controllers feature parameters that smoothly vary over gait phase and task according to a data-driven model. However, these recent efforts only use continuous impedance control during stance and instead utilize discrete transition logic to switch to kinematic control during swing, necessitating two separate models for the different parts of the stride. In contrast, this paper presents a controller that uses smooth impedance parameter trajectories throughout the gait, unifying the stance and swing periods under a single, continuous model. Furthermore, this paper proposes a basis model to represent inter-task relationships in the impedance parameters—a strategy that has previously been shown to improve model accuracy over classic linear interpolation methods. In the proposed controller, a weighted sum of Fourier series is used to model the impedance parameters of each joint as continuous functions of gait cycle progression and task. Fourier series coefficients are determined via convex optimization such that the controller best reproduces the joint torques and kinematics in a reference able-bodied dataset. Experiments with a powered knee-ankle prosthesis show that this simpler, unified model produces competitive results when compared to a more complex hybrid impedance-kinematic model over varying walking speeds and inclines. keywords: {Legged locomotion;Biological system modeling;Kinematics;Control systems;Trajectory;Impedance;Fourier series},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610071&isnumber=10609862

O. Dubois, A. Roby-Brami, R. Parry and N. Jarrassé, "Short term after-effects of small force fields applied by an upper-limb exoskeleton on inter-joint coordination," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 959-965, doi: 10.1109/ICRA57147.2024.10610645.Abstract: Exoskeleton technologies have numerous potential applications, ranging from improving human motor skills to aiding individuals in their daily activities. While exoskeletons are increasingly viewed, for example, as promising tools in industrial ergonomics, the effect of using them on human motor control, particularly on inter-joint coordination, remains relatively uncharted. This paper investigates the effects of generic low-amplitude force fields applied by an exoskeleton on motor strategies in asymptomatic users. The force fields mimic common perturbations encountered in exoskeletons, such as residual friction, over/under-tuned assistance, or structural elasticity. Fifty-five participants performed reaching tasks while connected to an arm exoskeleton, experiencing one of five tested force fields. Their movements before and after exposure to the exoskeleton force field were compared. The study focuses both on spatial and temporal changes in coordination using specific metrics. The results reveal that even brief exposure to a low- amplitude force field, or to uncompensated residual friction and dynamic forces, applied at the joint level can alter the interjoint coordination, while task performance remains unaffected. The tested force fields induced varying degrees of changes in joint contributions and synchronization. This study highlights the importance of monitoring coordination changes to fully understand the impact of exoskeletons on human motor control and thus enable safe and widespread adoption of those devices. keywords: {Performance evaluation;Motor drives;Friction;Perturbation methods;Exoskeletons;Force;Motors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610645&isnumber=10609862

Z. Ying, X. Zhang, S. Li, K. Nakashima, L. Shu and N. Sugita, "Real-time Dexterous Prosthesis Hand Control by Decoding Neural Information Based on EMG Decomposition," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 966-972, doi: 10.1109/ICRA57147.2024.10611356.Abstract: The vague interpretation of myoelectrical signals on the residual limb end makes restoring dexterous hand function in amputees still impossible. Understanding motor control between human motion intention and synaptic inputs to motor neurons also remains a significant challenge. The neural decoding methods of surface EMG signals remains challenging, which limit the application of robot hand in real life. Herein, we propose and substantiate a human-machine interface for motor control that introduces neural information of motor neurons in conjunction with the combination mechanism of muscle contraction. The interface firstly introduces a new concept of motor unit (MU) spike trains, which combines decoupling of the electrical activations on motor neuron axons with extraction of motion patterns from the discharge timings of the motor neuron pools. We realized a real-time implementation of the EMG decomposition algorithm on our developed prosthesis hand control system. The control scheme provides an accurate classification of intuitive hand motions, enabling the amputee to perform versatile finger movements of the prosthesis hand. The concept of motor neuron discharge timings was evaluated through experiments on one amputee participant and six able-bodied participants. The results show that the neuroprosthesis hand control scheme based on MU spike trains has the capacity of generating accurate and intuitive hand movements for amputees in a physical environment. keywords: {Motor drives;Accuracy;Neurons;Motors;Control systems;Real-time systems;Electromyography},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611356&isnumber=10609862

M. Yang, P. Grady, S. Brahmbhatt, A. B. Vasudevan, C. C. Kemp and J. Hays, "The Un-Kidnappable Robot: Acoustic Localization of Sneaking People," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 985-992, doi: 10.1109/ICRA57147.2024.10611514.Abstract: How easy is it to sneak up on a robot? We examine whether we can detect people using only the incidental sounds they produce as they move, even when they try to be quiet. To do so, we first collect a robotic dataset of high-quality 4-channel audio paired with 360° RGB data of people moving in different indoor settings. Using this dataset, we train models to predict if there is a moving person nearby and then their location using only audio. We implement our method on a robot, allowing it to track a single person moving quietly using only passive audio sensing. For demonstration videos, see our project page. keywords: {Location awareness;Tracking;Predictive models;Robot sensing systems;Acoustics;Sensors;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611514&isnumber=10609862

S. Arreghini, G. Abbate, A. Giusti and A. Paolillo, "Predicting the Intention to Interact with a Service Robot: the Role of Gaze Cues," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 993-999, doi: 10.1109/ICRA57147.2024.10610289.Abstract: For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience. We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a self-supervised way. Our main contribution is a study of the benefit of features representing the person’s gaze in this context. Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5 % to 91.2 %); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the system’s ability to adapt to new environments without external supervision. Qualitative experiments show practical applications with a waiter robot. keywords: {Accuracy;Service robots;User experience;Task analysis;Testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610289&isnumber=10609862

A. Gonçalves, P. Moreno, J. Forlizzi, L. G. Marques and A. Bernardino, "Non-Verbal Cues on Robot-Group Persuasion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1000-1006, doi: 10.1109/ICRA57147.2024.10611310.Abstract: When integrating robots into human daily life, persuasive power can be essential. However, there are often group dynamics which can complicate persuasion. This study focuses on how non-verbal cues, specifically gaze and hand gestures, affect the persuasiveness of a social robot. We have designed a protocol to include non-verbal cues in the social robot Vizzy (head and eye gaze, hand gestures) and test them in a series of experiments using the paradigm of the "Desert Survival Challenge". The goal of the robot is to persuade the participants of the game into changing their answers whilst avoiding negative feelings. It is hypothesized that the nonverbal cues will help avoid psychological reactance without diminishing compliance to the verbal requests issued by the robot. This phenomenon has been verified before for single person persuasion, but it is yet to be tested on groups. Thus, the goal of this project is to verify the effect of non-verbal cues in group persuasion by a robot and comparing it to single person persuasion. The results showed that the robot’s gestures increased compliance by the group and the gaze behaviour decreased psychological reactance. keywords: {Protocols;Social robots;Psychology;Medical services;Games;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611310&isnumber=10609862

W. Zu et al., "Language and Sketching: An LLM-driven Interactive Multimodal Multitask Robot Navigation Framework," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1019-1025, doi: 10.1109/ICRA57147.2024.10611462.Abstract: The socially-aware navigation system has evolved to adeptly avoid various obstacles while performing multiple tasks, such as point-to-point navigation, human-following, and -guiding. However, a prominent gap persists: in Human-Robot Interaction (HRI), the procedure of communicating commands to robots demands intricate mathematical formulations. Furthermore, the transition between tasks does not quite possess the intuitive control and user-centric interactivity that one would desire. In this work, we propose an LLM-driven interactive multimodal multitask robot navigation framework, termed LIM2N, to solve the above new challenge in the navigation field. We achieve this by first introducing a multimodal interaction framework where language and hand-drawn inputs can serve as navigation constraints and control objectives. Next, a reinforcement learning agent is built to handle multiple tasks with the received information. Crucially, LIM2N creates smooth cooperation among the reasoning of multimodal input, multitask planning, and adaptation and processing of the intelligent sensing modules in the complicated system. Detailed experiments are conducted in both simulation and the real world demonstrating that LIM2N has solid user needs understanding, alongside an enhanced interactive experience. keywords: {Navigation;Human-robot interaction;Reinforcement learning;Robot sensing systems;Solids;Human in the loop;Cognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611462&isnumber=10609862

S. Mu et al., "Dual-modal Tactile E-skin: Enabling Bidirectional Human-Robot Interaction via Integrated Tactile Perception and Feedback," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1026-1032, doi: 10.1109/ICRA57147.2024.10610074.Abstract: To foster an immersive and natural human-robot interaction (HRI), the implementation of tactile perception and feedback becomes imperative, effectively bridging the conventional sensory gap. In this paper, we propose a dual-modal electronic skin (e-skin) that integrates magnetic tactile sensing and vibration feedback for enhanced HRI. The dual-modal tactile e-skin offers multi-functional tactile sensing and programmable haptic feedback, underpinned by a layered structure comprised of flexible magnetic films, soft silicone elastomer, a Hall sensor and actuator array, and a microcontroller unit. The e-skin captures the magnetic field changes caused by subtle deformations through Hall sensors, employing deep learning for accurate tactile perception. Simultaneously, the actuator array generates mechanical vibrations to facilitate haptic feedback, delivering diverse mechanical stimuli. Notably, the dual-modal e-skin is capable of transmitting tactile information bidirectionally, enabling object recognition and fine-weighing operations. This bidirectional tactile interaction framework will enhance the immersion and efficiency of interactions between humans and robots. keywords: {Vibrations;Actuators;Microcontrollers;Human-robot interaction;Robot sensing systems;Skin;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610074&isnumber=10609862

G. Han, Q. Ye, A. Chen and J. Chen, "CAMInterHand: Cooperative Attention for Multi-View Interactive Hand Pose and Mesh Reconstruction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1041-1047, doi: 10.1109/ICRA57147.2024.10610469.Abstract: Interactive hand mesh reconstruction from singleview images poses a significant challenge with the severe occlusion and depth ambiguity inherent in interactive hand gestures. Recent approaches that employ probabilistic models and tokenpruned techniques have shown decent results in multi-view human body reconstruction. Nevertheless, these methods have not fully utilized multi-scale semantic information from multiview images and are not applicable in scenarios involving severe occlusion during dual-hand interactions. Simultaneously, current single-view methods independently reconstruct the left and right hands, which are ineffective in enhancing the interaction between both hands. To address these challenges, we propose CAMInterHand, a cooperative attention-based method for multi-view interactive hand pose and mesh reconstruction. Specifically, CAMInterHand extracts local pyramid features and global vertex features from multi-scale feature maps of multi-view images, enabling the exploration of rich local semantic information and facilitating effective feature alignment. Furthermore, CAMInterHand employs the cooperative attention fusion module to fuse all features from multi-view images, enhancing interactions among vertices of dual hands within global and local contexts. We conduct extensive experiments on the large-scale multi-view dataset InterHand2.6M and CAMInterHand achieves a substantial performance improvement over existing methods for multi-view and single-view interactive hand reconstruction. keywords: {Fuses;Semantics;Feature extraction;Probabilistic logic;Data mining;Robotics and automation;Image reconstruction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610469&isnumber=10609862

