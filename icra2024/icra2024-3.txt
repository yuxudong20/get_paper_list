R. Chen et al., "An LLM-driven Framework for Multiple-Vehicle Dispatching and Navigation in Smart City Landscapes," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2147-2153, doi: 10.1109/ICRA57147.2024.10610578.Abstract: In the context of smart cities, autonomous vehicles, such as unmanned delivery vehicles and taxis are gradually gaining acceptance. However, their application scenarios remain significantly fragmented. Typically, an Autonomous Multi-Functional Vehicle (AMFV) is not engaged in other scenarios when idle in a specific one. Currently, a unified system capable of coordinating and using these resources efficiently is lacking. Moreover, there is an absence of an advanced navigation algorithm for facilitating coordinated navigation among Heterogeneous Vehicles (HVs). To address these issues, we propose the LLM-driven Multi-vehicle Dispatching and navigation (LiMeda) framework. It comprises an LLM-driven scheduling module that facilitates efficient allocation considering task scenarios and vehicle information, which addresses the issue of incompatible vehicle resources across various smart city scenarios. And the other is a navigation module, founded on the Heterogeneous Agent Reinforcement Learning (HARL) framework we previously proposed, which can effectively perform cooperative navigation tasks among heterogeneous agents, assisting the cooperative task completion by HVs in a smart city. Experimental results show our method outperforms both traditional scheduling algorithms and Reinforcement Learning navigation algorithms in metric terms. Additionally, it shows remarkable scalability and generalization under varying city scales, vehicle numbers, and task numbers. keywords: {Navigation;Smart cities;Scheduling algorithms;Scalability;Robot kinematics;Reinforcement learning;Dispatching},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610578&isnumber=10609862

M. Zhang, Y. Shen, Z. Li, G. Pan and S. Lu, "A Retinex Structure-based Low-light Enhancement Model Guided by Spatial Consistency," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2154-2161, doi: 10.1109/ICRA57147.2024.10610021.Abstract: Images captured by robotics under low-light conditions are often plagued by several challenges, including diminished contrast, increased noise, loss of fine details, and unnatural color reproduction. These factors can significantly hinder the performance of computer vision tasks such as object detection and image segmentation. As a result, improving the quality of low-light images is of paramount importance for practical applications in the computer vision domain. To effectively address these challenges, we present a novel low-light image enhancement model, termed Spatial Consistency Retinex Network (SCRNet), which leverages the Retinex-based structure and is guided by the principle of spatial consistency. Specifically, our proposed model incorporates three levels of consistency: channel level, semantic level, and texture level, inspired by the principle of spatial consistency. These levels of consistency enable our model to adaptively enhance image features, ensuring more accurate and visually pleasing results. Extensive experimental evaluations on various low-light image datasets demonstrate that our proposed SCRNet outshines existing state-of-the-art methods, highlighting the potential of SCRNet as an effective solution for enhancing low-light images. keywords: {Adaptation models;Computer vision;Image segmentation;Image color analysis;Computational modeling;Semantics;Lighting},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610021&isnumber=10609862

H. -C. Lin, M. M. Marinho and K. Harada, "Autonomous Field-of-View Adjustment Using Adaptive Kinematic Constrained Control with Robot-Held Microscopic Camera Feedback," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2162-2168, doi: 10.1109/ICRA57147.2024.10610663.Abstract: Robotic systems for manipulation in millimeter scale often use a camera with high magnification for visual feedback of the target region. However, the limited field-of-view (FoV) of the microscopic camera necessitates camera motion to capture a broader workspace environment. In this work, we propose an autonomous robotic control method to constrain a robot-held camera within a designated FoV. Furthermore, we model the camera extrinsics as part of the kinematic model and use camera measurements coupled with a U-Net based tool tracking to adapt the complete robotic model during task execution. As a proof-of-concept demonstration, the proposed framework was evaluated in a bi-manual setup, where the microscopic camera was controlled to view a tool moving in a pre-defined trajectory. The proposed method allowed the camera to stay 94.1% of the time within the real FoV, compared to 54.4% without the proposed adaptive control. keywords: {Adaptation models;Visualization;Tracking;Microscopy;Robot vision systems;Kinematics;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610663&isnumber=10609862

Y. John, C. Hughes, G. Díaz-García, J. R. Marden and F. Bullo, "RoSSO: A High-Performance Python Package for Robotic Surveillance Strategy Optimization Using JAX," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2169-2175, doi: 10.1109/ICRA57147.2024.10610477.Abstract: To enable the computation of effective randomized patrol routes for single- or multi-robot teams, we present RoSSO, a Python package designed for solving Markov chain optimization problems. We exploit machine-learning techniques such as reverse-mode automatic differentiation and constraint parametrization to achieve superior efficiency compared to general-purpose nonlinear programming solvers. Additionally, we supplement a game-theoretic stochastic surveillance formulation in the literature with a novel greedy algorithm and multi-robot extension. We close with numerical results for a police district in downtown San Francisco that demonstrate RoSSO’s capabilities on our new formulations and the prior work. keywords: {Heating systems;Greedy algorithms;Measurement;Surveillance;Reinforcement learning;Programming;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610477&isnumber=10609862

C. S. Zapico, Y. Petillot and M. S. Erden, "Semi-autonomous surface-tracking tasks using omnidirectional mobile manipulators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2176-2182, doi: 10.1109/ICRA57147.2024.10611518.Abstract: Despite the potential of mobile manipulators and applications where robots require a force-controlled physical interaction with the environment, the majority of robot automation nowadays is still based on fixed manipulators for free-motion tasks (e.g. welding, pick and place, or painting). In this work, we propose a control solution for omnidirectional mobile manipulators in force-tracking tasks, interacting with unknown surface geometries and with a human teleoperator in the control loop. Keeping a teleoperator in the loop makes the system widely applicable to unstructured environments. With little effort, a human can take care of the mobile base navigation, self-collisions, and collisions with the environment, as well as selecting the area of the asset surface to process. The teleoperator interfaces with the robot platform by commanding motion in the mobile base to increase the arm’s workspace and manoeuvrability. The operator can also command the movement of the end-effector, sliding on the surface geometry to process a specific area. Alternatively, he can let the controller execute a parametric trajectory (spiral or raster) for an autonomous area coverage and meanwhile telecommand the base in order to keep the arm in configurations with good dexterity. The autonomous controller, on the other hand, takes responsibility for following the unknown contour on the manipulated surface by only taking observations from a force/torque sensor attached to the arm’s wrist, exerting a prescribed force, and handling the motion control in the base and the arm so that both can follow their respective task requests. Overall, we have developed a user-friendly control scheme, where an operator with little training and using a joystick, can guide the robot system to perform a physically interactive task on the surface of an asset. keywords: {Geometry;Wrist;Training;Symbiosis;Teleoperators;Welding;Redundancy},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611518&isnumber=10609862

Y. Ding, Y. Mao, C. Jiao and P. Ren, "Towards Optimal Lane-changing Coordination of CAVs in Multi-lane Mixed Traffic Scenarios," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2183-2189, doi: 10.1109/ICRA57147.2024.10611720.Abstract: Lane changing is a fundamental but challenging operation for moving vehicles. Connected and Automated Vehicles(CAVs) enable autonomous vehicles to cooperate with each other to accomplish the lane changing tasks, profiting from their communication ability. However, dispatching CAVs in mixed traffic remains difficult due to the stochastic behaviors and uncertain intentions of Human-Driven Vehicles(HDVs). To tackle this issue, this paper devises a coordination approach based on Conflict-Based Search(CBS) theory. Firstly, HDVs are accurately modeled as constraints to enable usage of CBS in the mixed traffic. Additionally, virtual goals are introduced to search CAVs’ priority and outlets along with path finding. Furthermore, we optimize the performance of CBS in dense traffic by defining the concept of following vehicles. Experiments show that performance is improved by utilizing new conflict prioritizing rules and a heuristic value calculation method that derived from following vehicles. Finally, we introduce grouping vehicles to extend the proposed method for solving extremely dense and large instances at a scale of more than one hundred without significant loss in efficiency. keywords: {Bellows;Transportation;Stochastic processes;Dispatching;Planning;Numerical models;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611720&isnumber=10609862

T. Do et al., "Reducing Non-IID Effects in Federated Autonomous Driving with Contrastive Divergence Loss," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2190-2196, doi: 10.1109/ICRA57147.2024.10611202.Abstract: Federated learning has been widely applied in autonomous driving since it enables training a learning model among vehicles without sharing users’ data. However, data from autonomous vehicles usually suffer from the non-independent-and-identically-distributed (non-IID) problem, which may cause negative effects on the convergence of the learning process. In this paper, we propose a new contrastive divergence loss to address the non-IID problem in autonomous driving by reducing the impact of divergence factors from transmitted models during the local learning process of each silo. We also analyze the effects of contrastive divergence in various autonomous driving scenarios, under multiple network infrastructures, and with different centralized/distributed learning schemes. Our intensive experiments on three datasets demonstrate that our proposed contrastive divergence loss significantly improves the performance over current state-of-the-art approaches. Our source code is available at https://github.com/aioz-ai/CDL. keywords: {Training;Federated learning;Source coding;Roads;Benchmark testing;Propagation losses;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611202&isnumber=10609862

Y. Tang et al., "ODD-based Query-time Scenario Mutation Framework for Autonomous Driving Scenario databases," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2197-2203, doi: 10.1109/ICRA57147.2024.10610412.Abstract: Large-scale scenario databases may contain hundreds of thousands of scenarios for the verification and validation (V&V) of autonomous vehicles (AV). Scenarios in the database are often labelled with semantic Operational Design Domain (ODD) tags (e.g., WeatherRainy, RoadTypeHighway and ActorTypeTruck) to be queried via exact tag matching. Such a scenario database design has two major limitations, i.e. combinatorial scenario generation inevitably leads to many redundant scenarios, and each ODD query matches only a small number of scenarios in the database (0.2% in our case study), rendering most of the database wealth wasted. We propose a novel scenario database design and the first ODD-based query-time scenario mutation framework to address the limitations. Our case study results show that the proposed framework has the potential to fully utilize all the database scenarios at query time while eliminating scenario redundancy in the database (in our case study, given the same ODD query, the number of final matched scenarios increased by 36 times, diversity increased by 99 times, and scenario database utilization rate increased from 0.2% to 36%). keywords: {Road transportation;Databases;Redundancy;Diversity reception;Semantics;Rendering (computer graphics);Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610412&isnumber=10609862

N. Ludlow, Y. Lyu and J. Dolan, "Hierarchical Learned Risk-Aware Planning Framework for Human Driving Modeling," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2223-2229, doi: 10.1109/ICRA57147.2024.10610354.Abstract: This paper presents a novel approach to modeling human driving behavior, designed for use in evaluating autonomous vehicle control systems in a simulation environments. Our methodology leverages a hierarchical forward-looking, risk-aware estimation framework with learned parameters to generate human-like driving trajectories, accommodating multiple driver levels determined by model parameters. This approach is grounded in multimodal trajectory prediction, using a deep neural network with LSTM-based social pooling to predict the trajectories of surrounding vehicles. These trajectories are used to compute forward-looking risk assessments along the ego vehicle’s path, guiding its navigation. Our method aims to replicate human driving behaviors by learning parameters that emulate human decision-making during driving. We ensure that our model exhibits robust generalization capabilities by conducting simulations, employing real-world driving data to validate the accuracy of our approach in modeling human behavior. The results reveal that our model effectively captures human behavior, showcasing its versatility in modeling human drivers in diverse highway scenarios. keywords: {Road transportation;Computational modeling;Predictive models;Data models;Behavioral sciences;Trajectory;Planning;Human modeling;Risk-aware;Social Pooling;Trajectory Prediction;Autonomous Vehicles;Simulation;Robotics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610354&isnumber=10609862

R. Karim, S. M. A. Shabestary and A. Rasouli, "DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for Trajectory Prediction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2230-2237, doi: 10.1109/ICRA57147.2024.10611124.Abstract: Predicting temporally consistent road users’ trajectories in a multi-agent setting is a challenging task due to the unknown characteristics of agents and their varying intentions. Besides using semantic map information and modeling interactions, it is important to build an effective mechanism capable of reasoning about behaviors at different levels of granularity.To this end, we propose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE) method. Unlike prior approaches, our approach 1) dynamically predicts agents’ goals irrespective of particular road structures, such as lanes, allowing the method to produce a more accurate estimation of destinations; 2) achieves map-compliant predictions by generating future trajectories in a coarse-to-fine fashion, where the coarser predictions at a lower frame rate serve as intermediate goals; and 3) uses an attention module designed to temporally align predicted trajectories via a masked attention operation.Using the common Argoverse benchmark dataset, we show that our method achieves state-of-the-art performance on various metrics, and further investigate the contributions of proposed modules via comprehensive ablation studies. keywords: {Measurement;Roads;Semantics;Estimation;Benchmark testing;Predictive models;Cognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611124&isnumber=10609862

Z. Huang, H. Liu, S. Shen and J. Ma, "Parallel Optimization with Hard Safety Constraints for Cooperative Planning of Connected Autonomous Vehicles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2238-2244, doi: 10.1109/ICRA57147.2024.10611158.Abstract: The development of connected autonomous vehicles (CAVs) facilitates the enhancement of traffic efficiency in complicated scenarios. Difficulties remain unsolved in developing an effective and efficient coordination strategy for CAVs. In this paper, we formulate the cooperative autonomous driving task of CAVs as an optimal control problem with safety conditions enforced as hard constraints, and propose a computationally-efficient parallel optimization framework to generate strategies for CAVs with the travel efficiency improved and the hard safety constraints satisfied. Specifically, all constraints involved are addressed appropriately with convex approximation, such that the convexity property of the reformulated optimization problem is exhibited. Then, a parallel optimization algorithm is presented to solve the reformulated optimization problem, with an embodied iterative nearest neighbor search strategy to determine the optimal passing sequence. It is noteworthy that the travel efficiency is enhanced and the computation burden is considerably alleviated with the proposed innovation development. We also examine the proposed method in CARLA simulator and perform thorough comparisons to demonstrate the effectiveness and efficiency of the proposed approach. keywords: {Technological innovation;Optimal control;Safety;Computational efficiency;Planning;Vehicle dynamics;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611158&isnumber=10609862

S. Holk, D. Marta and I. Leite, "POLITE: Preferences Combined with Highlights in Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2288-2295, doi: 10.1109/ICRA57147.2024.10610505.Abstract: Many solutions to address the challenge of robot learning have been devised, namely through exploring novel ways for humans to communicate complex goals and tasks in reinforcement learning (RL) setups. One way that experienced recent research interest directly addresses the problem by considering human feedback as preferences between pairs of trajectories (sequences of state-action pairs). However, when simply attributing a single preference to a pair of trajectories that contain many agglomerated steps, key pieces of information are lost in the process. We amplify the initial definition of preferences to account for highlights: state-action pairs of relatively high information (high/low reward) within a preferred trajectory. To include the additional information, we design novel regularization methods within a preference learning framework. To this extent, we present our method which is able to greatly reduce the necessary amount of preferences, by permitting the highlighting of favoured trajectories, in order to reduce the entropy of the credit assignment. We show the effectiveness of our work in both simulation and a user study, which analyzes the feedback given and its implications. We also use the total collected feedback to train a robot policy for socially compliant trajectories in a simulated social navigation environment. We release code and video examples at https://sites.google.com/view/rl-polite keywords: {Codes;Shape;Navigation;Design methodology;Reinforcement learning;Robot learning;Entropy},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610505&isnumber=10609862

P. Schaldenbrand, G. Parmar, J. -Y. Zhu, J. McCann and J. Oh, "CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2296-2302, doi: 10.1109/ICRA57147.2024.10610618.Abstract: Prior robot painting and drawing work, such as FRIDA, has focused on decreasing the sim-to-real gap and expanding input modalities for users, but the interaction with these systems generally exists only in the input stages. To support interactive, human-robot collaborative painting, we introduce the Collaborative FRIDA (CoFRIDA) robot painting framework, which can co-paint by modifying and engaging with content already painted by a human collaborator. To improve text-image alignment–FRIDA’s major weakness–our system uses pre-trained text-to-image models; however, pre-trained models in the context of real-world co-painting do not perform well because they (1) do not understand the constraints and abilities of the robot and (2) cannot perform co-painting without making unrealistic edits to the canvas and overwriting content. We propose a self-supervised fine-tuning procedure that can tackle both issues, allowing the use of pre-trained state-of-the-art text-image alignment models with robots to enable co-painting in the physical world. Our open-source approach, CoFRIDA, creates paintings and drawings that match the input text prompt more clearly than FRIDA, both from a blank canvas and one with human created work. More generally, our fine-tuning procedure successfully encodes the robot’s constraints and abilities into a foundation model, showcasing promising results as an effective method for reducing sim-to-real gaps. https://pschaldenbrand.github.io/cofrida/ keywords: {Adaptation models;Impedance matching;Semantics;Collaboration;Text to image;Planning;Noise measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610618&isnumber=10609862

J. Zheng, J. Zhang, K. Yang, K. Peng and R. Stiefelhagen, "MateRobot: Material Recognition in Wearable Robotics for People with Visual Impairments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2303-2309, doi: 10.1109/ICRA57147.2024.10610333.Abstract: People with Visual Impairments (PVI) typically recognize objects through haptic perception. Knowing objects and materials before touching is desired by the target users but under-explored in the field of human-centered robotics. To fill this gap, in this work, a wearable vision-based robotic system, MATERobot, is established for PVI to recognize materials and object categories beforehand. To address the computational constraints of mobile platforms, we propose a lightweight yet accurate model MATEViT to perform pixel-wise semantic segmentation, simultaneously recognizing both objects and materials. Our methods achieve respective 40.2% and 51.1% of mIoU on COCOStuff-10K and DMS datasets, surpassing the previous method with +5.7% and +7.0% gains. Moreover, on the field test with participants, our wearable system reaches a score of 28 in the NASA-Task Load Index, indicating low cognitive demands and ease of use. Our MATERobot demonstrates the feasibility of recognizing material property through visual cues and offers a promising step towards improving the functionality of wearable robots for PVI. The source code has been made publicly available at MATERobot. keywords: {Visualization;Source coding;Semantic segmentation;Visual impairment;Semantics;Wearable robots;Mobile applications},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610333&isnumber=10609862

P. Balatti et al., "Robot-Assisted Navigation for Visually Impaired through Adaptive Impedance and Path Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2310-2316, doi: 10.1109/ICRA57147.2024.10611071.Abstract: This paper presents a framework to navigate visually impaired people through unfamiliar environments by means of a mobile manipulator. The Human-Robot system consists of three key components: a mobile base, a robotic arm, and the human subject who gets guided by the robotic arm via physically coupling their hand with the cobot’s end-effector. These components, receiving a goal from the user, traverse a collision-free set of waypoints in a coordinated manner, while avoiding static and dynamic obstacles through an obstacle avoidance unit and a novel human guidance planner. With this aim, we also present a legs tracking algorithm that utilizes 2D LiDAR sensors integrated into the mobile base to monitor the human pose. Additionally, we introduce an adaptive pulling planner responsible for guiding the individual back to the intended path if they veer off course. This is achieved by establishing a target arm end-effector position and dynamically adjusting the impedance parameters in real-time through a impedance tuning unit. To validate the framework we present a set of experiments both in laboratory settings with 12 healthy blindfolded subjects and a proof-of-concept demonstration in a real-world scenario. keywords: {Target tracking;Navigation;Robot kinematics;End effectors;Real-time systems;Path planning;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611071&isnumber=10609862

T. Daab, N. Jaquier, C. Dreher, A. Meixner, F. Krebs and T. Asfour, "Incremental Learning of Full-Pose Via-Point Movement Primitives on Riemannian Manifolds," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2317-2323, doi: 10.1109/ICRA57147.2024.10610275.Abstract: Movement primitives (MPs) are compact representations of robot skills that can be learned from demonstrations and combined into complex behaviors. However, merely equipping robots with a fixed set of innate MPs is insufficient to deploy them in dynamic and unpredictable environments. Instead, the full potential of MPs remains to be attained via adaptable, large-scale MP libraries. In this paper, we propose a set of seven fundamental operations to incrementally learn, improve, and re-organize MP libraries. To showcase their applicability, we provide explicit formulations of the five spatial operations for libraries composed of Via-Point Movement Primitives (VMPs). By building on Riemannian manifold theory, our approach enables the incremental learning of all parameters of position and orientation VMPs within a library. Moreover, our approach stores a fixed number of parameters, thus complying with the essential principles of incremental learning. We evaluate our approach to incrementally learn a VMP library from sequentially-provided motion capture data. keywords: {Manifolds;Incremental learning;Buildings;Libraries;Motion capture;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610275&isnumber=10609862

E. Ballesteros, S. -Y. Lee, K. C. Carpenter and H. Harry Asada, "Supernumerary Robotic Limbs to Support Post-Fall Recoveries for Astronauts," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2324-2331, doi: 10.1109/ICRA57147.2024.10610849.Abstract: This paper proposes the utilization of Supernumerary Robotic Limbs (SuperLimbs) for augmenting astronauts during an Extra-Vehicular Activity (EVA) in a partial-gravity environment. We investigate the effectiveness of SuperLimbs in assisting astronauts to their feet following a fall. Based on preliminary observations from a pilot human study, we categorized post-fall recoveries into a sequence of statically stable poses called "waypoints". The paths between the waypoints can be modeled with a simplified kinetic motion applied about a specific point on the body. Following the characterization of post-fall recoveries, we designed a task-space impedance control with high damping and low stiffness, where the SuperLimbs provide an astronaut with assistance in post-fall recovery while keeping the human-in-the-loop scheme. In order to validate this control scheme, a full-scale wearable analog space suit was constructed and tested with a SuperLimbs prototype. Results from the experimentation found that without assistance, astronauts would impulsively exert themselves to perform a post-fall recovery, which resulted in high energy consumption and instabilities maintaining an upright posture, concurring with prior NASA studies. When the SuperLimbs provided assistance, the astronaut’s energy consumption and deviation in their tracking as they performed a post-fall recovery was reduced considerably. keywords: {Damping;Energy consumption;Tracking;NASA;Prototypes;Aerospace electronics;Impedance;Supernumerary Robotic Limbs;Human-Assistive Robotics;Human-Robot Interaction;Astronaut Ergonomics;Fall Recovery},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610849&isnumber=10609862

Y. Miyazaki and M. Higashimori, "Lissajous Curve-Based Vibrational Orbit Control of a Flexible Vibrational Actuator with a Structural Anisotropy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2332-2338, doi: 10.1109/ICRA57147.2024.10610781.Abstract: This paper proposes a novel flexible vibrational actuator with a structural anisotropy and its control method to diversify the vibrational behavior. First, the analytical model of the proposed actuator, which comprises a rectangular cross-sectional flexible beam and a rotational-type motor, is introduced. Regarding the structural anisotropy, the rotational axis of the motor is nonparallel to both principal axes of bending stiffness of the beam. Then, the vibrational phenomenon of the actuator is theoretically revealed. It is shown that using the synthetic wave input constituting two sine waves based on the resonance frequencies for the principal axes of the beam, the vibrational orbit of the tip of the beam can be controlled in the same manner as the Lissajous curve. Finally, the proposed method is experimentally validated. The Lissajous curve-based vibrational orbit control is performed using a prototype actuator. Furthermore, an application to underactuated-type locomotor is demonstrated. keywords: {Vibrations;Actuators;Anisotropic magnetoresistance;Prototypes;Resonant frequency;Motors;Orbits},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610781&isnumber=10609862

E. Sihite, A. Ramezani and M. Gharib, "Dynamic modeling of wing-assisted inclined running with a morphing multi-modal robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2339-2345, doi: 10.1109/ICRA57147.2024.10610678.Abstract: Robot designs can take many inspirations from nature, where there are many examples of highly resilient and fault-tolerant locomotion strategies to navigate complex terrains by using multi-functional appendages. For example, Chukar and Hoatzin birds can repurpose their wings for quadrupedal walking and wing-assisted incline running (WAIR) to climb steep surfaces. We took inspiration from nature and designed a morphing robot with multi-functional thruster-wheel appendages that allows the robot to change its mode of locomotion by transforming into a rover, quad-rotor, mobile inverted pendulum (MIP), and other modes. In this work, we derive a dynamic model and formulate a nonlinear model predictive controller to perform WAIR to showcase the unique capabilities of our robot. We implemented the model and controller in a numerical simulation and experiments to show their feasibility and the capabilities of our transforming multimodal robot. keywords: {Legged locomotion;Fault tolerance;Navigation;Fault tolerant systems;Predictive models;Numerical simulation;Birds},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610678&isnumber=10609862

H. Yong, F. Xu, C. Li, H. Ding and Z. Wu, "Design and Modeling of a Nested Bi-cavity-based Soft Growing Robot for Grasping in Constrained Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2346-2352, doi: 10.1109/ICRA57147.2024.10610986.Abstract: Soft growing robots with unique navigation (tip extension by eversion) hold great promise in rescue, medical, and industrial applications. Equipping them with grasping capability would enhance their usefulness in constrained environments for various applications. However, in traditional designs, the tip’s eversion naturally conflicts with grasping, and the addition of grippers at the tip would limit navigation inevitably in constrained environments. To realize grasping in such scenes without extra devices, we propose a nested bi-cavity-based growing soft robot (BIBOT). The new design consists of two coaxially nested cavities, where the inner and outer cavities extend synchronously by inversion and eversion of the film rolls. Such a bi-cavity design enables the BIBOT to navigate and grasp without relative movements between the body and environment, and avoids contact between the object and its surroundings as well. Further, a kinematics model is established and verified to precisely control its lengthening and steering by a feed mechanism. Finally, its capability in a constrained environment is demonstrated by navigating and grasping an object in a curved pipe with a variable internal diameter. keywords: {Navigation;Service robots;Grasping;Kinematics;Soft robotics;Feeds;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610986&isnumber=10609862

L. Yang, C. Zhang, R. Wang, Y. Zhang and L. Liu, "Optimized Design and Fabrication of Skeletal Muscle Actuators for Bio-syncretic Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2353-2359, doi: 10.1109/ICRA57147.2024.10611728.Abstract: In recent years, bio-syncretic robots actuated by living materials have received widespread attention. Among the common living materials, engineered skeletal muscle tissue (eSKT) has been the focus of researchers due to its high contraction force and good controllability. However, the current performance of eSKT is far from that of natural skeletal muscle tissue. In this paper, an optimized design method for eSKTs has been proposed. By combining simulation analysis with experiments, the eSKTs with multiple strips have been developed. The results show that under a specific volume (250 μL), the optimized strip structures can enhance the stability of eSKT and facilitate the penetration of nutrients and oxygen, leading to improved fusion of myoblasts and the directional arrangement of myotubes, thus improving the performance of eSKT. The eSKT with multiple strips exhibits a significant contraction force and has been successfully utilized in a bio-syncretic robot to demonstrate its actuation capability. This work may provide insights into the development of the field of bio-syncretic robots and even tissue engineering. keywords: {Fabrication;Strips;Design methodology;Force;Tissue engineering;Muscles;Nutrients},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611728&isnumber=10609862

C. Quintero-Peña, W. Thomason, Z. Kingston, A. Kyrillidis and L. E. Kavraki, "Stochastic Implicit Neural Signed Distance Functions for Safe Motion Planning under Sensing Uncertainty," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2360-2367, doi: 10.1109/ICRA57147.2024.10610773.Abstract: Motion planning under sensing uncertainty is critical for robots in unstructured environments, to guarantee safety for both the robot and any nearby humans. Most work on planning under uncertainty does not scale to high-dimensional robots such as manipulators, assumes simplified geometry of the robot or environment, or requires per-object knowledge of noise. Instead, we propose a method that directly models sensor-specific aleatoric uncertainty to find safe motions for high-dimensional systems in complex environments, without exact knowledge of environment geometry. We combine a novel implicit neural model of stochastic signed distance functions with a hierarchical optimization-based motion planner to plan low- risk motions without sacrificing path quality. Our method also explicitly bounds the risk of the path, offering trustworthiness. We empirically validate that our method produces safe motions and accurate risk bounds and is safer than baseline approaches. keywords: {Geometry;Uncertainty;Accuracy;Noise;Stochastic processes;Robot sensing systems;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610773&isnumber=10609862

A. Jamgochian, H. Buurmeijer, K. H. Wray, A. Corso and M. J. Kochenderfer, "Constrained Hierarchical Monte Carlo Belief-State Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2368-2374, doi: 10.1109/ICRA57147.2024.10611223.Abstract: Optimal plans in Constrained Partially Observable Markov Decision Processes (CPOMDPs) maximize reward objectives while satisfying hard cost constraints, generalizing safe planning under state and transition uncertainty. Unfortunately, online CPOMDP planning is extremely difficult in large or continuous problem domains. In many large robotic domains, hierarchical decomposition can simplify planning by using tools for low-level control given high-level action primitives (options). We introduce Constrained Options Belief Tree Search (COBeTS) to leverage this hierarchy and scale online search-based CPOMDP planning to large robotic problems. We show that if primitive option controllers are defined to satisfy assigned constraint budgets, then COBeTS will satisfy constraints anytime. Otherwise, COBeTS will guide the search towards a safe sequence of option primitives, and hierarchical monitoring can be used to achieve runtime safety. We demonstrate COBeTS in several safety-critical, constrained partially observable robotic domains, showing that it can plan successfully in continuous CPOMDPs while non-hierarchical baselines cannot. keywords: {Uncertainty;Monte Carlo methods;Costs;Runtime;Natural languages;Search problems;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611223&isnumber=10609862

J. Shen, R. Ren, A. Ruiz and F. Moreno-Noguer, "Estimating 3D Uncertainty Field: Quantifying Uncertainty for Neural Radiance Fields," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2375-2381, doi: 10.1109/ICRA57147.2024.10611116.Abstract: Current methods based on Neural Radiance Fields (NeRF) significantly lack the capacity to quantify uncertainty in their predictions, particularly on the unseen space including the occluded and outside scene content. This limitation hinders their extensive applications in robotics, where the reliability of model predictions has to be considered for tasks such as robotic exploration and planning in unknown environments. To address this, we propose a novel approach to estimate a 3D Uncertainty Field based on the learned incomplete scene geometry, which explicitly identifies these unseen regions. By considering the accumulated transmittance along each camera ray, our Uncertainty Field infers 2D pixel-wise uncertainty, exhibiting high values for rays directly casting towards occluded or outside the scene content. To quantify the uncertainty on the learned surface, we model a stochastic radiance field. Our experiments demonstrate that our approach is the only one that can explicitly reason about high uncertainty both on 3D unseen regions and its involved 2D rendered pixels, compared with recent methods. Furthermore, we illustrate that our designed uncertainty field is ideally suited for real-world robotics tasks, such as next-best-view selection. keywords: {Uncertainty;Three-dimensional displays;Robot vision systems;Stochastic processes;Predictive models;Neural radiance field;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611116&isnumber=10609862

M. Faroni and D. Berenson, "Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2382-2388, doi: 10.1109/ICRA57147.2024.10610323.Abstract: Robotic manipulation relies on analytical or learned models to simulate the system dynamics. These models are often inaccurate and based on offline information, so that the robot planner is unable to cope with mismatches between the expected and the actual behavior of the system (e.g., the presence of an unexpected obstacle). In these situations, the robot should use information gathered online to correct its planning strategy and adapt to the actual system response. We propose a sampling-based motion planning approach that uses an estimate of the model error and online observations to correct the planning strategy at each new replanning. Our approach adapts the cost function and the sampling bias of a kinodynamic motion planner when the outcome of the executed transitions is different from the expected one (e.g., when the robot unexpectedly collides with an obstacle) so that future trajectories will avoid unreliable motions. To infer the properties of a new transition, we introduce the notion of context-awareness, i.e., we store local environment information for each executed transition and avoid new transitions with context similar to previous unreliable ones. This is helpful for leveraging online information even if the simulated transitions are far (in the state-and-action space) from the executed ones. Simulation and experimental results show that the proposed approach increases the success rate in execution and reduces the number of replannings needed to reach the goal. keywords: {Robot motion;Adaptation models;Analytical models;Uncertainty;System dynamics;Cost function;Vectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610323&isnumber=10609862

E. Wiman, L. Widén, M. Tiger and F. Heintz, "Autonomous 3D Exploration in Large-Scale Environments with Dynamic Obstacles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2389-2395, doi: 10.1109/ICRA57147.2024.10610996.Abstract: Exploration in dynamic and uncertain real-world environments is an open problem in robotics and it constitutes a foundational capability of autonomous systems operating in most of the real-world. While 3D exploration planning has been extensively studied, the environments are assumed static or only reactive collision avoidance is carried out. We propose a novel approach to not only avoid dynamic obstacles but also include them in the plan itself, to deliberately exploit the dynamic environment in the agent’s favor. The proposed planner, Dynamic Autonomous Exploration Planner (DAEP), extends AEP to explicitly plan with respect to dynamic obstacles. Furthermore, addressing prior errors within AEP in DAEP has resulted in enhanced exploration within static environments. To thoroughly evaluate exploration planners in such settings we propose a new enhanced benchmark suite with several dynamic environments, including large-scale outdoor environments. DAEP outperforms state-of-the-art planners in dynamic and large-scale environments and is shown to be more effective at both exploration and collision avoidance. keywords: {Three-dimensional displays;Autonomous systems;Benchmark testing;Planning;Collision avoidance;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610996&isnumber=10609862

J. Liang et al., "MTG: Mapless Trajectory Generator with Traversability Coverage for Outdoor Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2396-2402, doi: 10.1109/ICRA57147.2024.10611319.Abstract: We present a novel learning-based trajectory generation algorithm for outdoor robot navigation. Our goal is to compute collision-free paths that also satisfy the environment-specific traversability constraints. Our approach is designed for global planning using limited onboard robot perception in mapless environments while ensuring comprehensive coverage of all traversable directions. Our formulation uses a Conditional Variational Autoencoder (CVAE) generative model that is enhanced with traversability constraints and an optimization formulation used for the coverage. We highlight the benefits of our approach over state-of-the-art trajectory generation approaches and demonstrate its performance in challenging and large outdoor environments, including around buildings, across intersections, along trails, and off-road terrain, using a Clearpath Husky and a Boston Dynamics Spot robot. In practice, our approach results in a 6% improvement in coverage of traversable areas and an 89% reduction in trajectory portions residing in non-traversable regions. Our video is here: https://youtu.be/3eJ2soAzXnU keywords: {Navigation;Computational modeling;Buildings;Generators;Trajectory;Planning;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611319&isnumber=10609862

D. Zheng and P. Tsiotras, "IBBT: Informed Batch Belief Trees for Motion Planning Under Uncertainty," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2403-2409, doi: 10.1109/ICRA57147.2024.10610244.Abstract: In this work, we propose the Informed Batch Belief Trees (IBBT) algorithm for motion planning under motion and sensing uncertainties. The original stochastic motion planning problem is divided into a deterministic motion planning problem and a graph search problem. First, we solve the deterministic planning problem using Rapidly-exploring Random Graph (RRG) to construct a nominal trajectory graph. Then, an informed cost-to-go heuristic for the original problem is computed based on the nominal trajectory graph. Finally, we grow a belief tree by searching the graph using the proposed heuristic. IBBT interleaves batch state sampling, nominal trajectory graph construction, heuristic computing, and searching over the graph to find belief space motion plans. IBBT is an anytime, incremental algorithm. With an increasing number of batches of samples added to the graph, the algorithm finds improved plans. IBBT is efficient by reusing results between sequential iterations. The belief tree search is an ordered search guided by an informed heuristic. We test IBBT in different planning environments. Our numerical investigation confirms that IBBT finds non-trivial motion plans and is faster compared with previous similar methods. keywords: {Uncertainty;Noise;Buildings;Search problems;Robot sensing systems;Planning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610244&isnumber=10609862

K. Ryu and N. Mehr, "Integrating Predictive Motion Uncertainties with Distributionally Robust Risk-Aware Control for Safe Robot Navigation in Crowds," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2410-2417, doi: 10.1109/ICRA57147.2024.10610404.Abstract: Ensuring safe navigation in human-populated environments is crucial for autonomous mobile robots. Although recent advances in machine learning offer promising methods to predict human trajectories in crowded areas, it remains unclear how one can safely incorporate these learned models into a control loop due to the uncertain nature of human motion, which can make predictions of these models imprecise. In this work, we address this challenge and introduce a distributionally robust chance-constrained model predictive control (DRCC-MPC) which: (i) adopts a probability of collision as a pre-specified, interpretable risk metric, and (ii) offers robustness against discrepancies between actual human trajectories and their predictions. We consider the risk of collision in the form of a chance constraint, providing an interpretable measure of robot safety. To enable real-time evaluation of chance constraints, we consider conservative approximations of chance constraints in the form of distributionally robust Conditional Value at Risk constraints. The resulting formulation offers computational efficiency as well as robustness with respect to out-of-distribution human motion. With the parallelization of a sampling-based optimization technique, our method operates in real-time, demonstrating successful and safe navigation in a number of case studies with real-world pedestrian data. keywords: {Uncertainty;Pedestrians;Navigation;Predictive models;Robustness;Real-time systems;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610404&isnumber=10609862

N. Mohammad, J. Higgins and N. Bezzo, "A GP-based Robust Motion Planning Framework for Agile Autonomous Robot Navigation and Recovery in Unknown Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2418-2424, doi: 10.1109/ICRA57147.2024.10610382.Abstract: For autonomous mobile robots, uncertainties in the environment and system model can lead to failure in the motion planning pipeline, resulting in potential collisions. In order to achieve a high level of robust autonomy, these robots should be able to proactively predict and recover from such failures. To this end, we propose a Gaussian Process (GP) based model for proactively detecting the risk of future motion planning failure. When this risk exceeds a certain threshold, a recovery behavior is triggered that leverages the same GP model to find a safe state from which the robot may continue towards the goal. The proposed approach is trained in simulation only and can generalize to real world environments on different robotic platforms. Simulations and physical experiments demonstrate that our framework is capable of both predicting planner failures and recovering the robot to states where planner success is likely, all while producing agile motion. keywords: {Uncertainty;Navigation;Pipelines;Gaussian processes;Planning;Mobile robots;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610382&isnumber=10609862

T. Makabe, K. Okada and M. Inaba, "Development of the Assembling System for Structure Transformable Humanoid with Attach-Lock-Detachable Magnetic Coupling," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2433-2439, doi: 10.1109/ICRA57147.2024.10611574.Abstract: We propose the method to adapt humanoids the ability to change the body structures that modular robots have by using Attach-Lock-Detachable Magnetic Couplings(ALDMag) to give the ability to detach and attach the robot body with an arm-type robot, and the system to manage the connection state of modularized body elements. Robots and we can use the ALDMag to attach and detach mechanical and electrical connections without actuators. Using xacro for writing the file of the robot model description of each module, we can construct a system that allows the robot to attach and detach modules during task operation. We demonstrated the effectiveness of the proposed method by achieving assembly experiments of a small robot with a life-size arm and experiments with environmental contacts by the small robot. keywords: {Couplings;Actuators;Humanoid robots;Writing;Manipulators;Information management;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611574&isnumber=10609862

D. -Y. Kim et al., "HyperLeg: Biomechanics-Inspired High-DOF Leg and Toe Mechanism for Highly Dynamic Motions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2456-2462, doi: 10.1109/ICRA57147.2024.10610527.Abstract: A human foot with high degrees of freedom (DOF) that has multi-DOF toe joints and a two-DOF ankle provides multiple benefits, such as increased stride length and walking speed, impact mitigation, and enhanced balancing. However, creating such mechanisms for legged robots has been challenging due to increased complexity, heavy weight, and vulnerability to impact. In this paper, a novel leg and toe mechanism inspired by human biomechanics, featuring a one-DOF knee joint, two-DOF ankle joint, and one-DOF toe joint, is developed. All actuators are located at the proximal part of the thigh frame to minimize the distal mass. High payload timing belts and unique linkage mechanisms are utilized in the transmission to achieve high backdrivability and high joint stiffness. Actuation torques are intentionally coupled inspired by human anatomy, enduring the high propulsive force to the ground for dynamic movements, such as jumping. The implemented leg and toe mechanisms weigh 8.16 kg, and the height from the ground to the hip center is 786 mm. The proposed mechanism has been proven to be effective through force test and distance jump experiments. keywords: {Legged locomotion;Ankle;Knee;Biomechanics;Dynamics;Force;Thigh},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610527&isnumber=10609862

C. Liang, B. Lin and H. Qian, "Design of a Towing System by Multi Autonomous Sailboats*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2463-2469, doi: 10.1109/ICRA57147.2024.10611188.Abstract: For researchers or administrators of relevant institutions who need to collect hydrological data of a certain water area, using autonomous sailboats to tow floating detection equipment is an energy-saving and convenient scheme for deploying detectors. However, due to the limited pulling force provided by a single autonomous sailboat, this scheme is not suitable for floating equipment with large masses. This paper proposes a new approach for multiple autonomous sailboats to tow floating objects. A system of linear arrangement and connection of two autonomous sailboats is considered an appropriate solution for towing heavy floating objects because of its ability to provide greater pulling force. The main part of the article introduces a new design of multi sailboat towing system that can tow floating objects to sail with or against wind. Repetitive experiments have been conducted at the test site equipped with a motion capture system to find the best strategy to control the sails and rudder, in order to increase the towing system’s pulling force and tacking success rate. Three connection modes are proposed, compared, and tested. The best one is applied to the sailboat’s towing system and improves its performance. keywords: {Industries;Force;Energy conservation;Transportation;Detectors;Fisheries;Motion capture},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611188&isnumber=10609862

Y. Kim, S. Lim, L. Hanmin, S. Kim, J. -C. Kim and D. Yun, "Non-Intrusive LiDAR Protection Module Emulating Bio-Inspired Wiping Motion for Outdoor Unmanned Vehicles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2470-2476, doi: 10.1109/ICRA57147.2024.10610438.Abstract: In this paper, we have developed a protection module for Light Detection and Ranging (LiDAR) sensors used in outdoor unmanned vehicles. Bio-inspired wiping motion was figured to have more efficient and excellent wiping performance than conventional cleaning methods for LiDAR sensors. An water wiping experiment confirmed that the finger wiping motion removed 35% more water than the translational wiping motion. Also, the theoretical analysis for the existence of an optimal rotational speed at maximum wiping performance was verified to be consistent with the experiment. The LiDAR distortion experiment results demonstrated no data distortion, showing an average error of up to 0.40% for detecting obstacles even when the acrylic cover rotates. Finally, a contamination protection experiment was conducted for water, powder, soil, and mud. As a result, although there was a change in the number of pointcloud and a decrease in the intensity of the sensor data after contamination, it was validated that the number of pointclouds and average intensity of data could be restored to at least 97% and 67% after being cleaned. keywords: {Laser radar;Fingers;Soil;Distortion;Cleaning;Water pollution;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610438&isnumber=10609862

Z. Fan and H. Zhang, "OSCaR: An Origami-Inspired Shape-Changing Robot for Ground Coverage Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2491-2497, doi: 10.1109/ICRA57147.2024.10610212.Abstract: This paper introduces a novel origami-inspired shape-changing robot OSCaR. The objective is to enhance the adaptability of vehicles engaged in ground coverage tasks, such as floor cleaning. The robot exhibits two distinct configurations: it can fold itself for agile navigation through tight spaces, and unfold to cover larger areas efficiently. The folding pattern has a deploy-to-stow ratio of 3 in the width dimension, and a kinematic model is established to simulate the deployment process for the pattern. The hinge design employs rolling contact elements to mitigate collision among the panels, particularly in regions with multiple colinear crease lines. Furthermore, the design exhibits one degree of freedom and features pivots, making it easy to actuate with motors. The system design of the prototype is also presented, including its structure, an embedded hardware system, and upper computer software. The results show that the robot has great adaptability in complex environments. keywords: {Navigation;Prototypes;Kinematics;Fasteners;Motors;Software;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610212&isnumber=10609862

A. Linard, A. Gautier, D. Duberg and J. Tumova, "Robust MITL planning under uncertain navigation times," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2498-2504, doi: 10.1109/ICRA57147.2024.10611704.Abstract: In environments like offices, the duration of a robot’s navigation between two locations may vary over time. For instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. In this work, we address the problem of routing in such environments with tasks expressed in Metric Interval Temporal Logic (MITL) – a rich robot task specification language that allows us to capture explicit time requirements. Our objective is to find a strategy that maximizes the temporal robustness of the robot’s MITL task. As the first step towards a solution, we define a Mixed-integer linear programming approach to solving the task planning problem over a Varying Weighted Transition System, where navigation durations are deterministic but vary depending on the time of day. Then, we apply this planner to optimize for MITL temporal robustness in Markov Decision Processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. Finally, we develop a receding horizon planner for Markov Decision Processes that preserves guarantees over MITL temporal robustness. We show the scalability of our planning algorithms in simulations of robotic tasks. keywords: {Navigation;Markov decision processes;Scalability;Routing;Robustness;Planning;Specification languages;Formal Methods;Planning Under Uncertainty;Temporal Robustness;Markov Decision Processes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611704&isnumber=10609862

K. Muvvala, A. M. Wells, M. Lahijanian, L. E. Kavraki and M. Y. Vardi, "Stochastic Games for Interactive Manipulation Domains," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2513-2519, doi: 10.1109/ICRA57147.2024.10611623.Abstract: As robots become more prevalent, the complexity of robot-robot, robot-human, and robot-environment interactions increases. In these interactions, a robot needs to consider not only the effects of its own actions, but also the effects of other agents’ actions and the possible interactions between agents. Previous works have considered reactive synthesis, where the human/environment is modeled as a deterministic, adversarial agent; as well as probabilistic synthesis, where the human/environment is modeled via a Markov chain. While they provide strong theoretical frameworks, there are still many aspects of human-robot interaction that cannot be fully expressed and many assumptions that must be made in each model. In this work, we propose stochastic games as a general model for human-robot interaction, which subsumes the expressivity of all previous representations. In addition, it allows us to make fewer modeling assumptions and leads to more natural and powerful models of interaction. We introduce the semantics of this abstraction and show how existing tools can be utilized to synthesize strategies to achieve complex tasks with guarantees. Further, we discuss the current computational limitations and improve the scalability by two orders of magnitude by a new way of constructing models for PRISM-games. keywords: {Computational modeling;Scalability;Semantics;Stochastic processes;Human-robot interaction;Games;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611623&isnumber=10609862

Z. Chen, Z. Zhou, L. Li and Z. Kan, "Active Inference for Reactive Temporal Logic Motion Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2520-2526, doi: 10.1109/ICRA57147.2024.10611484.Abstract: Reactive planning enables the robots to deal with dynamic events in uncertain environments. However, existing methods heavily rely on the predefined hard-coded robot behaviors, e.g, a pre-coded temporal logic formula that specifies how robot should react. Little attention has been paid for autonomous generation of reactive tasks specifications during the runtime. As a first attempt towards this goal, this work develops a real-time decision-making and motion planning framework. It allows the robot to follow a global task planned offline while taking proactive decisions and generating temporal logic specifications for local reactive tasks when encountering dynamic events. Specifically, inspired by the causal knowledge graph, a proposition graph is developed, based on which the decision module encode the environment and the task as the Boolean logic and linear temporal logic (LTL), respectively. Based on the established proposition graph and perceived environment, the agent can autonomously generate an LTL formula to realize the local temporary task. A joint sampling algorithm is then developed, in which the automaton states of local and global task are jointly considered to generate a feasible planning that satisfies both global and local tasks. Experiments demonstrate the effectiveness of the proposed decision-making and motion planning. keywords: {Runtime;Decision making;Dynamics;Automata;Knowledge graphs;Real-time systems;Inference algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611484&isnumber=10609862

J. X. Liu, A. Shah, E. Rosen, M. Jia, G. Konidaris and S. Tellex, "Skill Transfer for Temporal Task Specification," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2535-2541, doi: 10.1109/ICRA57147.2024.10611432.Abstract: Deploying robots in real-world environments, such as households and manufacturing lines, requires generalization across novel task specifications without violating safety constraints. Linear temporal logic (LTL) is a widely used task specification language with a compositional grammar that naturally induces commonalities among tasks while preserving safety guarantees. However, most prior work on reinforcement learning with LTL specifications treats every new task independently, thus requiring large amounts of training data to generalize. We propose LTL-Transfer, a zero-shot transfer algorithm that composes task-agnostic skills learned during training to safely satisfy a wide variety of novel LTL task specifications. Experiments in Minecraft-inspired domains show that after training on only 50 tasks, LTL-Transfer can solve over 90% of 100 challenging unseen tasks and 100% of 300 commonly used novel tasks without violating any safety constraints. We deployed LTL-Transfer at the task-planning level of a quadruped mobile manipulator to demonstrate its zero-shot transfer ability for fetch-and-deliver and navigation tasks. keywords: {Training;Navigation;Training data;Reinforcement learning;Manipulators;Safety;Specification languages},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611432&isnumber=10609862

G. Tanaka, Y. Takahashi and H. Iwata, "High Precision Paint Deposition Modeling Considering Variable Posture of Spray Painting Robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2542-2548, doi: 10.1109/ICRA57147.2024.10610968.Abstract: This study developed a high-precision paint deposition model that considers the position and direction of a spray-painting gun. Our angle-specific paint deposition model focused on the change in paint deposition due to the change in the painting angle; however, there was a problem with its versatility. We analyzed this problem, and the solution was achieved by separately modeling changes in the film thickness distribution using impact angle and spray distance, which were previously modeled together. For higher accuracy, a special function was proposed to convert the three-dimensional vector into two-dimensional coordinate values in the distribution function upper plane. To confirm the validity of our model, a painting test on an L-shaped surface was conducted, and the measured and predicted values were compared. The L-shaped surface is a typical shape in which the film thickness distribution changes with the angle; a complex path with varying distances and angles was employed. The results confirmed that the predicted values agreed well with the measured values in the L-shaped surface painting test, validating the developed model. keywords: {Analytical models;Accuracy;Shape;Service robots;Weapons;Predictive models;Mathematical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610968&isnumber=10609862

A. Benton, E. Solowjow and P. Akella, "Verifiable Learned Behaviors via Motion Primitive Composition: Applications to Scooping of Granular Media," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2549-2555, doi: 10.1109/ICRA57147.2024.10611279.Abstract: A robotic behavior model that can reliably generate behaviors from natural language inputs in real time would substantially expedite the adoption of industrial robots due to enhanced system flexibility. To facilitate these efforts, we construct a framework in which learned behaviors, created by a natural language abstractor, are verifiable by construction. Leveraging recent advancements in motion primitives and probabilistic verification, we construct a natural-language behavior abstractor that generates behaviors by synthesizing a directed graph over the provided motion primitives. If these component motion primitives are constructed according to the criteria we specify, the resulting behaviors are probabilistically verifiable. We demonstrate this verifiable behavior generation capacity in both simulation on an exploration task and on hardware with a robot scooping granular media. keywords: {Service robots;Natural languages;Directed graphs;Media;Probabilistic logic;Manipulators;Industrial robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611279&isnumber=10609862

D. A. Shell and J. M. O’Kane, "Knowledge acquisition plans: Generation, combination, and execution," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2556-2562, doi: 10.1109/ICRA57147.2024.10610628.Abstract: This paper contemplates the possibility of asking robots questions and having them use their ability to go out into the environment and probe it, in combination with what they already know of the world, to provide answers. We describe a method whereby a robot system efficiently answers such questions on the basis of reasoning about observations as they are made, interrelationships between multiple pieces of evidence, and what they imply.A central idea in the approach is to maintain a separation of concerns so that managing ‘what is known’ is decoupled from ‘how it is learned’. This idea is realized in a graph-based representation well-suited to algorithmic manipulation and composition, exposing synergies rife for optimization. We show how to use this representation to leverage both informational overlap between multiple simultaneous queries and availability of multiple robots working in concert to answer those queries. We demonstrate these ideas in a simple case study and present data illustrating how plan quality (in terms of cost to execute) can be improved through an optimization operation that is robot agnostic. keywords: {Costs;Knowledge acquisition;Cognition;Probes;Robots;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610628&isnumber=10609862

M. Coffey and A. Pierson, "Assessing Reputation to Improve Team Performance in Heterogeneous Multi-Robot Coverage," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2571-2577, doi: 10.1109/ICRA57147.2024.10611134.Abstract: When agents in a multi-robot team have limited knowledge about their relative performance, their teammates, or the environment, robots must observe individual performance variations and adapt accordingly. We propose robot reputation to assess the historical performance of agents and make future adaptations in a persistent coverage task. We consider a heterogeneous multi-robot team, where robots are equipped with different capabilities to serve discrete events in an environment. We utilize a heterogeneous coverage control approach to partition the space according to robot capabilities and the estimated probability density, such that the robot is responsible for serving the events in its assigned region. As the team serves events, we assign each robot a reputation, which is then used to adjust the size of a robot’s region, thus adjusting the amount of space a robot is responsible for serving. Our simulations show that using reputation to weigh the size of the Voronoi cells outperforms the case where we neglect reputation. keywords: {Costs;Aerospace electronics;Generators;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611134&isnumber=10609862

S. Chen, Y. Sun, P. Li, L. Zhou and C. -T. Lu, "Learning Decentralized Flocking Controllers with Spatio-Temporal Graph Neural Network," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2596-2602, doi: 10.1109/ICRA57147.2024.10610627.Abstract: Recently a line of research has delved into the use of graph neural networks (GNNs) for decentralized control in swarm robotics. However, it has been observed that relying solely on the states of immediate neighbors is insufficient to imitate a centralized control policy. To address this limitation, prior studies proposed incorporating L-hop delayed states into the computation. While this approach shows promise, it can lead to a lack of consensus among distant flock members and the formation of small clusters, consequently failing cohesive flocking behaviors. Instead, our approach leverages spatiotemporal GNN, named STGNN that encompasses both spatial and temporal expansions. The spatial expansion collects delayed states from distant neighbors, while the temporal expansion incorporates previous states from immediate neighbors. The broader information gathered from both expansions results in more effective and accurate predictions. We develop an expert algorithm for controlling a swarm of robots and employ imitation learning to train our decentralized STGNN model based on the expert algorithm. We simulate the proposed STGNN approach in various settings, demonstrating its decentralized capacity to emulate the global expert algorithm. Further, we implemented our approach to achieve cohesive flocking, leader following, and obstacle avoidance by a group of Crazyflie drones. The performance of STGNN underscores its potential as an effective and reliable approach for achieving cohesive flocking, leader following, and obstacle avoidance tasks. keywords: {Imitation learning;Swarm robotics;Prediction algorithms;Graph neural networks;Spatiotemporal phenomena;Reliability;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610627&isnumber=10609862

X. Wen, Y. Wang, X. Zheng, K. Wang, C. Xu and F. Gao, "Simultaneous Time Synchronization and Mutual Localization for Multi-robot System," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2603-2609, doi: 10.1109/ICRA57147.2024.10610915.Abstract: Mutual localization stands as a foundational component within various domains of multi-robot systems. Nevertheless, in relative pose estimation, time synchronization is usually underappreciated and rarely addressed, although it significantly influences estimation accuracy. In this paper, we introduce time synchronization into mutual localization to recover the time offset and relative poses between robots simultaneously. Under a constant velocity assumption in a short time, we fuse time offset estimation with our previous bearing-based mutual localization by a novel error representation. Based on the error model, we formulate a joint optimization problem and utilize semi-definite relaxation (SDR) to furnish a lossless relaxation. By solving the relaxed problem, time synchronization and relative pose estimation can be achieved when time drift between robots is limited. To enhance the application range of time offset estimation, we further propose an iterative method to recover the time offset from coarse to fine. Comparisons between the proposed method and the existing ones through extensive simulation tests present prominent benefits of time synchronization on mutual localization. Moreover, real-world experiments are conducted to show the practicality and robustness. keywords: {Location awareness;Accuracy;Fuses;Pose estimation;Robustness;Synchronization;Multi-robot systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610915&isnumber=10609862

F. Cladera, Z. Ravichandran, I. D. Miller, M. Ani Hsieh, C. J. Taylor and V. Kumar, "Enabling Large-scale Heterogeneous Collaboration with Opportunistic Communications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2610-2616, doi: 10.1109/ICRA57147.2024.10611469.Abstract: Multi-robot collaboration in large-scale environments with limited-sized teams and without external infrastructure is challenging, since the software framework required to support complex tasks must be robust to unreliable and intermittent communication links. In this work, we present MOCHA (Multi-robot Opportunistic Communication for Heterogeneous Collaboration), a framework for resilient multi-robot collaboration that enables large-scale exploration in the absence of continuous communications. MOCHA is based on a gossip communication protocol that allows robots to interact opportunistically whenever communication links are available, propagating information on a peer-to-peer basis. We demonstrate the performance of MOCHA through real-world experiments with commercial-off-the-shelf (COTS) communication hardware. We further explore the system’s scalability in simulation, evaluating the performance of our approach as the number of robots increases and communication ranges vary. Finally, we demonstrate how MOCHA can be tightly integrated with the planning stack of autonomous robots. We show a communication-aware planning algorithm for a high-altitude aerial robot executing a collaborative task while maximizing the amount of information shared with ground robots.The source code for MOCHA and the high-altitude UAV planning system is available open source 1. keywords: {Robot kinematics;Collaboration;Autonomous aerial vehicles;Planning;Peer-to-peer computing;Synchronization;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611469&isnumber=10609862

N. Karapetyan, A. B. Asghar, A. Bhaskar, G. Shi, D. Manocha and P. Tokekar, "AG-Cvg: Coverage Planning with a Mobile Recharging UGV and an Energy-Constrained UAV," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2617-2623, doi: 10.1109/ICRA57147.2024.10610339.Abstract: In this paper, we present an approach for coverage path planning for a team of an energy-constrained Unmanned Aerial Vehicle (UAV) and an Unmanned Ground Vehicle (UGV). Both the UAV and the UGV have predefined areas that they have to cover. The goal is to perform complete coverage by both robots while minimizing the coverage time. The UGV can also serve as a mobile recharging station. The UAV and UGV need to occasionally rendezvous for recharging. We propose a heuristic method to address this NP-Hard planning problem. Our approach involves initially determining coverage paths without factoring in energy constraints. Subsequently, we cluster segments of these paths and employ graph matching to assign UAV clusters to UGV clusters for efficient recharging management. We perform numerical analysis on real-world coverage applications and show that compared with a greedy approach our method reduces rendezvous overhead on average by 11.33%. We demonstrate proof-of-concept with a team of a VOXL m500 drone and a Clearpath Jackal ground vehicle, providing a complete system from the offline algorithm to the field execution. keywords: {Energy consumption;Satellites;Numerical analysis;Autonomous aerial vehicles;Land vehicles;Trajectory;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610339&isnumber=10609862

T. Hummer and S. Kriegman, "A non-cubic space-filling modular robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2624-2631, doi: 10.1109/ICRA57147.2024.10611176.Abstract: Space-filling building blocks of diverse shape permeate nature at all levels of organization, from atoms to honeycombs, and have proven useful in artificial systems, from molecular containers to clay bricks. But, despite the wide variety of space-filling polyhedra known to mathematics, only the cube has been explored in robotics. Thus, here we roboticize a non-cubic space-filling shape: the rhombic dodecahedron. This geometry offers an appealing alternative to cubes as it greatly simplifies rotational motion of one cell about the edge of another, and increases the number of neighbors each cell can communicate with and hold on to. To better understand the challenges and opportunities of these and other space-filling machines, we manufactured 48 rhombic dodecahedral cells and used them to build various superstructures. We report locomotive ability of some of the structures we built, and discuss the dis/advantages of the different designs we tested. We also introduce a strategy for genderless passive docking of cells that generalizes to any polyhedra with radially symmetrical faces. Future work will allow the cells to freely roll/rotate about one another so that they may realize the full potential of their unique shape. keywords: {Geometry;Shape;Organizations;Containers;Atoms;Robots;Faces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611176&isnumber=10609862

M. Cheng, H. Liu, D. Liu, H. Gu and X. Wang, "Optimal Containment Control of Multiple Quadrotors via Reinforcement Learning*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2632-2637, doi: 10.1109/ICRA57147.2024.10611262.Abstract: This paper explores the optimal containment control problem for nonlinear and underactuated quadrotors with multiple team leaders governed by nonlinear dynamics, employing the reinforcement learning. A cascade controller is formulated, comprising a position control component to ensure containment achievement and an attitude control component to govern rotational channel. The proposed optimal control protocols derived from historical data collected from quadrotor systems without requirement for exact knowledge of vehicle dynamics. The simulation illustrates the effectiveness of the proposed controller in managing a quadrotor team with multiple leaders. keywords: {Protocols;Simulation;Optimal control;Position control;Reinforcement learning;Trajectory;Nonlinear dynamical systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611262&isnumber=10609862

M. Lippi, M. C. Welle, A. Gasparri and D. Kragic, "Ensemble Latent Space Roadmap for Improved Robustness in Visual Action Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2638-2644, doi: 10.1109/ICRA57147.2024.10611385.Abstract: Planning in learned latent spaces helps to decrease the dimensionality of raw observations. In this work, we propose to leverage the ensemble paradigm to enhance the robustness of latent planning systems. We rely on our Latent Space Roadmap (LSR) framework, which builds a graph in a learned structured latent space to perform planning. Given multiple LSR framework instances, that differ either on their latent spaces or on the parameters for constructing the graph, we use the action information as well as the embedded nodes of the produced plans to define similarity measures. These are then utilized to select the most promising plans. We validate the performance of our Ensemble LSR (ENS-LSR) on simulated box stacking and grape harvesting tasks as well as on a real-world robotic T-shirt folding experiment. keywords: {Visualization;Stacking;Pipelines;Robustness;Planning;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611385&isnumber=10609862

Y. Kang et al., "Direct 3D model-based object tracking with event camera by motion interpolation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2645-2651, doi: 10.1109/ICRA57147.2024.10611576.Abstract: Event cameras are recent sensors that measure intensity changes in each pixel asynchronously. It is being used due to lower latency and higher temporal resolution compared to traditional frame-based camera. We propose a method of 3D model-based object tracking directly from events captured by event camera. To enable reliable and accurate tracking of objects, we use a new event representation and predict brightness increment images with motion interpolation. Results of object tracking show the new methods significantly improves tracking duration and robustness, both for perspective and fisheye cameras. Our implementation succeeds in tracking objects when the camera speed is reaching 2 m/s. keywords: {Interpolation;Solid modeling;Three-dimensional displays;Target tracking;Tracking;Robot vision systems;Brightness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611576&isnumber=10609862

A. Sengupta, K. Makki and A. Bartoli, "Using Specularities to Boost Non-Rigid Structure-from-Motion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2652-2659, doi: 10.1109/ICRA57147.2024.10610803.Abstract: Non-Rigid Structure-from-Motion (NRSfM) reconstructs the time-varying 3D shape of a deforming object from 2D point correspondences in monocular images. Despite promising use-cases such as the grasping of deformable objects and visual navigation in a non-rigid environment, NRSfM has had limited applications in robotics due to a lack of accuracy. To remedy this, we propose a new method which boosts the accuracy of NRSfM using sparse surface normals. Surface normal information is available from many sources, including structured lighting, homography decomposition of infinitesimal planes and shape priors. However, these sources are not always available. We thus propose a widely available new source of surface normals: the specularities. Our first technical contribution is a method which detects specular highlights and reconstructs the surface normals from it. It assumes that the light source is approximately localised, which is widely applicable in robotics applications such as endoscopy. Our second technical contribution is an NRSfM method which exploits a sparse surface normal set. For that, we propose a novel convex formulation and a globally optimal solution method. Experiments on photo-realistic synthetic data and real household and medical data show that the proposed method outperforms existing NRSfM methods.1 2 3 keywords: {Surface reconstruction;Accuracy;Three-dimensional displays;Shape;Navigation;Grasping;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610803&isnumber=10609862

J. Lu, F. Richter, S. Lin and M. C. Yip, "Tracking Snake-Like Robots in the Wild Using Only a Single Camera," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2660-2666, doi: 10.1109/ICRA57147.2024.10611438.Abstract: Robot navigation within complex environments requires precise state estimation and localization to ensure robust and safe operations. For ambulating mobile robots like robot snakes, traditional methods for sensing require multiple embedded sensors or markers, leading to increased complexity, cost, and increased points of failure. Alternatively, deploying an external camera in the environment is very easy to do, and marker-less state estimation of the robot from this camera’s images is an ideal solution: both simple and cost-effective. However, the challenge in this process is in tracking the robot under larger environments where the cameras may be moved around without extrinsic calibration, or maybe when in motion (e.g., a drone following the robot). The scenario itself presents a complex challenge: single-image reconstruction of robot poses under noisy observations. In this paper, we address the problem of tracking ambulatory mobile robots from a single camera. The method combines differentiable rendering with the Kalman filter. This synergy allows for simultaneous estimation of the robot’s joint angle and pose while also providing state uncertainty which could be used later on for robust control. We demonstrate the efficacy of our approach on a snake-like robot in both stationary and non-stationary (moving) cameras, validating its performance in both structured and unstructured scenarios. The results achieved show an average error of 0.05 m in localizing the robot’s base position and 6 degrees in joint state estimation. We believe this novel technique opens up possibilities for enhanced robot mobility and navigation in future exploratory and search-and-rescue missions. keywords: {Navigation;Tracking;Robot vision systems;Snake robots;Cameras;Rendering (computer graphics);Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611438&isnumber=10609862

J. Cao, J. Pang and K. Kitani, "Multi-Object Tracking by Hierarchical Visual Representations," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2667-2674, doi: 10.1109/ICRA57147.2024.10611201.Abstract: We propose a new visual hierarchical representation paradigm for multi-object tracking. It is more effective to discriminate between objects by attending to objects’ compositional visual regions and contrasting with the background contextual information instead of sticking to only the semantic visual cue such as bounding boxes. This compositional-semantic-contextual hierarchy is flexible to be integrated in different appearance-based multi-object tracking methods. We also propose an attention-based visual feature module to fuse the hierarchical visual representations. The proposed method achieves state-of-the-art accuracy and time efficiency among query-based methods on multiple multi-object tracking benchmarks. keywords: {Visualization;Target tracking;Accuracy;Fuses;Computational modeling;Semantics;Noise},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611201&isnumber=10609862

L. Saraceni, I. M. Motoi, D. Nardi and T. A. Ciarfuglia, "AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for robotics in precision agriculture," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2675-2682, doi: 10.1109/ICRA57147.2024.10610231.Abstract: The problem of multi-object tracking (MOT) consists in detecting and tracking all the objects in a video sequence while keeping a unique identifier for each object. It is a challenging and fundamental problem for robotics. In precision agriculture the challenge of achieving a satisfactory solution is amplified by extreme camera motion, sudden illumination changes, and strong occlusions. Most modern trackers rely on the appearance of objects rather than motion for association, which can be ineffective when most targets are static objects with the same appearance, as in the agricultural case. To this end, on the trail of SORT [5], we propose AgriSORT, a simple, online, real-time tracking-by-detection pipeline for precision agriculture based only on motion information that allows for accurate and fast propagation of tracks between frames. The main focuses of AgriSORT are efficiency, flexibility, minimal dependencies, and ease of deployment on robotic platforms. We test the proposed pipeline on a novel MOT benchmark specifically tailored for the agricultural context, based on video sequences taken in a table grape vineyard, particularly challenging due to strong self-similarity and density of the instances. Both the code and the dataset are available for future comparisons at: https://github.com/Lio320/AgriSORT keywords: {Precision agriculture;Target tracking;Pipelines;Video sequences;Robot vision systems;Lighting;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610231&isnumber=10609862

W. Wang et al., "Stereo-NEC: Enhancing Stereo Visual-Inertial SLAM Initialization with Normal Epipolar Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2691-2697, doi: 10.1109/ICRA57147.2024.10611458.Abstract: We propose an accurate and robust initialization approach for stereo visual-inertial SLAM systems. Unlike the current state-of-the-art method, which heavily relies on the accuracy of a pure visual SLAM system to estimate inertial variables without updating camera poses, potentially compromising accuracy and robustness, our approach offers a different solution. We realize the crucial impact of precise gyroscope bias estimation on rotation accuracy. This, in turn, affects trajectory accuracy due to the accumulation of translation errors. To address this, we first independently estimate the gyroscope bias and use it to formulate a maximum a posteriori problem for further refinement. After this refinement, we proceed to update the rotation estimation by performing IMU integration with gyroscope bias removed from gyroscope measurements. We then leverage robust and accurate rotation estimates to enhance translation estimation via 3-DoF bundle adjustment. Moreover, we introduce a novel approach for determining the success of the initialization by evaluating the residual of the normal epipolar constraint. Extensive evaluations on the EuRoC dataset illustrate that our method excels in accuracy and robustness. It outperforms ORB-SLAM3, the current leading stereo visual-inertial initialization method, in terms of absolute trajectory error and relative rotation error, while maintaining competitive computational speed. Notably, even with 5 keyframes for initialization, our method consistently surpasses the state-of-the-art approach using 10 keyframes in rotation accuracy. The open source code is available at https://github.com/ApdowJN/Stereo-NEC.git. keywords: {Bundle adjustment;Visualization;Accuracy;Simultaneous localization and mapping;Source coding;3-DOF;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611458&isnumber=10609862

A. Millane et al., "nvblox: GPU-Accelerated Incremental Signed Distance Field Mapping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2698-2705, doi: 10.1109/ICRA57147.2024.10611532.Abstract: Dense, volumetric maps are essential to enable robot navigation and interaction with the environment. To achieve low latency, dense maps are typically computed onboard the robot, often on computationally constrained hardware. Previous works leave a gap between CPU-based systems for robotic mapping which, due to computation constraints, limit map resolution or scale, and GPU-based reconstruction systems which omit features that are critical to robotic path planning, such as computation of the Euclidean Signed Distance Field (ESDF). We introduce a library, nvblox, that aims to fill this gap, by GPU-accelerating robotic volumetric mapping. Nvblox delivers a significant performance improvement over the state of the art, achieving up to a 177× speed-up in surface reconstruction, and up to a 31× improvement in distance field computation, and is available open-source 1. keywords: {Surface reconstruction;Navigation;Graphics processing units;Libraries;Path planning;Hardware;Low latency communication},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611532&isnumber=10609862

Y. Sun, L. Zheng, H. Chen and W. Zhang, "Multi-Resolution Planar Region Extraction for Uneven Terrains," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2706-2712, doi: 10.1109/ICRA57147.2024.10610269.Abstract: This paper studies the problem of extracting planar regions in uneven terrains from unordered point cloud measurements. Such a problem is critical in various robotic applications such as robotic perceptive locomotion. While existing approaches have shown promising results in effectively extracting planar regions from the environment, they often suffer from issues such as low computational efficiency or loss of resolution. To address these issues, we propose a multi-resolution planar region extraction strategy in this paper that balances the accuracy in boundaries and computational efficiency. Our method begins with a pointwise classification preprocessing module, which categorizes all sampled points according to their local geometric properties to facilitate multi-resolution segmentation. Subsequently, we arrange the categorized points using an octree, followed by an in-depth analysis of nodes to finish multi-resolution plane segmentation. The efficiency and robustness of the proposed approach are verified via synthetic and real-world experiments, demonstrating our method’s ability to generalize effectively across various uneven terrains while maintaining real-time performance, achieving frame rates exceeding 35 FPS. keywords: {Point cloud compression;Deep learning;Accuracy;Octrees;Noise;Robustness;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610269&isnumber=10609862

I. Kasahara, S. Agrawal, S. Engin, N. Chavan-Dafle, S. Song and V. Isler, "RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2713-2720, doi: 10.1109/ICRA57147.2024.10611694.Abstract: General scene reconstruction refers to the task of estimating the full 3D geometry and texture of a scene containing previously unseen objects. In many practical applications such as AR/VR, autonomous navigation, and robotics, only a single view of the scene may be available, making the scene reconstruction task challenging. In this paper, we present a method for scene reconstruction by structurally breaking the problem into two steps: rendering novel views via inpainting and 2D to 3D scene lifting. Specifically, we leverage the generalization capability of large visual language models (DALL•E 2) to inpaint the missing areas of scene color images rendered from different views. Next, we lift these inpainted images to 3D by predicting normals of the inpainted image and solving for the missing depth values. By predicting for normals instead of depth directly, our method allows for robustness to changes in depth distributions and scale. With rigorous quantitative evaluation, we show that our method outperforms multiple baselines while providing generalization to novel objects and scenes. Code and data is available at https://samsunglabs.github.io/RIC-project-page/. keywords: {Geometry;Visualization;Three-dimensional displays;Codes;Color;Rendering (computer graphics);Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611694&isnumber=10609862

A. Li, A. Hu, W. Xi, W. Yu and D. Zou, "Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2729-2736, doi: 10.1109/ICRA57147.2024.10611533.Abstract: Accurate and dense depth estimation with stereo cameras and LiDAR is an important task for automatic driving and robotic perception. While sparse hints from LiDAR points have improved cost aggregation in stereo matching, their effectiveness is limited by the low density and non-uniform distribution. To address this issue, we propose a novel stereo-LiDAR depth estimation network with Semi-Dense hint Guidance, named SDG-Depth. Our network includes a deformable propagation module for generating a semi-dense hint map and a confidence map by propagating sparse hints using a learned deformable window. These maps then guide cost aggregation in stereo matching. To reduce the triangulation error in depth recovery from disparity, especially in distant regions, we introduce a disparity-depth conversion module. Our method is both accurate and efficient. The experimental results on benchmark tests show its superior performance. Our code is available at https://github.com/SJTU-ViSYS/SDG-Depth. keywords: {Laser radar;Costs;Accuracy;Three-dimensional displays;Robot vision systems;Estimation;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611533&isnumber=10609862

S. Tourani, J. Reddy, S. Thakur, K. M. Krishna, M. Haris Khan and N. D. Reddy, "Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2737-2744, doi: 10.1109/ICRA57147.2024.10610738.Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has become available. This prompts the question of how to utilize this data for geometric reasoning of scenes. While many RGB-D registration methods rely on geometric and feature-based similarity, we take a different approach. We use cycle-consistent keypoints as salient points to enforce spatial coherence constraints during matching, improving correspondence accuracy. Additionally, we introduce a novel pose block that combines a GRU recurrent unit with transformation synchronization, blending historical and multi-view data. Our approach surpasses previous self-supervised registration methods on ScanNet and 3DMatch, even outperforming some older supervised methods. We also integrate our components into existing methods, showing their effectiveness. keywords: {Location awareness;Accuracy;Soft sensors;Pipelines;Spatial coherence;Benchmark testing;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610738&isnumber=10609862

S. Yuan et al., "MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2745-2751, doi: 10.1109/ICRA57147.2024.10610957.Abstract: In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the dataset’s applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in https://ntu-aris.github.io/MMAUD. keywords: {Accuracy;Laser radar;Estimation;Autonomous aerial vehicles;Robot sensing systems;Threat assessment;Trajectory;UAV;LIDAR;Audio;video fusion;Detection;Classification;Trajectory Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610957&isnumber=10609862

Y. Lu, Y. Chen, N. Ruozzi and Y. Xiang, "Mean Shift Mask Transformer for Unseen Object Instance Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2760-2766, doi: 10.1109/ICRA57147.2024.10610943.Abstract: Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to state-of-the-art methods for unseen object instance segmentation 1. keywords: {Instance segmentation;Training;Attention mechanisms;Neural networks;Clustering algorithms;Transformers;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610943&isnumber=10609862

H. Cao and S. Behnke, "SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2767-2773, doi: 10.1109/ICRA57147.2024.10610602.Abstract: We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements. The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP). This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency. We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics and shows great temporal consistency. keywords: {Measurement;Image segmentation;Three-dimensional displays;Laser radar;Semantics;Pipelines;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610602&isnumber=10609862

M. Lechner, R. Hasani, A. Amini, T. -H. Wang, T. A. Henzinger and D. Rus, "Overparametrization helps offline-to-online generalization of closed-loop control from pixels," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2774-2782, doi: 10.1109/ICRA57147.2024.10610284.Abstract: There is an ever-growing zoo of modern neural network models that can efficiently learn end-to-end control from visual observations. These advanced deep models, ranging from convolutional to Vision Transformers, from small to gigantic networks, have been extensively tested on offline image classification tasks. In this paper, we study these vision models with respect to the open-loop training to closed-loop generalization abilities, i.e., deployment realizes a causal feedback loop that is not present during training. This causality gap typically emerges in robotics applications such as autonomous driving, where a network is trained to imitate the control commands of a human. In this setting, two situations arise: 1) Closed-loop testing in-distribution, where the test environment shares properties with those of offline training data. 2) Closed-loop testing under distribution shifts and out-of-distribution. Contrary to recently reported results, we show that under proper training guidelines, all vision architectures perform indistinguishably well on in-distribution deployment, resolving the causality gap. In situation 2, We observe that scale is the strongest factor in improving closed-loop generalization regardless of the choice of the model architecture. Our results predict the trend that in the future we will see larger and larger models being used in offline-training-online-deployment imitation learning tasks in robotic applications. keywords: {Training;Visualization;Neural networks;Cause effect analysis;Training data;Transformers;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610284&isnumber=10609862

Z. Lin, Y. Chen and Z. Liu, "Hierarchical Human-to-Robot Imitation Learning for Long-Horizon Tasks via Cross-Domain Skill Alignment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2783-2790, doi: 10.1109/ICRA57147.2024.10610084.Abstract: For a general-purpose robot, it is desirable to imitate human demonstration videos that can effectively solve long-horizon tasks and perform novel ones. Recent advances in skill-based imitation learning have shown that extracting skill embedding from raw human videos is a promising paradigm to enable robots to cope with long-horizon tasks. However, generalization to unseen tasks in a different domain with a human prompt video poses a significant challenge due to the big embodiment and environment difference. To this end, we present Hierarchical Human-to-Robot Imitation Learning (H2RIL) that learns the mapping of cross-domain sensorimotor skills and utilizes it to generalize to unseen tasks given a human video in a different environment. To allow for generalizing zero-shot across environments and embodiments, H2RIL leverages task-agnostic play data for low-level policy training and paired human-robot data for both semantic and temporal skill embedding alignment. Extensive experiments in a simulated kitchen environment demonstrate that H2RIL significantly outperforms other prior baselines and is capable of generalizing to composable new tasks and adapting to Out-of-Distribution (OOD) tasks. keywords: {Training;Imitation learning;Semantics;Robot sensing systems;Task analysis;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610084&isnumber=10609862

Y. Liu and M. Hofert, "Policy Optimization by Looking Ahead for Model-based Offline Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2791-2797, doi: 10.1109/ICRA57147.2024.10610966.Abstract: Offline reinforcement learning (RL) aims to optimize a policy, based on pre-collected data, to maximize the cumulative rewards after performing a sequence of actions. Existing approaches learn a value function from historical data and then guide the updating of the policy parameters by maximizing the value function at a single time. Driven by the gap between maximizing the cumulative rewards of RL and the greedy strategy of existing methods, we propose an approach of policy optimization by looking ahead (POLA) to mitigate the gap. Concretely, we optimize the policy on both current and future states where the future states are predicted by a transition model. A trajectory contains numerous actions before the task is done. Performing the best action at each time does not mean an optimal trajectory in the end. We need to allow sub-optimal or negative actions occasionally. But existing methods focus on generating the optimal action at each time according to the maximizing Q-value principle. This motivates our looking ahead approach. Besides, hidden confounding factors may affect the decision making process. To that end, we incorporate the correlations among dimensions of the state into the policy, providing more information about the environment for the policy to make decisions. Empirical results on the Mujoco dataset show the effectiveness of the proposed approach. keywords: {Correlation;Accuracy;Decision making;Reinforcement learning;Predictive models;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610966&isnumber=10609862

N. Di Palo and E. Johns, "DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2798-2805, doi: 10.1109/ICRA57147.2024.10610923.Abstract: We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its endeffector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot. keywords: {Visualization;Computer vision;Codes;Imitation learning;Feature extraction;Transformers;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610923&isnumber=10609862

D. Yang, D. Tjia, J. Berg, D. Damen, P. Agrawal and A. Gupta, "Rank2Reward: Learning Shaped Reward Functions from Passive Video," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2806-2813, doi: 10.1109/ICRA57147.2024.10610873.Abstract: Teaching robots novel skills with demonstrations via human-in-the-loop data collection techniques like kinesthetic teaching or teleoperation puts a heavy burden on human supervisors. In contrast to this paradigm, it is often significantly easier to provide raw, action-free visual data of tasks being performed. Moreover, this data can even be mined from video datasets or the web. Ideally, this data can serve to guide robot learning for new tasks in novel environments, informing both "what" to do and "how" to do it. A powerful way to encode both the "what" and the "how" is to infer a well-shaped reward function for reinforcement learning. The challenge is determining how to ground visual demonstration inputs into a well-shaped and informative reward function. We propose a technique Rank2Reward for learning behaviors from videos of tasks being performed without access to any low-level states and actions. We do so by leveraging the videos to learn a reward function that measures incremental "progress" through a task by learning how to temporally rank the video frames in a demonstration. By inferring an appropriate ranking, the reward function is able to guide reinforcement learning by indicating when task progress is being made. This ranking function can be integrated into an adversarial imitation learning scheme resulting in an algorithm that can learn behaviors without exploiting the learned reward function. We demonstrate the effectiveness of Rank2Reward at learning behaviors from raw video on a number of tabletop manipulation tasks in both simulations and on a real-world robotic arm. We also demonstrate how Rank2Reward can be easily extended to be applicable to web-scale video datasets. Code and videos are available at https://rank2reward.github.io keywords: {Visualization;Codes;Imitation learning;Education;Reinforcement learning;Data collection;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610873&isnumber=10609862

E. Ellis, G. R. Ghosal, S. J. Russell, A. Dragan and E. Bıyık, "A Generalized Acquisition Function for Preference-based Reward Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2814-2821, doi: 10.1109/ICRA57147.2024.10611472.Abstract: Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic environment, an assistive robotics environment with domain transfer, and a natural language processing problem with real datasets demonstrate the superior performance of our querying method over the state-of-the-art information gain method. keywords: {Measurement;Learning systems;Autonomous systems;Education;Artificial neural networks;Natural language processing;Bayes methods},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611472&isnumber=10609862

C. Cao, Z. Yan, R. Lu, J. Tan and X. Wang, "Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2838-2844, doi: 10.1109/ICRA57147.2024.10610856.Abstract: Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GCRL algorithms and one offline safe RL algorithm. As a result, our method outperforms the existing state-of-the-art methods to a large extent. Furthermore, we validate the practicality and effectiveness of RbSL by deploying it on a real Panda manipulator. Code is available at https://github.com/Sunlighted/RbSL.git. keywords: {Training;Costs;Codes;Supervised learning;Reinforcement learning;Benchmark testing;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610856&isnumber=10609862

F. Yang, W. Zhou, Z. Liu, D. Zhao and D. Held, "Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2845-2851, doi: 10.1109/ICRA57147.2024.10610047.Abstract: Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. Our approach embeds safety constraints within the action space of a modified Markov Decision Process (MDP). The RL agent produces a sequence of actions that are transformed into safe trajectories by a trajectory optimizer, thereby effectively ensuring safety and increasing training stability. This novel approach excels in its performance on challenging Safety Gym tasks, achieving significantly higher rewards and near-zero safety violations during inference. The method’s real-world applicability is demonstrated through a safe and effective deployment in a real robot task of box-pushing around obstacles. Further insights are available from the videos and appendix on our website: https://sites.google.com/view/safemdp. keywords: {Training;Markov decision processes;Reinforcement learning;Safety;Task analysis;Trajectory optimization;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610047&isnumber=10609862

W. Zhang, J. Wang and Y. Yu, "Distributional Reinforcement Learning with Sample-set Bellman Update," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2852-2858, doi: 10.1109/ICRA57147.2024.10610740.Abstract: Distributional Reinforcement Learning (DRL) not only endeavors to optimize expected returns, but also strives to accurately characterize the full distribution of these returns, a key aspect in enhancing risk-aware decision-making. Previous DRL implementations often inappropriately treat statistical estimations as concrete samples, which undermines the integrity of learning. While several studies have addressed this issue, they frequently give rise to new complications, including computational burdens and diminished stochastic behavior. In our work, we present a novel DRL framework that leverages the Gaussian mixture model to adeptly depict the distribution of returns. This approach ensures precise, authentic sampling critical for robust learning, while also preserving computational tractability. Through extensive evaluation on a diverse array of 59 Atari games, our method not only surpasses the efficacy of prior DRL algorithms but also presents formidable competition to contemporary top-tier RL algorithms, signifying a substantial advancement in the field. keywords: {Decision making;Stochastic processes;Estimation;Reinforcement learning;Games;Concrete;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610740&isnumber=10609862

L. Berducci, S. Yang, R. Mangharam and R. Grosu, "Learning Adaptive Safety for Multi-Agent Systems," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2859-2865, doi: 10.1109/ICRA57147.2024.10611037.Abstract: Ensuring safety in dynamic multi-agent systems is challenging due to limited information about the other agents. Control Barrier Functions (CBFs) are showing promise for safety assurance but current methods make strong assumptions about other agents and often rely on manual tuning to balance safety, feasibility, and performance. In this work, we delve into the problem of adaptive safe learning for multi-agent systems with CBF. We show how emergent behaviour can be profoundly influenced by the CBF configuration, highlighting the necessity for a responsive and dynamic approach to CBF design. We present ASRL, a novel adaptive safe RL framework, to fully automate the optimization of policy and CBF coefficients, to enhance safety and long-term performance through reinforcement learning. By directly interacting with the other agents, ASRL learns to cope with diverse agent behaviours and maintains the cost violations below a desired limit. We evaluate ASRL in a multi-robot system and competitive multi-agent racing, against learning-based and control-theoretic approaches. We empirically demonstrate the efficacy of ASRL, and assess generalization and scalability to out-of-distribution scenarios. keywords: {Adaptation models;Costs;Uncertainty;Scalability;Reinforcement learning;Safety;Observability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611037&isnumber=10609862

N. Messikommer, Y. Song and D. Scaramuzza, "Contrastive Initial State Buffer for Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2866-2872, doi: 10.1109/ICRA57147.2024.10610528.Abstract: In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training convergence. keywords: {Training;Legged locomotion;Reinforcement learning;Contrastive learning;Data collection;Quadrupedal robots;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610528&isnumber=10609862

H. Honari, M. G. Tamizi and H. Najjaran, "Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2873-2879, doi: 10.1109/ICRA57147.2024.10611316.Abstract: Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a condition for SORL’s converged policy to guarantee safety and then use it to introduce an aggressiveness parameter that allows for fine-tuning the mentioned tradeoff. The experimental results obtained in seven different robotic environments indicate a considerable reduction in the number of safety violations along with higher, or competitive, policy returns, in comparison to six different state-of-the-art Safe RL methods. The results demonstrate the significant superiority of the proposed SORL algorithm in safety-critical applications. keywords: {Decision making;Reinforcement learning;Safety;Task analysis;Collision avoidance;Optimization;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611316&isnumber=10609862

L. Grossman and B. Plancher, "Differentially Encoded Observation Spaces for Perceptive Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2880-2886, doi: 10.1109/ICRA57147.2024.10611215.Abstract: Perceptive deep reinforcement learning (DRL) has lead to many recent breakthroughs for complex AI systems leveraging image-based input data. Applications of these results range from super-human level video game agents to dexterous, physically intelligent robots. However, training these perceptive DRL-enabled systems remains incredibly compute and memory intensive, often requiring huge training datasets and large experience replay buffers. This poses a challenge for the next generation of field robots that will need to be able to learn on the edge in order to adapt to their environments. In this paper, we begin to address this issue through differentially encoded observation spaces. By reinterpreting stored imagebased observations as a video, we leverage lossless differential video encoding schemes to compress the replay buffer without impacting training performance. We evaluate our approach with three state-of-the-art DRL algorithms and find that differential image encoding reduces the memory footprint by as much as 14.2× and 16.7× across tasks from the Atari 2600 benchmark and the DeepMind Control Suite (DMC) respectively. These savings also enable large-scale perceptive DRL that previously required paging between flash and RAM to be run entirely in RAM, improving the latency of DMC tasks by as much as 32%. keywords: {Training;Video games;Image coding;Monte Carlo methods;Memory management;Random access memory;Streaming media},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611215&isnumber=10609862

J. S. Roberts and J. Di, "Projected Task-Specific Layers for Multi-Task Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2887-2893, doi: 10.1109/ICRA57147.2024.10610483.Abstract: Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm. keywords: {Transfer learning;Employment;Reinforcement learning;Interference;Benchmark testing;Multitasking;Routing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610483&isnumber=10609862

C. Li et al., "Bi2Lane: Bi-Directional Temporal Refinement with Bi-Level Feature Aggregation for 3D Lane Detection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2894-2900, doi: 10.1109/ICRA57147.2024.10610794.Abstract: Monocular 3D lane detection has recently received increasing research attention in autonomous driving due to its application effectiveness and simplicity. However, depending solely on the limited semantic information from a single image makes current monocular detection methods unable to deal with complex scenarios, such as occluded, blurred, and unaligned scenes. In this study, we introduce an end-to-end framework named Bi2Lane which models temporal dependency in a continuous sequence. It recurrently utilizes detected lanes within historical frames as prior information to achieve robust lane detection. Additionally, Bi2Lane employs temporal reverse refinement together with temporal forward refinement to achieve bi-directional temporal refinement (BDTR) while maintaining a robust temporal dependency. For the refined features of different frames, we design a bi-level feature aggregation module (BLFA) to fuse them in both point-level and line-level manners, enabling a comprehensive feature representation to deal with complicated road scenes. Extensive experiments conducted on the OpenLane dataset demonstrate the superiority of Bi2Lane, achieving a notable F1 score of 63.8% using a simple ResNet50 backbone, surpassing the performance of existing state-of-the-art methods. keywords: {Three-dimensional displays;Lane detection;Fuses;Roads;Semantics;Bidirectional control;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610794&isnumber=10609862

J. Wasserman, G. Chowdhary, A. Gupta and U. Jain, "Exploitation-Guided Exploration for Semantic Embodied Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2901-2908, doi: 10.1109/ICRA57147.2024.10610117.Abstract: In the recent progress in embodied navigation and sim-to-robot transfer, modular policies have emerged as a de facto framework. However, there is more to compositionality beyond the decomposition of the learning load into modular components. In this work, we investigate a principled way to syntactically combine these components. Particularly, we propose Exploitation-Guided Exploration (XgX) where separate modules for exploration and exploitation come together in a novel and intuitive manner. We configure the exploitation module to take over in the deterministic final steps of navigation i.e. when the goal becomes visible. Crucially, an exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization. XgX, with effective decomposition and novel guidance, improves the state-of-the-art performance on the challenging object navigation task from 70% to 73%. Along with better accuracy, through targeted analysis, we show that XgX is also more efficient at goal-conditioned exploration. Finally, we show sim-to-real transfer to robot hardware and XgX performs over two-fold better than the best baseline from simulation benchmarking. Project page: xgxvisnav.github.io keywords: {Accuracy;Navigation;Semantics;Benchmark testing;Hardware;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610117&isnumber=10609862

P. Nourizadeh, M. Milford and T. Fischer, "Teach and Repeat Navigation: A Robust Control Approach," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2909-2916, doi: 10.1109/ICRA57147.2024.10611662.Abstract: Robot navigation requires an autonomy pipeline that is robust to environmental changes and effective in varying conditions. Teach and Repeat (T&R) navigation has shown high performance in autonomous repeated tasks under challenging circumstances, but research within T&R has predominantly focused on motion planning as opposed to motion control. In this paper, we propose a novel T&R system based on a robust motion control technique for a skid-steering mobile robot using sliding-mode control that effectively handles uncertainties that are particularly pronounced in the T&R task, where sensor noises, parametric uncertainties, and wheel-terrain interaction are common challenges. We first theoretically demonstrate that the proposed T&R system is globally stable and robust while considering the uncertainties of the closed-loop system. When deployed on a Clearpath Jackal robot, we then show the global stability of the proposed system in both indoor and outdoor environments covering different terrains, outperforming previous state-of-the-art methods in terms of mean average trajectory error and stability in these challenging environments. This paper makes an important step towards long-term autonomous T&R navigation with ensured safety guarantees. keywords: {Uncertainty;Navigation;Wheels;Robot sensing systems;Robustness;Odometry;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611662&isnumber=10609862

F. Lotfi et al., "Uncertainty-aware hybrid paradigm of nonlinear MPC and model-based RL for offroad navigation: Exploration of transformers in the predictive model," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2925-2931, doi: 10.1109/ICRA57147.2024.10610452.Abstract: In this paper, we investigate a hybrid scheme that combines nonlinear model predictive control (MPC) and model-based reinforcement learning (RL) for navigation planning of an autonomous model car across offroad, unstructured terrains without relying on predefined maps. Our innovative approach takes inspiration from BADGR, an LSTM-based network that primarily concentrates on environment modeling, but distinguishes itself by substituting LSTM modules with transformers to greatly elevate the performance of our model. Addressing uncertainty within the system, we train an ensemble of predictive models and estimate the mutual information between model weights and outputs, facilitating dynamic horizon planning through the introduction of variable speeds. Further enhancing our methodology, we incorporate a nonlinear MPC controller that accounts for the intricacies of the vehicle’s model and states. The model-based RL facet produces steering angles and quantifies inherent uncertainty. At the same time, the nonlinear MPC suggests optimal throttle settings, striking a balance between goal attainment speed and managing model uncertainty influenced by velocity. In the conducted studies, our approach excels over the existing baseline by consistently achieving higher metric values in predicting future events and seamlessly integrating the vehicle’s kinematic model for enhanced decision-making. The code and the evaluation data are available at (Github-repo). keywords: {Uncertainty;Navigation;Reinforcement learning;Predictive models;Transformers;Planning;Trajectory;Model-based RL;transformers;nonlinear MPC;uncertainty-aware planning;offroad navigation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610452&isnumber=10609862

C. Xu, C. Amato and L. L. S. Wong, "Robot Navigation in Unseen Environments using Coarse Maps," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2932-2938, doi: 10.1109/ICRA57147.2024.10611256.Abstract: Metric occupancy maps are widely used in autonomous robot navigation systems. However, when a robot is deployed in an unseen environment, building an accurate metric map is time-consuming. Can an autonomous robot directly navigate in previously unseen environments using coarse maps? In this work, we propose the Coarse Map Navigator (CMN), a navigation framework that can perform robot navigation in unseen environments using different coarse maps. To do so, CMN addresses two challenges: (1) novel and realistic visual observations; (2) error and misalignment on coarse maps. To tackle novel visual observations in unseen environments, CMN learns a deep perception model that maps the visual input from various pixel spaces to the local occupancy grid space. To tackle the error and misalignment on coarse maps, CMN extends the Bayesian filter and maintains a belief directly on coarse maps using the predicted local occupancy grids as observations. Using the latest belief, CMN extracts a global heuristic vector that guides the planner to find a local navigation action. Empirical results demonstrate that CMN achieves high navigation success rates in unseen environments, significantly outperforming baselines, and is robust to different coarse maps. keywords: {Measurement;Location awareness;Visualization;Systematics;Navigation;Cameras;Vectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611256&isnumber=10609862

T. Kitade, W. Yamada, K. Ochiai and M. Imai, "Bicode: A Hybrid Blinking Marker System for Event Cameras," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2939-2945, doi: 10.1109/ICRA57147.2024.10611033.Abstract: In the field of robotics, tag systems play an important role in various applications, such as object identification and robot control in real-world environments. While typical visual markers use two-dimensional (2D) patterns and RGB cameras for recognizing object IDs and poses, achieving long-distance recognition necessitates increasing marker size and camera magnification to ensure the required resolution. Furthermore, the growing adoption of event cameras in robotics captures rapid changes in pixel brightness but faces limitations in recognizing stationary 2D markers. Although compact blinker markers using blinking light-emitting diodes (LEDs) achieve long-distance recognition, they are constrained by the number of IDs or recognition speed when used with standard RGB cameras. In addition, recognizing object pose using only a single blinking LED presents challenges. To address these challenges, we introduce ‘Bicode,’ an indoor visual marker designed for event cameras. Bicode seamlessly integrates 2D and blinker markers within a single marker unit. We have developed prototypes of 2.5, 5, and 10 cm square acrylic 2D markers, each equipped with a single LED blinking at 1 kHz, enabling recognition with an event camera. Our experiments revealed the effects of marker size, LED light quantity, recognition distance, and angle, external lighting conditions, and camera or marker movement on accuracy. Notably, using the 5 cm marker, we confirmed its compatibility to recognize IDs at distances exceeding 20 m, and pose recognition at 2.5 m was confirmed. keywords: {Visualization;Two-dimensional displays;Robot vision systems;Robot control;Pose estimation;Prototypes;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611033&isnumber=10609862

H. Morimitsu, X. Zhu, R. M. Cesar, X. Ji and X. -C. Yin, "RAPIDFlow: Recurrent Adaptable Pyramids with Iterative Decoding for Efficient Optical Flow Estimation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2946-2952, doi: 10.1109/ICRA57147.2024.10610277.Abstract: Extracting motion information from videos with optical flow estimation is vital in multiple practical robot applications. Current optical flow approaches show remarkable accuracy, but top-performing methods have high computational costs and are unsuitable for embedded devices. Although some previous works have focused on developing low-cost optical flow strategies, their estimation quality has a noticeable gap with more robust methods. In this paper, we develop a novel method to efficiently estimate high-quality optical flow in embedded devices. Our proposed RAPIDFlow model combines efficient NeXt1D convolution blocks with a fully recurrent structure based on feature pyramids to decrease computational costs without significantly impacting estimation accuracy. The adaptable recurrent encoder produces multi-scale features with a single shared block, which allows us to adjust the pyramid length at inference time and make it more robust to changes in input size. Also, it enables our model to offer multiple tradeoffs between accuracy and speed to suit different applications. Experiments using a Jetson Orin NX embedded system on the MPI-Sintel and KITTI public benchmarks show that RAPIDFlow outperforms previous approaches by significant margins at faster speeds. Our code is available at https://github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/rapidflow. keywords: {Adaptation models;Accuracy;Embedded systems;Computational modeling;Estimation;Computational efficiency;Optical flow},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610277&isnumber=10609862

X. Fu, D. Zhang, L. Mo, K. Li and X. Zhao, "Design and Analysis of Soft Hybrid-Driven Manipulator with Variable Stiffness and Multiple Motion Patterns," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2979-2985, doi: 10.1109/ICRA57147.2024.10610612.Abstract: Soft manipulators offer the advantages of safety and adaptability. However, due to insufficient stiffness and single motion mode limitations, existing soft manipulators usually exhibit low load capacity and small working space. To address this problem, we propose a novel soft hybrid-driven manipulator with continuous stiffness control capability and multiple motion patterns (omnidirectional bending and extension). Furthermore, we develop kinematic and stiffness models based on the constant curvature assumption. The soft robot consists of a soft bellows actuator and inextensible rigid skeletons, which exhibit a high extension ratio and low drive pressure. With the antagonistic actuation of tendon-pulling and air-pushing, the robot can achieve independent control over stiffness and position in three-dimensional space. The performance associated with the designed soft hybrid-driven manipulator is experimentally verified. The robot can achieve an elongation of 198% and a maximum bending angle of up to 240°. The robot can also increase stiffness by increasing internal air pressure to resist deformation caused by external loads. Additionally, tracking experiments with various trajectories in space verify the accuracy of the kinematic model, which indicates that the soft manipulator can stabilize motion within a broad workspace. keywords: {Bellows;Deformation;Kinematics;Soft robotics;Pneumatic systems;Bending;Aerospace electronics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610612&isnumber=10609862

H. Matusik, C. Liu and D. Rus, "Directly 3D Printed, Pneumatically Actuated Multi-Material Robotic Hand," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2986-2992, doi: 10.1109/ICRA57147.2024.10610016.Abstract: Soft robotic manipulators with many degrees of freedom can carry out complex tasks safely around humans. However, manufacturing of soft robotic hands with several degrees of freedom requires a complex multi-step manual process, which significantly increases their cost. We present a design of a multi-material 15 DoF robotic hand with five fingers including an opposable thumb. Our design has 15 pneumatic actuators based on a series of hollow chambers that are driven by an external pressure system. The thumb utilizes rigid joints and the palm features internal rigid structure and soft skin. The design can be directly 3D printed using a multi-material additive manufacturing process without any assembly process and therefore our hand can be manufactured for less than 300 dollars. We test the hand in conjunction with a low-cost vision-based teleoperation system on different tasks. keywords: {Three-dimensional displays;Costs;Force;Thumb;Pneumatic systems;Soft robotics;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610016&isnumber=10609862

D. Xie, Y. Su, X. Shi, Z. Li and R. K. -y. Tong, "Soft Hand Extension Glove with Thumb Abduction and Extension Assistance," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2993-2999, doi: 10.1109/ICRA57147.2024.10610770.Abstract: Hand extension is crucial for stroke survivors with spasticity, where their fingers become rigid and their thumb remains curled within the palm. Due to the underactuated nature of the hand, the dominance of flexor muscles over extensors, and the limited surface area available, developing an extension glove with thumb assistance poses a challenge for researchers. This paper introduces a fully wearable soft hand extension glove based on the X-pouch and strap system, addressing the above challenges. The glove enables adequate finger extension, thumb abduction, and extension for high MAS score patients. Modelling and testing revealed extension torques of up to 2.7 Nm at the MCP joint and 0.67 Nm at the PIP and DIP joints. Performance evaluation, including comparison with existing methods, demonstrated the glove’s superior extension capabilities using a model hand with realistic stiffness. Furthermore, the glove’s effectiveness was confirmed through testing on a stroke patient with MAS = 2, validating its on-body functionality. keywords: {Performance evaluation;Thumb;Muscles;Robotics and automation;Testing;Electronics packaging},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610770&isnumber=10609862

H. Liu, C. Wu, S. Lin and Y. Chen, "Design and Characterization of a Soft Flat Tube Twisting Actuator," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3000-3005, doi: 10.1109/ICRA57147.2024.10609872.Abstract: Soft actuators have shown advantages of adaptiveness, large deformation, and safe human-robot interaction, making them suitable for various applications. Herein, a novel soft flat tube twisting actuator (SFTTA) is proposed. The SFTTA is composed of a folded flat tube sandwiched between two silicone rubber laminates. When inflated by compressed air, the folded corners of the flat tube tend to unfold, resulting in the twist of the actuator to a helical structure. The SFTTA has great scalability. It can be fabricated through simple processes with low-cost materials. For a sample SFTTA with the size of a human finger, it can twist 5400 at an air pressure of 300 kPa. In general, SFTTA based actuators can twist 9.6 degree per millimeter in length, which is significantly larger than previously reported soft twisting actuators. Additionally, the composite-like SFTTA allows mechanical property programming through the alteration of folding patterns of the flat tube and the material structure of the elastomer laminates. Finally, an extensible soft gripper based on flat tube actuators and a robotic wrist module are developed, and their rotation is realized by the proposed SFTTA actuator. keywords: {Wrist;Actuators;Scalability;Human-robot interaction;Soft robotics;Programming;Electron tubes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609872&isnumber=10609862

M. J. Mendoza, N. D. Naclerio and E. W. Hawkes, "High-Curvature, High-Force, Vine Robot for Inspection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3014-3021, doi: 10.1109/ICRA57147.2024.10610845.Abstract: Robot performance has advanced considerably both in and out of the factory, however in tightly constrained, unknown environments such as inside a jet engine or the human heart, current robots are less adept. In such cases where a borescope or endoscope can’t reach, disassembly or surgery are costly. One promising inspection device inspired by plant growth are "vine robots" that can navigate cluttered environments by extending from their tip. Yet, these vine robots are currently limited in their ability to simultaneously steer into tight curvatures and apply substantial forces to the environment. Here, we propose a plant-inspired method of steering by asymmetrically lengthening one side of the vine robot to enable high curvature and large force application. Our key development is the introduction of an extremely anisotropic, composite, wrinkled film with elastic moduli 400x different in orthogonal directions. The film is used as the vine robot body, oriented such that it can stretch over 120% axially, but only 3% circumferentially. With the addition of controlled layer jamming, this film enables a steering method inspired by plants in which the circumference of the robot is inextensible, but the sides can stretch to allow turns. This steering method and body pressure do not work against each other, allowing the robot to exhibit higher forces and tighter curvatures than previous vine robot architectures. This work advances the abilities of vine robots–and robots more generally–to not only access tightly constrained environments, but perform useful work once accessed. keywords: {Performance evaluation;Surgery;Inspection;Anisotropic;Soft robotics;Lumen;Production facilities},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610845&isnumber=10609862

K. L. Walker et al., "A Modular, Tendon Driven Variable Stiffness Manipulator with Internal Routing for Improved Stability and Increased Payload Capacity," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3030-3035, doi: 10.1109/ICRA57147.2024.10611527.Abstract: Stability and reliable operation under a spectrum of environmental conditions is still an open challenge for soft and continuum style manipulators. The inability to carry sufficient load and effectively reject external disturbances are two drawbacks which limit the scale of continuum designs, preventing widespread adoption of this technology. To tackle these problems, this work details the design and experimental testing of a modular, tendon driven bead-style continuum manipulator with tunable stiffness. By embedding the ability to independently control the stiffness of distinct sections of the structure, the manipulator can regulate it’s posture under greater loads of up to 1kg at the end-effector, with reference to the flexible state. Likewise, an internal routing scheme vastly improves the stability of the proximal segment when operating the distal segment, reducing deviations by at least 70.11%. Operation is validated when gravity is both tangential and perpendicular to the manipulator backbone, a feature uncommon in previous designs. The findings presented in this work are key to the development of larger scale continuum designs, demonstrating that flexibility and tip stability under loading can co-exist without compromise. keywords: {Motion segmentation;Prototypes;Bending;Manipulators;Routing;Reliability;Thermal stability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611527&isnumber=10609862

I. Fang et al., "EgoPAT3Dv2: Predicting 3D Action Target from 2D Egocentric Vision for Human-Robot Interaction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3036-3043, doi: 10.1109/ICRA57147.2024.10610283.Abstract: A robot’s ability to anticipate the 3D action target location of a hand’s movement from egocentric videos can greatly improve safety and efficiency in human-robot interaction (HRI). While previous research predominantly focused on semantic action classification or 2D target region prediction, we argue that predicting the action target’s 3D coordinate could pave the way for more versatile downstream robotics tasks, especially given the increasing prevalence of headset devices. This study expands EgoPAT3D, the sole dataset dedicated to egocentric 3D action target prediction. We augment both its size and diversity, enhancing its potential for generalization. Moreover, we substantially enhance the baseline algorithm by introducing a large pre-trained model and human prior knowledge. Remarkably, our novel algorithm can now achieve superior prediction outcomes using solely RGB images, eliminating the previous need for 3D point clouds and IMU input. Furthermore, we deploy our enhanced baseline algorithm on a real-world robotic platform to illustrate its practical utility in straightforward HRI tasks. The demonstrations showcase the real-world applicability of our advancements and may inspire more HRI use cases involving egocentric vision. All code and data are open-sourced and can be found on the project website. keywords: {Point cloud compression;Headphones;Three-dimensional displays;Robot kinematics;Semantics;Human-robot interaction;Prediction algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610283&isnumber=10609862

J. Ni et al., "Distribution-Aware Continual Test-Time Adaptation for Semantic Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3044-3050, doi: 10.1109/ICRA57147.2024.10610045.Abstract: Since autonomous driving systems usually face dynamic and ever-changing environments, continual test-time adaptation (CTTA) has been proposed as a strategy for transferring deployed models to continually changing target domains. However, the pursuit of long-term adaptation often introduces catastrophic forgetting and error accumulation problems, which impede the practical implementation of CTTA in the real world. Recently, existing CTTA methods mainly focus on utilizing a majority of parameters to fit target domain knowledge through self-training. Unfortunately, these approaches often amplify the challenge of error accumulation due to noisy pseudo-labels, and pose practical limitations stemming from the heavy computational costs associated with entire model updates. In this paper, we propose a distribution-aware tuning (DAT) method to make the semantic segmentation CTTA efficient and practical in real-world applications. DAT adaptively selects and updates two small groups of trainable parameters based on data distribution during the continual adaptation process, including domain-specific parameters (DSP) and task-relevant parameters (TRP). Specifically, DSP exhibits sensitivity to outputs with substantial distribution shifts, effectively mitigating the problem of error accumulation. In contrast, TRP are allocated to positions that are responsive to outputs with minor distribution shifts, which are fine-tuned to avoid the catastrophic forgetting problem. In addition, since CTTA is a temporal task, we introduce the Parameter Accumulation Update (PAU) strategy to collect the updated DSP and TRP in target domain sequences. We conducted extensive experiments on two widely-used semantic segmentation CTTA benchmarks, achieving competitive performance and efficiency compared to previous state-of-the-art methods. keywords: {Adaptation models;Sensitivity;Semantic segmentation;Computational modeling;Computational efficiency;Noise measurement;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610045&isnumber=10609862

J. Lu, B. Niu, H. Ma, J. Zhu and J. Ji, "STNet: Spatio-Temporal Fusion-Based SelfAttention for Slip Detection in Visuo-Tactile Sensors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3051-3056, doi: 10.1109/ICRA57147.2024.10610734.Abstract: Slip detection plays a pivotal role in the dexterity of robotics, improving the reliability and precision of manipulations but also contributing to safety, efficiency, and adaptability. Deep learning-based slip detection algorithms commonly difficult to concentrate on key features when faced with dense 3D shape data obtained by visuo-tactile sensors. Data from noncontact locations can interfere with slip judgements and the ignorance of interframe linkage can also lead to slip detection failure. In this paper, a new spatio-temporal sequences fusion-based self-attention, STNet, is proposed to perform slip detection by allocating more attention to the object-sensor contact area when processing complex 3D shape data. A binocular visuo-tactile system (BVTS) is designed and fabricated for dataset construction. The entire 3D shape dataset containing 4 motion patterns, including stationary, pressing, rolling and slipping. Self-attention architecture with and without spatio-temporal sequences fusion mechanism (denoted as STNet and TemNet, respectively) are trained based on the same dataset. The experiments show the validity of STNet, which can reach 98.91% slip detection accuracy. Meanwhile, the ablation studies confirm the effectiveness of the spatio-temporal sequences fusion mechanism. keywords: {Vibrations;Training;Three-dimensional displays;Shape;Prototypes;Pressing;Sensor phenomena and characterization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610734&isnumber=10609862

S. U. Lee, "Commonsense Spatial Knowledge-aware 3-D Human Motion and Object Interaction Prediction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3057-3063, doi: 10.1109/ICRA57147.2024.10610161.Abstract: We propose a novel 3-D human motion and object interaction prediction model that is aware of commonsense knowledge about human–object interaction. We jointly predict human joint motion and human–object interactions. The two prediction results are combined to enforce commonsense knowledge, such as "if the human right hand is predicted to be in contact with an object after 1 second, the distance between the right hand and an object should also be predicted to be small," explicit to the model. Our model uses the raw point cloud representation of the surrounding objects in the environment as input. Using raw point cloud representation allows us to model commonsense knowledge easily and improve accuracy. In particular, it does not require a separate perception system (e.g., object classification, object pose estimation, and so on), as in previous studies, and thus is robust to perception errors. Our model applies a cross-attention mechanism to fuse the environmental point cloud and past human joint poses. The surrounding environment context and past human joint poses are two heterogeneous inputs and cross-attention can be a powerful approach to fuse them. Our model is validated on the KIT Whole-Body Human Motion (WBHM) dataset. keywords: {Point cloud compression;Solid modeling;Fuses;Pose estimation;Predictive models;Encoding;Spatial databases},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610161&isnumber=10609862

L. Schulze and H. Lipson, "High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3064-3070, doi: 10.1109/ICRA57147.2024.10611047.Abstract: A robot self-model is a task-agnostic representation of the robot’s physical morphology that can be used for motion planning tasks in the absence of a classical geometric kinematic model. In particular, when the latter is hard to engineer or the robot’s kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of the robot’s workspace dimension. We demonstrate the capabilities of this model on motion planning tasks as an exemplary downstream application. keywords: {Geometry;Dynamics;Robot vision systems;Morphology;Kinematics;Cameras;Autonomous agents},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611047&isnumber=10609862

T. Nguyen et al., "Language-Conditioned Affordance-Pose Detection in 3D Point Clouds," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3071-3078, doi: 10.1109/ICRA57147.2024.10610008.Abstract: Affordance detection and pose estimation are of great importance in many robotic applications. Their combination helps the robot gain an enhanced manipulation capability, in which the generated pose can facilitate the corresponding affordance task. Previous methods for affodance-pose joint learning are limited to a predefined set of affordances, thus limiting the adaptability of robots in real-world environments. In this paper, we propose a new method for language-conditioned affordance-pose joint learning in 3D point clouds. Given a 3D point cloud object, our method detects the affordance region and generates appropriate 6-DoF poses for any unconstrained affordance label. Our method consists of an open-vocabulary affordance detection branch and a language-guided diffusion model that generates 6-DoF poses based on the affordance text. We also introduce a new high-quality dataset for the task of language-driven affordance-pose joint learning. Intensive experimental results demonstrate that our proposed method works effectively on a wide range of open-vocabulary affordances and outperforms other baselines by a large margin. In addition, we illustrate the usefulness of our method in real-world robotic applications. Our code and dataset are publicly available at https://3DAPNet.github.io. keywords: {Point cloud compression;Three-dimensional displays;Codes;Limiting;Affordances;Pose estimation;Diffusion models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610008&isnumber=10609862

S. Lim, Y. Yoo, J. K. Lee and B. -T. Zhang, "Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3079-3085, doi: 10.1109/ICRA57147.2024.10611029.Abstract: In this paper, we propose a novel method for plane clustering specialized in cluttered scenes using an RGB-D camera and validate its effectiveness through robot grasping experiments. Unlike existing methods, which focus on large- scale indoor structures, our approach—Multi-Object RANSAC emphasizes cluttered environments that contain a wide range of objects with different scales. It enhances plane segmentation by generating subplanes in Deep Plane Clustering (DPC) module, which are then merged with the final planes by postprocessing. DPC rearranges the point cloud by voting layers to make subplane clusters, trained in a self-supervised manner using pseudo-labels generated from RANSAC. Multi-Object RANSAC demonstrates superior plane instance segmentation performances over other recent RANSAC applications. We conducted an experiment on robot suction-based grasping, comparing our method with vision-based grasping network and RANSAC applications. The results from this real-world scenario showed its remarkable performance surpassing the baseline methods, highlighting its potential for advanced scene understanding and manipulation. keywords: {Point cloud compression;Instance segmentation;Clustering methods;Robot vision systems;Grasping;Cameras;Clutter},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611029&isnumber=10609862

S. Chatterjee, D. Doan and B. Calli, "Utilizing Inpainting for Training Keypoint Detection Algorithms Towards Markerless Visual Servoing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3086-3092, doi: 10.1109/ICRA57147.2024.10610006.Abstract: This paper presents a novel strategy to train keypoint detection models for robotics applications. Our goal is to develop methods that can robustly detect and track natural features on robotic manipulators. Such features can be used for vision-based control and pose estimation purposes, when placing artificial markers (e.g. ArUco) on the robot’s body is not possible or practical in runtime. Prior methods require accurate camera calibration and robot kinematic models in order to label training images for the keypoint locations. In this paper, we remove these dependencies by utilizing inpainting methods: In the training phase, we attach ArUco markers along the robot’s body and then label the keypoint locations as the center of those markers. We, then, use an inpainting method to reconstruct the parts of the robot occluded by the ArUco markers. As such, the markers are artificially removed from the training images, and labeled data is obtained to train markerless keypoint detection algorithms without the need for camera calibration or robot models. Using this approach, we trained a model for realtime keypoint detection and used the inferred keypoints as control features for an adaptive visual servoing scheme. We obtained successful control results with this fully model-free control strategy, utilizing natural robot features in the runtime and not requiring camera calibration or robot models in any stage of this process. keywords: {Training;Adaptation models;Runtime;Robot kinematics;Cameras;Feature extraction;Visual servoing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610006&isnumber=10609862

C. Kohler, A. S. Srikanth, E. Arora and R. Platt, "Symmetric Models for Visual Force Policy Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3101-3107, doi: 10.1109/ICRA57147.2024.10610728.Abstract: While it is generally acknowledged that force feedback is beneficial to robotic control, applications of policy learning to robotic manipulation typically only leverage visual feedback. Recently, symmetric neural models have been used to significantly improve the sample efficiency and performance of policy learning across a variety of robotic manipulation domains. This paper explores an application of symmetric policy learning to visual-force problems. We present Symmetric Visual Force Learning (SVFL), a novel method for robotic control which leverages visual and force feedback. We demonstrate that SVFL can significantly outperform state of the art baselines for visual force learning and report several interesting empirical findings related to the utility of learning force feedback control policies in both general manipulation tasks and scenarios with low visual acuity. keywords: {Visualization;Force feedback;Force;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610728&isnumber=10609862

