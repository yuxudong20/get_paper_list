S. W. Hyder et al., "Action Segmentation Using 2D Skeleton Heatmaps and Multi-Modality Fusion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1048-1055, doi: 10.1109/ICRA57147.2024.10610644.Abstract: This paper presents a 2D skeleton-based action segmentation method with applications in fine-grained human activity recognition. In contrast with state-of-the-art methods which directly take sequences of 3D skeleton coordinates as inputs and apply Graph Convolutional Networks (GCNs) for spatiotemporal feature learning, our main idea is to use sequences of 2D skeleton heatmaps as inputs and employ Temporal Convolutional Networks (TCNs) to extract spatiotemporal features. Despite lacking 3D information, our approach yields comparable/superior performances and better robustness against missing keypoints than previous methods on action segmentation datasets. Moreover, we improve the performances further by using both 2D skeleton heatmaps and RGB videos as inputs. To our best knowledge, this is the first work to utilize 2D skeleton heatmap inputs and the first work to explore 2D skeleton+RGB fusion for action segmentation. keywords: {Heating systems;Representation learning;Three-dimensional displays;Robot vision systems;Feature extraction;Skeleton;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610644&isnumber=10609862

W. Fan, H. Li, W. Si, S. Luo, N. Lepora and D. Zhang, "ViTacTip: Design and Verification of a Novel Biomimetic Physical Vision-Tactile Fusion Sensor," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1056-1062, doi: 10.1109/ICRA57147.2024.10611186.Abstract: Tactile sensing is significant for robotics since it can obtain physical contact information during manipulation. To capture multimodal contact information within a compact framework, we designed a novel sensor called ViTacTip, which seamlessly integrates both tactile and visual perception capabilities into a single, integrated sensor unit. ViTacTip features a transparent skin to capture fine features of objects during contact, which can be known as the see-through-skin mechanism. In the meantime, the biomimetic tips embedded in ViTacTip can amplify touch motions during tactile perception. For comparative analysis, we also fabricated a ViTac sensor devoid of biomimetic tips, as well as a TacTip sensor with opaque skin. Furthermore, we develop a Generative Adversarial Network (GAN)-based approach for modality switching between different perception modes, effectively alternating the emphasis between vision and tactile perception modes. We conducted a performance evaluation of the proposed sensor across three distinct tasks: i) grating identification, ii) pose regression, iii) contact localization and force estimation. In the grating identification task, ViTacTip demonstrated an accuracy of 99.72%, surpassing TacTip, which achieved 94.60%. It also exhibited superior performance in both pose and force estimation tasks with the minimum error of 0.08 mm and 0.03N, respectively, in contrast to ViTac’s 0.12 mm and 0.15N. Results indicate that ViTacTip outperforms single-modality sensors. keywords: {Biomimetics;Force;Estimation;Switches;Robot sensing systems;Skin;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611186&isnumber=10609862

W. Zheng, K. Liu, D. Guo, W. Yang, J. Zhu and H. Liu, "A Large-area Tactile Sensor for Distributed Force Sensing Using Highly Sensitive Piezoresistive Sponge," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1063-1069, doi: 10.1109/ICRA57147.2024.10610739.Abstract: Tactile sensing plays a critical role in enabling robots to interact safely with target objects in dynamic and unstructured environments. While various tactile sensors based on different sensing principles or different sensitive materials have been proposed, the development of flexible large-area tactile sensors for robots is still challenging. In this paper, a novel highly sensitive piezoresistive sponge based on multi-walled carbon nanotubes (MWCNTs) and polyurethane (PU) sponge is fabricated for pressure sensing. The sensing behavior of the piezoresistive sponge was experimentally evaluated, showing high sensitivity and fast response. Based on the piezoresistive sponge, a flexible large-area tactile sensor is designed for distributed force detection with electrical resistance tomography technology. The sensing performance of the sensor is validated by touch location, sensitivity analysis, real-time touch discrimination, and touch modality recognition. The experimental results indicate that the sensor performs well in detecting the position and force of contact in a large area. The sensor’s performance shows promise in embodied tactile sensing and human–robot interaction. keywords: {Fabrication;Costs;Sensitivity analysis;Inverse problems;Force;Tactile sensors;Reconstruction algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610739&isnumber=10609862

G. Brayshaw, B. Ward-Cherrier and M. J. Pearson, "A Neuromorphic System for the Real-time Classification of Natural Textures," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1070-1076, doi: 10.1109/ICRA57147.2024.10610401.Abstract: Tactile exploration of surfaces is a key component of everyday life, allowing us to make complex inferences about our environments even when vision is occluded. The emergence of biomimetic neuromorphic hardware in recent years has furthered our ability to create biologically plausible sensing solutions. While these platforms continue to improve in regards to latency and power consumption, within recent literature on tactile texture classification there is an emphasis on accuracy at the expense of real-time processing. In order for these tactile sensing systems to find use outside of experimental laboratory environments, it is key to design systems capable of capturing and processing data in real-time. Within this paper we present a system for the real-time classification of texture using a neuromorphic tactile sensor, a spiking neural network and a novel decision making algorithm. Our real-time system achieves classification accuracies of 94% on a dataset of 11 natural textile textures. Furthermore our system is capable of identifying textures at human-level performance in as little as 84ms. Additionally, benchmarking our system across CPU, GPU and Loihi2 hardware platforms resulted in a 96% reduction in power consumption on the neuromorphic platform. This system out-performed previous work by the authors and the state of art, both in terms of accuracy and classification speed. keywords: {Accuracy;Power demand;Neuromorphics;Tactile sensors;Spiking neural networks;Real-time systems;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610401&isnumber=10609862

Y. Fuchioka and M. Hamaya, "An Electromagnetism-Inspired Method for Estimating In-Grasp Torque from Visuotactile Sensors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1077-1083, doi: 10.1109/ICRA57147.2024.10611429.Abstract: Tactile sensing has become a popular sensing modality for robot manipulators, due to the promise of providing robots with the ability to measure the rich contact information that gets transmitted through its sense of touch. Among the diverse range of information accessible from tactile sensors, torques transmitted from the grasped object to the fingers through extrinsic environmental contact may be particularly important for tasks such as object insertion. However, tactile torque estimation has received relatively little attention when compared to other sensing modalities, such as force, texture, or slip identification. In this work, we introduce the notion of the Tactile Dipole Moment, which we use to estimate tilt torques from gel-based visuotactile sensors. This method does not rely on deep learning, sensor-specific mechanical, or optical modeling, and instead takes inspiration from electromechanics to analyze the vector field produced from 2D marker displacements. Despite the simplicity of our technique, we demonstrate its ability to provide accurate torque readings over two different tactile sensors and three object geometries, and highlight its practicality for the task of USB stick insertion with a compliant robot arm. These results suggest that simple analytical calculations based on dipole moments can sufficiently extract physical quantities from visuotactile sensors. keywords: {Mechanical sensors;Integrated optics;Torque;Tactile sensors;Manipulators;Universal Serial Bus;Vectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611429&isnumber=10609862

N. Yilmaz and U. Tumerdem, "Learning Contact for Haptic Feedback: Switching X-lateral Teleoperators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1092-1098, doi: 10.1109/ICRA57147.2024.10611571.Abstract: In this paper, we propose X-lateral teleoperation: a novel hybrid unilateral-bilateral teleoperation framework. Bilateral teleoperation enables kinesthetic coupling between the operator and the remote environment with haptic feedback. However, in free motion, unlike unilateral teleoperators, bilateral teleoperators reflect undesirable operational forces to the operator. The proposed X-lateral teleoperation framework benefits from a learning-based contact detection algorithm which triggers switching from unilateral teleoperation in free motion to bilateral teleoperation in contact. We also present a neural network based two-class classification technique to detect contacts even with environments not seen in training. In experiments with linear motors and Phantom Omni devices, using sensorless force estimation, we show that the proposed method can decrease operational forces significantly over transparencyoptimized bilateral architectures. keywords: {Training;Teleoperators;Neural networks;Force;Phantoms;Switches;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611571&isnumber=10609862

Y. Ma, J. A. Zhao and E. Adelson, "GelLink: A Compact Multi-phalanx Finger with Vision-based Tactile Sensing and Proprioception," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1107-1113, doi: 10.1109/ICRA57147.2024.10610823.Abstract: Compared to fully-actuated robotic end-effectors, underactuated ones are generally more adaptive, robust, and cost-effective. However, state estimation for underactuated hands is usually more challenging. Vision-based tactile sensors, like Gelsight, can mitigate this issue by providing high-resolution tactile sensing and accurate proprioceptive sensing. As such, we present GelLink, a compact, underactuated, linkage-driven robotic finger with low-cost, high-resolution vision-based tactile sensing and proprioceptive sensing capabilities. In order to reduce the amount of embedded hardware, i.e. the cameras and motors, we optimize the linkage transmission with a planar linkage mechanism simulator and develop a planar reflection simulator to simplify the tactile sensing hardware. As a result, GelLink only requires one motor to actuate the three phalanges, and one camera to capture tactile signals along the entire finger. Overall, GelLink is a compact robotic finger that shows adaptability and robustness when performing grasping tasks. The integration of vision- based tactile sensors can significantly enhance the capabilities of underactuated fingers and potentially broaden their future usage. keywords: {Couplings;Robot vision systems;Tactile sensors;Propioception;Motors;Cameras;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610823&isnumber=10609862

M. H. Tippur and E. H. Adelson, "RainbowSight: A Family of Generalizable, Curved, Camera-Based Tactile Sensors For Shape Reconstruction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1114-1120, doi: 10.1109/ICRA57147.2024.10609863.Abstract: Camera-based tactile sensors can provide high resolution positional and local geometry information for robotic manipulation. Curved and rounded fingers are often advantageous, but it can be difficult to derive illumination systems that work well within curved geometries. To address this issue, we introduce RainbowSight, a family of curved, compact, camera-based tactile sensors which use addressable RGB LEDs illuminated in a novel rainbow spectrum pattern. In addition to being able to scale the illumination scheme to different sensor sizes and shapes to fit on a variety of end effector configurations, the sensors can be easily manufactured and require minimal optical tuning to obtain high resolution depth reconstructions of an object deforming the sensor’s soft elastomer surface. Additionally, we show the advantages of our new hardware design and improvements in calibration methods for accurate depth map generation when compared to alternative lighting methods commonly implemented in previous camera-based tactile sensors. With these advancements, we make the integration of tactile sensors more accessible to roboticists by allowing them the flexibility to easily customize, fabricate, and calibrate camera-based tactile sensors to best fit the needs of their robotic systems. keywords: {Geometry;Surface reconstruction;Three-dimensional displays;Shape;Tactile sensors;Lighting;Light emitting diodes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609863&isnumber=10609862

Z. Gu, R. Guo, W. Yates, Y. Chen, Y. Zhao and Y. Zhao, "Walking-by-Logic: Signal Temporal Logic-Guided Model Predictive Control for Bipedal Locomotion Resilient to External Perturbations," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1121-1127, doi: 10.1109/ICRA57147.2024.10610811.Abstract: This study proposes a novel planning framework based on a model predictive control formulation that incorporates signal temporal logic (STL) specifications for task completion guarantees and robustness quantification. This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion push recovery, where the robot experiences unexpected disturbances. Existing recovery strategies often struggle with complex task logic reasoning and locomotion robustness evaluation, making them susceptible to failures due to inappropriate recovery strategies or insufficient robustness. To address this issue, the STL-guided framework generates optimal and safe recovery trajectories that simultaneously satisfy the task specification and maximize the locomotion robustness. Our framework outperforms a state-of-the-art locomotion controller in a high-fidelity dynamic simulation, especially in scenarios involving crossed-leg maneuvers. Furthermore, it demonstrates versatility in tasks such as locomotion on stepping stones, where the robot must select from a set of disjointed footholds to maneuver successfully. keywords: {Legged locomotion;Perturbation methods;Predictive models;Robustness;Planning;Logic;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610811&isnumber=10609862

M. Tucker, K. Li and A. D. Ames, "Synthesizing Robust Walking Gaits via Discrete-Time Barrier Functions with Application to Multi-Contact Exoskeleton Locomotion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1136-1142, doi: 10.1109/ICRA57147.2024.10610537.Abstract: Successfully achieving bipedal locomotion remains challenging due to real-world factors such as model uncertainty, random disturbances, and imperfect state estimation. In this work, we propose a novel metric for locomotive robustness – the estimated size of the hybrid forward invariant set associated with the step-to-step dynamics. Here, the forward invariant set can be loosely interpreted as the region of attraction for the discrete-time dynamics. We illustrate the use of this metric towards synthesizing nominal walking gaits using a simulation-in-the-loop learning approach. Further, we leverage discrete-time barrier functions and a sampling-based approach to approximate sets that are maximally forward invariant. Lastly, we experimentally demonstrate that this approach results in successful locomotion for both flat-foot walking and multi-contact walking on the Atalante lower-body exoskeleton. keywords: {Legged locomotion;Measurement;Uncertainty;Exoskeletons;Robustness;State estimation;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610537&isnumber=10609862

S. Fasano, J. Foster, S. Bertrand, C. DeBuys and R. Griffin, "Efficient, Dynamic Locomotion through Step Placement with Straight Legs and Rolling Contacts," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1143-1150, doi: 10.1109/ICRA57147.2024.10611056.Abstract: For humans, fast, efficient walking over flat ground represents the vast majority of locomotion that an individual experiences on a daily basis, and for an effective, real-world humanoid robot the same will likely be the case. In this work, we propose a locomotion controller for efficient walking over near-flat ground using a relatively simple, model-based controller that utilizes a novel combination of several interesting design features including an ALIP-based step adjustment strategy, stance leg length control as an alternative to center of mass height control, and rolling contact for heel-to-toe motion of the stance foot. We then present the results of this controller on our robot Nadia, both in simulation and on hardware. These results include validation of this controller’s ability to perform fast, reliable forward walking at 0.75 m/s along with backwards walking, side-stepping, turning in place, and push recovery. We also present an efficiency comparison between the proposed control strategy and our baseline walking controller over three steady-state walking speeds. Lastly, we demonstrate some of the benefits of utilizing rolling contact in the stance foot, specifically the reduction of necessary positive and negative work throughout the stride. keywords: {Legged locomotion;Humanoid robots;Turning;Hardware;Steady-state;Reliability;Foot},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611056&isnumber=10609862

G. A. Castillo, B. Weng, W. Zhang and A. Hereid, "Data-Driven Latent Space Representation for Robust Bipedal Locomotion Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1172-1178, doi: 10.1109/ICRA57147.2024.10610978.Abstract: This paper presents a novel framework for learning robust bipedal walking by combining a data-driven state representation with a Reinforcement Learning (RL) based locomotion policy. The framework utilizes an autoencoder to learn a low-dimensional latent space that captures the complex dynamics of bipedal locomotion from existing locomotion data. This reduced dimensional state representation is then used as states for training a robust RL-based gait policy, eliminating the need for heuristic state selections or the use of template models for gait planning. The results demonstrate that the learned latent variables are disentangled and directly correspond to different gaits or speeds, such as moving forward, backward, or walking in place. Compared to traditional template model-based approaches, our framework exhibits superior performance and robustness in simulation. The trained policy effectively tracks a wide range of walking speeds and demonstrates good generalization capabilities to unseen scenarios. keywords: {Legged locomotion;Training;Manifolds;Supervised learning;Reinforcement learning;Aerospace electronics;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610978&isnumber=10609862

Q. Zhao et al., "LIKO: LiDAR, Inertial, and Kinematic Odometry for Bipedal Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1180-1185, doi: 10.1109/ICRA57147.2024.10610222.Abstract: High-frequency and accurate state estimation is crucial for biped robots. This paper presents a tightly-coupled LiDAR-Inertial-Kinematic Odometry (LIKO) for biped robot state estimation based on an iterated extended Kalman filter. Beyond state estimation, the foot contact position is also modeled and estimated. This allows for both position and velocity updates from kinematic measurement. Additionally, the use of kinematic measurement results in an increased output state frequency of about 1kHz. This ensures temporal continuity of the estimated state and makes it practical for control purposes of biped robots. We also announce a biped robot dataset consisting of LiDAR, inertial measurement unit (IMU), joint encoders, force/torque (F/T) sensors, and motion capture ground truth to evaluate the proposed method. The dataset is collected during robot locomotion, and our approach reached the best quantitative result among other LIO-based methods and biped robot state estimation algorithms. The dataset and source code will be available at https://github.com/Mr-Zqr/LIKO. keywords: {Visualization;Laser radar;Accuracy;Source coding;Robot sensing systems;Sensors;Frequency measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610222&isnumber=10609862

J. Arrizabalaga, L. Pries, R. Laha, R. Li, S. Haddadin and M. Ryll, "Geometric Slosh-Free Tracking for Robotic Manipulators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1226-1232, doi: 10.1109/ICRA57147.2024.10610813.Abstract: This work focuses on the agile transportation of liquids with robotic manipulators. In contrast to existing methods that are either computationally heavy, system/container specific or dependant on a singularity-prone pendulum model, we present a real-time slosh-free tracking technique. This method solely requires the reference trajectory and the robot’s kinematic constraints to output kinematically feasible joint space commands. The crucial element underlying this approach consists on mimicking the end-effector’s motion through a virtual quadrotor, which is inherently slosh-free and differentially flat, thereby allowing us to calculate a slosh-free reference orientation. Through the utilization of a cascaded proportional-derivative (PD) controller, this slosh-free reference is transformed into task space acceleration commands, which, following the resolution of a Quadratic Program (QP) based on Resolved Acceleration Control (RAC), are translated into a feasible joint configuration. The validity of the proposed approach is demonstrated by simulated and real-world experiments on a 7 DoF Franka Emika Panda robot. keywords: {Liquids;Tracking;Transportation;Kinematics;Aerospace electronics;Real-time systems;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610813&isnumber=10609862

H. Wang, C. Ning, L. Li and W. Zhang, "Online-Learning-Based Distributionally Robust Motion Control with Collision Avoidance for Mobile Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1241-1246, doi: 10.1109/ICRA57147.2024.10610765.Abstract: Collision-free navigation is a critical issue in robotic systems as the environment is often dynamic and uncertain. This paper investigates a data-stream-driven motion control problem for mobile robots to avoid randomly moving obstacles when the probability distribution of the obstacle’s movement is partially observable through data and can be even time-varying. A data-stream-driven ambiguity set is firstly constructed from movement data by leveraging a Dirichlet process mixture model and is updated online using real-time data. Then we propose an Online-Learning-based Distributionally Robust Nonlinear Model Predictive Control (OL-DR-NMPC) approach for limiting the risk of collision through considering the worst-case distribution within the ambiguity set. To facilitate solving the OL-DR-NMPC problem, we reformulate it as a finite-dimensional nonlinear optimization problem. To cope with the bilinear matrix inequality constraints in the nonlinear problem, we develop a parabolic relaxation and a sequential algorithm, by which the problem is further transformed into polynomial-time solvable surrogates. The simulations using a quadrotor model are employed to demonstrate the effectiveness and advantages of the proposed method. keywords: {Navigation;Simulation;Real-time systems;Probability distribution;Safety;Mobile robots;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610765&isnumber=10609862

V. Fortineau, V. Padois and D. Daney, "Prediction of pose errors implied by external forces applied on robots: towards a metric for the control of collaborative robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1247-1253, doi: 10.1109/ICRA57147.2024.10610746.Abstract: The presented work tackles the question of quantifying the pose deviations of robots subject to external disturbance forces. While this question may not be central for large robots perfectly rejecting disturbances through high controller gains, it is an important factor when considering collaborative settings where smaller robots may be deviated from their task because of unmodeled physical interactions. This is all the more true with human-robot collaboration where human capacities may fluctuate over time and have to be compensated by a proper adaptation of the robot control. To move forward in this direction, this work first derives a deviation prediction methodology and exemplifies it using three largely employed control approaches. The proposed prediction method is then validated using simulated and real robot experiments both in single and multiple robots cases. The obtained results constitute a stepping stone towards a quantitative metric for robots adapting their behaviour to human motor fluctuation. keywords: {Measurement;Fluctuations;Friction;Robot control;Linear approximation;Prediction methods;Motors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610746&isnumber=10609862

W. Cheng, A. N. Foon Chan and D. Lau, "Iterative Learning Control for Deformable Open-Frame Cable-Driven Parallel Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1254-1260, doi: 10.1109/ICRA57147.2024.10610529.Abstract: This paper proposed an iterative learning control (ILC) scheme for deformable open-frame cable-driven parallel robots (D-CDPRs). In contrast to the straightforward inverse kinematics of the rigid frame cable-driven parallel robots (CDPRs), accurate modeling of the deformable frame poses challenges due to errors and uncertainties. To address these issues, the authors propose the use of ILC, a control strategy that modifies the control input over iterations based on previous results. ILC has been successfully applied to traditional cable robots, particularly in handling model uncertainty. The paper presents a novel ILC control scheme specifically designed for D-CDPRs, with a focus on reducing tracking errors over repetitive operations. Additionally, hardware experiments are conducted to validate the effectiveness and reliability of the proposed ILC approach. The results demonstrate the efficacy of ILC in mitigating tracking errors, even in scenarios where the dynamic model of the D-CDPRs is unknown. keywords: {Deformable models;Parallel robots;Uncertainty;Accuracy;Kinematics;Hardware;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610529&isnumber=10609862

G. He, G. Feng and B. Ding, "A Study of Force-Free Control Framework for Industrial Manipulator Tasks Based on High-Pass Filter," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1261-1267, doi: 10.1109/ICRA57147.2024.10610526.Abstract: Force-free control (FFC) allows for flexible manipulator motion in response to external forces, making it a vital component of human-robot interaction (HRI). Manual intervention may cause uneven forces on the manipulator or frequencies close to the natural frequency, and mechanical resonance can occur due to the inertia of the manipulator and adjustable equivalent stiffness of the controller. This paper proposes an FFC approach for industrial manipulators using a six-axis force/torque sensor (F/T sensor), implemented through a three-layer control architecture, consisting of motion control layer, Admittance Control layer and force decoupling layer. To mitigate the effects of mechanical resonance, a high-pass filter (HPF) is integrated with the F/T sensor and its impact is investigated. Experimental validation is conducted using both a simulation model and an industrial manipulator. Test results indicate that the proposed FFC architecture enables the manipulator not only to interact smoothly with external forces, but also to distinguish load forces at different frequencies and potentially address the issue of mechanical resonance between the manipulator and the applied load forces. keywords: {Resonant frequency;Human-robot interaction;Manuals;Manipulators;Robot sensing systems;Resonance;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610526&isnumber=10609862

M. Sommersperger, S. Dehghani, P. Matten, H. Roodaki and N. Navab, "Uncertainty-Aware Contextual Visualization for Human Supervision of OCT-Guided Autonomous Robotic Subretinal Injection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1268-1275, doi: 10.1109/ICRA57147.2024.10610319.Abstract: The injection of therapeutic agents into the sub-retinal space might allow improved treatment of age-related macular degeneration. Various robotic systems have been developed to achieve the required precision and, in combination with intraoperative Optical Coherence Tomography (iOCT) imaging, methods for autonomous robotic guidance have been proposed. In such systems, the robot’s cognition is often governed by machine learning algorithms, such as convolutional neural networks (CNNs), which provide semantic scene information from iOCT images. Although the robot performs a surgical task autonomously, human supervision is critical to monitor the robot’s execution and, if necessary, stop the robot or take control to avoid trauma to the patient. In this paper, we propose a novel visualization concept for improved human supervision of autonomous robotic subretinal injection that integrates uncertainty information of the data provided to the robot. We design a focus and context visualization that renders an automatically identified instrument-aligned B-scan in the context of the 3D OCT volume. Our visualization is enriched by augmenting the uncertainty information on the instrument-aligned B-scan. To dynamically model task-specific uncertainty, we introduce a weighting scheme to assign an importance factor to each pair of classes, controlling the impact of their confusion on the overall uncertainty. We demonstrate our visualization concept on iOCT volumes acquired at different stages during subretinal injection on ex-vivo porcine eyes. We show that our processing pipeline achieves sufficient update rates for surgical display and discuss the impact of our visualization concept on the acceptance of robotic task autonomy for subretinal injection procedures. keywords: {Uncertainty;Three-dimensional displays;Instruments;Pipelines;Semantics;Data visualization;Rendering (computer graphics);Medical Robots and Systems;Acceptability and Trust;Safety in HRI},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610319&isnumber=10609862

S. He, Y. Zhang, B. Huang, J. Lin, C. Shi and C. Hu, "A Track-based Colon Endoscopic Robot with Depth Perception Stereo Cameras for Haustral Fold Detection during Colonic Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1276-1282, doi: 10.1109/ICRA57147.2024.10611617.Abstract: Colon endoscopic robots represent a promising screening modality for the visualization of colon cancers with high sensitivity. However, current colonoscopy robots are often characterized by intricate and bulky mechanical structures, which pose practical challenges when moving through the complex and narrow environment of the colon. Moreover, these robots are typically equipped with a single camera, limiting their ability to accurately estimate the depth of haustral folds in the colon, which is of great importance for the active colonic navigation of the robots. To address these challenges, we develop a track-based stereoscopic endoscopic robot (TSER) which is equipped with four tracks positioned at the corners of its body. This innovative design maximizes the contact between the tracks and the colon wall, enhancing maneuverability. The tracks are constructed from de-molded polydimethylsiloxane (PDMS) and incorporate micro-patterns on their outer surfaces. We have proposed a straightforward strategy for detecting haustral folds using TSER’s stereo camera, which allows for precise identification of their position and depth. The TSER achieves an average motion speed of 9.8 mm/s in a bellows tube that contains silicone oil and a speed of 5.2 mm/s in an exvivo porcine intestinal segment. Impressively, the TSER boasts an 88.11% accuracy rate in haustral fold depth estimation, surpassing the performance of existing geometric shape fitting methods. These results demonstrate that the TSER holds great potential for effective and efficient movement and inspection within the colon, offering a promising solution for improved colon cancer screening. keywords: {Visualization;Sensitivity;Tracking;Navigation;Shape;Stereo image processing;Oils},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611617&isnumber=10609862

L. Beber, E. Lamon, D. Nardi, D. Fontanelli, M. Saveriano and L. Palopoli, "A Passive Variable Impedance Control Strategy with Viscoelastic Parameters Estimation of Soft Tissues for Safe Ultrasonography," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1298-1304, doi: 10.1109/ICRA57147.2024.10610167.Abstract: In the context of telehealth, robotic approaches have proven a valuable solution to in-person visits in remote areas, with decreased costs for patients and infection risks. In particular, in ultrasonography, robots have the potential to reproduce the skills required to acquire high-quality images while reducing the sonographer’s physical efforts. In this paper, we address the control of the interaction of the probe with the patient’s body, a critical aspect of ensuring safe and effective ultrasonography. We introduce a novel approach based on variable impedance control, allowing the real-time optimisation of compliant controller parameters during ultrasound procedures. This optimisation is formulated as a quadratic programming problem and incorporates physical constraints derived from viscoelastic parameter estimations. Safety and passivity constraints, including an energy tank, are also integrated to minimise potential risks during human-robot interaction. The proposed method’s efficacy is demonstrated through experiments on a patient’s dummy torso, highlighting its potential for achieving safe behaviour and accurate force control during ultrasound procedures, even in cases of contact loss. keywords: {Torso;Parameter estimation;Tracking;Telemedicine;Ultrasonography;Safety;Impedance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610167&isnumber=10609862

Y. Velikova, M. F. Azampour, W. Simson, M. Esposito and N. Navab, "Implicit Neural Representations for Breathing-compensated Volume Reconstruction in Robotic Ultrasound," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1316-1322, doi: 10.1109/ICRA57147.2024.10611443.Abstract: Ultrasound (US) imaging is widely used in diagnosing and staging abdominal diseases due to its lack of non-ionizing radiation and prevalent availability. However, significant inter-operator variability and inconsistent image acquisition hinder the widespread adoption of extensive screening programs. Robotic ultrasound systems have emerged as a promising solution, offering standardized acquisition protocols and the possibility of automated acquisition. Additionally, these systems enable access to 3D data via robotic tracking, enhancing volumetric reconstruction for improved ultrasound interpretation and precise disease diagnosis.However, the interpretability of 3D US reconstruction of abdominal images can be affected by the patient’s breathing motion. This study introduces a method to compensate for breathing motion in 3D US compounding by leveraging implicit neural representations. Our approach employs a robotic ultrasound system for automated screenings. To demonstrate the method’s effectiveness, we evaluate our proposed method for the diagnosis and monitoring of abdominal aorta aneurysms as a representative use case.Our experiments demonstrate that our proposed pipeline facilitates robust automated robotic acquisition, mitigating artifacts from breathing motion, and yields smoother 3D reconstructions for enhanced screening and medical diagnosis. keywords: {Ultrasonic imaging;Three-dimensional displays;Protocols;Ultrasonic variables measurement;Tracking;Pipelines;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611443&isnumber=10609862

J. Yang, Z. Zhao, M. Maier, K. Huang, N. Navab and M. Ali Nasseri, "Shadow-Based 3D Pose Estimation of Intraocular Instrument Using Only 2D Images," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1323-1329, doi: 10.1109/ICRA57147.2024.10611011.Abstract: In ophthalmic surgeries, such as vitreoretinal operations, surgeons rely on imaging systems, primarily microscopes, for real-time instrument monitoring and motion planning. However, novice surgeons struggle to extract 3D instrument positions from 2D microscope frames, necessitating extensive trial-and-error experience with the background that additional imaging modalities such as iOCT remain inaccessible in most operating rooms. Targeting intraocular assessment within the current surgical setup, this paper presents an imagebased pose estimation method to obtain real-time instrument tip positions in a standard 12mm-radius spherical eyeball model, which links floating instruments with on-the-retinal objects based on the intraocular shadowing principle. We validate this estimation method in a Unity simulator and verify its depth estimation capability using a specially designed eyeball phantom. Both simulator and phantom experiments demonstrate an average needle-tip estimation error within [1.0, 2.0] mm using only 2D microscope frames. keywords: {Three-dimensional displays;Shape;Microscopy;Pose estimation;Surgery;Phantoms;Retina},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611011&isnumber=10609862

M. Merlin et al., "Robot Task Planning Under Local Observability," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1362-1368, doi: 10.1109/ICRA57147.2024.10610876.Abstract: Real-world robot task planning is intractable in part due to partial observability. A common approach to reducing complexity is introducing additional structure into the decision process, such as mixed-observability, factored states, or temporally-extended actions. We propose the locally observable Markov decision process, a novel formulation that models task-level planning where uncertainty pertains to object-level attributes and where a robot has subroutines for seeking and accurately observing objects. This models sensors that are range-limited and line-of-sight—objects occluded or outside sensor range are unobserved, but the attributes of objects that fall within sensor view can be resolved via repeated observation. Our model results in a three-stage planning process: first, the robot plans using only observed objects; if that fails, it generates a target object that, if observed, could result in a feasible plan; finally, it attempts to locate and observe the target, replanning after each newly observed object. By combining LOMDPs with off-the-shelf Markov planners, we outperform state-of-the-art-solvers for both object-oriented POMDP and MDP analogues with the same task specification. We then apply the formulation to successfully solve a task on a mobile robot. keywords: {Uncertainty;Simultaneous localization and mapping;Navigation;Object oriented modeling;Sensor phenomena and characterization;Planning;Space exploration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610876&isnumber=10609862

C. Wu, R. Wang, M. Song, F. Gao, J. Mei and B. Zhou, "Real-time Whole-body Motion Planning for Mobile Manipulators Using Environment-adaptive Search and Spatial-temporal Optimization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1369-1375, doi: 10.1109/ICRA57147.2024.10610192.Abstract: Mobile manipulators have recently gained significant attention in the robotics community due to their superior potential in industrial and service applications. However, the high degree of freedom associated with mobile manipulators poses challenges in achieving real-time whole-body motion planning. To bridge the gap, this paper presents a motion planning method capable of generating high-quality, safe, agile and feasible trajectories for mobile manipulators in real time. First, we present a novel environment-adaptive path searching method, which can generate paths in real-time in various environments by adaptively adjusting searching dimension based on environment complexity. Additionally, we propose a real-time spatial-temporal trajectory optimization method that takes into account the whole-body safety, agility and dynamic feasibility of mobile manipulators. Moreover, task constraints are applied to ensure that the trajectory can fulfill specific task requirements. Simulation and real-world experiments demonstrate that our method is capable of generating whole-body trajectories in real-time in challenging environments. We will release our code to benefit the community. keywords: {Service robots;Dynamics;Transportation;Real-time systems;Planning;Safety;Complexity theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610192&isnumber=10609862

D. Rakovitis and D. Mronga, "Gaussian Mixture Likelihood-based Adaptive MPC for Interactive Mobile Manipulators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1392-1398, doi: 10.1109/ICRA57147.2024.10610115.Abstract: Mobile robots are nowadays frequently used for interaction tasks in the real world, e.g. for opening doors or for pick-and-place tasks. When used in real-world environments, adapting the robot controllers to uncertain contact dynamics is a significant challenge. Adaptive Model Predictive Control (AMPC) is an approach for controlling robot motions while adapting to uncertain or changing dynamics. However, most of the existing AMPC approaches used in mobile manipulation require either expert tuning or extensive training, making it very difficult to introduce novel or diverse tasks. In addition, the adjustment of several, independent environment parameters is usually not considered in the AMPC formulation. In this work, we introduce a hierarchical approach that uses Gaussian Mixture Models (GMMs) and Gaussian Mixture Regression (GMR) to predict the dynamic model parameters of MPC based on proprioceptive measurements and perform tasks with multiple unknown environmental parameters. The approach is evaluated in simulation and in real experiments on a mobile manipulator and compared to several baseline methods. It is shown that it outperforms standard MPC and an existing AMPC approach on several tasks such as carrying, pushing, and door opening. keywords: {Training;Robot motion;Dynamics;Propioception;Predictive models;Mobile robots;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610115&isnumber=10609862

J. Zhang et al., "GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1399-1405, doi: 10.1109/ICRA57147.2024.10610125.Abstract: Mobile manipulation constitutes a fundamental task for robotic assistants and garners significant attention within the robotics community. A critical challenge inherent in mobile manipulation is the effective observation of the target while approaching it for grasping. In this work, we propose a graspability-aware mobile manipulation approach powered by an online grasping pose fusion framework that enables a temporally consistent grasping observation. Specifically, the predicted grasping poses are online organized to eliminate the redundant, outlier grasping poses, which can be encoded as a grasping pose observation state for reinforcement learning. Moreover, on-the-fly fusing the grasping poses enables a direct assessment of graspability, encompassing both the quantity and quality of grasping poses. This assessment can subsequently serve as an observe-to-grasp reward, motivating the agent to prioritize actions that yield detailed observations while approaching the target object for grasping. Through extensive experiments conducted on the Habitat and Isaac Gym simulators, we find that our method attains a good balance between observation and manipulation, yielding high performance under various grasping metrics. Furthermore, we discover that the incorporation of temporal information from grasping poses aids in mitigating the sim-to-real gap, leading to robust performance in challenging real-world experiments. Project page: https://pku-epic.github.io/GAMMA/ keywords: {Measurement;Accuracy;Fuses;Habitats;Grasping;Reinforcement learning;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610125&isnumber=10609862

Q. Li, Q. Meng, Y. Qin, J. Chen, X. Ding and K. Xu, "Dynamic Interaction Control in Legged Mobile Manipulators: A Decoupled Approach," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1406-1412, doi: 10.1109/ICRA57147.2024.10611344.Abstract: Legged mobile manipulators are receiving much more attention. Mobile platforms can infinitely expand the workspace of robotic arms, providing more possibilities for robot application scenarios. Compared with wheeled mobile manipulators, legged mobile manipulators have higher requirements for cooperative control of legged robots and robotic arms. This work decouples the control of the robotic arm and the legged robot. On the legged robot side, we explicitly estimate the wrench exerted by the robotic arm on the base and bring it into the legged robot’s dynamics, and then use a nonlinear model predictive controller (NMPC) to control the legged robot. On the robotics arm side, we adopt an impedance controller to realize the end-effector’s force control, and the introduction of impedance control has improved the safety and interactivity of legged mobile manipulators. We conducted experiments on physical robot to compare the differences between decoupled control and independent control, and the results show that the stability and robustness of robot systems have improved using decoupled control. keywords: {Legged locomotion;Force;Dynamics;Manipulators;Robustness;Safety;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611344&isnumber=10609862

S. Jauhri, S. Lueth and G. Chalvatzaki, "Active-Perceptive Motion Generation for Mobile Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1413-1419, doi: 10.1109/ICRA57147.2024.10610714.Abstract: Mobile Manipulation (MoMa) systems incorporate the benefits of mobility and dexterity, due to the enlarged space in which they can move and interact with their environment. However, even when equipped with onboard sensors, e.g., an embodied camera, extracting task-relevant visual information in unstructured and cluttered environments, such as households, remains challenging. In this work, we introduce an active perception pipeline for mobile manipulators to generate motions that are informative toward manipulation tasks, such as grasping in unknown, cluttered scenes. Our proposed approach, ActPerMoMa, generates robot paths in a receding horizon fashion by sampling paths and computing path-wise utilities. These utilities trade-off maximizing the visual Information Gain (IG) for scene reconstruction and the task-oriented objective, e.g., grasp success, by maximizing grasp reachability. We show the efficacy of our method in simulated experiments with a dual-arm TIAGo++ MoMa robot performing mobile grasping in cluttered scenes with obstacles. We empirically analyze the contribution of various utilities and parameters, and compare against representative baselines both with and without active perception objectives. Finally, we demonstrate the transfer of our mobile grasping strategy to the real world, indicating a promising direction for active-perceptive MoMa. keywords: {Visualization;Robot kinematics;Robot vision systems;Pipelines;Active perception;Grasping;Performance gain},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610714&isnumber=10609862

G. Bellegarda, M. Shafiee and A. Ijspeert, "Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided Quadruped Locomotion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1420-1427, doi: 10.1109/ICRA57147.2024.10611128.Abstract: We present a framework for learning visually-guided quadruped locomotion by integrating exteroceptive sensing and central pattern generators (CPGs), i.e. systems of coupled oscillators, into the deep reinforcement learning (DRL) framework. Through both exteroceptive and proprioceptive sensing, the agent learns to coordinate rhythmic behavior among different oscillators to track velocity commands, while at the same time override these commands to avoid collisions with the environment. We investigate several open robotics and neuroscience questions: 1) What is the role of explicit interoscillator couplings between oscillators, and can such coupling improve sim-to-real transfer for navigation robustness? 2) What are the effects of using a memory-enabled vs. a memory-free policy network with respect to robustness, energy-efficiency, and tracking performance in sim-to-real navigation tasks? 3) How do animals manage to tolerate high sensorimotor delays, yet still produce smooth and robust gaits? To answer these questions, we train our perceptive locomotion policies in simulation and perform sim-to-real transfers to the Unitree Go1 quadruped, where we observe robust navigation in a variety of scenarios. Our results show that the CPG, explicit interoscillator couplings, and memory-enabled policy representations are all beneficial for energy efficiency, robustness to noise and sensory delays of 90 ms, and tracking performance for successful sim-to-real transfer for navigation tasks. keywords: {Couplings;Navigation;Robot kinematics;Robot sensing systems;Robustness;Delays;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611128&isnumber=10609862

K. King and S. Revzen, "Self-Righting Shell for Robotic Hexapod," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1436-1442, doi: 10.1109/ICRA57147.2024.10610914.Abstract: Decimeter scale robots in human environments are small relative to obstacles they encounter, making them prone to flipping over and needing to self-right. We present a multifaceted shell that by its geometry alone enables the hexapedal robot MediumANT to passively self-right without the need for additional sensory feedback. We designed the shell by specifying the cross-sectional geometry in the yz and xy planes such that the robot returns to an upright position by rolling around the longitudinal (x) axis, and then tweaked the design to reduce the number of faces. We then attached the shell to the robot by modifying some of its chassis structural plates to extend to and support the shell. We evaluated the effectiveness of the shell in two experimental scenarios: passive righting – balancing the robot on each face of the shell before releasing the robot – and an intentional fall – walking the robot off a ledge at various approach angles. As intended by our design, the robot recovered the upright orientation from all starting faces in the passive righting test and righted itself and continued walking in all falling trials. This work presents an example of using biologically inspired simplicity to solve what would otherwise be a technically challenging problem. keywords: {Legged locomotion;Geometry;Structural plates;Robot sensing systems;Biology;Faces;self-righting;multi-legged;robot},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610914&isnumber=10609862

G. Bellegarda, M. Shafiee, M. E. Özberk and A. Ijspeert, "Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1443-1450, doi: 10.1109/ICRA57147.2024.10610141.Abstract: Legged robots are becoming increasingly agile in exhibiting dynamic behaviors such as running and jumping. Usually, such behaviors are either optimized and engineered offline (i.e. the behavior is designed for before it is needed), either through model-based trajectory optimization, or through deep learning-based methods involving millions of timesteps of simulation interactions. Notably, such offline-designed locomotion controllers cannot perfectly model the true dynamics of the system, such as the motor dynamics. In contrast, in this paper, we consider a quadruped jumping task that we rapidly optimize online. We design foot force profiles parameterized by only a few parameters which we optimize for directly on hardware with Bayesian Optimization. The force profiles are tracked at the joint level, and added to Cartesian PD impedance control and Virtual Model Control to stabilize the jumping motions. After optimization, which takes only a handful of jumps, we show that this control architecture is capable of diverse and omnidirectional jumps including forward, lateral, and twist (turning) jumps, even on uneven terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward, and jump-turn over 2 rad. keywords: {Force;Dynamics;Motors;Turning;Hardware;Biology;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610141&isnumber=10609862

D. Bright, S. Shield and A. Patel, "AeroDima: Cheetah-Inspired Aerodynamic Tail Design for Rapid Maneuverability," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1451-1456, doi: 10.1109/ICRA57147.2024.10610202.Abstract: Scientists have long theorized that the cheetah’s tail contributes to its impressive maneuvrability at high speeds by stabilizing its body. This has inspired the design of several agile robots, including Dima - a wheeled platform that used cheetah-inspired inertial tail swings to better execute rapid acceleration and turning motions. Subsequent research suggests that the effectiveness of the cheetah’s tail might be enhanced by aerodynamic effects. In this paper, we introduce AeroDima: a follow-up to the original Dima design that uses aerodynamic drag on the tail as the primary mechanism for generating the stabilizing torque. The resulting sail-like tail is substantially lighter than the original, but still improves the performance of the platform, allowing it to enter turns at a higher speed without toppling. While the yaw rate of the robot was actually higher without the tail, the tail substantially reduced unwanted roll, confirming that this appendage increases maneuvrability by increasing stability, rather than by directly contributing to lateral acceleration. keywords: {Torque;Tail;Aerodynamics;Turning;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610202&isnumber=10609862

J. Wang, J. Cheng, J. Hu, W. Gao and S. Zhang, "Spined Torso Renders Advanced Mobility for Quadrupedal Locomotion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1457-1463, doi: 10.1109/ICRA57147.2024.10610113.Abstract: Animals possessing spinal columns often exhibit exceptional agility for highly dynamic locomotion. The spine grants the trunk with increased degrees of freedom, thereby endowing diverse postures. This paper presents the development of a robot STRAY for quadrupedal locomotion, featuring a four-degree-of-freedom spine design. Using trajectory based reinforcement learning techniques, STRAY is able to trot and bound dynamically using its spine. Simulation results reveal the positive roles of spinal movement, such as twisting, extension, retraction and rotation, in helping STRAY realize efficient locomotion. Preliminary results from experiments demonstrate that STRAY can achieve a trotting gait of approximately 0.6 m/s and a bounding gait of 0.7 m/s, with desired velocities of 0.8 m/s and 1.0 m/s, respectively. The results also indicate that reinforcement learning is a feasible way to investigate how the spine should be used in dynamic quadrupedal locomotion and achieve more possibilities in the future. keywords: {Legged locomotion;Torso;Animals;Simulation;Dynamics;Reinforcement learning;Motors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610113&isnumber=10609862

Y. Pan, R. A. I. Khan, C. Zhang, A. Zhang and H. Shang, "Pegasus: a Novel Bio-inspired Quadruped Robot with Underactuated Wheeled-Legged Mechanism *," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1464-1469, doi: 10.1109/ICRA57147.2024.10611633.Abstract: This paper presents the design and analysis of Pegasus, a quadrupedal wheeled robot grounded in biomimicry principles. Pegasus offers two distinct motion modes, including a wheeled motion and a hybrid wheeled-legged motion, enabling adaptability across various tasks and environmental conditions. The robot draws inspiration from the joint structures of quadruped animals and incorporates biomimetic features. At the robot’s ankle joint, we imitate the articulation of a radiusulna joint to enhance the wheeled motion’s agility. Additionally, we establish comprehensive mathematical models for adaptive dynamics model, providing a robust theoretical foundation for subsequent motion planning and high-precision control. A novel telescopic vehicle mode is also proposed for complex wheel-leg hybrid motion, offering optimized solutions for intricate robot locomotion. Furthermore, we employ parallel underactuated MPC controllers for each leg at the control level, contributing to heightened motion precision and stability. Extensive validation through physical platform experiments highlights the effectiveness and feasibility of the proposed controllers, offering substantial support for real-world applications in robotics. keywords: {Legged locomotion;Biomimetics;Dynamics;Mathematical models;Stability analysis;Planning;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611633&isnumber=10609862

J. Lu et al., "LeapRun: A Dynamic Soft Robot with Running and Jumping Capabilities," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1470-1476, doi: 10.1109/ICRA57147.2024.10610431.Abstract: In the natural world, insects exhibit remarkable locomotion capabilities through a combination of running and jumping. However, replicating this versatile locomotion in a soft robot poses technical and design complexities. Here, we propose a dynamic soft robot named LeapRun that possesses agile locomotion and the ability to perform continuous jumping. To achieve this, a prototype soft robot (weight of 300 mg, size of 30 mm × 15 mm × 5 mm), composed of piezoelectric thin film, shape memory alloy, magnet-locking mechanism, and corresponding support structures, is fabricated. Experimental results demonstrate a maximum moving speed of 15 cm/s and a maximum jumping height of 8.7 cm. Continuous jumping of steps and crossing of complex rugged surfaces are realized. Besides, integrated with the power source, wireless communication module, and control module, the untethered operation is also presented, showcasing the potential for multiple applications in search and rescue, exploration, and monitoring. keywords: {Wireless communication;Magnetic films;Shape memory alloys;Insects;Prototypes;Soft robotics;Complexity theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610431&isnumber=10609862

S. Even, H. Gordon, H. Yang and Y. Ozkan-Aydin, "Machine Learning-Driven Burrowing with a Snake-Like Robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1477-1483, doi: 10.1109/ICRA57147.2024.10610264.Abstract: Subterranean burrowing is inherently difficult for robots because of the high forces experienced as well as the high amount of uncertainty in this domain. Because of the difficulty in modeling forces in granular media, we propose the use of a novel machine-learning control strategy to obtain optimal techniques for vertical self-burrowing. In this paper, we realize a snake-like bio-inspired robot that is equipped with an IMU and two triple-axis magnetometers. Utilizing magnetic field strength as an analog for depth, a novel deep learning architecture was proposed based on sinusoidal and random data in order to obtain a more efficient strategy for vertical self-burrowing. This strategy was able to outperform many other standard burrowing techniques and was able to automatically reach targeted burrowing depths. We hope these results will serve as a proof of concept for how optimization can be used to unlock the secrets of navigating in the subterranean world more efficiently. keywords: {Training;Uncertainty;Quantization (signal);Neural networks;Training data;Media;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610264&isnumber=10609862

P. Spino and D. Rus, "Towards Centimeter-Scale Underwater Mobile Robots: An Architecture for Capable µAUVs," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1484-1490, doi: 10.1109/ICRA57147.2024.10610474.Abstract: Underwater robots are indispensable for aquatic exploration, yet their size and complexity often limit broader application. This research presents a pioneering micro autonomous underwater vehicle (µAUV) design. This robot is distinguished by its utilization of mass-produced drone components, novel jet propulsion mechanisms, and multifunctional spherical shell. Its architecture is modular, appendage-free, and largely seal-free. Preliminary tests highlight its motion capabilities and set new benchmarks for centimeter-scale µAUV advancements. keywords: {Structural rings;Autonomous underwater vehicles;Propulsion;Benchmark testing;Robot sensing systems;Sensors;Complexity theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610474&isnumber=10609862

X. Chao, I. Hameed, D. Navarro-Alarcon and X. Jing, "Untethered Bimodal Robotic Fish with Tunable Bistability," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1491-1497, doi: 10.1109/ICRA57147.2024.10610967.Abstract: In nature, fish are excellent swimmers due to their flexible and precise control of tail, which allows them to freely transform between the smooth flapping and the motion of rapid response so that they can move with dexterity. Here, inspired by the versatile motion abilities of fish, a novel robotic fish has been developed, featuring the capability of adaptable bistability. Through tuning the bistability, the robot can acquire two locomotion modes, namely monostable and bistable modes, and it can also swim at different energy barrier that needs to be overcome to realize the bistable motion. The theoretical models are derived to facilitate the control of the robot and the understanding of its nonlinear behavior. The impact of the tunable bistability on the swimming and turning performance is investigated through extensive experiments. The study effectively demonstrates the robotic fish’s capability to swiftly and efficiently navigate through mode switches, enabled by its tunable bistability. This feature is essential for underwater robots to perform tasks in intricate environments. keywords: {Navigation;Transforms;Tail;Fish;Turning;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610967&isnumber=10609862

C. M. Sourkounis, T. Kwasnitschka and A. Raatz, "Tendon-Driven Continuum Robot for Deep-Sea Application," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1498-1504, doi: 10.1109/ICRA57147.2024.10611177.Abstract: The extreme conditions of the deep sea require the use of large and expensive diving robots designed to withstand the high pressure in these depths. In order to reduce the costs for sediment sampling in the deep sea and thus facilitate the explorations of rare deep-sea ecosystems, the goal of this research is to design an alternative manipulator for deep-sea suction sampling. Instead of relying on heavy hydraulic rigid manipulators that deep-sea diving robots are commonly equipped with, we introduce a new concept for a lightweight actuation system that can be used in combination with a traditional diving robot and a suction sampling system. The proposed concept consists of a series of rigid links connected by angled swivel joints. Each segment is actuated by tendons, which allows for continuous bending. The system can be adapted to various sizes of host systems, and the links and joints are printed in place, simplifying the manufacturing process. keywords: {Manufacturing processes;Costs;Ecosystems;Hydraulic systems;Bending;Manipulators;Sediments},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611177&isnumber=10609862

M. Rosette et al., "WAVE: An open-source underWater Arm-Vehicle Emulator," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1505-1511, doi: 10.1109/ICRA57147.2024.10611358.Abstract: Underwater vehicle manipulator systems (UVMS) are increasingly popular platforms for performing subsea operations that require precision manipulation. While there is high demand for fully autonomous or even semi-autonomous systems, most UVMS still require human support teams. Developing new hardware and algorithms for autonomous underwater manipulation is challenging. Simulations do not capture the full complexity of the underwater environment, and deploying a UVMS at sea for testing/validation is resource-intensive and expensive. In this paper, we present a physical testbed for underwater manipulation that bridges the gap between simulation and full field trials. The underWater Arm-Vehicle Emulator (WAVE) is a 10-degree of freedom system designed to replicate an inspection-class UVMS. WAVE includes an underwater perception sensor and has 2 operating modes: rigid or passive-mode. In passive-mode, the ROV body can pitch similar to how a dynamically-coupled underactuated UVMS without pitch control would rotate during manipulation tasks. To validate the overall design and passive pitch concept, we evaluated the testbed during underwater experiments in energetic conditions at a wave basin. To support continued research and development in underwater robotics, we make the design open-access and freely available to the community. keywords: {Couplings;Dynamics;Robot sensing systems;Hardware;End effectors;Complexity theory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611358&isnumber=10609862

S. M. Lee, J. Liu, J. L. Chien, W. H. Ng, M. Lim and S. Foong, "Rapid Resistography with Passive Overhead-perching Mechanism in an Unmanned Aerial System for Wood Structure Inspection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1554-1560, doi: 10.1109/ICRA57147.2024.10611159.Abstract: This paper presents an aerial robotic platform for rapid remote elevated overhead-perching drill operations for wood health inspection. The platform features an innovative passive prismatic-gripper mechanism affixed to the aerial robot’s top, facilitating overhead drilling. The primary aim is to enhance the safety and efficiency of elevated wood structure inspection using the resistography method, which involves drilling into wooden structures to identify internal voids. The research centers on two key enabling technologies: a gripper mechanism for secure attachment to target surfaces and a tethered drill configuration for drilling operations. The novel gripper mechanism enables drilling on large planar surfaces and even small beam-width structures. The paper concludes with discussions on design simulations and drill resistance experiments, highlighting the effectiveness of the proposed approach in detecting internal cavities within wooden structures. keywords: {Drilling;Surface resistance;Inspection;Safety;Grippers;Robots;Resistography;Wood structure inspection;Tethered Aerial Robotics;Prismatic Gripper Mechanism;Internal cavity detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611159&isnumber=10609862

Y. Yuan and M. Ryll, "Dual Quaternion Control of UAVs with Cable-suspended Load," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1561-1567, doi: 10.1109/ICRA57147.2024.10610170.Abstract: Modeling the kinematics and dynamics of robotics systems with suspended loads using dual quaternions has not been explored so far. This paper introduces a new innovative control strategy using dual quaternions for UAVs with cable-suspended loads, focusing on the sling load lifting and tracking problems. By utilizing the mathematical efficiency and compactness of dual quaternions, a unified representation of the UAV and its suspended load’s dynamics and kinematics is achieved, facilitating the realization of load lifting and trajectory tracking. The simulation results have tested the proposed strategy’s accuracy, efficiency, and robustness. This study makes a substantial contribution to present this novel control strategy that harnesses the benefits of dual quaternions for cargo UAVs. Our work also holds promise for inspiring future innovations in under-actuated systems control using dual quaternions. keywords: {Technological innovation;Uncertainty;Trajectory tracking;Quaternions;Simulation;Kinematics;Autonomous aerial vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610170&isnumber=10609862

X. Guo, G. He, M. Mousaei, J. Geng, G. Shi and S. Scherer, "Aerial Interaction with Tactile Sensing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1576-1582, doi: 10.1109/ICRA57147.2024.10611282.Abstract: While the field of autonomous Uncrewed Aerial Vehicles (UAVs) has grown rapidly, most applications only focus on passive visual tasks. Aerial interaction aims to execute tasks involving physical interactions, which offers a way to assist humans in high-altitude and high-risk operations. Tactile sensors, being both cost-effective and lightweight, are capable of sensing contact information including force distribution, as well as recognizing local textures. In this paper, we pioneer the use of vision-based tactile sensors on fully actuated UAVs in dynamic aerial manipulation tasks. We introduce a pipeline utilizing tactile feedback for force tracking via a hybrid motion-force controller and a method for wall texture detection during aerial interactions. Our experiments demonstrate that our system can effectively replace or complement traditional force/torque (F/T) sensors. Compared with only using the F/T sensor, our approach offers two solutions: substitution with tactile sensing, achieving comparable flight performance, or integration of tactile sensing with F/T sensor feedback, leading to around 16% improvement in position tracking accuracy. Our algorithm achieves 93.4% accuracy in real-time texture recognition, which further escalates to 100% in post-contact analysis. To the best of our knowledge, this is the first work to incorporate a vision-based tactile sensor into aerial interaction tasks. keywords: {Visualization;Accuracy;Tracking;Force;Pipelines;Tactile sensors;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611282&isnumber=10609862

W. Yan, G. Chen, Z. Zhang and H. Wang, "A Meter-scale Ornithopter Capable of Jumping Take-off," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1583-1589, doi: 10.1109/ICRA57147.2024.10610300.Abstract: Flapping wing air vehicles(FWAV) or ornithopters are bio-inspired aerial robots that mimic the flying principles of insects and birds. Autonomous take-off is an important capability for FWAV to enhance its performance and extend its working time, which is equipped by almost every kind of bird. As a common method of take-off for birds, jumping take-off has a great ability to adapt to different terrain and high energy efficiency compared with running and rotor-based take-off. Despite recent research, there is no FWAV capable of jumping take-off to this day. In this paper, we present a process to realize the jumping take-off of a meter-scale FWAV from flat ground. To lower the mechanical complexity, we eliminate the design of traditional robotic legs. Instead, we realize steady standing through a tripod-like structure that consists of two wings and a jumping mechanism. Two flapping wings are directly driven by two independent servos. Three carbon fiber springs are employed to build a lightweight jumping module with high elastic energy. We build the dynamic model to analyze the aerodynamic effect during the jumping phase and realize a stable transition to flapping flight. This work lays the foundation for outdoor flight without human assistance. keywords: {Legged locomotion;Insects;Birds;Aerodynamics;Energy efficiency;Complexity theory;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610300&isnumber=10609862

D. Lee, S. Hwang, J. Byun, S. J. Lee and H. Jin Kim, "Autonomous aerial perching and unperching using omnidirectional tiltrotor and switching controller," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1590-1596, doi: 10.1109/ICRA57147.2024.10610445.Abstract: Aerial unperching of multirotors has received little attention as opposed to perching that has been investigated to elongate operation time. This study presents a new aerial robot capable of both perching and unperching autonomously on/from a ferromagnetic surface during flight, and a switching controller to avoid rotor saturation and mitigate overshoot during transition between free-flight and perching. To enable stable perching and unperching maneuvers on/from a vertical surface, a lightweight (≈ 1 kg), fully actuated tiltrotor that can hover at 90◦ pitch angle is first developed. We design a perching/unperching module composed of a single servomotor and a magnet, which is then mounted on the tiltrotor. A switching controller including exclusive control modes for transitions between free-flight and perching is proposed. Lastly, we propose a simple yet effective strategy to ensure robust perching in the presence of measurement and control errors and avoid collisions with the perching site immediately after unperching. We validate the proposed framework in experiments where the tiltrotor successfully performs perching and unperching on/from a vertical surface during flight. We further show effectiveness of the proposed transition mode in the switching controller by ablation studies where large overshoot and even collision with a perching site occur. To the best of the authors’ knowledge, this work presents the first autonomous aerial unperching framework using a fully actuated tiltrotor. keywords: {Measurement uncertainty;Rotors;Switches;Control systems;Autonomous aerial vehicles;Task analysis;Servomotors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610445&isnumber=10609862

B. Chapdelaine, M. Celce, C. Vidal, L. Birglen and B. Monsarrat, "Simulation and Experimental Validation of an Autonomous Perching and Takeoff Method for a Multirotor UAV on Vertical Surfaces using a Suction Cup," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1617-1623, doi: 10.1109/ICRA57147.2024.10610925.Abstract: This paper details the simulation and experimental validation of an autonomous perching and take-off method for a multirotor unmanned aerial vehicle (UAV) using a suction cup perching mechanism on vertical surfaces. The suction cup interaction with different surface types is characterized with experimental tests to accurately model the perching manoeuvre. The resulting model is used to develop a realistic hardware-in-the-loop (HIL) simulation of the perching and take-off manoeuvre of the UAV in Gazebo. A control method is developed to automate the perching and take-off manoeuvre. The method is tested in simulation and is experimentally validated with flight tests. Comparisons between simulation and experimental data demonstrate that the simulation is accurate and can be used to continue the development of autonomous perching methods. keywords: {Laser radar;Accuracy;Hardware-in-the-loop simulation;Autonomous aerial vehicles;Aerodynamics;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610925&isnumber=10609862

J. Sun, S. Wu, J. Dong and J. He, "Field-VIO: Stereo Visual-Inertial Odometry Based on Quantitative Windows in Agricultural Open Fields," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1624-1630, doi: 10.1109/ICRA57147.2024.10611284.Abstract: In agricultural open fields, accurate autonomous localization of robots requires long-term data correlation to reduce cumulative error. Our article presents a Stereo Visual-Inertial Odometry (VIO) system based on ORB-SLAM3 to address the malfunction of the Loop Closure Detection (LCD) methods in this environment. In this method, we first propose a concept of quantitative windows to describe the robot’s trajectory along the crop rows. We design a driving state quantification algorithm and accurately separate the quantitative windows between the crop rows. Our system constructs spatial constraints according to the parallelism between the quantitative windows. We apply an anomaly correction method to maintain the constructed parallel matching relationship and implement holistic pose correction for keyframes within abnormal quantitative windows. Our system demonstrated excellent performance over long distances in experiments on the Rosario dataset, verifying its effectiveness in reducing cumulative positioning error in agricultural open fields. keywords: {Location awareness;Correlation;Semantics;Crops;Parallel processing;Liquid crystal displays;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611284&isnumber=10609862

H. Li and J. Stueckler, "Online Calibration of a Single-Track Ground Vehicle Dynamics Model by Tight Fusion with Visual-Inertial Odometry," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1631-1637, doi: 10.1109/ICRA57147.2024.10610157.Abstract: Wheeled mobile robots need the ability to estimate their motion and the effect of their control actions for navigation planning. In this paper, we present ST-VIO, a novel approach which tightly fuses a single-track dynamics model for wheeled ground vehicles with visual-inertial odometry (VIO). Our method calibrates and adapts the dynamics model online to improve the accuracy of forward prediction conditioned on future control inputs. The single-track dynamics model approximates wheeled vehicle motion under specific control inputs on flat ground using ordinary differential equations. We use a singularity-free and differentiable variant of the single-track model to enable seamless integration as dynamics factor into VIO and to optimize the model parameters online together with the VIO state variables. We validate our method with real-world data in both indoor and outdoor environments with different terrain types and wheels. In experiments, we demonstrate that ST-VIO can not only adapt to wheel or ground changes and improve the accuracy of prediction under new control inputs, but can even improve tracking accuracy. keywords: {Adaptation models;Accuracy;Dynamics;Wheels;Predictive models;Mathematical models;Land vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610157&isnumber=10609862

J. Hu, X. Lang, F. Zhang, Y. Mao and G. Huang, "Square-Root Inverse Filter-based GNSS-Visual-Inertial Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1646-1652, doi: 10.1109/ICRA57147.2024.10611666.Abstract: While Global Navigation Satellite System (GNSS) is often used to provide global positioning if available, its intermittency and/or inaccuracy calls for fusion with other sensors. In this paper, we develop a novel GNSS-Visual-Inertial Navigation System (GVINS) that fuses visual, inertial, and raw GNSS measurements within the square-root inverse sliding window filtering (SRI-SWF) framework in a tightly coupled fashion, which thus is termed SRI-GVINS. In particular, for the first time, we deeply fuse the GNSS pseudorange, Doppler shift, single-differenced pseudorange, and double-differenced carrier phase measurements, along with the visual-inertial measurements. Inherited from the SRI-SWF, the proposed SRI-GVINS gains significant numerical stability and computational efficiency over the start-of-the-art methods. Additionally, we propose to use a filter to sequentially initialize the reference frame transformation till converges, rather than collecting measurements for batch optimization. We also perform online calibration of GNSS-IMU extrinsic parameters to mitigate the possible extrinsic parameter degradation. The proposed SRI-GVINS is extensively evaluated on our own collected UAV datasets and the results demonstrate that the proposed method is able to suppress VIO drift in real-time and also show the effectiveness of online GNSS-IMU extrinsic calibration. The experimental validation on the public datasets further reveals that the proposed SRI-GVINS outperforms the state-of-the-art methods in terms of both accuracy and efficiency. keywords: {Global navigation satellite system;Visualization;Phase measurement;Fuses;Atmospheric measurements;Particle measurements;Time measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611666&isnumber=10609862

W. Xie et al., "Omnidirectional Dense SLAM for Back-to-back Fisheye Cameras," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1653-1660, doi: 10.1109/ICRA57147.2024.10610351.Abstract: We propose a real-time visual-inertial dense SLAM system that utilizes the online data streams from back-to-back dual fisheye cameras setup, providing 360◦ coverage of the environment. Firstly, we employ a sliding-window-based front-end to estimate real-time poses from the binocular fisheye images and IMU data. Then, we implement a lightweight panoramic depth completion network based on multi-basis depth representation. The network takes panoramic images (obtained by stitching dual-fisheye images with extrinsics and intrinsic parameters) and sparse depths (generated by the front-end local tracking) as input and predicts multiple depth bases along with corresponding confidence as output. The final dense depth is the linear combination of the multiple depth bases. Thanks to the multi-basis depth representation, we can continuously optimize the 360° depth with the traditional optimizer to achieve higher global consistency in depth. We conducted experiments on both simulated and real-world datasets to evaluate our method. The results demonstrate that the proposed method outperforms SoTA methods in terms of depth prediction and 3D reconstruction. In addition, we develop a demo that can run on a mobile to demonstrate the real-time capabilities of our method. keywords: {Simultaneous localization and mapping;Three-dimensional displays;Cameras;Real-time systems;Robotics and automation;Streams},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610351&isnumber=10609862

M. Lisondra, J. Kim, R. Murai, K. Zareinia and S. Saeedi, "Visual Inertial Odometry using Focal Plane Binary Features (BIT-VIO)," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1661-1668, doi: 10.1109/ICRA57147.2024.10610838.Abstract: Focal-Plane Sensor-Processor Arrays (FPSP)s are an emerging technology that can execute vision algorithms directly on the image sensor. Unlike conventional cameras, FPSPs perform computation on the image plane – at individual pixels – enabling high frame rate image processing while consuming low power, making them ideal for mobile robotics. FPSPs, such as the SCAMP-5, use parallel processing and are based on the Single Instruction Multiple Data (SIMD) paradigm. In this paper, we present BIT-VIO, the first Visual Inertial Odometry (VIO) which utilises SCAMP-5. BIT-VIO is a loosely-coupled iterated Extended Kalman Filter (iEKF) which fuses together the visual odometry running fast at 300 FPS with predictions from 400 Hz IMU measurements to provide accurate and smooth trajectories. Project Page: https://sites.google.com/view/bit-vio/home keywords: {Visualization;Single instruction multiple data;Robot vision systems;Trajectory;Odometry;Kalman filters;Velocity measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610838&isnumber=10609862

D. Kumar, S. Gopinath, K. Dantu and S. Y. Ko, "JacobiGPU: GPU-Accelerated Numerical Differentiation for Loop Closure in Visual SLAM," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1687-1693, doi: 10.1109/ICRA57147.2024.10611512.Abstract: In this paper, we introduce JacobiGPU, a technique that uses a GPU to improve the efficiency of loop closure in visual-inertial SLAM systems, particularly when approximating Jacobians using the Finite Difference Method (FDM). Traditional FDM techniques often face computational overhead due to repeated perturbations in pose graphs. We address this overhead with a novel methodology, leveraging strategic graph partitioning and an optimized approach to Jacobian approximation. By integrating JacobiGPU into ORB-SLAM3’s g2o, we enhance the linearization process. Our evaluation, conducted on 12 sequences of varying lengths from the EuRoC and TUM-VI datasets, demonstrated a speedup of up to 4.23x in the linearization stage and an overall enhancement of up to 2.08x in the overall optimization process. keywords: {Jacobian matrices;Visualization;Simultaneous localization and mapping;Perturbation methods;Graphics processing units;Frequency division multiplexing;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611512&isnumber=10609862

Y. Wang et al., "MAVIS: Multi-Camera Augmented Visual-Inertial SLAM using SE2(3) Based Exact IMU Pre-integration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1694-1700, doi: 10.1109/ICRA57147.2024.10609982.Abstract: We present a novel optimization-based Visual-Inertial SLAM system designed for multiple partially over-lapped camera systems, named MAVIS. Our framework fully exploits the benefits of wide field-of-view from multi-camera systems, and the metric scale measurements provided by an inertial measurement unit (IMU). We introduce an improved IMU pre-integration formulation based on the exponential function of an automorphism of SE2(3), which can effectively enhance tracking performance under fast rotational motion and extended integration time. Furthermore, we extend conventional front-end tracking and back-end optimization module designed for monocular or stereo setup towards multi-camera systems, and introduce implementation details that contribute to the performance of our system in challenging scenarios. The practical validity of our approach is supported by our experiments on public datasets. Our MAVIS won the first place in all the vision-IMU tracks (single and multi-session SLAM) on Hilti SLAM Challenge 2023 with 1.7 times the score compared to the second place 1. keywords: {Simultaneous localization and mapping;Measurement units;Tracking;System performance;Inertial navigation;Cameras;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609982&isnumber=10609862

J. Park, J. Lee, E. Choi and Y. Cho, "Salience-guided Ground Factor for Robust Localization of Delivery Robots in Complex Urban Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1701-1708, doi: 10.1109/ICRA57147.2024.10611696.Abstract: In urban environments for delivery robots, particularly in areas such as campuses and towns, many custom features defy standard road semantic categorizations. Addressing this challenge, our paper introduces a method leveraging Salient Object Detection (SOD) to extract these unique features, employing them as pivotal factors for enhanced robot loop closure and localization. Traditional geometric feature-based localization is hampered by fluctuating illumination and appearance changes. Our preference for SOD over semantic segmentation sidesteps the intricacies of classifying a myriad of non-standardized urban features. To achieve consistent ground features, the Motion Compensate IPM (MC-IPM) technique is implemented, capitalizing on motion for distortion compensation and subsequently selecting the most pertinent salient ground features through moment computations. For thorough evaluation, we validated the saliency detection and localization performances to the real urban scenarios. Project page: https://sites.google.com/view/salient-ground-feature/home. keywords: {Location awareness;Simultaneous localization and mapping;Semantic segmentation;Urban areas;Semantics;Lighting;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611696&isnumber=10609862

Y. Feng et al., "Block-Map-Based Localization in Large-Scale Environment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1709-1715, doi: 10.1109/ICRA57147.2024.10610122.Abstract: Accurate localization is an essential technology for the flexible navigation of robots in large-scale environments. Both SLAM-based and map-based localization will increase the computing load due to the increase in map size, which will affect downstream tasks such as robot navigation and services. To this end, we propose a localization system based on Block Maps (BMs) to reduce the computational load caused by maintaining large-scale maps. Firstly, we introduce a method for generating block maps and the corresponding switching strategies, ensuring that the robot can estimate the state in large-scale environments by loading local map information. Secondly, global localization according to Branch-and-Bound Search (BBS) in the 3D map is introduced to provide the initial pose. Finally, a graph-based optimization method is adopted with a dynamic sliding window that determines what factors are being marginalized whether a robot is exposed to a BM or switching to another one, which maintains the accuracy and efficiency of pose tracking. Comparison experiments are performed on publicly available large-scale datasets. Results show that the proposed method can track the robot pose even though the map scale reaches more than 6 kilometers, while efficient and accurate localization is still guaranteed on NCLT [6] and M2DGR [35]. Codes and data will be publicly available on https://github.com/YixFeng/blocklocalization. keywords: {Location awareness;Accuracy;Uncertainty;Three-dimensional displays;Navigation;Optimization methods;Switches},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610122&isnumber=10609862

H. Li, J. Guo and D. Song, "Subsurface Feature-based Ground Robot/Vehicle Localization Using a Ground Penetrating Radar," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1716-1722, doi: 10.1109/ICRA57147.2024.10611579.Abstract: Robot localization using subsurface features captured by Ground-Penetrating Radar (GPR) complements and improves robustness over existing common sensor modalities, as subsurface features are less sensitive to weather, season and surface scene changes. Here, we propose a novel subsurface feature-based localization method that uses only GPR measurements with a known subsurface map. An efficient feature descriptor, the dominant energy curve (DEC), is designed to identify different locations in cluttered conditions. Specifically, image processing techniques that involve background segmentation, energy point detection, and energy curve refinement are designed to extract DEC features from a 2D radargram. With DECs features obtained, a metric subsurface feature map is constructed. Finally, we perform robot localization by feature matching under a particle swarm optimization framework. We have implemented our method and tested it with the public CMU-GPR dataset. The results show that our algorithm improves accuracy and robustness with real-time performance for robot localization tasks. Specifically, the mean localization error is 0.50 m for all cases. keywords: {Location awareness;Ground penetrating radar;Feature extraction;Robot sensing systems;Robot localization;Robustness;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611579&isnumber=10609862

C. Bai, R. Fu and X. Gao, "Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1723-1729, doi: 10.1109/ICRA57147.2024.10611582.Abstract: State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD3, an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map. keywords: {Point cloud compression;Location awareness;Laser radar;Structure from motion;Pipelines;Cameras;Synchronization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611582&isnumber=10609862

P. Pfreundschuh, H. Oleynikova, C. Cadena, R. Siegwart and O. Andersson, "COIN-LIO: Complementary Intensity-Augmented LiDAR Inertial Odometry," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1730-1737, doi: 10.1109/ICRA57147.2024.10610938.Abstract: We present COIN-LIO, a LiDAR Inertial Odometry pipeline that tightly couples information from LiDAR intensity with geometry-based point cloud registration. The focus of our work is to improve the robustness of LiDAR-inertial odometry in geometrically degenerate scenarios, like tunnels or flat fields. We project LiDAR intensity returns into an image, and present a novel image processing pipeline that produces filtered images with improved brightness consistency within the image as well as across different scenes. We effectively leverage intensity as an additional modality, using our new feature selection scheme that detects uninformative directions in the point cloud registration and explicitly selects patches with complementary image information. Photometric error minimization in the image patches is then fused with inertial measurements and point-to-plane registration in an iterated Extended Kalman Filter. The proposed approach improves accuracy and robustness on a public dataset. We additionally publish a new dataset, that captures five real-world environments in challenging, geometrically degenerate scenes. By using the additional photometric information, our approach shows drastically improved robustness against geometric degeneracy in environments where all compared baseline approaches fail. keywords: {Point cloud compression;Laser radar;Accuracy;Pipelines;Measurement uncertainty;Minimization;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610938&isnumber=10609862

K. Koide, S. Oishi, M. Yokozuka and A. Banno, "MegaParticles: Range-based 6-DoF Monte Carlo Localization with GPU-Accelerated Stein Particle Filter," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1738-1744, doi: 10.1109/ICRA57147.2024.10610447.Abstract: This paper presents a 6-DoF range-based Monte Carlo localization method with a GPU-accelerated Stein particle filter. To update a massive amount of particles, we propose a Gauss-Newton-based Stein variational gradient descent (SVGD) with iterative neighbor particle search. This method uses SVGD to collectively update particle states with gradient and neighborhood information, which provides efficient particle sampling. For an efficient neighbor particle search, it uses locality sensitive hashing and iteratively updates the neighbor list of each particle over time. The neighbor list is then used to propagate the posterior probabilities of particles over the neighbor particle graph. The proposed method is capable of evaluating one million particles in real-time on a single GPU and enables robust pose initialization and re-localization without an initial pose estimate. In experiments, the proposed method showed an extreme robustness to complete sensor occlusion (i.e., kidnapping), and enabled pinpoint sensor localization without any prior information. keywords: {Location awareness;Monte Carlo methods;Posterior probability;Graphics processing units;Robot sensing systems;Particle filters;6-DOF},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610447&isnumber=10609862

K. Koide, S. Oishi, M. Yokozuka and A. Banno, "Tightly Coupled Range Inertial Localization on a 3D Prior Map Based on Sliding Window Factor Graph Optimization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1745-1751, doi: 10.1109/ICRA57147.2024.10611195.Abstract: This paper presents a range inertial localization algorithm for a 3D prior map. The proposed algorithm tightly couples scan-to-scan and scan-to-map point cloud registration factors along with IMU factors on a sliding window factor graph. The tight coupling of the scan-to-scan and scan-to-map registration factors enables a smooth fusion of sensor ego-motion estimation and map-based trajectory correction that results in robust tracking of the sensor pose under severe point cloud degeneration and defective regions in a map. We also propose an initial sensor state estimation algorithm that robustly estimates the gravity direction and IMU state and helps perform global localization in 3- or 4-DoF for system initialization without prior position information. Experimental results show that the proposed method outperforms existing state-of-the-art methods in extremely severe situations where the point cloud data becomes degenerate, there are momentary sensor interruptions, or the sensor moves along the map boundary or into unmapped regions. keywords: {Location awareness;Point cloud compression;Couplings;Three-dimensional displays;Robot sensing systems;Reliability engineering;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611195&isnumber=10609862

S. Carmichael, R. Agrawal, R. Vasudevan and K. A. Skinner, "SPOT: Point Cloud Based Stereo Visual Place Recognition for Similar and Opposing Viewpoints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1752-1758, doi: 10.1109/ICRA57147.2024.10610293.Abstract: Recognizing places from an opposing viewpoint during a return trip is a common experience for human drivers. However, the analogous robotics capability, visual place recognition (VPR) with limited field of view cameras under 180 degree rotations, has proven to be challenging to achieve. To address this problem, this paper presents Same Place Opposing Trajectory (SPOT), a technique for opposing viewpoint VPR that relies exclusively on structure estimated through stereo visual odometry (VO). The method extends recent advances in lidar descriptors and utilizes a novel double (similar and opposing) distance matrix sequence matching method. We evaluate SPOT on a publicly available dataset with 6.7-7.6 km routes driven in similar and opposing directions under various lighting conditions. The proposed algorithm demonstrates remarkable improvement over the state-of-the-art, achieving up to 91.7% recall at 100% precision in opposing viewpoint cases, while requiring less storage than all baselines tested and running faster than all but one. Moreover, the proposed method assumes no a priori knowledge of whether the viewpoint is similar or opposing, and also demonstrates competitive performance in similar viewpoint cases. keywords: {Point cloud compression;Visualization;Laser radar;Robot vision systems;Lighting;Cameras;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610293&isnumber=10609862

C. Qing, R. Zeng, X. Wu, Y. Shi and G. Ma, "An Onboard Framework for Staircases Modeling Based on Point Clouds," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1759-1765, doi: 10.1109/ICRA57147.2024.10610407.Abstract: The detection of traversable regions on staircases and the physical modeling constitutes pivotal aspects of the mobility of legged robots. This paper presents an onboard framework tailored to the detection of traversable regions and the modeling of physical attributes of staircases by point cloud data. To mitigate the influence of illumination variations and the overfitting due to the dataset diversity, a series of data augmentations are introduced to enhance the training of the fundamental network. A curvature suppression cross-entropy(CSCE) loss is proposed to reduce the ambiguity of prediction on the boundary between traversable and non-traversable regions. Moreover, a measurement correction based on the pose estimation of stairs is introduced to calibrate the output of raw modeling that is influenced by tilted perspectives. Lastly, we collect a dataset pertaining to staircases and introduce new evaluation criteria. Through a series of rigorous experiments conducted on this dataset, we substantiate the superior accuracy and generalization capabilities of our proposed method. Codes, models, and datasets will be available at https://github.com/szturobotics/Stair-detection-and-modeling-project. keywords: {Training;Point cloud compression;Legged locomotion;Pose estimation;Lighting;Stairs;Data augmentation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610407&isnumber=10609862

S. Jung, J. Lee, X. Meng, B. Boots and A. Lambert, "V-STRONG: Visual Self-Supervised Traversability Learning for Off-road Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1766-1773, doi: 10.1109/ICRA57147.2024.10611227.Abstract: Reliable estimation of terrain traversability is critical for the successful deployment of autonomous systems in wild, outdoor environments. Given the lack of large-scale annotated datasets for off-road navigation, strictly-supervised learning approaches remain limited in their generalization ability. To this end, we introduce a novel, image-based self-supervised learning method for traversability prediction, leveraging a state-of-the-art vision foundation model for improved out-of-distribution performance. Our method employs contrastive representation learning using both human driving data and instance-based segmentation masks during training. We show that this simple, yet effective, technique drastically outperforms recent methods in predicting traversability for both on- and off-trail driving scenarios. We compare our method with recent baselines on both a common benchmark as well as our own datasets, covering a diverse range of outdoor environments and varied terrain types. We also demonstrate the compatibility of resulting costmap predictions with a model-predictive controller. Finally, we evaluate our approach on zero- and few-shot tasks, demonstrating unprecedented performance for generalization to new environments. Videos and additional material can be found here: https://sites.google.com/view/visual-traversability-learning. keywords: {Training;Representation learning;Image segmentation;Visualization;Navigation;Predictive models;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611227&isnumber=10609862

Y. Jeon, E. I. Son and S. -W. Seo, "Follow the Footprints: Self-supervised Traversability Estimation for Off-road Vehicle Navigation based on Geometric and Visual Cues," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1774-1780, doi: 10.1109/ICRA57147.2024.10611198.Abstract: In this study, we address the off-road traversability estimation problem, that predicts areas where a robot can navigate in off-road environments. An off-road environment is an unstructured environment comprising a combination of traversable and non-traversable spaces, which presents a challenge for estimating traversability. This study highlights three primary factors that affect a robot’s traversability in an off-road environment: surface slope, semantic information, and robot platform. We present two strategies for estimating traversability, using a guide filter network (GFN) and footprint supervision module (FSM). The first strategy involves building a novel GFN using a newly designed guide filter layer. The GFN interprets the surface and semantic information from the input data and integrates them to extract features optimized for traversability estimation. The second strategy involves developing an FSM, which is a self-supervision module that utilizes the path traversed by the robot in pre-driving, also known as a footprint. This enables the prediction of traversability that reflects the characteristics of the robot platform. Based on these two strategies, the proposed method overcomes the limitations of existing methods, which require laborious human supervision and lack scalability. Extensive experiments in diverse conditions, including automobiles and unmanned ground vehicles, herbfields, woodlands, and farmlands, demonstrate that the proposed method is compatible for various robot platforms and adaptable to a range of terrains. Code is available at https://github.com/yurimjeon1892/FtFoot. keywords: {Visualization;Navigation;Scalability;Semantics;Estimation;Self-supervised learning;Information filters},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611198&isnumber=10609862

Y. Shen, M. Liu, H. Lu and X. Chen, "TSCM: A Teacher-Student Model for Vision Place Recognition Using Cross-Metric Knowledge Distillation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1789-1795, doi: 10.1109/ICRA57147.2024.10611612.Abstract: Visual place recognition (VPR) plays a pivotal role in autonomous exploration and navigation of mobile robots within complex outdoor environments. While cost-effective and easily deployed, camera sensors are sensitive to lighting and weather changes, and even slight image alterations can greatly affect VPR efficiency and precision. Existing methods overcome this by exploiting powerful yet large networks, leading to significant consumption of computational resources. In this paper, we propose a high-performance teacher and lightweight student distillation framework called TSCM. It exploits our devised cross-metric knowledge distillation to narrow the performance gap between the teacher and student models, maintaining superior performance while enabling minimal computational load during deployment. We conduct comprehensive evaluations on large-scale datasets, namely Pittsburgh30k and Pittsburgh250k. Experimental results demonstrate the superiority of our method over baseline models in terms of recognition accuracy and model parameter efficiency. Moreover, our ablation studies show that the proposed knowledge distillation technique surpasses other counterparts. The code of our method has been released at https://github.com/nubot-nudt/TSCM. keywords: {Visualization;Accuracy;Navigation;Computational modeling;Robot vision systems;Real-time systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611612&isnumber=10609862

K. Aoki, K. Koide, S. Oishi, M. Yokozuka, A. Banno and J. Meguro, "3D-BBS: Global Localization for 3D Point Cloud Scan Matching Using Branch-and-Bound Algorithm," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1796-1802, doi: 10.1109/ICRA57147.2024.10610810.Abstract: This paper presents an accurate and fast 3D global localization method, 3D-BBS, that extends the existing branchand-bound (BnB)-based 2D scan matching (BBS) algorithm. To reduce memory consumption, we utilize a sparse hash table for storing hierarchical 3D voxel maps. To improve the processing cost of BBS in 3D space, we propose an efficient roto-translational space branching. Furthermore, we devise a batched BnB algorithm to fully leverage GPU parallel processing. Through experiments in simulated and real environments, we demonstrated that the 3D-BBS enabled accurate global localization with only a 3D LiDAR scan roughly aligned in the gravity direction and a 3D pre-built map. This method required only 878 msec on average to perform global localization and outperformed state-of-the-art global registration methods in terms of accuracy and processing speed. keywords: {Location awareness;Point cloud compression;Three-dimensional displays;Accuracy;Laser radar;Memory management;Graphics processing units},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610810&isnumber=10609862

H. Zhao, M. Yao, X. Xiao and B. Zheng, "DynaInsRemover: A Real-time Dynamic Instance-Aware Static 3D LiDAR Mapping Framework for Dynamic Environment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1803-1809, doi: 10.1109/ICRA57147.2024.10610211.Abstract: Dynamic objects diversify the distribution of point cloud in the map, degrading the performance of the robotic downstream tasks. To address this problem, we present a novel real-time dynamic instance-aware static mapping framework called DynaInsRemover, which exploits the geometric discrepancies between instances to efficiently remove dynamic objects and preserve more details of static map. It contains the Instance Occupancy Check module for initial dynamic instance proposal and the Instance Belief Update module for reverting false positives. We quantitatively evaluate our approach performance on the SemanticKITTI dataset and validate it in a real-world environment. Experimental evaluations show that our method achieves very promising results in dynamic environments. The implementation of our method is available as open source at: https://github.com/Zhaohuanfeng/DynaInsRemover.git. keywords: {Point cloud compression;Three-dimensional displays;Laser radar;Real-time systems;Windows;Proposals;Object recognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610211&isnumber=10609862

G. Tang, K. M. Jatavallabhula and A. Torralba, "Efficient 3D Instance Mapping and Localization with Neural Fields," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1818-1824, doi: 10.1109/ICRA57147.2024.10611715.Abstract: We tackle the problem of learning an implicit scene representation for 3D instance segmentation from a sequence of posed RGB images. Towards this, we introduce 3DIML, a novel framework that efficiently learns a label field that may be rendered from novel viewpoints to produce view-consistent instance segmentation masks. 3DIML significantly improves upon training and inference runtimes of existing implicit scene representation based methods. Opposed to prior art that optimizes a neural field in a self-supervised manner, requiring complicated training procedures and loss function design, 3DIML leverages a two-phase process. The first phase, InstanceMap, takes as input 2D segmentation masks of the image sequence generated by a frontend instance segmentation model, and associates corresponding masks across images to 3D labels. These almost view-consistent pseudolabel masks are then used in the second phase, InstanceLift, to supervise the training of a neural label field, which interpolates regions missed by InstanceMap and resolves ambiguities. Additionally, we introduce InstanceLoc, which enables near realtime localization of instance masks given a trained label field and an off-the-shelf image segmentation model by fusing outputs from both. We evaluate 3DIML on sequences from the Replica and ScanNet datasets and demonstrate 3DIML’s effectiveness under mild assumptions for the image sequences. We achieve a 14-24× speedup over existing implicit scene representation methods with comparable quality, showcasing its potential to facilitate faster and more effective 3D scene understanding. keywords: {Instance segmentation;Training;Location awareness;Solid modeling;Three-dimensional displays;Runtime;Refining},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611715&isnumber=10609862

M. Noseworthy, S. Shaw, C. C. Kessens and N. Roy, "Amortized Inference for Efficient Grasp Model Adaptation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1886-1892, doi: 10.1109/ICRA57147.2024.10610789.Abstract: In robotic applications such as bin-picking or block-stacking, learned predictive models have been developed for manipulation of objects with varying but known dynamic properties (e.g., mass distributions and friction coefficients). When a robot encounters a new object, these properties are often difficult to observe and must be inferred through interaction, which can be expensive in both inference time and number of interactions. We propose an encoder/decoder action-feasibility model to efficiently adapt to new objects by estimating their unobserved properties through interaction. The encoder predicts a distribution over the unobserved parameters while the decoder predicts action feasibility, which can be used in an uncertainty-aware planner. An explicit representation of uncertainty in the encoder enables information-gathering heuristics to minimize adaptation interactions. The amortized distributions are efficient to compute and perform comparably to particle-based distributions in a grasping domain. Finally, we deploy our method on a Panda robot to grasp heavy objects. keywords: {Geometry;Adaptation models;Uncertainty;Computational modeling;Grasping;Predictive models;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610789&isnumber=10609862

H. Duan, Y. Li, D. Li, W. Wei, Y. Huang and P. Wang, "Learning Realistic and Reasonable Grasps for Anthropomorphic Hand in Cluttered Scenes," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1893-1899, doi: 10.1109/ICRA57147.2024.10610646.Abstract: Grasping is one of the most fundamental skills for humans to interact with objects. However, it remains a challenging problem for anthropomorphic hands, due to the lack of object affordance understanding and high-dimensional grasp planning. In this work, we propose an anthropomorphic hand grasping framework to learn realistic and reasonable grasps in cluttered scenes, which tackles the problem in three items: 1) graspable point segmentation; 2) hand grasp generation and 3) grasp optimization. Specifically, our method generates high-quality hand grasps efficiently without complete object models by learning graspable points, associated grasp configurations from observed point cloud in a parallel manner and optimizing predicted grasps based on hand-object contacts. Simulation experiments show that our model generates physical plausible grasps for the anthropomorphic hand effectively with over 70% success rate. Real-world experiments demonstrate that the model trained in simulation performs satisfactorily in real-world scenarios for unseen objects. keywords: {Point cloud compression;Affordances;Grasping;Predictive models;Planning;Optimization;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610646&isnumber=10609862

H. Chen, B. Xu and S. Leutenegger, "FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1900-1906, doi: 10.1109/ICRA57147.2024.10611233.Abstract: We present FuncGrasp, a framework that can infer dense yet reliable grasp configurations for unseen objects using one annotated object and single-view RGB-D observation via categorical priors. Unlike previous works that only transfer a set of grasp poses, FuncGrasp aims to transfer infinite configurations parameterized by an object-centric continuous grasp function across varying instances. To ease the transfer process, we propose Neural Surface Grasping Fields (NSGF), an effective neural representation defined on the surface to densely encode grasp configurations. Further, we exploit function-to-function transfer using sphere primitives to establish semantically meaningful categorical correspondences, which are learned in an unsupervised fashion without any expert knowledge. We showcase the effectiveness through extensive experiments in both simulators and the real world. Remarkably, our framework significantly outperforms several strong baseline methods in terms of density and reliability for generated grasps. keywords: {Training;Degradation;Filtering;Semantics;Grasping;Reliability engineering;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611233&isnumber=10609862

N. W. Alharthi and M. Brandão, "Physical and Digital Adversarial Attacks on Grasp Quality Networks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1907-1902, doi: 10.1109/ICRA57147.2024.10610886.Abstract: Grasp Quality Networks are important components of grasping-capable autonomous robots, as they allow them to evaluate grasp candidates and select the one with highest chance of success. The widespread use of pick-and-place robots and Grasp Quality Networks raises the question of whether such systems are vulnerable to adversarial attacks, as that could lead to large economic damage. In this paper we propose two kinds of attacks on Grasp Quality Networks, one assuming physical access to the workspace (to place or attach a new object) and another assuming digital access to the camera software (to inject a pixel-intensity change on a single pixel). We then use evolutionary optimization to obtain attacks that simultaneously minimize the noticeability of the attacks and the chance that selected grasps are successful. Our experiments show that both kinds of attack lead to drastic drops in algorithm performance, thus making them important attacks to consider in the cybersecurity of grasping robots. Source code can be found at https://github.com/Naif-W-Alharthi/Physical-and-Digital-Attacks-on-Grasping-Networks keywords: {Economics;Prevention and mitigation;Source coding;Software algorithms;Grasping;Software;Protection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610886&isnumber=10609862

C. Mitash, M. Hussein, J. Vanbaar, V. Terhuja and K. Katyal, "Scaling Object-centric Robotic Manipulation with Multimodal Object Identification," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1913-1920, doi: 10.1109/ICRA57147.2024.10611181.Abstract: Robotic manipulation is a key enabler for automation in the fulfillment logistics sector. Such robotic systems require perception and manipulation capabilities to handle a wide variety of objects. Existing systems either operate on a closed set of objects or perform object-agnostic manipulation which lacks the capability for deliberate and reliable manipulation at scale. Object identification (ID) unlocks the ability for large-scale, object-centric manipulation by mapping object segments to one of the previously seen objects from a database. Nevertheless, it is often limited by the availability of reference data or coverage for objects in a database. In this work, we propose to perform object identification with multiple reference databases, including images and text references, each with a different coverage and matching challenge. We propose a training strategy that tackles the challenges of learning domain-invariant image embeddings, image-text matching and fusing predictions from different sources. We perform experiments over a recent benchmark with over 190K+ unique objects, extend the dataset with the additional reference sources and propose an evaluation strategy that simulates coverage for different reference sources. Model trained with the proposed learning pipeline shows robust performance over a range of simulation experiments. keywords: {Training;Image segmentation;Automation;Databases;Pipelines;Benchmark testing;Object recognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611181&isnumber=10609862

Y. -S. Tsai, P. -C. Yeh, C. -H. Huang, I. -C. Hsueh and C. -C. Lan, "A Force-Controlled Gripper Capable of Measuring Mechanical Properties of an Object," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1929-1935, doi: 10.1109/ICRA57147.2024.10610496.Abstract: Various sensorized grippers have been developed to handle delicate objects safely. These grippers have sensors mounted on their fingers’ surface that provide direct force measurements. However, multiple sensors are often required on one finger, leading to significant sensor placement and wire routing complexity. Finger-based sensors are limited to sensing external gripping force, and fingers cannot be easily replaced to meet the requirements of objects with specific geometries. To overcome the complexity and limitations of finger surface sensors, this paper proposes a force-controlled two-fingered gripper that relies on the deformation sensing of elastic elements in the drivetrain to obtain finger force. By using a minimum number of optical encoders placed in the drivetrain, accurate position and force sensing can be achieved at any location of each finger. When gripping an object, the size and stiffness of the object can thus be accurately measured. Simulation and experimental results demonstrate the proposed gripper’s merits. We expect this new gripper to provide a more competitive solution for robots that need to manipulate objects and check their mechanical qualities at the same time. keywords: {Force measurement;Deformation;Force;Optical variables measurement;Robot sensing systems;Size measurement;Sensors;Force-controlled gripper;object size and stiffness;planar spring;optical encoder;repeatability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610496&isnumber=10609862

F. Ostyn, B. Vanderborght and G. Crevecoeur, "Accelerating Robotic Picking of Rigid Objects with a Compliant Pneumatic Gripper and an Impact-Aware Trajectory Plan," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1944-1949, doi: 10.1109/ICRA57147.2024.10611505.Abstract: Industrial robots are capable of moving at high speed. Each time they come into contact with their environment, e.g. to pick up an object, they decelerate to a near standstill. A solution involving a compliant pneumatic gripper and adapted trajectory plan is presented to initiate contact at a higher speed while remaining within hardware limits. By adding overload clutches in either the robot arm or gripper, tolerance to errors is provided. The key parameters such as gripper compliance and maximum allowed initial impact velocity are identified. Results show that by properly optimizing these parameters, robot picking of rigid objects can be accelerated. The complete high-speed picking solution is experimentally verified. A time reduction of 16% was obtained when making contact at 0.65 m/s. keywords: {Service robots;Pneumatic systems;Industrial robots;Manipulators;Hardware;Trajectory;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611505&isnumber=10609862

C. L. Yako, J. Nowak, S. Yuan and K. Salisbury, "Vertical Vibratory Transport of Grasped Parts Using Impacts," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1950-1956, doi: 10.1109/ICRA57147.2024.10610769.Abstract: In this paper, we use impact-induced acceleration in conjunction with periodic stick-slip to successfully and quickly transport parts vertically against gravity. We show analytically that vertical vibratory transport is more difficult than its horizontal counterpart, and provide guidelines for achieving optimal vertical vibratory transport of a part. Namely, such a system must be capable of quickly realizing high accelerations, as well as supply normal forces at least several times that required for static equilibrium. We also show that for a given maximum acceleration, there is an optimal normal force for transport. To test our analytical guidelines, we built a vibrating surface using flexures and a voice coil actuator that can accelerate a magnetic ram into various materials to generate impacts. The surface was used to transport a part against gravity. Experimentally obtained motion tracking data confirmed the theoretical model. A series of grasping tests with a vibrating-surface equipped parallel jaw gripper confirmed the design guidelines. keywords: {Tracking;Design methodology;Random access memory;Life estimation;Grasping;Magnetic analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610769&isnumber=10609862

X. Wang et al., "Bionic Soft Fingers with Hybrid Variable Stiffness Mechanisms for Multimode Grasping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1957-1963, doi: 10.1109/ICRA57147.2024.10611680.Abstract: This paper presents a novel Bionic Soft Finger (BSF) that aims to overcome the limitations of conventional rigid manipulators in terms of adaptability and safety, as well as the challenges faced by soft hands regarding carrying capacity and stability. The BSF design uses a hybrid variable stiffness mechanism combining memory alloy actuators with particle jamming to achieve the desired bending angle and actuator stiffness. Our innovative approach utilizes a bionic finger design that incorporates a memory alloy skeleton and a water-cooled recirculation system, leading to a substantial reduction in the time required for each operation. Through the integration of particle jamming, we have enhanced the overall stiffness and performance of the manipulator, enabling load capacities of up to 3N per finger and more than twice the stiffness of a normal condition. Additionally, our design enables multimode grasping and incorporates a liquid metal strain sensor (METT) for real-time monitoring of finger bending angles. Comparative analyses demonstrate that our design exhibits superior stiffness and enables five-mode grasping in comparison to pneumatic actuators. We believe that bionic soft fingers present a promising solution for enhancing adaptability, safety, and performance in human-robot interaction applications. keywords: {Metals;Grasping;Bending;Manipulators;Stability analysis;Skeleton;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611680&isnumber=10609862

M. Li, F. Zhao, X. Li, M. Li, S. Liu and M. Li, "Design and Fabrication of a Novel Miniature Magnetic Gripper," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1964-1970, doi: 10.1109/ICRA57147.2024.10611688.Abstract: Small-scale robots hold significant promise in the field of minimally invasive surgery (MIS). In this paper, we present a miniature magnetic gripper and develop a data-driven kinematic model. The gripper comprises four fingers, wherein each finger has a maximum size not exceeding 3mm, 4mm and 5.5mm in three dimensions. By integrating permanent magnets and elastic ropes as internal actuation elements into the fingers, the gripper is equipped with the capability to open-close under an external magnetic field, facilitating the manipulation of small objects in confined spaces. Modeling and analysis of the magnetic gripper are undertaken, wherein the relationship between the open angle and the external magnetic field is established. The average error between the experimentally observed open angles and the model-predicted values is 2.31°. Subsequent experiments demonstrated the necessity of the magnetic gripper model for precise manipulation, verified its excellent sensitivity to magnetic fields, and demonstrated its potential for future applications in MIS. keywords: {Magnetic field measurement;Torque;Sensitivity;Magnetic confinement;Magnetoelasticity;Fingers;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611688&isnumber=10609862

P. Gümbel and K. Dröder, "Design of Highly Repeatable and Multi-Functional Grippers for Precision Handling with Articulated Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1971-1977, doi: 10.1109/ICRA57147.2024.10611540.Abstract: This paper presents a novel approach to designing, a low-cost gripper that is highly repeatable and functionally integrated. The gripper is optimized to compensate for gripping errors with particular consideration to potential challenges of articulated robots. The primary design goal is to achieve maximum repeatability during the gripping and releasing stages of a pick-and-place process for a chip-like silicon die. The design is centered around a custom printed circuit board integrates functionality for vision-based error compensation, vacuum level monitoring, part contact detection, and detection of abnormal vibrations. We detail our design requirements and specific design choices for the mechanical and electronic design and provide qualitative and quantitative experimental validation of the achieved repeatability and the integrated functions. keywords: {Vibrations;Printed circuits;Error compensation;Silicon;Grippers;Robots;Monitoring},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611540&isnumber=10609862

M. Hansjosten, J. Baumgärtner and J. Fleischer, "Generalized Partially Destructive Disassembly Planning for Robotic Disassembly," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1978-1984, doi: 10.1109/ICRA57147.2024.10610546.Abstract: While robotic assembly is a well researched topic, recycling and disassembly of products are also becoming ever more important as we transition to a more sustainable economy. In disassembly, we are typically only interested in a subset of product parts, which opens the possibility of using destructive processes such as tearing, cutting, or milling to speed up the disassembly. Currently, such destructive actions are only included as predefined case-specific actions such as milling away a screw head. By contrast, this paper presents a generalized approach to destructive disassembly planning that can automatically derive destructive disassembly actions from a symbolic representation of the disassembly state. Viable destructive actions are identified and verified only based on the underlying geometric model, circumventing the need for their explicit definition. We showcase the performance of this system both virtually on several test parts and physically by destructively and non-destructively disassembling a model of an electric motor using a robot manipulator with a multitool end effector. keywords: {Robotic assembly;Geometry;Geometric modeling;Milling;Fasteners;Cost function;End effectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610546&isnumber=10609862

M. Bortolon, T. Tsesmelis, S. James, F. Poiesi and A. D. Bue, "IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single image and a NeRF model," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1985-1991, doi: 10.1109/ICRA57147.2024.10610425.Abstract: We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera pose of a given image, building on the Neural Radiance Fields (NeRF) formulation. IFFNeRF is specifically designed to operate in real-time and eliminates the need for an initial pose guess that is proximate to the sought solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface points from within the NeRF model. From these sampled points, we cast rays and deduce the color for each ray through pixel-level view synthesis. The camera pose can then be estimated as the solution to a Least Squares problem by selecting correspondences between the query image and the resulting bundle. We facilitate this process through a learned attention mechanism, bridging the query image embedding with the embedding of parameterized rays, thereby matching rays pertinent to the image. Through synthetic and real evaluation settings, we show that our method can improve the angular and translation error accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing at 34fps on consumer hardware and not requiring the initial pose guess. Project page: https://mbortolon97.github.io/frenerf/ keywords: {Accuracy;Image color analysis;Robot vision systems;Pose estimation;Memory management;Neural radiance field;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610425&isnumber=10609862

T. Ma et al., "VeloVox: A Low-Cost and Accurate 4D Object Detector with Single-Frame Point Cloud of Livox LiDAR," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1992-1998, doi: 10.1109/ICRA57147.2024.10610427.Abstract: Combining motion prediction in LiDAR-based 3D object detection is an effective method for improving overall accuracy, especially the downstream autonomous driving tasks. The recent development of low-cost LiDARs (e.g. Livox LiDAR) enables us to explore such 4D perception systems with a lower budget and higher performance. In this paper, we propose a 4D object detector, VeloVox, to establish accurate object detection and velocity estimation with a single-frame point cloud of Livox LiDAR. Based on the non-repetitive scanning pattern and point-level temporal nature, we propose a two-stage module to enhance the spatial-temporal point feature interaction along the time dimension. The aggregated feature also benefits a more accurate proposal refinement. To demonstrate the performance, comparison of VeloVox with several SOTA detector based baselines is evaluated on our in-house dataset and synthesized dataset built under Carla simulation. Code will be released at https://github.com/PJLab-ADG/VeloVox. keywords: {Point cloud compression;Laser radar;Accuracy;Three-dimensional displays;Estimation;Object detection;Detectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610427&isnumber=10609862

Y. Li, S. Yu, D. Wang and J. Jiao, "MTRadSSD: A Multi-Task Single-Stage Detector for Object Detection and Free Space Analysis in Radar Point Clouds*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 1999-2005, doi: 10.1109/ICRA57147.2024.10611324.Abstract: Environmental perception tasks such as object detection and free space detection based on 3+1D radar severely suffer from the disorder and sparsity of point cloud. To tackle this problem, we propose a novel Multi-Task Radar-based Single Stage Detector, termed MTRadSSD, where we adopt instance-aware sampling strategies to discover multi-class road users and propose an occupancy map tool based on kernel density estimation (KDE) to make predictions in bird’s eye view (BEV). The denoised occupancy map also plays key role in generating polygon represented free space in the scene. As a result, our elaborated sampling strategies effectively retained useful semantic information and narrowed the difference of detection performance across object categories. Meanwhile, our MTRadSSD outperforms those state-of-the-art approaches in terms of real-time requirement and detection accuracy. In detail, the proposed method achieves an satisfactory speed of ˜ View-of-Delft (VOD). With IoU thresholds 0.5/0.25/0.25 the average prediction precision (AP) of easy-level objects (cars, pedestrians and cyclists) reaches at competitive 52.2%, 61.1%, 86.3%, respectively, while mean IoU of free space is 87.8%. Especially, the occupancy map also makes difference in improving prediction precision of object orientation dramatically to averaged 64.0%. keywords: {Point cloud compression;Pedestrians;Accuracy;Spaceborne radar;Roads;Radar detection;Object detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611324&isnumber=10609862

C. Wang, Y. Qin, Z. Kang, N. Ma and R. Zhang, "Toward Accurate Camera-based 3D Object Detection via Cascade Depth Estimation and Calibration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2006-2012, doi: 10.1109/ICRA57147.2024.10610281.Abstract: Recent camera-based 3D object detection is limited by the precision of transforming from image to 3D feature spaces, as well as the accuracy of object localization within the 3D space. This paper aims to address such a fundamental problem of camera-based 3D object detection: How to effectively learn depth information for accurate feature lifting and object localization. Different from previous methods which directly predict depth distributions by using a supervised estimation model, we propose a cascade framework consisting of two depth-aware learning paradigms. First, a depth estimation (DE) scheme leverages relative depth information to realize the effective feature lifting from 2D to 3D spaces. Furthermore, a depth calibration (DC) scheme introduces depth reconstruction to further adjust the 3D object localization perturbation along the depth axis. In practice, the DE is explicitly realized by using both the absolute and relative depth optimization loss to promote the precision of depth prediction, while the capability of DC is implicitly embedded into the detection Transformer through a depth denoising mechanism in the training phase. The entire model training is accomplished through an end-to-end manner. We propose a baseline detector and evaluate the effectiveness of our proposal with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a comparable performance with 55.9%/45.7% NDS/mAP. Furthermore, we conduct extensive experiments to demonstrate its generality based on various detectors with about +2% NDS improvements. keywords: {Location awareness;Training;Three-dimensional displays;Accuracy;Estimation;Object detection;Detectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610281&isnumber=10609862

M. Jeon, J. Seo and J. Min, "DA-RAW: Domain Adaptive Object Detection for Real-World Adverse Weather Conditions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2013-2020, doi: 10.1109/ICRA57147.2024.10611219.Abstract: Despite the success of deep learning-based object detection methods in recent years, it is still challenging to make the object detector reliable in adverse weather conditions such as rain and snow. For the robust performance of object detectors, unsupervised domain adaptation has been utilized to adapt the detection network trained on clear weather images to adverse weather images. While previous methods do not explicitly address weather corruption during adaptation, the domain gap between clear and adverse weather can be decomposed into two factors with distinct characteristics: a style gap and a weather gap. In this paper, we present an unsupervised domain adaptation framework for object detection that can more effectively adapt to real-world environments with adverse weather conditions by addressing these two gaps separately. Our method resolves the style gap by concentrating on style-related information of high-level features using an attention module. Using self-supervised contrastive learning, our framework then reduces the weather gap and acquires instance features that are robust to weather corruption. Extensive experiments demonstrate that our method outperforms other methods for object detection in adverse weather conditions. keywords: {Rain;Snow;Object detection;Detectors;Contrastive learning;Robustness;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611219&isnumber=10609862

A. C. Stutts, D. Erricolo, S. Ravi, T. Tulabandhula and A. R. Trivedi, "Mutual Information-calibrated Conformal Feature Fusion for Uncertainty-Aware Multimodal 3D Object Detection at the Edge," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2029-2035, doi: 10.1109/ICRA57147.2024.10609987.Abstract: In the expanding landscape of AI-enabled robotics, robust quantification of predictive uncertainties is of great importance. Three-dimensional (3D) object detection, a critical robotics operation, has seen significant advancements; however, the majority of current works focus only on accuracy and ignore uncertainty quantification. Addressing this gap, our novel study integrates the principles of conformal inference (CI) with information theoretic measures to perform lightweight, Monte Carlo-free uncertainty estimation within a multimodal framework. Through a multivariate Gaussian product of the latent variables in a Variational Autoencoder (VAE), features from RGB camera and LiDAR sensor data are fused to improve the prediction accuracy. Normalized mutual information (NMI) is leveraged as a modulator for calibrating uncertainty bounds derived from CI based on a weighted loss function. Our simulation results show an inverse correlation between inherent predictive uncertainty and NMI throughout the model’s training. The framework demonstrates comparable or better performance in KITTI 3D object detection benchmarks to similar methods that are not uncertainty-aware, making it suitable for real-time edge robotics. keywords: {Deep learning;Training;Uncertainty;Three-dimensional displays;Accuracy;Image edge detection;Simulation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609987&isnumber=10609862

J. Wei, X. Song, W. Liu, L. Kneip, H. Li and P. Ji, "RGB-based Category-level Object Pose Estimation via Decoupled Metric Scale Recovery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2036-2042, doi: 10.1109/ICRA57147.2024.10611723.Abstract: While showing promising results, recent RGB-D camera-based category-level object pose estimation methods have restricted applications due to the heavy reliance on depth sensors. RGB-only methods provide an alternative to this problem yet suffer from inherent scale ambiguity stemming from monocular observations. In this paper, we propose a novel pipeline that decouples the 6D pose and size estimation to mitigate the influence of imperfect scales on rigid transformations. Specifically, we leverage a pre-trained monocular estimator to extract local geometric information, mainly facilitating the search for inlier 2D-3D correspondence. Meanwhile, a separate branch is designed to directly recover the metric scale of the object based on category-level statistics. Finally, we advocate using the RANSAC-PnP algorithm to robustly solve for 6D object pose. Extensive experiments have been conducted on both synthetic and real datasets, demonstrating the superior performance of our method over previous state-of-the-art RGB-based approaches, especially in terms of rotation accuracy. Code: https://github.com/goldoak/DMSR. keywords: {Measurement;Codes;Accuracy;Pose estimation;Pipelines;Sensors;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611723&isnumber=10609862

J. Li, L. Jin, X. Song, Y. Chen, N. Li and X. Qin, "Implicit Coarse-to-Fine 3D Perception for Category-level Object Pose Estimation from Monocular RGB Image," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2043-2050, doi: 10.1109/ICRA57147.2024.10610570.Abstract: Category-level object pose estimation demonstrates robust generalization capabilities that benefit robotics applications. However, exclusive reliance on RGB images without leveraging any 3D information introduces ambiguity in the translation and size of objects, leading to suboptimal performance. In this paper, we propose a framework for category-level pose estimation from a single RGB image in an end-to-end manner, i.e., Feature Auxiliary Perception Network (FAP-Net). To address inaccurate pose estimation caused by the inherent ambiguity of RGB images, we design a coarse-to-fine approach that first harnesses geometry supervision to facilitate coarse 3D feature perception and subsequently refines the features based on pose and size constraints. Experimental results on REAL275 and CAMERA25 demonstrate that FAP-Net achieves significant improvements (14.7% on 10°10cm and 11.4% on IoU50 on the real-scene REAL275 dataset) over the state-of-the-art and real-time inference (42 FPS). keywords: {Geometry;Three-dimensional displays;Pose estimation;Grasping;Robustness;Real-time systems;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610570&isnumber=10609862

K. Shirai et al., "Vision-Language Interpreter for Robot Task Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2051-2058, doi: 10.1109/ICRA57147.2024.10611112.Abstract: Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated with four new evaluation metrics. Experimental results show that ViLaIn can generate syntactically correct problems with more than 99% accuracy and valid plans with more than 58% accuracy. Our code and dataset are available at https://github.com/omron-sinicx/ViLaIn. keywords: {Measurement;Accuracy;Codes;Large language models;Refining;Linguistics;Market research},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611112&isnumber=10609862

B. Wang, X. Chen and L. Zhao, "Trust-Region Neural Moving Horizon Estimation for Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2059-2065, doi: 10.1109/ICRA57147.2024.10611703.Abstract: Accurate disturbance estimation is essential for safe robot operations. The recently proposed neural moving horizon estimation (NeuroMHE), which uses a portable neural network to model the MHE’s weightings, has shown promise in further pushing the accuracy and efficiency boundary. Currently, NeuroMHE is trained through gradient descent, with its gradient computed recursively using a Kalman filter. This paper proposes a trust-region policy optimization method for training NeuroMHE. We achieve this by providing the second-order derivatives of MHE, referred to as the MHE Hessian. Remarkably, we show that many of the intermediate results used to obtain the gradient, especially the Kalman filter, can be efficiently reused to compute the MHE Hessian. This offers linear computational complexity with respect to the MHE horizon. As a case study, we evaluate the proposed trust region NeuroMHE on real quadrotor flight data for disturbance estimation. Our approach demonstrates highly efficient training in under 5 min using only 100 data points. It outperforms a state-of-the-art neural estimator by up to 68.1% in force estimation accuracy, utilizing only 1.4% of its network parameters. Furthermore, our method showcases enhanced robustness to network initialization compared to the gradient descent counterpart. keywords: {Training;Accuracy;Neural networks;Force;Estimation;Optimization methods;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611703&isnumber=10609862

W. Xia et al., "Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2073-2080, doi: 10.1109/ICRA57147.2024.10610744.Abstract: Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation. To effectively prompt LLMs with the kinematic structure of different objects, we design a unified kinematic knowledge parser, which represents various articulated objects as a unified textual description containing kinematic joints and contact location. Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method. Our evaluation spanned 48 instances across 16 distinct categories, revealing that our framework not only outperforms traditional methods on 8 seen categories but also shows a powerful zero-shot capability for 8 unseen articulated object categories with only 17 demonstrations. Moreover, the real-world experiments on 7 different object categories prove our framework’s adaptability in practical scenarios. Code is released at https://github.com/GeWu-Lab/LLM_articulated_object_manipulation. keywords: {Solid modeling;Three-dimensional displays;Large language models;Imitation learning;Kinematics;Reinforcement learning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610744&isnumber=10609862

Z. Zhou, J. Song, K. Yao, Z. Shu and L. Ma, "ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2081-2088, doi: 10.1109/ICRA57147.2024.10610065.Abstract: Motivated by the substantial achievements of Large Language Models (LLMs) in the field of natural language processing, recent research has commenced investigations into the application of LLMs for complex, long-horizon sequential task planning challenges in robotics. LLMs are advantageous in offering the potential to enhance the generalizability as task-agnostic planners and facilitate flexible interaction between human instructors and planning systems. However, task plans generated by LLMs often lack feasibility and correctness. To address this challenge, we introduce ISR-LLM, a novel framework that improves LLM-based planning through an iterative self-refinement process. The framework operates through three sequential steps: preprocessing, planning, and iterative self-refinement. During preprocessing, an LLM translator is employed to convert natural language input into a Planning Domain Definition Language (PDDL) formulation. In the planning phase, an LLM planner formulates an initial plan, which is then assessed and refined in the iterative self-refinement step by a validator. We examine the performance of ISR-LLM across three distinct planning domains. Our experimental results show that ISR-LLM is able to achieve markedly higher success rates in sequential task planning compared to state-of-the-art LLM-based planners. Moreover, it also preserves the broad applicability and generalizability of working with natural language instructions. keywords: {Large language models;Data preprocessing;Reliability engineering;Natural language processing;Planning;Iterative methods;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610065&isnumber=10609862

R. Kausar, F. Zayer, J. Viegas and J. Dias, "Efficient Hybrid Neuromorphic-Bayesian Model for Olfaction Sensing: Detection and Classification," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2089-2095, doi: 10.1109/ICRA57147.2024.10611648.Abstract: Olfaction sensing in autonomous robotics faces challenges in dynamic operations, energy efficiency, and edge processing. It necessitates a machine learning algorithm capable of managing real-world odor interference, ensuring resource efficiency for mobile robotics, and accurately estimating gas features for critical tasks such as odor mapping, localization, and alarm generation. This paper introduces a hybrid approach that exploits neuromorphic computing in combination with probabilistic inference to address these demanding requirements. Our approach implements a combination of a convolutional spiking neural network for feature extraction and a Bayesian spiking neural network for odor detection and identification. The developed algorithm is rigorously tested on a dataset for sensor drift compensation for robustness evaluation. Additionally, for efficiency evaluation, we compare the energy consumption of our model with a non-spiking machine learning algorithm under identical dataset and operating conditions. Our approach demonstrates superior efficiency alongside comparable accuracy outcomes. keywords: {Adaptation models;Machine learning algorithms;Spiking neural networks;Robot sensing systems;Rendering (computer graphics);Probabilistic logic;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611648&isnumber=10609862

Q. Zhang, Y. Yang, H. Fang, R. Geng and P. Jensfelt, "DeFlow: Decoder of Scene Flow Network in Autonomous Driving," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2105-2111, doi: 10.1109/ICRA57147.2024.10610278.Abstract: Scene flow estimation determines a scene’s 3D motion field, by predicting the motion of points in the scene, especially for aiding tasks in autonomous driving. Many networks with large-scale point clouds as input use voxelization to create a pseudo-image for real-time running. However, the voxelization process often results in the loss of point-specific features. This gives rise to a challenge in recovering those features for scene flow tasks. Our paper introduces DeFlow which enables a transition from voxel-based features to point features using Gated Recurrent Unit (GRU) refinement. To further enhance scene flow estimation performance, we formulate a novel loss function that accounts for the data imbalance between static and dynamic points. Evaluations on the Argoverse 2 scene flow task reveal that DeFlow achieves state-of-the-art results on large-scale point cloud data, demonstrating that our network has better performance and efficiency compared to others. The code is available at https://github.com/KTH-RPL/deflow. keywords: {Point cloud compression;Three-dimensional displays;Estimation;Sensor fusion;Feature extraction;Real-time systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610278&isnumber=10609862

H. Wang, X. Tan, X. Qiu and C. Qu, "Subequivariant Reinforcement Learning Framework for Coordinated Motion Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2112-2118, doi: 10.1109/ICRA57147.2024.10610563.Abstract: Effective coordination is crucial for motion control with reinforcement learning, especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with reinforcement learning. This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency. keywords: {Motion planning;Couplings;Process control;Reinforcement learning;Complexity theory;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610563&isnumber=10609862

A. Rosyid and B. El-Khasawneh, "A Large-scale Suction-based Climbing Parallel Robot for Wall Painting Application," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2119-2125, doi: 10.1109/ICRA57147.2024.10610067.Abstract: This paper presents a large-scale climbing robot that employs a parallel mechanism with three translational degrees of freedom as its locomotion method. Using a robot frame having a triangular pyramid shape, the robot provides a good stability during the locomotion and task execution. Three suction cups, called the perimeter cups, are attached to the vertices of the robot’s pyramid base, whereas three other suction cups called the middle cups, are attached to the end-effector of the parallel mechanism. The climbing motion is made by attaching and releasing the perimeter and middle cups one after another. The synchronization between the parallel mechanism’s motion and the suction cups during locomotion, as well as the improved gait trajectory, was established to ensure successful climbing. The control scheme of the robot integrates the servo control, the suction control, and the application control in a modular fashion. The successful climbing of the robot proves the scalability of the proposed climbing robot using active suction cups with an optimized design. Finally, a painting application was presented to demonstrate the robot’s capability to perform a wall painting task. keywords: {Shape;Buildings;Robot sensing systems;Climbing robots;End effectors;Trajectory;Synchronization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610067&isnumber=10609862

L. Zhang, K. Bai, Q. Li, Z. Chen and J. Zhang, "A Collision-Aware Cable Grasping Method in Cluttered Environment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2126-2132, doi: 10.1109/ICRA57147.2024.10610559.Abstract: We introduce a Cable Grasping-Convolutional Neural Network (CG-CNN) designed to facilitate robust cable grasping in cluttered environments. Utilizing physics simulations, we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers. We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts. The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques. Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot’s controller for execution. Grasping efficacy is assessed across both synthetic and real-world settings. Given our model’s implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing contemporary state-of-the-art approaches. Supplementary materials can be found at https://leizhang-public.github.io/cg-cnn/. keywords: {Sensitivity;Neural networks;MIMICs;Grasping;Predictive models;Robot sensing systems;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610559&isnumber=10609862

M. Schwegel and A. Kugi, "A Simple Computationally Efficient Path ILC for Industrial Robotic Manipulators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2133-2139, doi: 10.1109/ICRA57147.2024.10610623.Abstract: In this paper, a numerically efficient flexible control scheme for the absolute accuracy of industrial robots is presented and experimentally validated. A model-based controller that leverages all typically available parameters is combined with an online path iterative learning controller (ILC). The ILC law is employed to compensate for the unknown residual error dynamics caused by elastic and transmission effects. The proposed approach combines several benefits, including the possibility of a continuous execution of trials, a straightforward generalization of the learned data to different execution speeds, and learning from partial trials. The experimental validations on a 6-axis industrial robot with a laser tracker absolute measurement system show a 95% improvement in absolute accuracy after two trials. When the laser tracker is removed, the learned feedforward controller can sustain the accuracy achieved even without trial-by-trial learning. keywords: {Accuracy;Service robots;Measurement by laser beam;Industrial robots;Manipulators;Motors;Numerical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610623&isnumber=10609862

B. Alt et al., "RoboGrind: Intuitive and Interactive Surface Treatment with Industrial Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2140-2146, doi: 10.1109/ICRA57147.2024.10611143.Abstract: Surface treatment tasks such as grinding, sanding or polishing are a vital step of the value chain in many industries, but are notoriously challenging to automate. We present RoboGrind, an integrated system for the intuitive, interactive automation of surface treatment tasks with industrial robots. It combines a sophisticated 3D perception pipeline for surface scanning and automatic defect identification, an interactive voice-controlled wizard system for the AI-assisted bootstrapping and parameterization of robot programs, and an automatic planning and execution pipeline for force-controlled robotic surface treatment. RoboGrind is evaluated both under laboratory and real-world conditions in the context of refabricating fiberglass wind turbine blades. keywords: {Three-dimensional displays;Service robots;Pipelines;Knowledge based systems;Industrial robots;Wind turbines;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611143&isnumber=10609862

