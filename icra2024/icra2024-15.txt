S. Kuroki et al., "GenDOM: Generalizable One-shot Deformable Object Manipulation with Parameter-Aware Policy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14792-14799, doi: 10.1109/ICRA57147.2024.10611378.Abstract: Due to the inherent uncertainty in their deformability during motion, previous methods in deformable object manipulation, such as rope and cloth, often required hundreds of real-world demonstrations to train a manipulation policy for each object, which hinders their applications in our ever-changing world. To address this issue, we introduce GenDOM, a framework that allows the manipulation policy to handle different deformable objects with only a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable object parameters and training it with a diverse range of simulated deformable objects so that the policy can adjust actions based on different object parameters. At the time of inference, given a new object, GenDOM can estimate the deformable object parameters with only a single real-world demonstration by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations in a differentiable physics simulator. Empirical validations on both simulated and real-world object manipulation setups clearly show that our method can manipulate different objects with a single demonstration and significantly outperforms the baseline in both environments (a 62% improvement for in-domain ropes and a 15% improvement for out-of-distribution ropes in simulation, as well as a 26% improvement for ropes and a 50% improvement for cloths in the real world), demonstrating the effectiveness of our approach in one-shot deformable object manipulation. https://sites.google.com/view/gendom/home. keywords: {Deformable models;Training;Point cloud compression;Uncertainty;Task analysis;Robotics and automation;Physics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611378&isnumber=10609862

C. -H. Kung et al., "RiskBench: A Scenario-based Benchmark for Risk Identification," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14800-14807, doi: 10.1109/ICRA57147.2024.10610270.Abstract: Intelligent driving systems aim to achieve a zero-collision mobility experience, requiring interdisciplinary efforts to enhance safety performance. This work focuses on risk identification, the process of identifying and analyzing risks stemming from dynamic traffic participants and unexpected events. While significant advances have been made in the community, the current evaluation of different risk identification algorithms uses independent datasets, leading to difficulty in direct comparison and hindering collective progress toward safety performance enhancement. To address this limitation, we introduce RiskBench, a large-scale scenario-based benchmark for risk identification. We design a scenario taxonomy and augmentation pipeline to enable a systematic collection of ground truth risks under diverse scenarios. We assess the ability of ten algorithms to (1) detect and locate risks, (2) anticipate risks, and (3) facilitate decision-making. We conduct extensive experiments and summarize future research on risk identification. Our aim is to encourage collaborative endeavors in achieving a society with zero collisions. We have made our dataset and benchmark toolkit publicly at this project webpage. keywords: {Systematics;Heuristic algorithms;Taxonomy;Pipelines;Benchmark testing;Robustness;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610270&isnumber=10609862

D. Griesser, M. O. Franz and G. Umlauf, "Enhancing Inland Water Safety: The Lake Constance Obstacle Detection Benchmark," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14808-14814, doi: 10.1109/ICRA57147.2024.10610600.Abstract: Autonomous navigation on inland waters requires an accurate understanding of the environment in order to react to possible obstacles. Deep learning is a promising technique to detect obstacles robustly. However, supervised deep learning models require large data-sets to adjust their weights and to generalize to unseen data. Therefore, we equipped our research vessel with a laser scanner and a stereo camera to record a novel obstacle detection data-set for inland waters. We annotated 1974 stereo images and lidar point clouds with 3d bounding boxes. Furthermore, we provide an initial approach and a suitable metric2 to compare the results on the test data-set. The data-set is publicly available3 and seeks to make a contribution towards increasing the safety on inland waters. keywords: {Deep learning;Training;Point cloud compression;Three-dimensional displays;Laser radar;Lakes;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610600&isnumber=10609862

C. Parikh, R. Saluja, C. V. Jawahar and R. Kiran Sarvadevabhatla, "IDD-X: A Multi-View Dataset for Ego-relative Important Object Localization and Explanation in Dense and Unstructured Traffic," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14815-14821, doi: 10.1109/ICRA57147.2024.10609989.Abstract: Intelligent vehicle systems require a deep understanding of the interplay between road conditions, surrounding entities, and the ego vehicle’s driving behavior for safe and efficient navigation. This is particularly critical in developing countries where traffic situations are often dense and unstructured with heterogeneous road occupants. Existing datasets, predominantly geared towards structured and sparse traffic scenarios, fall short of capturing the complexity of driving in such environments. To fill this gap, we present IDD-X, a large-scale dual-view driving video dataset. With 697K bounding boxes, 9K important object tracks, and 1-12 objects per video, IDD-X offers comprehensive ego-relative annotations for multiple important road objects covering 10 categories and 19 explanation label categories. The dataset also incorporates rearview information to provide a more complete representation of the driving environment. We also introduce custom-designed deep networks aimed at multiple important object localization and per-object explanation prediction. Overall, our dataset and introduced prediction models form the foundation for studying how road conditions and surrounding entities affect driving behavior in complex traffic situations. keywords: {Location awareness;Navigation;Intelligent vehicles;Annotations;Roads;Predictive models;Developing countries},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609989&isnumber=10609862

J. Fang et al., "LiDAR-CS Dataset: LiDAR Point Cloud Dataset with Cross-Sensors for 3D Object Detection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14822-14829, doi: 10.1109/ICRA57147.2024.10611136.Abstract: Over the past few years, there has been remarkable progress in research on 3D point clouds and their use in autonomous driving scenarios has become widespread. However, deep learning methods heavily rely on annotated data and often face domain generalization issues. Unlike 2D images whose domains usually pertain to the texture information present in them, the features derived from a 3D point cloud are affected by the distribution of the points. The lack of a 3D domain adaptation benchmark leads to the common practice of training a model on one benchmark (e.g. Waymo) and then assessing it on another dataset (e.g. KITTI). This setting results in two distinct domain gaps: scenarios and sensors, making it difficult to analyze and evaluate the method accurately. To tackle this problem, this paper presents ${\color{Red}\text{LiDAR}}$ Dataset with ${\color{Red}\text{C}}{\text{ross}} - {\color{Red}\text{S}}{\text{ensors}}$ (LiDAR-CS Dataset), which contains large-scale annotated LiDAR point cloud under six groups of different sensors but with the same corresponding scenarios, captured from hybrid realistic LiDAR simulator. To our knowledge, LiDAR-CS Dataset is the first dataset that addresses the sensor-related gaps in the domain of 3D object detection in real traffic. Furthermore, we evaluate and analyze the performance using various baseline detectors and demonstrated its potential applications. Project page: https://opendriving.github.io/lidar-cs. keywords: {Point cloud compression;Training;Solid modeling;Three-dimensional displays;Laser radar;Object detection;Detectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611136&isnumber=10609862

P. Mortimer, R. Hagmanns, M. Granero, T. Luettel, J. Petereit and H. -J. Wuensche, "The GOOSE Dataset for Perception in Unstructured Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14838-14844, doi: 10.1109/ICRA57147.2024.10611298.Abstract: The potential for deploying autonomous systems can be significantly increased by improving the perception and interpretation of the environment. However, the development of deep learning-based techniques for autonomous systems in unstructured outdoor environments poses challenges due to limited data availability for training and testing. To address this gap, we present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines. This initiative aims to establish a common framework, enabling the seamless inclusion of existing datasets and a fast way to enhance the perception capabilities of various robots operating in unstructured environments. This framework also makes it possible to query data for specific weather conditions or sensor setups from a database in future. The dataset, pre-trained models for offroad perception, and additional documentation can be found at https://goose-dataset.de/. keywords: {Point cloud compression;Training;Autonomous systems;Navigation;Semantic segmentation;Ontologies;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611298&isnumber=10609862

D. Sheng, A. Yang, J. -R. Rizzo and C. Feng, "NYC-Indoor-VPR: A Long-Term Indoor Visual Place Recognition Dataset with Semi-Automatic Annotation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14853-14859, doi: 10.1109/ICRA57147.2024.10610564.Abstract: Visual Place Recognition (VPR) in indoor environments is beneficial to humans and robots for better localization and navigation. It is challenging due to appearance changes at various frequencies, and difficulties of obtaining ground truth metric trajectories for training and evaluation. This paper introduces the NYC-Indoor-VPR dataset, a unique and rich collection of over 36,000 images compiled from 13 distinct crowded scenes in New York City taken under varying lighting conditions with appearance changes. Each scene has multiple revisits across a year. To establish the ground truth for VPR, we propose a semiautomatic annotation approach that computes the positional information of each image. Our method specifically takes pairs of videos as input and yields matched pairs of images along with their estimated relative locations. The accuracy of this matching is refined by human annotators, who utilize our annotation software to correlate the selected keyframes. Finally, we present a benchmark evaluation of several state-of-the-art VPR algorithms using our annotated dataset, revealing its challenge and thus value for VPR research. keywords: {Training;Visualization;Annotations;Urban areas;Software algorithms;Benchmark testing;Software},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610564&isnumber=10609862

D. Cheng et al., "TreeScope: An Agricultural Robotics Dataset for LiDAR-Based Mapping of Trees in Forests and Orchards," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14860-14866, doi: 10.1109/ICRA57147.2024.10611103.Abstract: Data collection for forestry, timber, and agriculture relies on manual techniques which are labor-intensive and time-consuming. We seek to demonstrate that robotics offers improvements over these techniques and can accelerate agricultural research, beginning with semantic segmentation and diameter estimation of trees in forests and orchards. We present TreeScope v1.0, the first robotics dataset for precision agriculture and forestry addressing the counting and mapping of trees in forestry and orchards. TreeScope provides LiDAR data from agricultural environments collected with robotics platforms, such as UAV and mobile robot platforms carried by vehicles and human operators. In the first release of this dataset, we provide ground-truth data with over 1,800 manually annotated semantic labels for tree stems and field-measured tree diameters. We share benchmark scripts for these tasks that researchers may use to evaluate the accuracy of their algorithms. Finally, we run our open-source diameter estimation and off-the-shelf semantic segmentation algorithms and share our baseline results.The dataset can be found at https://treescope.org, and the data pre-processing and benchmark code is available at https://github.com/KumarRobotics/treescope. keywords: {Precision agriculture;Forests;Laser radar;Semantic segmentation;Semantics;Estimation;Manuals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611103&isnumber=10609862

S. Narayanan, D. Jayaraman and M. Chandraker, "Long-HOT: A Modular Hierarchical Approach for Long-Horizon Object Transport," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14867-14874, doi: 10.1109/ICRA57147.2024.10611192.Abstract: We aim to address key challenges in long-horizon embodied exploration and navigation by proposing a long-horizon object transport task called Long-HOT and a novel modular framework for temporally extended navigation. Agents in Long-HOT need to efficiently find and pick up target objects that are scattered in the environment, carry them to a goal location with load constraints, and optionally have access to a container. We propose a modular topological graph-based transport policy (HTP) that explores efficiently with the help of weighted frontiers. Our hierarchical approach uses a combination of motion planning algorithms to reach point goals within explored locations and object navigation policies for moving towards semantic targets at unknown locations. Experiments on both our proposed Habitat transport task and on MultiOn benchmarks show that our method outperforms baselines and prior works. Further, we analyze the agent’s behavior for the usage of the container and demonstrate meaningful generalization to harder transport scenes with training only on simpler versions of the task. keywords: {Training;Navigation;Semantics;Habitats;Containers;Benchmark testing;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611192&isnumber=10609862

B. Vu, T. Migimatsu and J. Bohg, "COAST: COnstraints And STreams for Task and Motion Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14875-14881, doi: 10.1109/ICRA57147.2024.10611670.Abstract: Task and Motion Planning (TAMP) algorithms solve long-horizon robotics tasks by integrating task planning with motion planning; the task planner proposes a sequence of actions towards a goal state and the motion planner verifies whether this action sequence is geometrically feasible for the robot. However, state-of-the-art TAMP algorithms do not scale well with the difficulty of the task and require an impractical amount of time to solve relatively small problems. We propose Constraints and Streams for Task and Motion Planning (COAST), a probabilistically-complete, sampling-based TAMP algorithm that combines stream-based motion planning with an efficient, constrained task planning strategy. We validate COAST on three challenging TAMP domains and demonstrate that our method outperforms baselines in terms of cumulative task planning time by an order of magnitude. You can find more supplementary materials on our project ${\color{blue}{\mathbf{website}}}$. keywords: {Bridges;Algorithms;Probabilistic logic;Planning;Task analysis;Streams;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611670&isnumber=10609862

T. Löw and S. Calinon, "Extending the Cooperative Dual-Task Space in Conformal Geometric Algebra," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14882-14887, doi: 10.1109/ICRA57147.2024.10610558.Abstract: In this work, we are presenting an extension of the cooperative dual-task space (CDTS) in conformal geometric algebra. The CDTS was first defined using dual quaternion algebra and is a well established framework for the simplified definition of tasks using two manipulators. By integrating conformal geometric algebra, we aim to further enhance the geometric expressiveness and thus simplify the modeling of various tasks. We show this formulation by first presenting the CDTS and then its extension that is based around a cooperative pointpair. This extension keeps all the benefits of the original formulation that is based on dual quaternions, but adds more tools for geometric modeling of the dual-arm tasks. We also present how this CGACDTS can be seamlessly integrated with an optimal control framework in geometric algebra that was derived in previous work. In the experiments, we demonstrate how to model different objectives and constraints using the CGA-CDTS. Using a setup of two Franka Emika robots we then show the effectiveness of our approach using model predictive control in real world experiments. keywords: {Algebra;Quaternions;Geometric modeling;Optimal control;Aerospace electronics;End effectors;Task analysis;Geometric Algebra;Dual-Arm Manipulation;Optimal Control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610558&isnumber=10609862

T. Xue, A. Razmjoo and S. Calinon, "D-LGP: Dynamic Logic-Geometric Program for Reactive Task and Motion Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14888-14894, doi: 10.1109/ICRA57147.2024.10610285.Abstract: Many real-world sequential manipulation tasks involve a combination of discrete symbolic search and continuous motion planning, collectively known as combined task and motion planning (TAMP). However, prevailing methods often struggle with the computational burden and intricate combinatorial challenges, limiting their applications for online replanning in the real world. To address this, we propose Dynamic Logic-Geometric Program (D-LGP), a novel approach integrating Dynamic Tree Search and global optimization for efficient hybrid planning. Through empirical evaluation on three benchmarks, we demonstrate the efficacy of our approach, showcasing superior performance in comparison to state-of-the-art techniques. We validate our approach through simulation and demonstrate its reactive capability to cope with online uncertainty and external disturbances in the real world. Project webpage: https://sites.google.com/view/dyn-lgp. keywords: {Uncertainty;Limiting;Dynamics;Benchmark testing;Planning;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610285&isnumber=10609862

J. Gao, P. Xie, X. Gao, Z. Sun, J. Wang and M. Q. . -H. Meng, "Indoor Exploration and Simultaneous Trolley Collection Through Task-Oriented Environment Partitioning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14895-14901, doi: 10.1109/ICRA57147.2024.10610500.Abstract: In this paper, we present a simultaneous exploration and object search framework for the application of autonomous trolley collection. For environment representation, a task-oriented environment partitioning algorithm is presented to extract diverse information for each sub-task. First, LiDAR data is classified as potential objects, walls, and obstacles after outlier removal. Segmented point clouds are then transformed into a hybrid map with the following functional components: object proposals to avoid missing trolleys during exploration; room layouts for semantic space segmentation; and polygonal obstacles containing geometry information for efficient motion planning. For exploration and simultaneous trolley collection, we propose an efficient exploration-based object search method. First, a traveling salesman problem with precedence constraints (TSP-PC) is formulated by grouping frontiers and object proposals. The next target is selected by prioritizing object search while avoiding excessive robot backtracking. Then, feasible trajectories with adequate obstacle clearance are generated by topological graph search. We validate the proposed framework through simulations and demonstrate the system with real-world autonomous trolley collection tasks. keywords: {Point cloud compression;Laser radar;Motion segmentation;Semantics;Layout;Search problems;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610500&isnumber=10609862

M. Toussaint, J. Ortiz-Haro, V. N. Hartmann, E. Karpas and W. Hönig, "Effort Level Search in Infinite Completion Trees with Application to Task-and-Motion Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14902-14908, doi: 10.1109/ICRA57147.2024.10611722.Abstract: Solving a Task-and-Motion Planning (TAMP) problem can be represented as a sequential (meta-) decision process, where early decisions concern the skeleton (sequence of logic actions) and later decisions concern what to compute for such skeletons (e.g., action parameters, bounds, RRT paths, or full optimal manipulation trajectories). We consider the general problem of how to schedule compute effort in such hierarchical solution processes. More specifically, we introduce infinite completion trees as a problem formalization, where before we can expand or evaluate a node, we have to solve a preemptible computational sub-problem of a priori unknown compute effort. Infinite branchings represent an infinite choice of random initializations of computational sub-problems. Decision making in such trees means to decide on where to invest compute or where to widen a branch. We propose a heuristic to balance branching width and compute depth using polynomial level sets. We show completeness of the resulting solver and that a round robin baseline strategy used previously for TAMP becomes a special case. Experiments confirm the robustness and efficiency of the method on problems including stochastic bandits and a suite of TAMP problems, and compare our approach to a round robin baseline. An appendix comparing the framework to bandit methods and proposing a corresponding tree policy version is found on the supplementary webpage 1. keywords: {Schedules;Systematics;Search problems;Skeleton;Robustness;Trajectory;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611722&isnumber=10609862

W. Jin and A. Martinoli, "Sense in Motion with Belief Clustering: Efficient Gas Source Localization with Mobile Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14909-14916, doi: 10.1109/ICRA57147.2024.10611440.Abstract: Given the patchy nature of gas plumes and the slow response of conventional gas sensors, the use of mobile robots for Gas Source Localization (GSL) tasks presents significant challenges. These aspects increase the difficulties in obtaining gas measurements, encompassing both qualitative and quantitative aspects. Most existing model-based GSL algorithms rely on lengthy stops at each sampling point to ensure accurate gas measurements. However, this approach not only prolongs the time required for a single measurement but also hinders sampling during robot motion, thus exacerbating the scarcity of available gas measurements. In this work, our goal is to push the boundaries in terms of continuity in sampling to enhance system efficiency. Firstly, we decouple and comprehensively evaluate the impact of both plume dynamics and gas sensor properties on the GSL performance. Secondly, we demonstrate that adopting a continuous sampling strategy, which has been generally overlooked in prior research, markedly enhances the system efficiency by obviating the prolonged measurement pauses and leveraging all the data gathered during the robot motion. Thirdly, we further expand the capabilities of the continuous sampling by introducing a novel informative path-planning strategy, which takes into account all the information gathered along the robot's movement. The proposed method is evaluated in both simulation and reality under different scenarios emulating indoor environmental conditions. keywords: {Location awareness;Robot motion;Robot sensing systems;Time measurement;Sensors;Motion measurement;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611440&isnumber=10609862

K. T. Ly, V. Semenov, M. Risiglione, W. Merkt and I. Havoutis, "R-LGP: A Reachability-guided Logic-geometric Programming Framework for Optimal Task and Motion Planning on Mobile Manipulators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14917-14923, doi: 10.1109/ICRA57147.2024.10611389.Abstract: This paper presents an optimization-based solution to task and motion planning (TAMP) on mobile manipulators. Logic-geometric programming (LGP) has shown promising capabilities for optimally dealing with hybrid TAMP problems that involve abstract and geometric constraints. However, LGP does not scale well to high-dimensional systems (e.g. mobile manipulators) and can suffer from obstacle avoidance issues due to local minima. In this work, we extend LGP with a sampling-based reachability graph to enable solving optimal TAMP on high-DoF mobile manipulators. The proposed reachability graph can incorporate environmental information (obstacles) to provide the planner with sufficient geometric constraints. This reachability-aware heuristic efficiently prunes infeasible sequences of actions in the continuous domain, hence, it reduces replanning by securing feasibility at the final full path trajectory optimization. Our framework proves to be time-efficient in computing optimal and collision-free solutions, while outperforming the current state of the art on metrics of success rate, planning time, path length and number of steps. We validate our framework on the physical Toyota HSR robot and report comparisons on a series of mobile manipulation tasks of increasing difficulty. Videos of the experiments are available here. keywords: {Measurement;Kinematics;Programming;Manipulators;Planning;Task analysis;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611389&isnumber=10609862

S. Levit, J. Ortiz-Haro and M. Toussaint, "Solving Sequential Manipulation Puzzles by Finding Easier Subproblems," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14924-14930, doi: 10.1109/ICRA57147.2024.10610974.Abstract: We consider a set of challenging sequential manipulation puzzles, where an agent has to interact with multiple movable objects and navigate narrow passages. Such settings are notoriously difficult for Task-and-Motion Planners, as they require interdependent regrasps and solving hard motion planning problems.In this paper, we propose to search over sequences of easier pick-and-place subproblems, which can lead to the solution of the manipulation puzzle. Our method combines a heuristic-driven forward search of subproblems with an optimization-based Task-and-Motion Planning solver. To guide the search, we introduce heuristics to generate and prioritize useful subgoals. We evaluate our approach on various manually designed and automatically generated scenes, demonstrating the benefits of auxiliary subproblems in sequential manipulation planning. keywords: {Navigation;Manuals;Search problems;Planning;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610974&isnumber=10609862

M. Mehl, M. Bartholdt, S. F. G. Ehlers, T. Seel and M. Schappler, "Adaptive State Estimation with Constant-Curvature Dynamics Using Force-Torque Sensors with Application to a Soft Pneumatic Actuator," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14939-14945, doi: 10.1109/ICRA57147.2024.10610370.Abstract: Using compliant materials leads to continuum robots undergoing large deformations. Their nonlinear behavior motivates the use of model-based controllers. They require state estimation as an essential step to be deployed. Available sensors are usually realized by introducing rigid bodies to the soft robot or inserting soft sensors made of materials different from the robot itself. Both approaches result in changes in the system’s dynamics. Optical measurements are problematic, especially in confined spaces. This can be avoided when the sensor is located at the robot's base. This paper studies the state estimation of a pneumatically actuated soft robot using the measured forces and torques at its base. For the first time, this is done using an unscented Kalman filter without restraining the dynamics to a planar or quasi-static motion while applying it to a real system. Real-time capability is achieved with our implementation. The state estimation is tested in a Cosserat rod simulation and on the physical system. The position is estimated with an accuracy of three to five millimeters for a 130 millimeter long pneumatic robot. keywords: {Accuracy;Computational modeling;Dynamics;Soft robotics;Pneumatic systems;Bending;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610370&isnumber=10609862

J. M. Bern, W. C. May, A. Osborn, F. Stella, S. Zargarzadeh and J. Hughes, "A Soft Robot Inverse Kinematics for Virtual Reality," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14957-14963, doi: 10.1109/ICRA57147.2024.10611603.Abstract: We show how a variety of techniques from Computer Graphics can be leveraged to intuitively control the shape (configuration) of arbitrary 3D Soft Robots in VR. Our pipeline, Virtual Reality Soft Robot Inverse Kinematics (VR-Soft IK), overcomes fundamental limitations of general-purpose drag-and-drop soft robot control interfaces by leaving the 2D computer screen for 3D Virtual Reality (VR). VR-Soft IK uses a simulation based on the Finite Element Method (FEM) and a control method based on sensitivity analysis. Additionally, we show that our general control pipeline can be fused with techniques from 3D character animation to skin our simulation with a high-resolution surface mesh, pointing a way toward Mixed Reality Soft Robots. This full Skinned VR-Soft IK pipeline uses skeletal animation and GPU picking. We demonstrate the utility of our pipeline by doing real-time, open-loop control of the real-world 3D soft robotic arm Helix. keywords: {Three-dimensional displays;Pipelines;Mixed reality;Virtual reality;Kinematics;Soft robotics;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611603&isnumber=10609862

U. Yoo, Z. Lopez, J. Ichnowski and J. Oh, "POE: Acoustic Soft Robotic Proprioception for Omnidirectional End-effectors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14980-14987, doi: 10.1109/ICRA57147.2024.10611598.Abstract: Shape estimation is crucial for precise control of soft robots. However, soft robot shape estimation and proprioception are challenging due to their complex deformation behaviors and infinite degrees of freedom. Their continuously deforming bodies complicate integrating rigid sensors and reliably estimating its shape. In this work, we present Proprioceptive Omnidirectional End-effector (POE), a tendon-driven soft robot with six embedded microphones. We first introduce novel applications of 3D reconstruction methods to acoustic signals from the microphones for soft robot shape proprioception. To improve the proprioception pipeline’s training efficiency and model prediction consistency, we present POE-M. POE-M predicts key point positions from acoustic signal observations and uses an energy-minimization method to reconstruct a physically admissible high-resolution mesh of POE. We evaluate mesh reconstruction on simulated data and the POE-M pipeline with real-world experiments. Ablation studies suggest POE-M’s guidance of the key points during the mesh reconstruction process provides robustness and stability to the pipeline. POE-M reduced the maximum Chamfer distance error by 23.1 % compared to the state-of-the-art end-to-end soft robot proprioception models and achieved 4.91 mm average Chamfer distance error during evaluation. Supplemental materials, experiment data, and visualizations are available at sites.google.com/view/acoustic-poe. keywords: {Training;Three-dimensional displays;Shape;Pipelines;Propioception;Estimation;Soft robotics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611598&isnumber=10609862

S. Deng et al., "A Data-Driven Approach to Geometric Modeling of Systems with Low-Bandwidth Actuator Dynamics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14988-14994, doi: 10.1109/ICRA57147.2024.10610880.Abstract: It is challenging to perform system identification on soft robots due to their underactuated, high-dimensional dynamics. In this work, we present a data-driven modeling framework, based on geometric mechanics (also known as gauge theory) that can be applied to systems with low-bandwidth control of the system’s internal configuration. This method constructs a series of connected models comprising actuator and locomotor dynamics based on data points from stochastically perturbed, repeated behaviors. By deriving these connected models from general formulations of dissipative Lagrangian systems with symmetry, we offer a method that can be applied broadly to robots with first-order, low-pass actuator dynamics, including swelling-driven actuators used in hydrogel crawlers. These models accurately capture the dynamics of the system shape and body movements of a simplified swimming robot model. We further apply our approach to a stimulus-responsive hydrogel simulator that captures the complexity of chemomechanical interactions that drive shape changes in biomedically relevant micromachines. Finally, we propose an approach of numerically optimizing control signals by iteratively refining models, which is applied to optimize the input waveform for the hydrogel crawler. This transfer to realistic environments provides promise for applications in locomotor design and biomedical engineering. keywords: {Actuators;Shape;Hydrogels;Biological system modeling;Crawlers;Refining;Geometric modeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610880&isnumber=10609862

Z. J. Patterson, C. D. Santina and D. Rus, "Modeling and Control of Intrinsically Elasticity Coupled Soft-Rigid Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 14995-15001, doi: 10.1109/ICRA57147.2024.10610229.Abstract: While much work has been done recently in the realm of model-based control of soft robots and soft-rigid hybrids, most works examine robots that have an inherently serial structure. While these systems have been prevalent in the literature, there is an increasing trend toward designing soft-rigid hybrids with intrinsically coupled elasticity between various degrees of freedom. In this work, we seek to address the issues of modeling and controlling such structures, particularly when underactuated. We introduce several simple models for elastic coupling, typical of those seen in these systems. We then propose a controller that compensates for the elasticity, and we prove its stability with Lyapunov methods without relying on the elastic dominance assumption. This controller is applicable to the general class of underactuated soft robots. After evaluating the controller in simulated cases, we then develop a simple hardware platform to evaluate both the models and the controller. Finally, using the hardware, we demonstrate a novel use case for underactuated, elastically coupled systems in "sensorless" force control. keywords: {Couplings;Elasticity;Soft robotics;Robot sensing systems;Market research;Hardware;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610229&isnumber=10609862

M. Arifin, Y. Kage, Y. Yang, A. Schmitz and S. Sugano, "A Combination of a Controllable Clutch and an Oscillating Slider Crank Mechanism for Ease of Direct-Teaching with Various Payloads," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15002-15008, doi: 10.1109/ICRA57147.2024.10610146.Abstract: Direct teaching is a straightforward way of teaching new motion to robots. Active methods with torque sensors, for example, can be used so that the robot can follow the movements of the human, but such methods introduce delays. Alternatively, series clutch actuators are easily backdrivable without delay. However, vertical joints are subject to gravity torques, which need to be compensated when disengaging the clutch. We implemented passive gravity compensation to counteract the robot’s weight, but this mechanism cannot compensate for varying payloads, as adjustable passive gravity compensation is relatively slow and mechanically complex. The varying payload causes an unintended joint movement, i.e. the arm falls down on its own, which is unacceptable during direct teaching. Therefore, this paper demonstrates how the torque output controlled with series clutch actuators can be used to compensate for varying payloads while maintaining high backdrivability. The proposed method is evaluated on a collaborative robot with a clutch in series for each actuator. Real-world experiments with payloads from 0 to 3 kg are conducted. During the experiments, the operator force is measured to evaluate the proposed method. keywords: {Actuators;Torque;Force measurement;Education;Robot sensing systems;Delays;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610146&isnumber=10609862

M. Levy, N. Saini and A. Shrivastava, "WayEx: Waypoint Exploration using a Single Demonstration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15009-15016, doi: 10.1109/ICRA57147.2024.10611088.Abstract: We propose WayEx, a new method for learning complex goal-conditioned robotics tasks from a single demonstration. Our approach distinguishes itself from existing imitation learning methods by demanding fewer expert examples and eliminating the need for information about the actions taken during the demonstration. This is accomplished by introducing a new reward function and employing a knowledge expansion technique. We demonstrate the effectiveness of WayEx, our waypoint exploration strategy, across six diverse tasks, showcasing its applicability in various environments. Notably, our method significantly reduces training time by ∼50% as compared to traditional reinforcement learning methods. WayEx obtains a higher reward than existing imitation learning methods given only a single demonstration. Furthermore, we demonstrate its success in tackling complex environments where standard approaches fall short. Appendix is available at: https://waypoint-ex.github.io. keywords: {Training;Imitation learning;Reinforcement learning;Task analysis;Standards;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611088&isnumber=10609862

A. Straižys, M. Burke and S. Ramamoorthy, "Generating robotic elliptical excisions with human-like tool-tissue interactions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15017-15023, doi: 10.1109/ICRA57147.2024.10610990.Abstract: In surgery, the application of appropriate force levels is critical for the success and safety of a given procedure. While many studies are focused on measuring in situ forces, little attention has been devoted to relating these observed forces to surgical techniques. Answering questions like "Can certain changes to a surgical technique result in lower forces and increased safety margins?" could lead to improved surgical practice, and importantly, patient outcomes. However, such studies would require a large number of trials and professional surgeons, which is generally impractical to arrange. Instead, we show how robots can learn several variations of a surgical technique from a smaller number of surgical demonstrations and interpolate learnt behaviour via a parameterised skill model. This enables a large number of trials to be performed by a robotic system and the analysis of surgical techniques and their downstream effects on tissue. Here, we introduce a parameterised model of the elliptical excision skill and apply a Bayesian optimisation scheme to optimise the excision behaviour with respect to expert ratings, as well as individual characteristics of excision forces. Results show that the proposed framework can successfully align the generated robot behaviour with subjects across varying levels of proficiency in terms of excision forces. keywords: {Force measurement;Force;Surgery;Safety;Bayes methods;Robots;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610990&isnumber=10609862

F. Stulp, A. Colomé and C. Torras, "Fitting Parameters of Linear Dynamical Systems to Regularize Forcing Terms in Dynamical Movement Primitives," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15024-15030, doi: 10.1109/ICRA57147.2024.10610581.Abstract: Due to their flexibility and ease of use, Dynamical Movement Primitives (DMPs) are widely used in robotics applications and research. DMPs combine linear dynamical systems to achieve robustness to perturbations and adaptation to moving targets with non-linear function approximators to fit a wide range of demonstrated trajectories.We propose a novel DMP formulation with a generalized logistic function as a delayed goal system. This formulation inherently has low initial jerk, and generates the bell-shaped velocity profiles that are typical of human movement. As the novel formulation is more expressive, it is able to fit a wide range of human demonstrations well, also without a non-linear forcing term. We exploit this increased expressiveness by automating the fitting of the dynamical system parameters through opti-mization. Our experimental evaluation demonstrates that this optimization regularizes the forcing term, and improves the interpolation accuracy of parametric DMPs. keywords: {Interpolation;Accuracy;Perturbation methods;Fitting;Robustness;Dynamical systems;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610581&isnumber=10609862

H. Fang et al., "AirExo: Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15031-15038, doi: 10.1109/ICRA57147.2024.10610799.Abstract: While humans can use parts of their arms other than the hands for manipulations like gathering and supporting, whether robots can effectively learn and perform the same type of operations remains relatively unexplored. As these manipulations require joint-level control to regulate the complete poses of the robots, we develop AirExo, a low-cost, adaptable, and portable dual-arm exoskeleton, for teleoperation and demonstration collection. As collecting teleoperated data is expensive and time-consuming, we further leverage AirExo to collect cheap in-the-wild demonstrations at scale. Under our in-the-wild learning framework, we show that with only 3 minutes of the teleoperated demonstrations, augmented by diverse and extensive in-the-wild data collected by AirExo, robots can learn a policy that is comparable to or even better than one learned from teleoperated demonstrations lasting over 20 minutes. Experiments demonstrate that our approach enables the model to learn a more general and robust policy across the various stages of the task, enhancing the success rates in task completion even with the presence of disturbances. Project website: airexo.github.io. keywords: {Costs;Atmospheric modeling;Exoskeletons;Manipulators;Robustness;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610799&isnumber=10609862

D. Papadimitriou and D. S. Brown, "Bayesian Constraint Inference from User Demonstrations Based on Margin-Respecting Preference Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15039-15046, doi: 10.1109/ICRA57147.2024.10611095.Abstract: It is crucial for robots to be aware of the presence of constraints in order to acquire safe policies. However, explicitly specifying all constraints in an environment can be a challenging task. State-of-the-art constraint inference algorithms learn constraints from demonstrations, but tend to be computationally expensive and prone to instability issues. In this paper, we propose a novel Bayesian method that infers constraints based on preferences over demonstrations. The main advantages of our proposed approach are that it 1) infers constraints without calculating a new policy at each iteration, 2) uses a simple and more realistic ranking of groups of demonstrations, without requiring pairwise comparisons over all demonstrations, and 3) adapts to cases where there are varying levels of constraint violation. Our empirical results demonstrate that our proposed Bayesian approach infers constraints of varying severity, more accurately than state-of-the-art constraint inference methods. Code and videos: https://sites.google.com/berkeley.edu/pbicrl. keywords: {Adaptation models;Codes;Inference algorithms;Bayes methods;Task analysis;Robots;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611095&isnumber=10609862

W. Zhi, T. Zhang and M. Johnson-Roberson, "Instructing Robots by Sketching: Learning from Demonstration via Probabilistic Diagrammatic Teaching," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15047-15053, doi: 10.1109/ICRA57147.2024.10611349.Abstract: Learning from Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions intuitively. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, then applies ray-tracing to find corresponding regions in 3D Cartesian space, and fits a probabilistic model of motion trajectories to these regions. New motion trajectories, which mimic those sketched by the user, can then be generated from the probabilistic model. We empirically validate our framework both in simulation and on real robots, which include a fixed-base manipulator and a quadruped-mounted manipulator. keywords: {Solid modeling;Three-dimensional displays;Education;Ray tracing;Probabilistic logic;Manipulators;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611349&isnumber=10609862

J. Wang, O. Donca and D. Held, "Learning Distributional Demonstration Spaces for Task-Specific Cross-Pose Estimation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15054-15060, doi: 10.1109/ICRA57147.2024.10611004.Abstract: Relative placement tasks are an important category of tasks in which one object needs to be placed in a desired pose relative to another object. Previous work has shown success in learning relative placement tasks from just a small number of demonstrations when using relational reasoning networks with geometric inductive biases. However, such methods cannot flexibly represent multimodal tasks, like a mug hanging on any of n racks. We propose a method that incorporates additional properties that enable learning multimodal relative placement solutions, while retaining the provably translation-invariant and relational properties of prior work. We show that our method is able to learn precise relative placement tasks with only 10-20 multimodal demonstrations with no human annotations across a diverse set of objects within a category. Supplementary information can be found on the website: https://sites.google.com/view/tax-posed/home. keywords: {Point cloud compression;Annotations;Estimation;Cognition;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611004&isnumber=10609862

A. Abyaneh, M. S. Guzmán and H. -C. Lin, "Globally Stable Neural Imitation Policies," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15061-15067, doi: 10.1109/ICRA57147.2024.10610791.Abstract: Imitation learning mitigates the resource-intensive nature of learning policies from scratch by mimicking expert behavior. While existing methods can accurately replicate expert demonstrations, they often exhibit unpredictability in unexplored regions of the state space, thereby raising major safety concerns when facing perturbations. We propose SNDS, an imitation learning approach aimed at efficient training of scalable neural policies while formally ensuring global stability. SNDS leverages a neural architecture that enables the joint training of the policy and its associated Lyapunov candidate to ensure global stability throughout the learning process. We validate our approach through extensive simulations and deploy the trained policies on a real-world manipulator arm. The results confirm SNDS’s ability to address instability, accuracy, and computational intensity challenges highlighted in the literature, positioning it as a promising solution for scalable and stable policy learning in complex environments. keywords: {Training;Perturbation methods;Imitation learning;Computer architecture;Reliability theory;Manipulators;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610791&isnumber=10609862

H. Wang, W. Zhi, G. Batista and R. Chandra, "Pedestrian Trajectory Prediction Using Dynamics-based Deep Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15068-15075, doi: 10.1109/ICRA57147.2024.10609993.Abstract: Pedestrian trajectory prediction plays an important role in autonomous driving systems and robotics. Recent work utilizing prominent deep learning models for pedestrian motion prediction makes limited a priori assumptions about human movements, resulting in a lack of explainability and explicit constraints enforced on predicted trajectories. We present a dynamics-based deep learning framework with a novel asymptotically stable dynamical system integrated into a Transformer-based model. We use an asymptotically stable dynamical system to model human goal-targeted motion by enforcing the human walking trajectory, which converges to a predicted goal position, and to provide the Transformer model with prior knowledge and explainability. Our framework features the Transformer model that works with a goal estimator and dynamical system to learn features from pedestrian motion history. The results show that our framework outperforms prominent models using five benchmark human motion datasets. keywords: {Deep learning;Legged locomotion;Pedestrians;Predictive models;Benchmark testing;Transformers;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609993&isnumber=10609862

L. Tao et al., "POAQL: A Partially Observable Altruistic Q-Learning Method for Cooperative Multi-Agent Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15076-15082, doi: 10.1109/ICRA57147.2024.10610745.Abstract: Multi-Agent Path Finding (MAPF) is an important issue in multi-agent cooperation. Many studies apply MultiAgent Reinforcement Learning (MARL) to solve MAPF in partially observable settings. The objective of cooperative MARL is to maximize the cumulative team reward. Nevertheless, in partially observable settings, the team reward is misleading due to unpredictable factors from the behavior and state of unobserved agents. To address this issue, we propose a Partially Observable Altruistic Q-learning (POAQL) method. POAQL considers the cumulative reward of the observed subteam instead of the whole team, where Altruistic Q-learning plays an important role in learning the subteam action value. In addition, we design a new conflict resolution without additional guidance to emphasize the cooperative nature of MARL frameworks. Experimental results show that POAQL outperforms existing reinforcement learning methods in terms of efficiency and performance. keywords: {Q-learning;Costs;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610745&isnumber=10609862

T. Yoneda et al., "Statler: State-Maintaining Language Models for Embodied Reasoning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15083-15091, doi: 10.1109/ICRA57147.2024.10610634.Abstract: There has been a significant research interest in employing large language models to empower intelligent robots with complex reasoning. Existing work focuses on harnessing their abilities to reason about the histories of their actions and observations. In this paper, we explore a new dimension in which large language models may benefit robotics planning. In particular, we propose Statler, a framework in which large language models are prompted to maintain an estimate of the world state, which are often unobservable, and track its transition as new actions are taken. Our framework then conditions each action on the estimate of the current world state. Despite being conceptually simple, our Statler framework significantly outperforms strong competing methods (e.g., Code-as-Policies) on several robot planning tasks. Additionally, it has the potential advantage of scaling up to more challenging long-horizon planning tasks. We release our code here. keywords: {Codes;Large language models;Cognition;Planning;History;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610634&isnumber=10609862

Y. Gan et al., "Multi-Granular Transformer for Motion Prediction with LiDAR," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15092-15098, doi: 10.1109/ICRA57147.2024.10610704.Abstract: Motion prediction has been an essential component of autonomous driving systems since it handles highly uncertain and complex scenarios involving moving agents of different types. In this paper, we propose a Multi-Granular TRansformer (MGTR) framework, an encoder-decoder network that exploits context features in different granularities for different kinds of traffic agents. To further enhance MGTR’s capabilities, we leverage LiDAR point cloud data by incorporating LiDAR semantic features from an off-the-shelf LiDAR feature extractor. We evaluate MGTR on Waymo Open Dataset motion prediction benchmark and show that the proposed method achieved state-of-the-art performance, ranking 1st on its leaderboard 1. keywords: {Point cloud compression;Laser radar;Pedestrians;Semantics;Predictive models;Benchmark testing;Transformers;Motion Prediction;Transformer;Autonomous Driving},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610704&isnumber=10609862

A. Böhm et al., "What Matters for Active Texture Recognition With Vision-Based Tactile Sensors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15099-15105, doi: 10.1109/ICRA57147.2024.10610274.Abstract: This paper explores active sensing strategies that employ vision-based tactile sensors for robotic perception and classification of fabric textures. We formalize the active sampling problem in the context of tactile fabric recognition and provide an implementation of information-theoretic exploration strategies based on minimizing predictive entropy and variance of probabilistic models. Through ablation studies and human experiments, we investigate which components are crucial for quick and reliable texture recognition. Along with the active sampling strategies, we evaluate neural network architectures, representations of uncertainty, influence of data augmentation, and dataset variability. By evaluating our method on a previously published Active Clothing Perception Dataset and on a real robotic system, we establish that the choice of the active exploration strategy has only a minor influence on the recognition accuracy, whereas data augmentation and dropout rate play a significantly larger role. In a comparison study, while humans achieve 66.9% recognition accuracy, our best approach reaches 90.0% in under 5 touches, highlighting that vision-based tactile sensors are highly effective for fabric texture recognition. keywords: {Accuracy;Uncertainty;Neural networks;Tactile sensors;Predictive models;Data augmentation;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610274&isnumber=10609862

M. Farsang, M. Lechner, D. Lung, R. Hasani, D. Rus and R. Grosu, "Learning with Chemical versus Electrical Synapses Does it Make a Difference?," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15106-15112, doi: 10.1109/ICRA57147.2024.10611016.Abstract: Bio-inspired neural networks have the potential to advance our understanding of neural computation and improve the state-of-the-art of AI systems. Bio-electrical synapses directly transmit neural signals, by enabling fast current flow between neurons. In contrast, bio-chemical synapses transmit neural signals indirectly, through neurotransmitters. Prior work showed that interpretable dynamics for complex robotic control, can be achieved by using chemical synapses, within a sparse, bio-inspired architecture, called Neural Circuit Policies (NCPs). However, a comparison of these two synaptic models, within the same architecture, remains an unexplored area. In this work we aim to determine the impact of using chemical synapses compared to electrical synapses, in both sparse and all-to-all connected networks. We conduct experiments with autonomous lane-keeping through a photorealistic autonomous driving simulator to evaluate their performance under diverse conditions and in the presence of noise. The experiments highlight the substantial influence of the architectural and synaptic-model choices, respectively. Our results show that employing chemical synapses yields noticeable improvements compared to electrical synapses, and that NCPs lead to better results in both synaptic models. keywords: {Wiring;Biological system modeling;Neurons;Computer architecture;Robot sensing systems;Robustness;Synapses},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611016&isnumber=10609862

B. Pan, J. Jiao, J. Pang and J. Cheng, "Distill-then-prune: An Efficient Compression Framework for Real-time Stereo Matching Network on Edge Devices," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15113-15120, doi: 10.1109/ICRA57147.2024.10611085.Abstract: In recent years, numerous real-time stereo matching methods have been introduced, but they often lack accuracy. These methods attempt to improve accuracy by introducing new modules or integrating traditional methods. However, the improvements are only modest. In this paper, we propose a novel strategy by incorporating knowledge distillation and model pruning to overcome the inherent trade-off between speed and accuracy. As a result, we obtained a model that maintains real-time performance while delivering high accuracy on edge devices. Our proposed method involves three key steps. Firstly, we review state-of-the-art methods and design our lightweight model by removing redundant modules from those efficient models through a comparison of their contributions. Next, we leverage the efficient model as the teacher to distill knowledge into the lightweight model. Finally, we systematically prune the lightweight model to obtain the final model. Through extensive experiments conducted on two widely-used benchmarks, Sceneflow and KITTI, we perform ablation studies to analyze the effectiveness of each module and present our state-of-the-art results. keywords: {Performance evaluation;Training;Accuracy;Quantization (signal);Reviews;Design methodology;Neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611085&isnumber=10609862

F. Li, P. Hu, Q. Song and R. Huang, "Incremental 3D Reconstruction through a Hybrid Explicit-and-Implicit Representation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15121-15127, doi: 10.1109/ICRA57147.2024.10610868.Abstract: 3D reconstruction is an important task in computer vision and is widely used in robotics and autonomous driving. When building large-scale scenes, limitations in computing resources and the difficulty of accessing the entire dataset in a single task are inevitable. Therefore, an incremental reconstruction approach is desired. On the one hand, traditional explicit 3D reconstruction methods such as SLAM and SFM require global optimization, which means that time and space resources increase dramatically with the growth of training data. On the other hand, implicit methods like Neural Radiation Fields (NeRF) suffer from catastrophic forgetting if trained incrementally. In this paper, we incrementally reconstruct 3D models in a hybrid representation, where the density of the radiation field is formulated by a voxel grid, and the view-dependent color information of the points is inferred by a shallow MLP. The expansion of the voxel grid and the distillation of the shallow MLP are efficient in this case. Experimental results demonstrate that our incremental method achieves a level of accuracy on par with approaches employing global optimization techniques. keywords: {Solid modeling;Three-dimensional displays;Simultaneous localization and mapping;Neural networks;Training data;Reconstruction algorithms;Rendering (computer graphics)},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610868&isnumber=10609862

G. M. Caddeo, A. Maracani, P. D. Alfano, N. A. Piga, L. Rosasco and L. Natale, "Sim2Real Bilevel Adaptation for Object Surface Classification using Vision-Based Tactile Sensors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15128-15134, doi: 10.1109/ICRA57147.2024.10610028.Abstract: In this paper, we address the Sim2Real gap in the field of vision-based tactile sensors for classifying object surfaces. We train a Diffusion Model to bridge this gap using a relatively small dataset of real-world images randomly collected from unlabeled everyday objects via the DIGIT sensor. Subsequently, we employ a simulator to generate images by uniformly sampling the surface of objects from the YCB Model Set. These simulated images are then translated into the real domain using the Diffusion Model and automatically labeled to train a classifier. During this training, we further align features of the two domains using an adversarial procedure. Our evaluation is conducted on a dataset of tactile images obtained from a set of ten 3D-printed YCB objects. The results reveal a total accuracy of 81.9%, a significant improvement compared to the 34.7% achieved by the classifier trained solely on simulated images. This demonstrates the effectiveness of our approach. We further validate our approach using the classifier on a 6D object pose estimation task from tactile data. keywords: {Training;Bridges;Accuracy;Pose estimation;Tactile sensors;Manuals;Diffusion models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610028&isnumber=10609862

S. Jadav, J. Heidersberger, C. Ott and D. Lee, "Shared Autonomy via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstrations*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15151-15157, doi: 10.1109/ICRA57147.2024.10610761.Abstract: This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture. For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner. Therefore, two key components are addressed in this work: motion generation and shared autonomy. We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance. Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority. Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces. System passivity is ensured by an energy-tank based task passivation strategy. The framework’s efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot. keywords: {Collaboration;Generators;Encoding;Trajectory;Impedance;Task analysis;Force control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610761&isnumber=10609862

T. Osa and T. Harada, "Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behaviors and Adversarial Style Sampling for Assistive Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15158-15164, doi: 10.1109/ICRA57147.2024.10611719.Abstract: Autonomous assistance of people with motor impairments is one of the most promising applications of autonomous robotic systems. Recent studies have reported encouraging results using deep reinforcement learning (RL) in the healthcare domain. Previous studies showed that assistive tasks can be formulated as multi-agent RL, wherein there are two agents: a caregiver and a care-receiver. However, policies trained in multi-agent RL are often sensitive to the policies of other agents. In such a case, a trained caregiver’s policy may not work for different care-receivers. To alleviate this issue, we propose a framework that learns a robust caregiver’s policy by training it for diverse care-receiver responses. In our framework, diverse care-receiver responses are autonomously learned through trials and errors. In addition, to robustify the care-giver’s policy, we propose a strategy for sampling a care-receiver’s response in an adversarial manner during the training. We evaluated the proposed method using tasks in an Assistive Gym. We demonstrate that policies trained with a popular deep RL method are vulnerable to changes in policies of other agents and that the proposed framework improves the robustness against such changes. keywords: {Training;Medical services;Robot sensing systems;Motors;Deep reinforcement learning;Robustness;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611719&isnumber=10609862

L. Chen, Z. J. Hu, Y. Huang, E. Burdet and F. R. y. Baena, "Human Robot Shared Control in Surgery: A Performance Assessment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15165-15171, doi: 10.1109/ICRA57147.2024.10611642.Abstract: While surgical robots, such as the da Vinci Surgical System, have become prevalent in minimally invasive surgery, they are predominantly used by the human operator to directly teleoperate the tools. This paper aims to analyse the different methods of human robot shared control in the surgical domain. We propose a reinforcement learning algorithm, transverse generative adversarial imitation learning (tGAIL), which is employed to train the robot from the expert’s demonstration and show competitive generalization ability compared to inverse reinforcement learning and conventional GAIL. We then propose a priority-changing shared control method to effectively combine the surgeon and robot’s strengths by dynamically adjusting control priority based on the deviation distance. We show that using this method in a supervision framework boosts the performance of the human operator when completing the peg transfer task. By learning from the expert and collaborating with the human during the task, the intelligent agent can help to reduce operation time by 31.7% and the human input by 60.5% compared to direct teleoperation. keywords: {Minimally invasive surgery;Medical robotics;Imitation learning;Heuristic algorithms;Reinforcement learning;Needles;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611642&isnumber=10609862

L. Zha et al., "Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15172-15179, doi: 10.1109/ICRA57147.2024.10610455.Abstract: Today’s robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Human corrective feedback is a crucial form of guidance to enable such generalization. However, adapting to and learning from online human corrections is a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can be arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings. DROC is able to respond to a sequence of online language corrections that address failures in both high-level task plans and low-level skill primitives. We demonstrate that DROC effectively distills the relevant information from the sequence of online corrections in a knowledge base and retrieves that knowledge in settings with new task or object instances. DROC outperforms other techniques that directly generate robot code via LLMs [1] by using only half of the total number of corrections needed in the first round and requires little to no corrections after two iterations. We show further results and videos on our project website: https://sites.google.com/stanford.edu/droc. keywords: {Visualization;Codes;Large language models;Knowledge based systems;Task analysis;Robots;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610455&isnumber=10609862

F. Shao and F. Ficuciello, "An Intuitive Manual Guidance Scheme to Operate Rotation and Translation Simultaneously," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15180-15186, doi: 10.1109/ICRA57147.2024.10610298.Abstract: During certain human-robot collaboration tasks, the operator interacts with the robot by hand guidance to adjust the end-effector pose for spatial operations. The rotational operation is less intuitive to humans than translation. In fact, imagining the path to the target orientation is more challenging. In the literature related to control strategies for robot manual guidance, it is usually proposed to control translation and rotation independently. Our research explored and quantified the factors that influence operational intuition. A Virtual Fixture spatial guidance framework with intuition maintenance is proposed. This novel guidance scheme enables operators to effortlessly and simultaneously control both orientation and position in an intuitive way. High operation precision and efficiency can be achieved without interfering with the main task by exploring the null space with constraint optimization. keywords: {Geometry;Constraint optimization;Fixtures;Null space;Manuals;End effectors;Maintenance;hand guidance;human-robot collaboration;virtual fixture},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610298&isnumber=10609862

H. Zheng, Z. J. Hu, Y. Huang, X. Cheng, Z. Wang and E. Burdet, "A User-Centered Shared Control Scheme with Learning from Demonstration for Robotic Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15195-15201, doi: 10.1109/ICRA57147.2024.10611089.Abstract: The utilization of shared control in the realm of surgical robotics augments precision and safety by amalgamating human expertise with autonomous assistance. This paper proposes a user-centered shared control framework enabling a robot to learn from expert demonstration, predict operators’ intent and modulate control authority to provide natural assistance when needed. We employ deep inverse reinforcement learning (IRL) to enable the robot to learn path planning from expert demonstrations with fast convergence, subsequently enhancing the policy with a potential field method. The control authority is allocated seamlessly between the human operator and the autonomous agent based on the prediction of operators’ movement from an adaptive filter and fuzzy logic inference. The proposed method is executed using the da Vinci Research Kit (dVRK) robot in a simulation environment, and its effectiveness is assessed through user performance evaluation in a trajectory tracking task. Compared to direct control and simple shared control, the proposed shared control scheme exhibits superior tracking accuracy and trajectory smoothness under external disturbances. Subjective responses underscore users’ perception of the method’s efficacy in enhancing their performance. keywords: {Performance evaluation;Fuzzy logic;Medical robotics;Trajectory tracking;Surgery;Reinforcement learning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611089&isnumber=10609862

M. Riechmann, A. Kirsch, M. Koenig and J. Rexilius, "Virtual Borders in 3D: Defining a Drone’s Movement Space Using Augmented Reality," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15202-15208, doi: 10.1109/ICRA57147.2024.10610259.Abstract: Robots are increasingly finding their way into home environments, where they can assist with household tasks like vacuuming or surveilling. While the robots can navigate on their own, users might not want them to go everywhere or not in a specific way. For example, users might not want a drone to fly over a table where important letters and the newspaper are stored, even though it is the shortest path to the goal. Therefore, an application is required, that is easy to learn and to apply even for inexperienced users.In this paper, we present a framework that uses a tablet as augmented reality (AR) device to modify a robot’s movement space in 3D. A user can define virtual borders in the real world with the tablet and add them to a map, changing the navigational behavior of the robot. The framework is evaluated by a user study with inexperienced participants that verifies our approach. Further analyses show, that even complex scenarios can be covered with our framework. keywords: {Three-dimensional displays;Navigation;Task analysis;Robots;Augmented reality;Drones},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610259&isnumber=10609862

R. Bhaskara, H. Viswanath and A. Bera, "Trajectory Prediction for Robot Navigation using Flow-Guided Markov Neural Operator," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15209-15216, doi: 10.1109/ICRA57147.2024.10611154.Abstract: Predicting pedestrian movements remains a complex and persistent challenge in robot navigation research. We must evaluate several factors to achieve accurate predictions, such as pedestrian interactions, the environment, crowd density, and social and cultural norms. Accurate prediction of pedestrian paths is vital for ensuring safe human-robot interaction, especially in robot navigation. Furthermore, this research has potential applications in autonomous vehicles, pedestrian tracking, and human-robot collaboration. Therefore, in this paper, we introduce FlowMNO, an Optical Flow-Integrated Markov Neural Operator designed to capture pedestrian behavior across diverse scenarios. Our paper models trajectory prediction as a Markovian process, where future pedestrian coordinates depend solely on the current state. This problem formulation eliminates the need to store previous states. We conducted experiments using standard benchmark datasets like ETH, HOTEL, ZARA1, ZARA2, UCY, and RGB-D pedestrian datasets. Our study demonstrates that FlowMNO outperforms some of the state-of-the-art deep learning methods like LSTM, GAN, and CNN-based approaches, by approximately 86.46% when predicting pedestrian trajectories. Thus, we show that FlowMNO can seamlessly integrate into robot navigation systems, enhancing their ability to navigate crowded areas smoothly. keywords: {Pedestrians;Accuracy;Navigation;Robot kinematics;Computational modeling;Estimation;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611154&isnumber=10609862

S. Pohland, A. Tan, P. Dutta and C. Tomlin, "Stranger Danger! Identifying and Avoiding Unpredictable Pedestrians in RL-based Social Robot Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15217-15224, doi: 10.1109/ICRA57147.2024.10610413.Abstract: Reinforcement learning (RL) methods for social robot navigation show great success navigating robots through large crowds of people, but the performance of these learning-based methods tends to degrade in particularly challenging or unfamiliar situations due to the models’ dependency on representative training data. To ensure human safety and comfort, it is critical that these algorithms handle uncommon cases appropriately, but the low frequency and wide diversity of such situations present a significant challenge for these data-driven methods. To overcome this challenge, we propose modifications to the learning process that encourage these RL policies to maintain additional caution in unfamiliar situations. Specifically, we improve the Socially Attentive Reinforcement Learning (SARL) policy by (1) modifying the training process to systematically introduce deviations into a pedestrian model, (2) updating the value network to estimate and utilize pedestrian-unpredictability features, and (3) implementing a reward function to learn an effective response to pedestrian unpredictability. Compared to the original SARL policy, our modified policy maintains similar navigation times and path lengths, while reducing the number of collisions by 82% and reducing the proportion of time spent in the pedestrians’ personal space by up to 19 percentage points for the most difficult cases. We also describe how to apply these modifications to other RL policies and demonstrate that some key high-level behaviors of our approach transfer to a physical robot. keywords: {Training;Learning systems;Pedestrians;Navigation;Social robots;Training data;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610413&isnumber=10609862

H. Chung, J. Oh, J. Heo, G. Lee and S. Oh, "MAC-ID: Multi-Agent Reinforcement Learning with Local Coordination for Individual Diversity," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15233-15239, doi: 10.1109/ICRA57147.2024.10610314.Abstract: With the increase of robots navigating through crowded environments in our daily lives, the demand for designing a socially-aware navigation method considering humanrobot interaction has risen. When developing and assessing socially-aware navigation methods, pedestrian motion modeling plays a significant role. However, existing pedestrian models often struggle in complex environments and do not have the capacity to generate diverse pedestrian styles.In this paper, we propose multi-agent reinforcement learning with local coordination for individual diversity (MAC-ID), which can synthesize diverse pedestrian motions via local coordination factor (LCF). Our experiments have demonstrated that the manipulation of the LCF induces interpretable changes in pedestrian behaviors, along with a superior performance compared to existing pedestrian motion models. For evaluating socially-aware navigation methods using MAC-ID, we present a novel benchmark called BSON. It offers realistic and diverse social environments with pedestrians modeled via MAC-ID. We have trained and compared various navigation methods in BSON using a newly proposed metric called socially-aware navigation score (SNS). Through BSON, users can evaluate their socially-aware navigation methods and compare them to baselines. keywords: {Measurement;Pedestrians;Navigation;Robot kinematics;Reinforcement learning;Benchmark testing;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610314&isnumber=10609862

Q. Liu and M. Brandão, "Generating Environment-based Explanations of Motion Planner Failure: Evolutionary and Joint-Optimization Algorithms," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15263-15269, doi: 10.1109/ICRA57147.2024.10611577.Abstract: Motion planning algorithms are important components of autonomous robots, which are difficult to understand and debug when they fail to find a solution to a problem. In this paper we propose a solution to the failure-explanation problem, which are automatically-generated environment-based explanations. These explanations reveal the objects in the environment that are responsible for the failure, and how their location in the world should change so as to make the planning problem feasible.Concretely, we propose two methods—one based on evolutionary optimization and another on joint trajectory-and-environment continuous-optimization. We show that the evolutionary method is well-suited to explain sampling-based motion planners, or even optimization-based motion planners in situations where computation speed is not a concern (e.g. post-hoc debugging). However, the optimization-based method is 4000 times faster and thus more attractive for interactive applications, even though at the cost of a slightly lower success rate. We demonstrate the capabilities of the methods through concrete examples and quantitative evaluation. keywords: {Point cloud compression;Costs;Debugging;User interfaces;Lead;Approximation algorithms;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611577&isnumber=10609862

X. Liu, B. Wang and Z. Li, "Vision-based Wearable Steering Assistance for People with Impaired Vision in Jogging," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15270-15275, doi: 10.1109/ICRA57147.2024.10610660.Abstract: Outdoor sports pose a challenge for people with impaired vision. The demand for higher-speed mobility inspired us to develop a vision-based wearable steering assistance. To ensure broad applicability, we focused on a representative sports environment, the athletics track. Our efforts centered on improving the speed and accuracy of perception, enhancing planning adaptability for the real world, and providing swift and safe assistance for people with impaired vision. In perception, we engineered a lightweight multitask network capable of simultaneously detecting track lines and obstacles. Additionally, due to the limitations of existing datasets for supporting multi-task detection in athletics tracks, we diligently collected and annotated a new dataset (MAT) containing 1000 images. In planning, we integrated the methods of sampling and spline curves, addressing the planning challenges of curves. Meanwhile, we utilized the positions of the track lines and obstacles as constraints to guide people with impaired vision safely along the current track. Our system is deployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it demonstrated adaptability in different sports scenarios, assisting users in achieving free movement of 400meter at an average speed of 1.34 m/s, meeting the level of normal people in jogging. Our MAT dataset is publicly available from https://github.com/snoopy-l/MAT keywords: {Visualization;Accuracy;Tracking;Multitasking;Planning;Splines (mathematics);Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610660&isnumber=10609862

L. Mišković, T. Brecelj, M. Dežman and T. Petrič, "Active, Quasi-Passive, Pneumatic, and Portable Knee Exoskeleton with Bidirectional Energy Flow for Efficient Air Recovery in Sit-Stand Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15292-15298, doi: 10.1109/ICRA57147.2024.10610457.Abstract: While existing literature encompasses exoskeleton-assisted sit-stand tasks, the integration of energy recovery mechanisms remains unexplored. To push the boundaries further, this study introduces a portable pneumatic knee exoskeleton that operates in both quasi-passive and active modes, where active mode is utilized for aiding in standing up (power generation), thus the energy flows from the exoskeleton to the user, and quasi-passive mode for aiding in sitting down (power absorption), where the device absorbs and can store energy in the form of compressed air, leading to energy savings in active mode. The absorbed energy can be stored and later reused without compromising exoskeleton transparency in the meantime. In active mode, an air pump inflates the pneumatic artificial muscle (PAM), which stores the compressed air, that can then be released into a pneumatic cylinder to generate torque. All electronic and pneumatic components are integrated into the system, and the exoskeleton weighs 3.9 kg with a maximum torque of 20 Nm at the knee joint. The paper describes the mechatronic design, mathematical model and includes a pilot study with an able-bodied subject performing sit-to-stand tasks. The results show that the exoskeleton can recover energy while assisting the subject and reducing mean muscle activity by ~31%. Further results highlight air regeneration’s potential for energy saving in portable pneumatic exoskeletons, showing that the proposed device extends exoskeleton operation by ~27%. keywords: {Performance evaluation;Technological innovation;Torque;Mechatronics;Exoskeletons;Energy conservation;Muscles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610457&isnumber=10609862

Y. Chen, G. Chen, J. Ye, X. Qiu and X. Li, "Safe and Individualized Motion Planning for Upper-limb Exoskeleton Robots Using Human Demonstration and Interactive Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15307-15313, doi: 10.1109/ICRA57147.2024.10610552.Abstract: A typical application of upper-limb exoskeleton robots is deployment in rehabilitation training, helping patients to regain manipulative abilities. However, as the patient is not always capable of following the robot, safety issues may arise during the training. Due to the bias in different patients, an individualized scheme is also important to ensure that the robot suits the specific conditions (e.g., movement habits) of a patient, hence guaranteeing effectiveness. To fulfill this requirement, this paper proposes a new motion planning scheme for upper-limb exoskeleton robots, which drives the robot to provide customized, safe, and individualized assistance using both human demonstration and interactive learning. Specifically, the robot first learns from a group of healthy subjects to generate a reference motion trajectory via probabilistic movement primitives (ProMP). It then learns from the patient during the training process to further shape the trajectory inside a moving safe region. The interactive data is fed back into the ProMP iteratively to enhance the individualized features for as long as the training process continues. The robot tracks the individualized trajectory under a variable impedance model to realize the assistance. Finally, the experimental results are presented in this paper to validate the proposed control scheme. keywords: {Training;Tracking;Shape;Exoskeletons;Trajectory;Safety;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610552&isnumber=10609862

A. Rigo, M. Hu, S. K. Gupta and Q. Nguyen, "Hierarchical Optimization-based Control for Whole-body Loco-manipulation of Heavy Objects," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15322-15328, doi: 10.1109/ICRA57147.2024.10611656.Abstract: In recent years, the field of legged robotics has seen growing interest in enhancing the capabilities of these robots through the integration of articulated robotic arms. However, achieving successful loco-manipulation, especially involving interaction with heavy objects, is far from straightforward, as object manipulation can introduce substantial disturbances that impact the robot’s locomotion. This paper presents a novel framework for legged loco-manipulation that considers whole-body coordination through a hierarchical optimization-based control framework. First, an online manipulation planner computes the manipulation forces and manipulated object task-based reference trajectory. Then, pose optimization aligns the robot’s trajectory with kinematic constraints. The resultant robot reference trajectory is executed via a linear MPC controller incorporating the desired manipulation forces into its prediction model. Our approach has been validated in simulation and hardware experiments, highlighting the necessity of whole-body optimization compared to the baseline locomotion MPC when interacting with heavy objects. Experimental results with Unitree Aliengo, equipped with a custom-made robotic arm, showcase its ability to lift and carry an 8kg payload and manipulate doors. keywords: {Legged locomotion;Robot kinematics;Kinematics;Predictive models;Hardware;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611656&isnumber=10609862

R. Scalise, E. Caglar, B. Boots and C. C. Kessens, "Toward Self-Righting and Recovery in the Wild: Challenges and Benchmarks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15329-15335, doi: 10.1109/ICRA57147.2024.10611544.Abstract: Self-recovery is a critical capability for robust, agile robots operating in the real world. Given truly challenging terrain, it is nearly inevitable that, at some point, the robot will fail and subsequently need to recover if it is to continue its task. One critical subset of recovery is standing back up after falling down (aka "self-righting"), an essential early milestone for babies learning to walk, and an existential capability for animals. While some robots can be designed with multiple orientations for mobility, most seeking to affect the world would significantly benefit from planners/policies that facilitate self-righting whenever possible.In this work, we present a series of challenges that outline why recovery in the wild is difficult. We then present a set of benchmark policies trained in simulation using deep reinforcement learning (RL) and the Student-Teacher approach. Finally, we evaluate the performance of these policies on a set of benchmark contexts in simulation, and provide baseline validation on a physical robot. keywords: {Training;Measurement;Pediatrics;Animals;Benchmark testing;Deep reinforcement learning;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611544&isnumber=10609862

T. Makabe, K. Okada and M. Inaba, "Design of Morphable StateNet Based on Pseudo-Generalization of Standing Up Motions for Humanoid with Variable Body Structure," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15336-15342, doi: 10.1109/ICRA57147.2024.10610947.Abstract: In this paper, we explain the Morphable StateNet as the StateNet with pseudo-generalized behaviors for robots with various degree-of-freedom arrangements and link lengths. Pseudo-generalization is performed by analytically calculating joint angles that satisfy the desired support conditions, focusing on link lengths and antigravity joints that contribute to motion, with constraints placed on the contact conditions between the environment and the robot body. We apply Morphable StateNet to the standing-up motion of humanoids with variable body structures and conduct evaluation experiments. We have demonstrated the usefulness of the proposed method in environments with low friction coefficients with the environment by conducting evaluations using both a simulator and an actual humanoid. keywords: {Legged locomotion;Deformation;Friction;Humanoid robots;Wheels;Focusing;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610947&isnumber=10609862

Y. Li, Y. Zhang, A. Razmjoo and S. Calinon, "Representing Robot Geometry as Distance Fields: Applications to Whole-body Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15351-15357, doi: 10.1109/ICRA57147.2024.10611674.Abstract: In this work, we propose a novel approach to represent robot geometry as distance fields (RDF) that extends the principle of signed distance fields (SDFs) to articulated kinematic chains. Our method employs a combination of Bernstein polynomials to encode the signed distance for each robot link with high accuracy and efficiency while ensuring the mathematical continuity and differentiability of SDFs. We further leverage the kinematics chain of the robot to produce the SDF representation in joint space, allowing robust distance queries in arbitrary joint configurations. The proposed RDF representation is differentiable and smooth in both task and joint spaces, enabling its direct integration to optimization problems. Additionally, the 0-level set of the robot corresponds to the robot surface, which can be seamlessly integrated into whole-body manipulation tasks. We conduct various experiments in both simulations and with 7-axis Franka Emika robots, comparing against baseline methods, and demonstrating its effectiveness in collision avoidance and whole-body manipulation tasks. Project page: https://sites.google.com/view/lrdf/home keywords: {Geometry;Accuracy;Shape;Kinematics;Resource description framework;Polynomials;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611674&isnumber=10609862

X. Wu, A. Albu-Schäffer and A. Dietrich, "Singularity-Robust Prioritized Whole-Body Tracking and Interaction Control With Smooth Task Transitions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15358-15365, doi: 10.1109/ICRA57147.2024.10610961.Abstract: In this work, we propose a singularity-robust whole-body control framework that ensures smooth task transitions while maintaining strict priorities. The weighted generalized inverse is adopted to derive a hierarchical control law compatible with singular and redundant tasks. Moreover, a smooth activation matrix is proposed to continuously shape both null-space projectors and task-level control actions. Validation has been conducted in MATLAB/Simulink and MuJoCo simulations with Rollin’ Justin. keywords: {Jacobian matrices;Shape;Task analysis;Robots;Matlab},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610961&isnumber=10609862

T. Portela, G. B. Margolis, Y. Ji and P. Agrawal, "Learning Force Control for Legged Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15366-15372, doi: 10.1109/ICRA57147.2024.10611066.Abstract: Controlling the contact force during interactions is an inherent requirement for locomotion and manipulation tasks. Current reinforcement learning approaches to locomotion and manipulation rely implicitly on forceful interaction to accomplish tasks but do not explicitly regulate it. This paper proposes a reinforcement learning task specification that focuses on matching desired contact force levels. Integrating force control with the coordination of a robot’s body and arm, we present an end-to-end policy for legged manipulator control. Force control enables us to realize compliant gripper and whole-body pulling movements that have not been previously demonstrated using a learned policy. It also facilitates a characterization of the force-tracking performance of learned policies in simulation and the real world, indicating their performance potential for force-critical tasks. Video is available at the project website: https://tif-twirl-13.github.io/learning-compliance. keywords: {Legged locomotion;Robot kinematics;Force;Reinforcement learning;Manipulators;Task analysis;Force control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611066&isnumber=10609862

G. Tatiya, J. Francis, H. -H. Wu, Y. Bisk and J. Sinapov, "MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15381-15387, doi: 10.1109/ICRA57147.2024.10609998.Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC. keywords: {Visualization;Grounding;Humanoid robots;Robot sensing systems;Robot learning;Distance measurement;Haptic interfaces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609998&isnumber=10609862

H. Ren and A. H. Qureshi, "Neural Rearrangement Planning for Object Retrieval from Confined Spaces Perceivable by Robot’s In-hand RGB-D Sensor," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15388-15394, doi: 10.1109/ICRA57147.2024.10610092.Abstract: Rearrangement planning for object retrieval tasks from confined spaces is a challenging problem, primarily due to the lack of open space for robot motion and limited perception. Several traditional methods exist to solve object retrieval tasks, but they require overhead cameras for perception and a time-consuming exhaustive search to find a solution and often make unrealistic assumptions, such as having identical, simple geometry objects in the environment. This paper presents a neural object retrieval framework that efficiently performs rearrangement planning of unknown, arbitrary objects in confined spaces to retrieve the desired object using a given robot grasp. Our method actively senses the environment with the robot’s in-hand camera. It then selects and relocates the non-target objects such that they do not block the robot path homotopy to the target object, thus also aiding an underlying path planner in quickly finding robot motion sequences. Furthermore, we demonstrate our framework in challenging scenarios, including real-world cabinet-like environments with arbitrary household objects. The results show that our framework achieves the best performance among all presented methods and is, on average, two orders of magnitude computationally faster than the best-performing baselines. keywords: {Robot motion;Geometry;Robot vision systems;Cameras;Search problems;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610092&isnumber=10609862

Y. He et al., "MMPI: a Flexible Radiance Field Representation by Multiple Multi-plane Images Blending," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15395-15401, doi: 10.1109/ICRA57147.2024.10611248.Abstract: This paper presents a flexible representation of neural radiance fields based on multi-plane images (MPI), for high-quality view synthesis of complex scenes. MPI with Normalized Device Coordinate (NDC) parameterization is widely used in NeRF learning for its simple definition, easy calculation, and powerful ability to represent unbounded scenes. However, existing NeRF works that adopt MPI representation for novel view synthesis can only handle simple forward-facing unbounded scenes (e.g., the scenes in the LLFF dataset), where the input cameras are all observing in similar directions with small relative translations. Hence, extending these MPIbased methods to more complex scenes like large-range or even 360-degree scenes is very challenging. In this paper, we explore the potential of MPI and show that MPI can synthesize high-quality novel views of complex scenes with diverse camera distributions and view directions, which are not only limited to simple forward-facing scenes. Our key idea is to encode the neural radiance field with multiple MPIs facing different directions and blend them with an adaptive blending operation. For each region of the scene, the blending operation gives larger blending weights to those advantaged MPIs with stronger local representation abilities while giving lower weights to those with weaker representation abilities. Such blending operation automatically modulates the multiple MPIs to appropriately represent the diverse local density and color information. Experiments on the KITTI dataset and ScanNet dataset demonstrate that our proposed MMPI synthesizes high-quality images from diverse camera pose distributions and is fast to train, outperforming the previous fast-training NeRF methods for novel view synthesis. Moreover, we show that MMPI can encode extremely long trajectories and produce novel view renderings, demonstrating its potential in applications like autonomous driving. Our demo video is available at https://youtube.com/watch?v=mbNKwN5urC8. keywords: {Image color analysis;Neural radiance field;Cameras;Rendering (computer graphics);Reliability engineering;Trajectory;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611248&isnumber=10609862

D. Joho, J. Schwinn and K. Safronov, "Neural Implicit Swept Volume Models for Fast Collision Detection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15402-15408, doi: 10.1109/ICRA57147.2024.10611687.Abstract: Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that it is able to speed up a commercial bin picking application. keywords: {Robot motion;Geometry;Solid modeling;Accuracy;Computational modeling;Machine learning;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611687&isnumber=10609862

S. Hausler, D. Hall, S. Mahendren and P. Moghadam, "Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15409-15415, doi: 10.1109/ICRA57147.2024.10610982.Abstract: Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural field dataset for evaluating registration problems. We provide an exhaustive set of experiments and ablation studies to identify the performance of our approach, while also discussing limitations to provide future direction to the research community on open challenges in utilizing neural fields in unconstrained environments. keywords: {Point cloud compression;Geometry;Analytical models;Three-dimensional displays;Robot kinematics;Neural networks;Transforms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610982&isnumber=10609862

J. Zhou et al., "3D-OAE: Occlusion Auto-Encoders for Self-Supervised Learning on Point Clouds," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15416-15423, doi: 10.1109/ICRA57147.2024.10610588.Abstract: The manual annotation for large-scale point clouds is still tedious and unavailable for many harsh real-world tasks. Self-supervised learning, which is used on raw and unlabeled data to pre-train deep neural networks, is a promising approach to address this issue. Existing works usually take the common aid from auto-encoders to establish the self-supervision by the self-reconstruction schema. However, the previous auto-encoders merely focus on the global shapes and do not distinguish the local and global geometric features apart. To address this problem, we present a novel and efficient self-supervised point cloud representation learning framework, named 3D Occlusion Auto-Encoder (3D-OAE), to facilitate the detailed supervision inherited in local regions and global shapes. We propose to randomly occlude some local patches of point clouds and establish the supervision via inpainting the occluded patches using the remaining ones. Specifically, we design an asymmetrical encoder-decoder architecture based on standard Transformer, where the encoder operates only on the visible subset of patches to learn local patterns, and a lightweight decoder is designed to leverage these visible patterns to infer the missing geometries via self-attention. We find that occluding a very high proportion of the input point cloud (e.g. 75%) will still yield a nontrivial self-supervisory performance, which enables us to achieve 3-4 times faster during training but also improve accuracy. Experimental results show that our approach outperforms the state-of-the-art on a diverse range of down-stream discriminative and generative tasks. Code is available at https://github.com/junshengzhou/3D-OAE. keywords: {Point cloud compression;Training;Representation learning;Three-dimensional displays;Shape;Self-supervised learning;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610588&isnumber=10609862

J. Shi, J. Qian, Y. J. Ma and D. Jayaraman, "Composing Pre-Trained Object-Centric Representations for Robotics From "What" and "Where" Foundation Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15424-15432, doi: 10.1109/ICRA57147.2024.10610695.Abstract: There have recently been large advances both in pre-training visual representations for robotic control and segmenting unknown category objects in general images. To leverage these for improved robot learning, we propose POCR, a new framework for building pre-trained object-centric representations for robotic control. Building on theories of "what-where" representations in psychology and computer vision, we use segmentations from a pre-trained model to stably locate across timesteps, various entities in the scene, capturing "where" information. To each such segmented entity, we apply other pre-trained models that build vector descriptions suitable for robotic control tasks, thus capturing "what" the entity is. Thus, our pre-trained object-centric representations for control are constructed by appropriately combining the outputs of off-the-shelf pre-trained models, with no new training. On various simulated and real robotic tasks, we show that imitation policies for robotic manipulators trained on POCR achieve better performance and systematic generalization than state of the art pre-trained representations for robotics, as well as prior object-centric representations that are typically trained from scratch. keywords: {Training;Representation learning;Visualization;Systematics;Computational modeling;Buildings;Vectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610695&isnumber=10609862

C. -L. Kuo, Y. -W. Chao and Y. -T. Chen, "SKT-Hang: Hanging Everyday Objects via Object-Agnostic Semantic Keypoint Trajectory Generation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15433-15439, doi: 10.1109/ICRA57147.2024.10610266.Abstract: We study the problem of hanging a wide range of grasped objects on diverse supporting items. Hanging objects is a ubiquitous task that is encountered in numerous aspects of our everyday lives. However, both the objects and supporting items can exhibit substantial variations in their shapes and structures, bringing two challenging issues: (1) determining the task-relevant geometric structures across different objects and supporting items, and (2) identifying a robust action sequence to accommodate the shape variations of supporting items. To this end, we propose Semantic Keypoint Trajectory (SKT), an object-agnostic representation that is highly versatile and applicable to various everyday objects. We also propose Shape-conditioned Trajectory Deformation Network (SCTDN), a model that learns to generate SKT by deforming a template trajectory based on the task-relevant geometric structure features of the supporting items. We conduct extensive experiments and demonstrate substantial improvements in our framework over existing robot hanging methods in the success rate and inference time. Finally, our simulation-trained framework shows promising hanging results in the real world. For videos and supplementary materials, please visit our project webpage: https://hcis-lab.github.io/SKT-Hang/. keywords: {Deformable models;Shape;Deformation;Semantics;Trajectory;Object recognition;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610266&isnumber=10609862

L. Li, A. Linger, M. Millhäusler, V. Tsiminaki, Y. Li and D. Dai, "Object-centric Cross-modal Feature Distillation for Event-based Object Detection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15440-15447, doi: 10.1109/ICRA57147.2024.10610325.Abstract: Event cameras are gaining popularity due to their unique properties, such as their low latency and high dynamic range. One task where these benefits can be crucial is real-time object detection. However, RGB detectors still outperform event-based detectors due to the sparsity of the event data and missing visual details. In this paper, we propose a cross-modality feature distillation method that can focus on regions where the knowledge distillation works best to shrink the detection performance gap between these two modalities. We achieve this by using an object-centric slot attention mechanism that can iteratively decouple feature maps into object-centric features and corresponding pixel-features used for distillation. We evaluate our novel distillation approach on a synthetic and a real event dataset with aligned grayscale images as a teacher modality. We show that object-centric distillation allows to significantly improve the performance of the event-based student object detector, nearly halving the performance gap with respect to the teacher. keywords: {Visualization;Image coding;Detectors;Object detection;Gray-scale;Feature extraction;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610325&isnumber=10609862

J. Wang, C. Hu, J. Kang, J. Liu, L. Ma and H. Liao, "A Novel Robotic Bronchoscope with a Spring-Based Extensible Segment for Improving Steering Ability," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15448-15454, doi: 10.1109/ICRA57147.2024.10611673.Abstract: Bronchoscopy, as an essential minimally invasive diagnostic and therapeutic modality, assumes a pivotal role in the early detection of lung cancer. However, the complex anatomy of the airway and the fixed length of the bronchoscope’s bending segment, along with its external propulsion property, pose challenges, including the risk of bleeding. This paper introduces a 4 mm diameter robot-assisted bronchoscope with a spring-based extensible segment. By manipulating two driven rods, the segment can be lengthened or shortened. The advantages of the extensible segment are discussed in two main aspects through theoretical analysis and experimentation. Firstly, the extensible segment enables the bronchoscope to move in a follow-the-leader motion mode or fixed-angle motion mode, navigating through narrow corners that are inaccessible to fixed-length bronchoscopes. It can also be shortened to increase its stiffness when it reaches the target position, creating a stable surgical platform for procedures like biopsies. In addition, a tailored master device has been developed to control the extensible bronchoscope in an isotropic manner. Phantom experiments confirm the feasibility and effectiveness of the extensible bronchoscope. keywords: {Solid modeling;Minimally invasive surgery;Navigation;Motion segmentation;Biopsy;Robot vision systems;Phantoms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611673&isnumber=10609862

C. D’Ambrosia et al., "Robust Surgical Tool Tracking with Pixel-based Probabilities for Projected Geometric Primitives," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15455-15462, doi: 10.1109/ICRA57147.2024.10610378.Abstract: Controlling robotic manipulators via visual feedback requires a known coordinate frame transformation between the robot and the camera. Uncertainties in mechanical systems as well as camera calibration create errors in this coordinate frame transformation. These errors result in poor localization of robotic manipulators and create a significant challenge for applications that rely on precise interactions between manipulators and the environment. In this work, we estimate the camera-to-base transform and joint angle measurement errors for surgical robotic tools using an image based insertion-shaft detection algorithm and probabilistic models. We apply our proposed approach in both a structured environment as well as an unstructured environment and measure to demonstrate the efficacy of our methods. keywords: {Visualization;Uncertainty;Medical robotics;Robot kinematics;Robot vision systems;Transforms;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610378&isnumber=10609862

J. Guo, J. Wang, Z. Li, T. Jia, Q. Dou and Y. -H. Liu, "Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-template Matching," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15463-15470, doi: 10.1109/ICRA57147.2024.10611030.Abstract: Soft tissue tracking is crucial for computer-assisted interventions. Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches. However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery. To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template. Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames. To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates. Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-term dynamics modeling by introducing global temporal compensation. We evaluate our approach on the public SurgT benchmark, which is generated from Hamlyn, SCARED, and Kidney boundary datasets. The experimental results show that Ada-Tracker achieves superior accuracy and performs more robustly against prior works. Code is available at https://github.com/wrld/Ada-Tracker. keywords: {Deformable models;Adaptation models;Accuracy;Tracking;Deformation;Biological tissues;Surgery},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611030&isnumber=10609862

X. Liang, F. Liu, Y. Zhang, Y. Li, S. Lin and M. Yip, "Real-to-Sim Deformable Object Manipulation: Optimizing Physics Models with Residual Mappings for Robotic Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15471-15477, doi: 10.1109/ICRA57147.2024.10610263.Abstract: Accurate deformable object manipulation (DOM) is essential for achieving autonomy in robotic surgery, where soft tissues are being displaced, stretched, and dissected. Many DOM methods can be powered by simulation, which ensures realistic deformation by adhering to the governing physical constraints and allowing for model prediction and control. However, real soft objects in robotic surgery, such as membranes and soft tissues, have complex, anisotropic physical parameters that a simulation with simple initialization from cameras may not fully capture. To use the simulation techniques in real surgical tasks, the real-to-sim gap needs to be properly compensated. In this work, we propose an online, adaptive parameter tuning approach for simulation optimization that (1) bridges the real-to-sim gap between a physics simulation and observations obtained 3D perceptions through estimating a residual mapping and (2) optimizes its stiffness parameters online. Our method ensures a small residual gap between the simulation and observation and improves the simulation’s predictive capabilities. The effectiveness of the proposed mechanism is evaluated in the manipulation of both a thin-shell and volumetric tissue, representative of most tissue scenarios. This work contributes to the advancement of simulation-based deformable tissue manipulation and holds potential for improving surgical autonomy. keywords: {Deformable models;Three-dimensional displays;Biological tissues;Robot vision systems;Surgery;Predictive models;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610263&isnumber=10609862

B. Y. Cho and A. Kuntz, "Efficient and Accurate Mapping of Subsurface Anatomy via Online Trajectory Optimization for Robot Assisted Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15478-15484, doi: 10.1109/ICRA57147.2024.10610456.Abstract: Robotic surgical subtask automation has the potential to reduce the per-patient workload of human surgeons. There are a variety of surgical subtasks that require geometric information of subsurface anatomy, such as the location of tumors, which necessitates accurate and efficient surgical sensing. In this work, we propose an automated sensing method that maps 3D subsurface anatomy to provide such geometric knowledge. We model the anatomy via a Bayesian Hilbert map-based probabilistic 3D occupancy map. Using the 3D occupancy map, we plan sensing paths on the surface of the anatomy via a graph search algorithm, A* search, with a cost function that enables the trajectories generated to balance between exploration of unsensed regions and refining the existing probabilistic understanding. We demonstrate the performance of our proposed method by comparing it against 3 different methods in several anatomical environments including a real-life CT scan dataset. The experimental results show that our method efficiently detects relevant subsurface anatomy with shorter trajectories than the comparison methods, and the resulting occupancy map achieves high accuracy. keywords: {Accuracy;Three-dimensional displays;Medical robotics;Costs;Computed tomography;Robot sensing systems;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610456&isnumber=10609862

R. C. Peter et al., "Lens Capsule Tearing in Cataract Surgery using Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15501-15508, doi: 10.1109/ICRA57147.2024.10611714.Abstract: Cataract is the leading cause of blindness worldwide with an increasing number of patients due to changing demographics, making automation an important part in future surgical treatment. In this work, we focus on a substep of cataract surgery, the Continuous Curvilinear Capsulorhexis (CCC). With a high complexity, this task is an ideal candidate for Reinforcement Learning (RL) in simulation. First, we present an interactive and physically realistic simulation based on the Finite Element Method (FEM) that mimics the tearing behavior of soft tissue during CCC. Then, we train and evaluate RL models in simulation, demonstrating that the trained policies can complete the CCC in 85% of cases. We also show that applying domain randomization techniques make the policy more robust against changes in geometrical and biomechanical boundary conditions. keywords: {Cataracts;Training;Instruments;Surgery;Phantoms;Reinforcement learning;Boundary conditions},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611714&isnumber=10609862

Q. Yu et al., "Orbit-Surgical: An Open-Simulation Framework for Learning Surgical Augmented Dexterity," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15509-15516, doi: 10.1109/ICRA57147.2024.10611637.Abstract: Physics-based simulations have accelerated progress in robot learning for driving, manipulation, and locomotion. Yet, a fast, accurate, and robust surgical simulation environment remains a challenge. In this paper, we present Orbit-Surgical, a physics-based surgical robot simulation framework with photorealistic rendering in NVIDIA Omniverse. We provide 14 benchmark surgical tasks for the da Vinci Research Kit (dVRK) and Smart Tissue Autonomous Robot (STAR) which represent common subtasks in surgical training. Orbit-Surgical leverages GPU parallelization to train reinforcement learning and imitation learning algorithms to facilitate study of robot learning to augment human surgical skills. Orbit-Surgical also facilitates realistic synthetic data generation for active perception tasks. We demonstrate Orbit-Surgical sim-to-real transfer of learned policies onto a physical dVRK robot.Project website: orbit-surgical.github.io keywords: {Training;Medical robotics;Imitation learning;Stars;Reinforcement learning;Rendering (computer graphics);Orbits},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611637&isnumber=10609862

H. Zhao, R. Li and Q. Quan, "Relaxed Hover Solution Based Control for a Bi-copter with Rotor and Servo Stuck Failure," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15517-15523, doi: 10.1109/ICRA57147.2024.10611350.Abstract: As the usage of bi-copters increases in military and civilian fields, the demand for reliable bi-copters is on the rise. This study focuses on controlling a bi-copter under rotor or servo stuck failure. A relaxed hover solution is derived for the bi-copter, by solving an optimization problem subject to rotor and servo stuck failures. The solution is used for designing a reduced attitude controller based on linear quadratic regulator (LQR). To ensure hover capability, we introduce a position controller based on a cascaded-PID. The numerical simulations are conducted to demonstrate that position control is possible, even with complete rotor or servo stuck failure, by driving the bi-copter into relaxed hover state through the abandonment of the yaw channel. Meanwhile, the FTC scheme is examined under constant wind disturbances and uncertainties in the rotational damping parameters. keywords: {Damping;Uncertainty;Regulators;Rotors;Position control;Numerical simulation;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611350&isnumber=10609862

R. Caballero, P. Piękos, E. Feron and J. Schmidhuber, "Utilizing a Malfunctioning 3D Printer by Modeling Its Dynamics with Machine Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15562-15569, doi: 10.1109/ICRA57147.2024.10611304.Abstract: To create a self-repairing 3D printer, it must continue operating even after experiencing corruption. This work focuses on developing a method to effectively utilize a malfunctioning printer for reliable printing. This method can be applied by the printer itself for self-repair and enhance the reliability of commercial 3D printers. We achieve this by modeling the dynamics of the corrupted printer using a machine learning model that by observing one trajectory infers the corrupted printer dynamics to improve its accuracy. Our method is evaluated on a digital twin of the 3D printer, demonstrating its capability to enable the printer to operate reliably, even when encountering new corruptions not encountered during training. The scripts are public on https://github.com/piotrpiekos/adaptive-printer. keywords: {Training;Solid modeling;Three-dimensional displays;Accuracy;Machine learning;Hardware;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611304&isnumber=10609862

J. J. Van Beers, P. Solanki and C. C. De Visser, "A novel metric for detecting quadrotor loss-of-control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15570-15576, doi: 10.1109/ICRA57147.2024.10610662.Abstract: Unmanned aerial vehicles (UAVs) are becoming an integral part of both industry and society. In particular, the quadrotor is now invaluable across a plethora of fields and recent developments, such as the inclusion of aerial manipulators, only extends their versatility. As UAVs become more widespread, preventing loss-of-control (LOC) is an ever growing concern. Unfortunately, LOC is not clearly defined for quadrotors, or indeed, many other autonomous systems. Moreover, any existing definitions are often incomplete and restrictive. A novel metric, based on actuator capabilities, is introduced to detect LOC in quadrotors. The potential of this metric for LOC detection is demonstrated through both simulated and real quadrotor flight data. It is able to detect LOC induced by actuator faults without explicit knowledge of the occurrence and nature of the failure. The proposed metric is also sensitive enough to detect LOC in more nuanced cases, where the quadrotor remains undamaged but nevertheless losses control through an aggressive yawing manoeuvre. As the metric depends only on system and actuator models, it is sufficiently general to be applied to other systems. keywords: {Measurement;Industries;Actuators;Autonomous systems;Manipulators;Autonomous aerial vehicles;Quadrotors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610662&isnumber=10609862

F. Toledo, T. Woodlief, S. Elbaum and M. B. Dwyer, "Specifying and Monitoring Safe Driving Properties with Scene Graphs," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15577-15584, doi: 10.1109/ICRA57147.2024.10610973.Abstract: With the proliferation of autonomous vehicles (AVs) comes the need to ensure they abide by safe driving properties. Specifying and monitoring such properties, however, is challenging because of the mismatch between the semantic space over which typical driving properties are asserted (e.g., vehicles, pedestrians, intersections) and the sensed inputs of AVs. Existing efforts either assume for such semantic data to be available or develop bespoke methods for capturing it. Instead, this work introduces a framework that can extract scene graphs (SGs) from sensor inputs to capture the entities related to the AV, and a domain-specific language that enables building propositions over those graphs and composing them through temporal logic. We implemented the framework to monitor for specification violations of 3 top AVs from the CARLA Autonomous Driving Leaderboard, and found that the AVs violated 71% of properties during at least one test. Artifact available at https://github.com/less-lab-uva/SGSM. keywords: {Space vehicles;Runtime;Pedestrians;Semantics;Robot sensing systems;Logic;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610973&isnumber=10609862

J. H. Kang, N. Dhanaraj, S. Wadaskar and S. K. Gupta, "Using Large Language Models to Generate and Apply Contingency Handling Procedures in Collaborative Assembly Applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15585-15592, doi: 10.1109/ICRA57147.2024.10610875.Abstract: In manufacturing, minimizing operational delays is crucial for efficiency and resilience. Therefore, efficiently handling contingencies is essential in human-robot teams working on assembly (i.e., collaborative assembly) applications. This paper introduces a novel approach to generating contingency handling procedures by leveraging recent advances in Large Language Models (LLMs). Our approach uses LLMs to update the required tasks in hierarchical task networks (HTNs) to handle contingencies. The results demonstrate that our approach can handle various contingencies in assembly applications and minimize the impact on the assembly completion time. keywords: {Sequential analysis;Large language models;Contingency management;Teamwork;Delays;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610875&isnumber=10609862

Y. An, J. Yang, J. Li, B. He, Y. Guo and G. -Z. Yang, "Skill Learning in Robot-Assisted Micro-Manipulation Through Human Demonstrations with Attention Guidance," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15601-15607, doi: 10.1109/ICRA57147.2024.10610945.Abstract: For the development of robotic systems for micromanipulation, it is challenging to design appropriate control strategies due to either the lack of sufficient information for feedback or the difficulty in extracting subtle yet critical visual features. With the same system under the teleoperated mode, however, human operators seem to be able to complete the task more successfully with an inherent motion and control strategy. The extraction of implicit human attention during the task and integration of this with robot control could provide crucial guidance in the design of feature extraction and motion control algorithms. In this paper, a micro-assembly task of miniature thin membrane sensors is considered. For human demonstrations, we collected data from repeated tests performed by ten operators following three motion strategies. The human attention during the task is explored according to the coordinates of the eye gaze, and then a neural network with gaze-guided attention is trained to segment the visual Region of Interest (ROI). After quantitative evaluation of operator results in terms of success rate, efficiency, reset time, and the Index of Pupillary Activity (IPA), an optimized motion strategy based on the "palpation" framework was derived. Consequently, we apply this strategy to automated tasks and achieve superior results than human operators, showing an average task completion time of 34.8±5.9s and a success rate of over 90%. keywords: {Visualization;Robot kinematics;Motion segmentation;Robot control;Neural networks;Feature extraction;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610945&isnumber=10609862

Y. Jiang, X. Fu, C. Zhong, T. Li, H. Lu and S. Liu, "Automated Surgical Knot Tying on Mini-Incision with Micro-Suture based on Dual-Arm Nanorobot under Stereo Microscope," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15608-15614, doi: 10.1109/ICRA57147.2024.10610173.Abstract: Knot tying is an essential task for robotic surgery, which is routinely realized by dual-arm robotic manipulation. Despite the well-established protocol and progress at macro scale so far, there remain challenges to further advance robotic knot tying technique, particularly in terms of decreasing space consumption with better dexterity, higher precision, and well biomechanical compatibility. In this paper, we propose a novel dual-arm nanorobotic system setup for automated knot tying performed on mini-incision under stereo microscope, featured by an additional rotation degree of freedom mounted on each arm. With this setup, an optimized motion trajectory planning under standard knot-tying protocol is also presented in order to support tying knots with shorter and thinner suture. Leveraging the natural advantage of nanorobotics and microscope, the proposed system is capable of tying consecutive throws with micro-suture on mini-incision, like in vascular anastomosis or microsurgery. We successfully evaluated the knot tying system on 2.0 mm wide bionic blood vessel with 30 mm long #8-0 micro-suture. We finally tested the mechanical strength of the knots for potential medical assessment. keywords: {Trajectory planning;Microscopy;Blood vessels;Microsurgery;Arms;Routing protocols;Synchronization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610173&isnumber=10609862

H. Yang et al., "Weakly-Supervised Depth Completion during Robotic Micromanipulation from a Monocular Microscopic Image," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15615-15621, doi: 10.1109/ICRA57147.2024.10611357.Abstract: Obtaining three-dimensional information, especially the z-axis depth information, is crucial for robotic micromanipulation. Due to the unavailability of depth sensors such as lidars in micromanipulation setups, traditional depth acquisition methods such as depth from focus or depth from defocus directly infer depth from microscopic images and suffer from poor resolution. Alternatively, micromanipulation tasks obtain accurate depth information by detecting the contact between an end-effector and an object (e.g., a cell). Despite its high accuracy, only sparse depth data can be obtained due to its low efficiency. This paper aims to address the challenge of acquiring dense depth information during robotic cell micromanipulation. A weakly-supervised depth completion network is proposed to take cell images and sparse depth data obtained by contact detection as input to generate a dense depth map. A two-stage data augmentation method is proposed to augment the sparse depth data, and the depth map is optimized by a network refinement method. The experimental results show that the MAE value of the depth prediction error is less than 0.3 µm, which proves the accuracy and effectiveness of the method. This deep learning network pipeline can be seamlessly integrated with the robotic micromanipulation tasks to provide accurate depth information. keywords: {Deep learning;Accuracy;Laser radar;Microscopy;Pipelines;Robot sensing systems;Data augmentation;Biological Cell Manipulation;Automation at Micro/Nano Scales;Deep Learning;Depth Completion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611357&isnumber=10609862

A. Wang et al., "Dynamic Adaptive Imaging System on Optoelectronic Tweezers Platform," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15622-15627, doi: 10.1109/ICRA57147.2024.10611608.Abstract: Optoelectronic tweezers (OET) has shown great promise in various applications, especially in the precise manipulation of microparticles and microorganisms on a micron and nanometer scale. This technology significantly enhances the efficiency of single-cell sorting and the development of antibody-based drugs. However, conventional OET platforms are limited by issues such as low autofocusing accuracy, restricted imaging field of view, and uneven illumination. To overcome these limitations, we have innovatively developed a dynamic adaptive imaging system. By incorporating peak-finding and in situ Gaussian blur compensation algorithms, we achieved rapid automatic focusing and illumination shadow compensation across an expanded field of view. At the same time, the system can also dynamically adjust compensation parameters under different lighting conditions. Our system has successfully completed comprehensive scanning of the optoelectronic tweezers chip, achieving a 60% reduction in autofocus time and a 15.8% improvement in lighting uniformity. Moreover, this imaging system demonstrates robust versatility and can serve as a reference for other optical systems. keywords: {Micrometers;Adaptive systems;Accuracy;Microorganisms;Heuristic algorithms;Lighting;Optical imaging},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611608&isnumber=10609862

H. Han et al., "Development of a 3-RRS Micromanipulator Based on Origami-Inspired Spherical Joint," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15636-15641, doi: 10.1109/ICRA57147.2024.10610385.Abstract: In recent years, micromanipulation technology has achieved extensive applications in industry and life science. Improving the precision and bandwidth of the micromanipulator and simultaneously reducing size, weight, and cost pose significant challenges to the existing micromanipulator design and fabrication methods. Here, we propose a 3-RRS micromanipulator with an origami-inspired spherical joint based on the PC-MEMS process, aiming for miniaturization and cost-effectiveness. The spherical joint allows rotations of 140° around the x-axis approximately, 140° around the y-axis approximately, and 20° around the z-axis approximately. The micromanipulator has weights of 0.8 g, dimensions of 16 mm × 16 mm × 22 mm, and workspace of 0.7 mm3. The end platform of the micromanipulator can be equipped with various effectors to accomplish different kinds of tasks. Experimental results validated its high precision and bandwidth, exhibiting its potential to perform intricate micromanipulation tasks. keywords: {Industries;Fabrication;Costs;Micromanipulators;Design methodology;Bandwidth;Life sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610385&isnumber=10609862

D. Torielli et al., "Wearable Haptics for a Marionette-inspired Teleoperation of Highly Redundant Robotic Systems," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15670-15676, doi: 10.1109/ICRA57147.2024.10610788.Abstract: The teleoperation of complex, kinematically redundant robots with loco-manipulation capabilities represents a challenge for human operators, who have to learn how to operate the many degrees of freedom of the robot to accomplish a desired task. In this context, developing an easy-to-learn and easy-to-use human-robot interface is paramount. Recent works introduced a novel teleoperation concept, which relies on a virtual physical interaction interface between the human operator and the remote robot equivalent to a "Marionette" control, but whose feedback was limited to only visual feedback on the human side. In this paper, we propose extending the "Marionette" interface by adding a wearable haptic interface to cope with the limitations given by the previous works. Leveraging the additional haptic feedback modality, the human operator gains full sensorimotor control over the robot, and the awareness about the robot’s response and interactions with the environment is greatly improved. We evaluated the proposed interface and the related teleoperation framework with naive users, assessing the teleoperation performance and the user experience with and without haptic feedback. The conducted experiments consisted in a loco-manipulation mission with the CENTAURO robot, a hybrid leg-wheel quadruped with a humanoid dual-arm upper body. keywords: {Visualization;Humanoid robots;Robot sensing systems;User experience;Haptic interfaces;Sensors;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610788&isnumber=10609862

A. Suarez, A. Gonzalez-Morgado and A. Ollero, "Lightweight and Compliant Bilateral Teleoperation System with Anthropomorphic Arms for Aerial and Ground Service Operations," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15685-15691, doi: 10.1109/ICRA57147.2024.10611383.Abstract: This paper presents a bilateral teleoperation system based on smart servos for the realization of dexterous manipulation tasks with aerial robots or in ground service applications, facilitating the transferability of cognitive capabilities of human workers to robots operating remotely or in high altitude workspaces. The system consists of a pair of lightweight and compliant anthropomorphic dual arm manipulators (LiCAS) in leader-follower configuration. The leader dual arm (LDA) captures the movements of the operator’s arms to obtain the desired joint references, sent to the follower dual arm (FDA) to reproduce in a natural and intuitive way the manipulation task. A model of the smart servos is derived, exploiting the feedback from the FDA actuators to provide the kinesthetic feedback to the LDA, using the pulse width modulation signal (PWM) along with the joint speed to estimate the interaction torque. The mechanical joint compliance of the FDA allows the passive accommodation of the arms to the physical interactions with the manipulated objects or the environment, whereas the very low weight of the arms (1.0 kg LDA, 2.5 kg FDA) and the human-size and human-like kinematics facilitate their use in a wide variety of applications. The performance of the system is evaluated using an industrial task board for benchmarking, and in two illustrative bimanual aerial manipulation tasks. keywords: {Actuators;Torque;Kinematics;Pulse width modulation;Benchmark testing;Manipulators;Autonomous aerial vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611383&isnumber=10609862

B. Kizilkaya, C. She, G. Zhao and M. Ali Imran, "Intelligent Mode-switching Framework for Teleoperation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15692-15698, doi: 10.1109/ICRA57147.2024.10611333.Abstract: Teleoperation can be very difficult due to limited perception, high communication latency, and limited degrees of freedom (DoFs) at the operator side. Autonomous teleoperation is proposed to overcome this difficulty by predicting user intentions and performing some parts of the task autonomously to decrease the demand on the operator and increase the task completion rate. However, decision-making for mode-switching is generally assumed to be done by the operator, which brings an extra DoF to be controlled by the operator and introduces extra mental demand. On the other hand, the communication perspective is not investigated in the current literature, although communication imperfections and resource limitations are the main bottlenecks for teleoperation. In this study, we propose an intelligent mode-switching framework by jointly considering mode-switching and communication systems. User intention recognition is done at the operator side. Based on user intention recognition, a deep reinforcement learning (DRL) agent is trained and deployed at the operator side to seamlessly switch between autonomous and teleoperation modes. A real-world data set is collected from our teleoperation testbed to train both user intention recognition and DRL algorithms. Our results show that the proposed framework can achieve up to 50% communication load reduction with improved task completion probability. keywords: {Switching systems;Decision making;Switches;Deep reinforcement learning;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611333&isnumber=10609862

M. Antonsen, F. Chinello and Q. Zhang, "Design Octree-Based Method to Improve Model-Mediated Teleoperation in Tactile Internet," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15707-15713, doi: 10.1109/ICRA57147.2024.10610295.Abstract: In this paper, we propose a model-mediated tele-operation (MMT) system using an octree-based model (OBM) to spatially map the environment impedance for the emerging use cases in Tactile Internet. Different from the existing just-noticeable-difference (JND) based MMT, our method avoids continuous transmission of environment impedance. Moreover, it allows the local model to generate accurate force feedback and reduces the number of model updates. Furthermore, the OBM can be deployed with or without previous knowledge of the environment. An online estimation of the OBM is proposed using a JND and a rate-of-change threshold. An offline estimation method is also proposed when the geometry and impedance parameters of the remote environment are known. In addition, a point cloud-based force rendering algorithm is tailored to use the OBM, thereby allowing the generating of force feedback for complex environments. An experiment without human-in-the-loop was conducted, showing that for an online estimated OBM, the accuracy of the force feedback was improved by up to 44 percent while using less than half the number of model updates when compared to JND-based MMT. Another experiment with a human operator interacting with a virtual environment showed that using an offline estimated OBM improves the accuracy of the force feedback and is reliable against packet loss and short temporal breakdown of the communication link. keywords: {Tactile Internet;Visualization;Accuracy;System performance;Force feedback;Force;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610295&isnumber=10609862

L. Chen, A. Naceri, A. Swikir, S. Hirche and S. Haddadin, "Autonomous and Teleoperation Control of a Drawing Robot Avatar," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15714-15720, doi: 10.1109/ICRA57147.2024.10610310.Abstract: A drawing robot avatar is a robotic system that allows for telepresence-based drawing, enabling users to remotely control a robotic arm and create drawings in real-time from a remote location. The proposed control framework aims to improve bimanual robot telepresence quality by reducing the user workload and required prior knowledge through the automation of secondary or auxiliary tasks. The introduced novel method calculates the near-optimal Cartesian end-effector pose in terms of visual feedback quality for the attached eye-to-hand camera with motion constraints in consideration. The effectiveness is demonstrated by conducting user studies of drawing reference shapes using the implemented robot avatar compared to stationary and teleoperated camera pose conditions. Our results demonstrate that the proposed control framework offers improved visual feedback quality and drawing performance. keywords: {Visualization;Telepresence;Automation;Shape;Avatars;Robot vision systems;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610310&isnumber=10609862

J. Park, I. S. Choi, S. -W. Choi and K. Kim, "Adaptive Haptic Control Interface for Safeguarding Robotic Teleoperation in Hazardous Steelmaking Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15721-15727, doi: 10.1109/ICRA57147.2024.10611107.Abstract: Steel mill is one of the most extreme and hazardous working environments due to molten iron erupted from blast furnace. Current manual labor to remove lump iron near the outlet, which is essential to prevent lump iron from scattering or blocking of molten iron, is performed by equipped human workers using a long stick tool. Thus, implementation of robotic teleoperation system is in demand to ensure the safety of workers. However, the conventional command interface is not intuitive for tool manipulation (i.e. pivoting, sweeping). Besides, haptic interface, which is used to render interaction results efficiently, still limits performance due to narrow workspace and insufficient kinesthetic feedback output compared to requirements. This paper proposes a novel haptic command interface (POstick) specified to lump iron removal task with two types (KF and VF). Both POsticks have rod-shaped end tip which is identical to actual tool already used to accelerate training. POstick-KF has large workspace and high kinesthetic feedback output satisfying requirements. Further, POstick-VF has strength with unlimited workspace at the expense of the amount of haptic information from simple vibrotactile feedback. User study to compare the performance of POsticks and conventional interface reveals that POstick-KF and VF showed superior interaction and tracking ability, respectively. Moreover, these two properties are in trade-off relationship that cannot be compatible. Finally, we proposed a seamless and automatic conversion mechanism from POstick-VF to KF, and vice versa, to cover up inherent limits of haptic devices. keywords: {Training;Target tracking;Service robots;Scattering;Iron;Haptic interfaces;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611107&isnumber=10609862

G. Quere, F. Stulp, D. Filliat and J. Silvério, "A probabilistic approach for learning and adapting shared control skills with the human in the loop," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15728-15734, doi: 10.1109/ICRA57147.2024.10610956.Abstract: Assistive robots promise to be of great help to wheelchair users with motor impairments, for example for activities of daily living. Using shared control to provide task-specific assistance – for instance with the Shared Control Templates (SCT) framework – facilitates user control, even with low-dimensional input signals. However, designing SCTs is a laborious task requiring robotic expertise. To facilitate their design, we propose a method to learn one of their core components – active constraints – from demonstrated end-effector trajectories. We use a probabilistic model, Kernelized Movement Primitives, which additionally allows adaptation from user commands to improve the shared control skills, during both design and execution. We demonstrate that the SCTs so acquired can be successfully used to pick up an object, as well as adjusted for new environmental constraints, with our assistive robot EDAN. keywords: {Wheelchairs;Null space;Modulation;Probabilistic logic;Assistive robots;Motors;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610956&isnumber=10609862

R. Wang, Z. Guo, Y. Chen, X. Wang and B. M. Chen, "Air Bumper: A Collision Detection and Reaction Framework for Autonomous MAV Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15735-15741, doi: 10.1109/ICRA57147.2024.10611410.Abstract: Autonomous navigation in unknown environments with obstacles remains challenging for micro aerial vehicles (MAVs) due to their limited onboard computing and sensing resources. Although various collision avoidance methods have been developed, it is still possible for drones to collide with unobserved obstacles due to unpredictable disturbances, sensor limitations, and control uncertainty. Instead of completely avoiding collisions, this article proposes Air Bumper, a collision detection and reaction framework, for fully autonomous flight in 3D environments to improve flight safety. Our framework only utilizes the onboard inertial measurement unit (IMU) to detect and estimate collisions. We further design a collision recovery control for rapid recovery and collision-aware mapping to integrate collision information into general LiDAR-based sensing and planning frameworks. Our simulation and experimental results show that the drone can rapidly detect, estimate, and recover from collisions with obstacles in 3D space and continue the flight smoothly with the help of the collision-aware map. In addition, we will open-source the implementation of Air Bumper on GitHub1. keywords: {Three-dimensional displays;Uncertainty;Software algorithms;Software;Planning;Sensors;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611410&isnumber=10609862

Y. Tao et al., "Learning to Explore Indoor Environments using Autonomous Micro Aerial Vehicles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15758-15764, doi: 10.1109/ICRA57147.2024.10610464.Abstract: In this paper, we address the challenge of exploring unknown indoor environments using autonomous aerial robots with Size Weight and Power (SWaP) constraints. The SWaP constraints induce limits on mission time requiring efficiency in exploration. We present a novel exploration framework that uses Deep Learning (DL) to predict the most likely indoor map given the previous observations, and Deep Reinforcement Learning (DRL) for exploration, designed to run on modern SWaP constraints neural processors. The DL-based map predictor provides a prediction of the occupancy of the unseen environment while the DRL-based planner determines the best navigation goals that can be safely reached to provide the most information. The two modules are tightly coupled and run onboard allowing the vehicle to safely map an unknown environment. Extensive experimental and simulation results show that our approach surpasses state-of-the-art methods by 50-60% in efficiency, which we measure by the fraction of the explored space as a function of the trajectory length. keywords: {Program processors;Simulation;Software algorithms;Full stack;Robot sensing systems;Indoor environment;Space exploration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610464&isnumber=10609862

M. Kulkarni and K. Alexis, "Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15781-15788, doi: 10.1109/ICRA57147.2024.10610287.Abstract: This work contributes a novel deep navigation policy that enables collision-free flight of aerial robots based on a modular approach exploiting deep collision encoding and reinforcement learning. The proposed solution builds upon a deep collision encoder that is trained on both simulated and real depth images using supervised learning such that it compresses the high-dimensional depth data to a low-dimensional latent space encoding collision information while accounting for the robot size. This compressed encoding is combined with an estimate of the robot’s odometry and the desired target location to train a deep reinforcement learning navigation policy that offers low-latency computation and robust sim2real performance. A set of simulation and experimental studies in diverse environments are conducted and demonstrate the efficiency of the emerged behavior and its resilience in real-life deployments. keywords: {Image coding;Navigation;Supervised learning;Noise;Robot sensing systems;Encoding;Odometry},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610287&isnumber=10609862

A. Mukuddem and P. Amayo, "Osiris: Building Hierarchical Representations for Agricultural Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15797-15803, doi: 10.1109/ICRA57147.2024.10610723.Abstract: 3D scene graphs have recently emerged as a powerful and human-understandable way of representing complex 3D environments. These describe environments through a layered or hierarchical graph where nodes represent different spatial concepts (from low-level geometry to higher-level scene-scale reasoning) and the edges between them represent relationships. While these representations have shown great promise in indoor well-structured environments, their use in outdoor structured environments such as agricultural environments has been under-explored. A key challenge here is that concepts and structures often observed in urban indoor environments cannot be easily transferred to these novel scenes.Motivated by this challenge, this paper presents Osiris which is a 3D scene graph builder for agricultural environments. We first propose a structure of the hierarchical graph for agricultural environments consisting of rowed crops and through our proposed system Osiris incrementally construct a 3D scene graph of agricultural environments from data taken onboard a mobile robot. We validate and evaluate the performance of Osiris using real-world data collected at several farms and show that this system is able to accurately get to the underlying structure of these agricultural environments while presenting a metrically accurate and human-understandable representation. keywords: {Measurement;Three-dimensional displays;Robot kinematics;Plants (biology);Semantics;Robot vision systems;Crops},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610723&isnumber=10609862

M. Niemeyer, J. Arkenau, S. Pütz and J. Hertzberg, "Streamlined Acquisition of Large Sensor Data for Autonomous Mobile Robots to Enable Efficient Creation and Analysis of Datasets," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15804-15810, doi: 10.1109/ICRA57147.2024.10611096.Abstract: The increasing usage of modern AI techniques represents a transforming shift in the robotics domain. Training and accessing new models requires substantial amounts of application-specific data, but the limited resources onboard mobile robots (like processing power, network bandwidth, etc.) pose a challenge for the development of efficient data recording and provisioning pipelines. Furthermore, accessing specific information based on a combination of spatial, temporal and semantic information is generally not supported by currently available tools. In this paper, we present a methodology which allows the efficient recording of robotic sensor data streams. We show that our approach reduces the overall time needed until the data can be served via the spatio-temporal-semantic query interface of the semantic environment representation SEEREP. We further present that the maximum sensor data rate which can be stored to disk in real-time is increased for large robotic data types like images and point clouds in comparison to frequently employed solutions within the ROS ecosystem. keywords: {Training;Point cloud compression;Semantics;Pipelines;Data acquisition;Streaming media;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611096&isnumber=10609862

Q. Pan, D. Wang, J. Lian, Y. Dong and C. Qiu, "Development of an Automatic Sweet Pepper Harvesting Robot and Experimental Evaluation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15811-15817, doi: 10.1109/ICRA57147.2024.10610866.Abstract: The aging population and diminishing working population in agriculture motivate the development of autonomous harvesting robots. Although autonomous harvesting is expanding rapidly, the commercial application of sweet pepper harvesting robots still faces challenges. This paper presents the development of a sweet pepper harvesting robot and reports its experimental verification, which mainly includes end-effector design, visual perception, and grasping pose control. The end-effector adopts electrical control, mainly composed of a servo-electric two-finger parallel clamping module, a swing-cutting module, and a fruit recovery device. Equipped with a tactile sensor array, it can accurately sense the sweet pepper peduncle position and the end-effector state (harvesting failure) to complete the precise cutting. An end-effector grasping pose control algorithm of the manipulator is proposed, which can control the end-effector to grasp along the direction of the fruit peduncle and perpendicular to the tangent direction of the picking point by estimating the pose of the sweet pepper peduncle. Finally, the robot and proposed method were verified in a plant factory. The experimental findings demonstrate that the developed harvesting robot can complete robust detection of fruit peduncles and non-destructive picking of sweet pepper, with an average picking time of about 15 seconds. keywords: {Sociology;Tactile sensors;Grasping;End effectors;Production facilities;Sensors;Statistics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610866&isnumber=10609862

M. Asano and T. Fukao, "LiDAR-based Robot Transplanter," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 15818-15824, doi: 10.1109/ICRA57147.2024.10611087.Abstract: In Japan, labor shortage of agriculture is becoming increasingly severe due to the lack of farmers and aging. Therefore, the development of automation of vegetable production such as transplanting, harvesting and transporting is required. In this paper, a self-localization method by using LiDAR and a robust control method of a transplanter are proposed for accurate transplanting. In this system, the path of transplanter is generated by using 3D point cloud data, and the transplanting part follows it and plant seedlings of cabbage accurately. Path generation is performed considering vehicle tilt in the roll direction depending on the environment of grooves. An accurate calculation of lateral and angular position of the transplanting part is also proposed. For path following control, sliding-mode control and inverse optimal control are applied to transplanter. The experimental results demonstrated the effectiveness of these proposed methods and problems we have to tackle on. Basically, it was possible to perform automated transplanting accurately, but there was an occasional problem of offset error from 0. It was confirmed that inverse optimal control is superior to sliding-mode control and is more robust to environmental changes. keywords: {Robust control;Point cloud compression;Accuracy;Three-dimensional displays;Laser radar;Optimal control;Production},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611087&isnumber=10609862

