H. -D. Jang, J. -H. Park and D. E. Chang, "Particle Filter with Stable Embedding for State Estimation of the Rigid Body Attitude System on the Set of Unit Quaternions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6166-6171, doi: 10.1109/ICRA57147.2024.10610922.Abstract: This paper presents a novel method for state estimation of rigid body attitude system evolving on the manifold S3, which is crucial in robotics and drone applications. We introduce a particle filter with stable embedding that extends the system into Euclidean space while ensuring stability of the manifold. Our particle filter with stable embedding enables accurate state estimation by maintaining estimated state values in close proximity to the manifold, while requiring significantly fewer computational resources than the standard exponential-map-based method that keeps state estimates on the manifold. Furthermore, our method facilitates the application of usual techniques designed for particle filters in Euclidean spaces, to the manifold system, as is, without any modification. The accuracy and the efficiency of our particle filter are confirmed both by simulation and by real drone experiments. keywords: {Manifolds;Accuracy;Quaternions;Particle filters;Stability analysis;Performance analysis;State estimation;Stable embedding;manifold;particle filter;attitude estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610922&isnumber=10609862

R. Ferede, C. De Wagter, D. Izzo and G. C. H. E. de Croon, "End-to-end Reinforcement Learning for Time-Optimal Quadcopter Flight," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6172-6177, doi: 10.1109/ICRA57147.2024.10611665.Abstract: Aggressive time-optimal control of quadcopters poses a significant challenge in the field of robotics. The state-of-the-art approach leverages reinforcement learning (RL) to train optimal neural policies. However, a critical hurdle is the sim-to-real gap, often addressed by employing a robust inner loop controller —an abstraction that, in theory, constrains the optimality of the trained controller, necessitating margins to counter potential disturbances. In contrast, our novel approach introduces high-speed quadcopter control using end-to-end RL (E2E) that gives direct motor commands. To bridge the reality gap, we incorporate a learned residual model and an adaptive method that can compensate for modeling errors in thrust and moments. We compare our E2E approach against a state-of-the-art network that commands thrust and body rates to an INDI inner loop controller, both in simulated and real-world flight. E2E showcases a significant 1.39-second advantage in simulation and a 0.17-second edge in real-world testing, highlighting end-to-end reinforcement learning’s potential. The performance drop observed from simulation to reality shows potential for further improvement, including refining strategies to address the reality gap or exploring offline reinforcement learning with real flight data. keywords: {Adaptation models;Sensitivity;Refining;Reinforcement learning;Voltage;Robustness;Task analysis;Time optimal control;Reinforcement Learning;end-to-end control;reality gap;sim-to-real transfer;abstraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611665&isnumber=10609862

Z. Chen, S. Mo, B. Zhang, J. Li and H. Cheng, "Robust Control for Bidirectional Thrust Quadrotors under Instantaneously Drastic Disturbances," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6186-6192, doi: 10.1109/ICRA57147.2024.10611241.Abstract: Quadrotors may crash and cause severe accidents under instantaneously drastic disturbances. To mitigate the effect of such disturbances, these critical issues should be considered: efficient disturbance observation and compensation, full attitude controllability, and instant output power generation of the quadrotor. In this paper, to keep the quadrotor stable even under suddenly drastic disturbances, a novel control framework is presented to by integrating the advantages of active disturbance rejection control (ADRC) as well as geometric control for a quadrotor with bidirectional thrust capabilities. Moreover, to strengthen the adaptability under significant disturbances, a novel switching strategy is introduced into the control framework by virtue of the quadrotor’s bidirectional thrust capabilities. The ADRC scheme is performed when the disturbances are within a range; alternatively, if the disturbances surpass the preset range and the desired control is beyond the ultimate output of the quadrotor, the quadrotor compliantly responds by executing a 180° flip reverse flight to handle such drastic disturbances. Numerical and real-world experiments demonstrate that the proposed robust control strategy has superior performance adapts to instantaneously drastic disturbances. keywords: {Robust control;Attitude control;Switches;Numerical simulation;Controllability;Computer crashes;Robotics and automation;Bidirectional Thrust Quadrotor;Suddenly Drastic Disturbances;Active Disturbance Rejection Control;Geometric Control;Model Predictive Control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611241&isnumber=10609862

Z. He, N. Zhao, Y. Luo, S. Long, X. Luo and H. Deng, "A Multi-modal Hybrid Robot with Enhanced Traversal Performance*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6193-6198, doi: 10.1109/ICRA57147.2024.10609980.Abstract: Current multi-modal hybrid robots with flight and wheeled modes have fallen into the dilemma that they can only avoid obstacles by re-taking off when encountering obstacles due to the poor performance of wheeled obstacle-crossing. To tackle this problem, this paper presents a novel multi-modal hybrid robot with the ability to actively adjust the wheel’s size, which is inspired by the behavior of the turtle’s legs when it encounters obstacles, to enhance the traversal performance. In detail, we describe the hardware design that allows the robot to achieve a modal switch between flight and wheeled modes through foldable structures and variable wheel diameters; then, we present the architecture to control these two morphing mechanisms. After that, we establish the theoretical kinematic models for both the foldable arm and variable wheel and carry out extensive experiments to test the performance of the foldable arm, the variable-diameter wheel, as well as the traversal performance of the robot. Experimental results show that the proposed multimodal robot can realize the function of a quadrotor, respond quickly with full-scale folding within 0.9 s, climb a maximum slope of 36°, and traverse narrow passageways, which exhibit superior mobility and environmental adaptability. keywords: {Legged locomotion;Wheels;Prototypes;Manipulators;Sensors;Mobile robots;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609980&isnumber=10609862

B. Kim, H. Seong and D. H. Shim, "Topological Exploration using Segmented Map with Keyframe Contribution in Subterranean Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6199-6205, doi: 10.1109/ICRA57147.2024.10610605.Abstract: Existing exploration algorithms mainly generate frontiers using random sampling or motion primitive methods within a specific sensor range or search space. However, frontiers generated within constrained spaces lead to back-and-forth maneuvers in large-scale environments, thereby diminishing exploration efficiency. To address this issue, we propose a method that utilizes a 3D dense map to generate Segmented Exploration Regions (SERs) and generate frontiers from a global-scale perspective. In particular, this paper presents a novel topological map generation approach that fully utilizes Line-of-Sight (LOS) features of LiDAR sensor points to enhance exploration efficiency inside large-scale subterranean environments. Our topological map contains the contributions of keyframes that generate each SER, enabling rapid exploration through a switch between local path planning and global path planning to each frontier. The proposed method achieved higher explored volume generation than the state-of-the-art algorithm in a large-scale simulation environment and demonstrated a 62% improvement in explored volume increment performance. For validation, we conducted field tests using UAVs in real subterranean environments, demonstrating the efficiency and speed of our method. keywords: {Three-dimensional displays;Laser radar;Motion segmentation;Line-of-sight propagation;Switches;Robot sensing systems;Data structures},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610605&isnumber=10609862

A. Lynch, C. Duguid, J. Buzzatto and M. Liarokapis, "A Powerline Inspection UAV Equipped with Dexterous, Lockable Gripping Mechanisms for Autonomous Perching and Contact Rolling," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6206-6211, doi: 10.1109/ICRA57147.2024.10610783.Abstract: Inspection of powerlines is a hard problem that requires humans to operate in remote locations and dangerous conditions. This paper proposes a quadcopter unmanned aerial vehicle (UAV) equipped with rolling-capable perching mechanisms and a depth-vision system for the purpose of autonomous power line inspection. The perching mechanism grips onto the power line, allowing the UAV to withstand external forces such as wind disturbances. Once engaged and applying the desired gripping force, the perching mechanism requires no power through the use of a ratcheting serial elastic transmission, allowing the UAV to perch indefinitely. The depth-vision system automates the perching and unperching procedures by estimating the position and pose of the UAV relative to the powerline. These measurements are sent to a local position controller that guides the UAV to and from the power line. Once perched, rollers in the fingers of the perching mechanism drive the UAV along the powerline, providing a close-up platform for inspection equipment. The proposed system was tested in an outdoor testing environment and shown to autonomously perch and unperch from a steel cable. The grippers force application was analysed and the UAVs powerless robust perch is demonstrated by total disconnect of power while perched. These results suggest that such a system could be a valuable tool for the upkeep of electricity networks. keywords: {Force;Inspection;Autonomous aerial vehicles;Conductors;Steel;Solar panels;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610783&isnumber=10609862

K. Goel and W. Tabib, "GIRA: Gaussian Mixture Models for Inference and Robot Autonomy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6212-6218, doi: 10.1109/ICRA57147.2024.10611216.Abstract: This paper introduces the open-source framework, GIRA, which implements fundamental robotics algorithms for reconstruction, pose estimation, and occupancy modeling using compact generative models. Compactness enables perception in the large by ensuring that the perceptual models can be communicated through low-bandwidth channels during large-scale mobile robot deployments. The generative property enables perception in the small by providing high-resolution reconstruction capability. These properties address perception needs for diverse robotic applications, including multi-robot exploration and dexterous manipulation. State-of-the-art perception systems construct perceptual models via multiple disparate pipelines that reuse the same underlying sensor data, which leads to increased computation, redundancy, and complexity. GIRA bridges this gap by providing a unified perceptual modeling framework using Gaussian mixture models (GMMs) as well as a novel systems contribution, which consists of GPUaccelerated functions to learn GMMs 10-100x faster compared to existing CPU implementations. Because few GMM-based frameworks are open-sourced, this work seeks to accelerate innovation and broaden adoption of these techniques. keywords: {Point cloud compression;Adaptation models;Technological innovation;Computational modeling;Pose estimation;Robot sensing systems;Software},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611216&isnumber=10609862

G. Picardi, A. Astolfi and M. Calisti, "Seabed intervention with an underwater legged robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6247-6253, doi: 10.1109/ICRA57147.2024.10611135.Abstract: Efficiently performing intervention tasks underwater is crucial in various commercial and scientific sectors; however, propeller-driven vehicles face limitations due to their floating nature. In Remotely Operated Vehicles (ROVs) operations, this can be compensated by the ability of the operator, but they come with high operational costs. Instead, Autonomous Underwater Vehicles (AUVs) have shown promise, but demonstrated intervention tasks are limited to controlled environments or docked. To address these limitations, we focused on the use of Underwater Legged Robots (ULRs), which offer greater stability and agile seabed mobility thanks to their legged propulsion system. This paper presents the field demonstration of teleoperated pick-and-place tasks using the ULR SILVER2 for which a novel stance control, Graphic User Interface (GUI), and tendon-driven gripper have been developed based on the lessons learned through several hours of field use. The methodology is validated through four field trials, including missions in both shallow water and open sea environments. The trials involve picking and placing various objects, such as plastic bottles, bags, and cans. The results demonstrate successful teleoperated object grasping and manipulation in real-world conditions, with collection times ranging from a few minutes to around ten minutes. Overall, this research contributes to advancing the capabilities of ULRs and lays the foundation for future underwater intervention missions in various scientific and industrial applications, aligning with the goals of the Decade of Ocean Science for Sustainable Development. keywords: {Legged locomotion;Remotely guided vehicles;Service robots;Oceans;Propulsion;Plastic products;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611135&isnumber=10609862

F. L. Busch, N. Bauschmann, S. Haddadin, R. Seifried and D. A. Duecker, "Predicting against the Flow: Boosting Source Localization by Means of Field Belief Modeling using Upstream Source Proximity," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6254-6259, doi: 10.1109/ICRA57147.2024.10610144.Abstract: Time-effective and accurate source localization with mobile robots is crucial in safety-critical scenarios, e.g. leakage detection. This becomes particular challenging in realistic cluttered scenarios, i.e. in the presence of complex current flows or wind. Traditional methods often fall short due to simplifications or limited onboard resources.We propose to combine source localization with a Gaussian Markov Random Field (GMRF). This allows to improve source localization hypotheses by building on the GMRF’s concentration and flow field belief that are continuously updated by gathered measurements. We introduce the upstream source proximity (USP) as a natural metric that exploits the joint knowledge represented in the field belief’s concentration and flow field, i.e. predicting sources upstream. As a result, our method yields a computationally efficient source localization and field belief module providing substantially more stable gradients than conventional concentration gradient-based methods.We demonstrate the suitability of our approach in a series of numerical experiments covering complex source location scenarios. With regard to computational requirements, the method achieves update rates of 10Hz on a RaspberryPi4B. keywords: {Location awareness;Measurement;Wind;Computational modeling;Position measurement;Predictive models;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610144&isnumber=10609862

W. Qi, Q. Sun and H. Qian, "A Turning Radius Prediction Scheme for Sailing Robots under Complex Marine Environment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6260-6265, doi: 10.1109/ICRA57147.2024.10611373.Abstract: This paper presents a strategy for predicting the turning radius of a sailing robot with consideration of aerodynamic and hydrodynamic interferences from the marine environment. The turning radius is initially obtained based on three consecutive designated points during the turning process, which is regarded as the baseline method. Subsequently, on the basis of our constructed turning datasets, a model is trained using Gaussian process regression (GPR) to achieve radius prediction. The feasibility and effectiveness of the proposed scheme have been validated in both simulation and experiments (conducted with OceanVoy as shown in Fig. 1). Under experimental circumstances, the Mean Absolute Error (MAE) of the turning radius produced by the trained prediction model is 0.58m. Furthermore, it has been observed that during longterm sailing covering a distance of 1200km, apart from wind speed and robot velocity, the tidal range also has a significant impact on the navigation of sailing robots. keywords: {Navigation;Wind speed;Gaussian processes;Predictive models;Turning;Hydrodynamics;Risk management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611373&isnumber=10609862

O. Kwon et al., "WayIL: Image-based Indoor Localization with Wayfinding Maps," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6274-6281, doi: 10.1109/ICRA57147.2024.10610480.Abstract: This paper tackles a localization problem in large-scale indoor environments with wayfinding maps. A wayfinding map abstractly portrays the environment, and humans can localize themselves based on the map. However, when it comes to using it for robot localization, large geometrical discrepancies between the wayfinding map and the real world make it hard to use conventional localization methods. Our objective is to estimate a robot pose within a wayfinding map, utilizing RGB images from perspective cameras. We introduce two different imagination modules which are inspired by how humans can comprehend and interpret their surroundings for localization purposes. These modules jointly learn how to effectively observe the first-person-view (FPV) world to interpret bird-eye-view (BEV) maps. Providing explicit guidance to the two imagination modules significantly improves the precision of the localization system. We demonstrate the effectiveness of the proposed approach using real-world datasets, which are collected from various large-scale crowded indoor environments. The experimental results show that, in 85% of scenarios, the proposed localization system can estimate its pose within 3m in large indoor spaces. Project Site: https://rllab-snu.github.io/projects/WayIL/ keywords: {Location awareness;Accuracy;Navigation;Semantics;Robot vision systems;Robot localization;Particle filters},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610480&isnumber=10609862

W. Song, R. Yan, B. Lei and T. Okatani, "Globalizing Local Features: Image Retrieval Using Shared Local Features with Pose Estimation for Faster Visual Localization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6290-6297, doi: 10.1109/ICRA57147.2024.10610786.Abstract: Visual localization is an important sub-task in SfM and visual SLAM that involves estimating a 6-DoF camera pose for an input query image relative to a given 3D model of the environment. The most accurate approach is a hierarchical one that splits the task into two stages: image retrieval and camera pose estimation. Each stage requires different image features, with global features compactly encoding holistic image information for the first stage and local features encoding the appearance around salient image points for the second stage. While existing methods use independent networks to extract these features, one for global and one for local, this strategy is suboptimal in terms of computational efficiency. In this paper, we propose a novel approach that achieves state-of-the-art inference accuracy with significantly improved efficiency. Our approach’s core component is SuperGF, a network that aggregates local features optimized for camera pose estimation to create a global feature that enables precise image retrieval. Through extensive experiments on the standard benchmark tests, we demonstrate that the method offers a better trade-off between accuracy and computational cost. keywords: {Location awareness;Visualization;Accuracy;Image coding;Three-dimensional displays;Pose estimation;Image retrieval},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610786&isnumber=10609862

L. Chen, W. Chen, R. Wang and M. Pollefeys, "Leveraging Neural Radiance Fields for Uncertainty-Aware Visual Localization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6298-6305, doi: 10.1109/ICRA57147.2024.10610126.Abstract: As a promising fashion for visual localization, scene coordinate regression (SCR) has seen tremendous progress in the past decade. Most recent methods usually adopt neural networks to learn the mapping from image pixels to 3D scene coordinates, which requires a vast amount of annotated training data. We propose to leverage Neural Radiance Fields (NeRF) to generate training samples for SCR. Despite NeRF’s efficiency in rendering, many of the rendered data are polluted by artifacts or only contain minimal information gain, which can hinder the regression accuracy or bring unnecessary computational costs with redundant data. These challenges are addressed in three folds in this paper: (1) A NeRF is designed to separately predict uncertainties for the rendered color and depth images, which reveal data reliability at the pixel level. (2) SCR is formulated as deep evidential learning with epistemic uncertainty, which is used to evaluate information gain and scene coordinate quality. (3) Based on the three arts of uncertainties, a novel view selection policy is formed that significantly improves data efficiency. Experiments on public datasets demonstrate that our method could select the samples that bring the most information gain and promote the performance with the highest efficiency. keywords: {Location awareness;Training;Visualization;Uncertainty;Three-dimensional displays;Robot kinematics;Training data},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610126&isnumber=10609862

A. Schperberg, Y. Tanaka, S. Mowlavi, F. Xu, B. Balaji and D. Hong, "OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6314-6320, doi: 10.1109/ICRA57147.2024.10610160.Abstract: State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot’s trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated in hardware using a quadruped robot on various terrains, yielding a 65% improvement on the Root Mean Squared Error compared to our VIO SLAM baseline. Code example: https://github.com/AlexS28/OptiState keywords: {Legged locomotion;Accuracy;Filtering;Logic gates;Robot sensing systems;Transformers;Kalman filters},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610160&isnumber=10609862

J. Morlana, J. D. Tardós and J. M. M. Montiel, "ColonMapper: topological mapping and localization for colonoscopy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6329-6336, doi: 10.1109/ICRA57147.2024.10610426.Abstract: We propose a topological mapping and localization system able to operate on real human colonoscopies, despite significant shape and illumination changes. The map is a graph where each node codes a colon location by a set of real images, while edges represent traversability between nodes. For close-in-time images, where scene changes are minor, place recognition can be successfully managed with the recent transformers-based local feature matching algorithms. However, under long-term changes –such as different colonoscopies of the same patient– feature-based matching fails. To address this, we train on real colonoscopies a deep global descriptor achieving high recall with significant changes in the scene. The addition of a Bayesian filter boosts the accuracy of long-term place recognition, enabling relocalization in a previously built map. Our experiments show that ColonMapper is able to autonomously build a map and localize against it in two important use cases: localization within the same colonoscopy or within different colonoscopies of the same patient. Code: github.com/jmorlana/ColonMapper. keywords: {Location awareness;Visualization;Simultaneous localization and mapping;Codes;Shape;Colonoscopy;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610426&isnumber=10609862

J. Loo and D. Hsu, "Scene Action Maps: Behavioural Maps for Navigation without Metric Information," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6354-6360, doi: 10.1109/ICRA57147.2024.10610489.Abstract: Humans are remarkable in their ability to navigate without metric information. We can read abstract 2D maps, such as floor-plans or hand-drawn sketches, and use them to navigate in unseen rich 3D environments, without requiring prior traversals to map out these scenes in detail. We posit that this is enabled by the ability to represent the environment abstractly as interconnected navigational behaviours, e.g., "follow the corridor" or "turn right", while avoiding detailed, accurate spatial information at the metric level. We introduce the Scene Action Map (SAM), a behavioural topological graph, and propose a learnable map-reading method, which parses a variety of 2D maps into SAMs. Map-reading extracts salient information about navigational behaviours from the overlooked wealth of pre-existing, abstract and inaccurate maps, ranging from floor-plans to sketches. We evaluate the performance of SAMs for navigation, by building and deploying a behavioural navigation stack on a quadrupedal robot. Videos and more information is available at: https://scene-action-maps.github.io. keywords: {Accuracy;Three-dimensional displays;Navigation;Affordances;Buildings;Dynamic range;Distance measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610489&isnumber=10609862

I. Bogoslavskyi, K. Zampogiannis and R. Phan, "Fast and Robust Normal Estimation for Sparse LiDAR Scans," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6389-6395, doi: 10.1109/ICRA57147.2024.10611556.Abstract: Light Detection and Ranging (LiDAR) technology has proven to be an important part of many robotics systems. Surface normals estimated from LiDAR data are commonly used for a variety of tasks in such systems. As most of the today’s mechanical LiDAR sensors produce sparse data, estimating normals from a single scan in a robust manner poses difficulties.In this paper, we address the problem of estimating normals for sparse LiDAR data avoiding the typical issues of smoothing out the normals in high curvature areas.Mechanical LiDARs rotate a set of rigidly mounted lasers. One firing of such a set of lasers produces an array of points where each point’s neighbor is known due to the known firing pattern of the scanner. We use this knowledge to connect these points to their neighbors and label them using the angles of the lines connecting them. When estimating normals at these points, we only consider points with the same label as neighbors. This allows us to avoid estimating normals in high curvature areas.We evaluate our approach on various data, both self-recorded and publicly available, acquired using various sparse LiDAR sensors. We show that using our method for normal estimation leads to normals that are more robust in areas with high curvature which leads to maps of higher quality. We also show that our method only incurs a constant factor runtime overhead with respect to a lightweight baseline normal estimation procedure and is therefore suited for operation in computationally demanding environments. keywords: {Mechanical sensors;Laser radar;Runtime;Smoothing methods;Firing;Lasers;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611556&isnumber=10609862

B. Liu et al., "OmniColor: A Global Camera Pose Optimization Approach of LiDAR-360Camera Fusion for Colorizing Point Clouds," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6396-6402, doi: 10.1109/ICRA57147.2024.10610292.Abstract: A Colored point cloud, as a simple and efficient 3D representation, has many advantages in various fields, including robotic navigation and scene reconstruction. This representation is now commonly used in 3D reconstruction tasks relying on cameras and LiDARs. However, fusing data from these two types of sensors is poorly performed in many existing frameworks, leading to unsatisfactory mapping results, mainly due to inaccurate camera poses. This paper presents Omni-Color, a novel and efficient algorithm to colorize point clouds using an independent 360-degree camera. Given a LiDAR-based point cloud and a sequence of panorama images with initial coarse camera poses, our objective is to jointly optimize the poses of all frames for mapping images onto geometric reconstructions. Our pipeline works in an off-the-shelf manner that does not require any feature extraction or matching process. Instead, we find optimal poses by directly maximizing the photometric consistency of LiDAR maps. In experiments, we show that our method can overcome the severe visual distortion of omnidirectional images and greatly benefit from the wide field of view (FOV) of 360-degree cameras to reconstruct various scenarios with accuracy and stability. The code will be released at https://github.com/liubonan123/OmniColor/. keywords: {Point cloud compression;Visualization;Three-dimensional displays;Laser radar;Accuracy;Robot vision systems;Virtual reality},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610292&isnumber=10609862

Y. Cui, Q. Ye, Q. Liu, A. Chen, G. Li and J. Chen, "InterRep: A Visual Interaction Representation for Robotic Grasping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6448-6454, doi: 10.1109/ICRA57147.2024.10610870.Abstract: Recently, pre-trained vision models have gained significant attention in motor control, showcasing impressive performance across diverse robotic learning tasks. While previous works predominantly concentrate on the significance of the pre-training phase, the equally important task of extracting more effective representations based on existing pre-trained visual models remains unexplored. To better leverage the representation capabilities of pre-trained models for robotic grasping, we propose InterRep, a novel interaction representation method that possesses not only the strengths of pre-trained models, known for their robustness in noisy environments and their proficiency in recognizing essential features, but also the capacity of capturing dynamic interaction details and local geometric features during the grasping process. Based on the novel representation, we introduce a deep reinforcement learning method to learn generalizable grasping policies. The experimental results demonstrate that our proposed representation outperforms the baselines in terms of both training speed and generalization. For the generalized grasping tasks with dexterous robotic hands, our method boasts a success rate nearly 20% higher than methods using the global features of the entire image from pre-trained models. In addition, our proposed representation method demonstrates promising performance when applied to a different robotic hand and task. It also exhibits excellent performance on real robots with a success rate of 70%. keywords: {Training;Visualization;Motor drives;Shape;Grasping;Feature extraction;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610870&isnumber=10609862

H. J. Choi and N. Figueroa, "Towards Feasible Dynamic Grasping: Leveraging Gaussian Process Distance Field, SE(3) Equivariance, and Riemannian Mixture Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6455-6461, doi: 10.1109/ICRA57147.2024.10611601.Abstract: This paper introduces a novel approach to improve robotic grasping in dynamic environments by integrating Gaussian Process Distance Fields (GPDF), SE(3) equivariant networks, and Riemannian Mixture Models. The aim is to enable robots to grasp moving objects effectively. Our approach comprises three main components: object shape reconstruction, grasp sampling, and implicit grasp pose selection. GPDF accurately models the shape of objects, which is essential for precise grasp planning. SE(3) equivariance ensures that the sampled grasp poses are equivariant to the object’s pose changes, enhancing robustness in dynamic scenarios. Riemannian Gaussian Mixture Models are employed to assess reachability, providing a feasible and adaptable grasping strategies. Feasible grasp poses are targeted by novel task or joint space reactive controllers formulated using Gaussian Mixture Models and Gaussian Processes. This method resolves the challenge of discrete grasp pose selection, enabling smoother grasping execution. Experimental validation confirms the effectiveness of our approach in generating feasible grasp poses and achieving successful grasps in dynamic environments. By integrating these advanced techniques, we present a promising solution for enhancing robotic grasping capabilities in real-world scenarios. keywords: {Shape;Dynamics;Grasping;Mixture models;Planning;Collision avoidance;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611601&isnumber=10609862

H. Yan, H. -S. Fang and C. Lu, "A Surprisingly Efficient Representation for Multi-Finger Grasping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6462-6469, doi: 10.1109/ICRA57147.2024.10611424.Abstract: The problem of grasping objects using a multi-finger hand has received significant attention in recent years. However, it remains challenging to handle a large number of unfamiliar objects in real and cluttered environments. In this work, we propose a representation that can be effectively mapped to the multi-finger grasp space. Based on this representation, we develop a simple decision model that generates accurate grasp quality scores for different multi-finger grasp poses using only hundreds to thousands of training samples. We demonstrate that our representation performs well on a real robot and achieves a success rate of 78.64% after training with only 500 real-world grasp attempts and 87% with 4500 grasp attempts. Additionally, we achieve a success rate of 84.51% in a dynamic human-robot handover scenario using a multi-finger hand. keywords: {Training;Adaptation models;Accuracy;Supervised learning;Training data;Grasping;Handover},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611424&isnumber=10609862

F. Zhao, D. Tsetserukou and Q. Liu, "GrainGrasp: Dexterous Grasp Generation with Fine-grained Contact Guidance," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6470-6476, doi: 10.1109/ICRA57147.2024.10610035.Abstract: One goal of dexterous robotic grasping is to allow robots to handle objects with the same level of flexibility and adaptability as humans. However, it remains a challenging task to generate an optimal grasping strategy for dexterous hands, especially when it comes to delicate manipulation and accurate adjustment the desired grasping poses for objects of varying shapes and sizes. In this paper, we propose a novel dexterous grasp generation scheme called GrainGrasp that provides fine-grained contact guidance for each fingertip. In particular, we employ a generative model to predict separate contact maps for each fingertip on the object point cloud, effectively capturing the specifics of finger-object interactions. In addition, we develop a new dexterous grasping optimization algorithm that solely relies on the point cloud as input, eliminating the necessity for complete mesh information of the object. By leveraging the contact maps of different fingertips, the proposed optimization algorithm can generate precise and determinable strategies for human-like object grasping. Experimental results confirm the efficiency of the proposed scheme. Our code is available at https://github.com/wmtlab/GrainGrasp. keywords: {Point cloud compression;Codes;Accuracy;Shape;Optimization methods;Grasping;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610035&isnumber=10609862

J. Lee, Z. Sun, Z. Dong, F. Chen and H. S. Stuart, "Regrasping on Printed Circuit Boards with the Smart Suction Cup," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6477-6483, doi: 10.1109/ICRA57147.2024.10610153.Abstract: The disposal of waste electrical and electronic equipment (WEEE) presents a sustainability challenge, particularly for waste printed circuit boards (PCBs). PCBs are challenging to sort out from other waste materials in part because traditional industrial end-effectors struggle to reliably grip these irregularly shaped objects with unmodeled surface-mounted components. Vision-based separators, while effective for object categorization, face challenges with identifying precise grasp points on PCB surfaces. This paper studies regrasping control to enhance suction cup grasping performance on PCBs, addressing issues arising from uneven surfaces and intricate features that interfere with suction sealing. We categorize PCBs into two recycling levels – with large surface features intact or removed – and conduct experiments on both stationary and conveyor belt setups with realistic vision-based grasp planners. Results show that jumping regrasping improves pick-and-place success rate. Haptically driven jumping – using the Smart Suction Cup – is especially useful for unprocessed waste PCBs with large surface mount parts. The proposed method offers a promising solution to enhance the efficiency and reliability of robotic grasping in recycling applications. keywords: {Particle separators;Surface mount technology;Grasping;Electronic waste;Recycling;Object recognition;Integrated circuit reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610153&isnumber=10609862

Y. Dong and F. T. Pokorny, "Quasi-static Soft Fixture Analysis of Rigid and Deformable Objects," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6513-6520, doi: 10.1109/ICRA57147.2024.10611593.Abstract: We present a sampling-based approach to reasoning about the caging-based manipulation of rigid and a simplified class of deformable 3D objects subject to energy constraints. Towards this end, we propose the notion of soft fixtures extending earlier work on energy-bounded caging to include a broader set of energy function constraints, such as gravitational and elastic potential energy of 3D deformable objects. Previous methods focused on establishing provably correct algorithms to compute lower bounds or analytically exact estimates of escape energy for a very restricted class of known objects with low-dimensional configuration spaces, such as planar polygons. We instead propose a practical sampling-based approach that is applicable in higher-dimensional configuration spaces, but only produces a sequence of upper-bound estimates that, however, appear to converge rapidly to actual escape energy. We present 8 simulation experiments demonstrating the applicability of our approach to various complex quasi-static manipulation scenarios. Quantitative results indicate the effectiveness of our approach in providing upper-bound estimates for escape energy in quasi-static manipulation scenarios. Two real-world experiments also show that the computed normalized escape energy estimates appear to correlate strongly with the probability of escape of an object under randomized pose perturbation1. keywords: {Potential energy;Three-dimensional displays;Heuristic algorithms;Fixtures;Cognition;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611593&isnumber=10609862

H. Ichikura and M. Higashimori, "In-Hand Rolling Manipulation Based on Ball-on-Cloth System," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6521-6527, doi: 10.1109/ICRA57147.2024.10609867.Abstract: This paper presents a novel in-hand rolling manipulation method in which a ball on a cloth attached to fingertips is controlled using flexible and adaptive deformation of the cloth. First, an analytical model of the ball-on-cloth system is introduced. The shape of the cloth is simplified, and the rolling constraint of the ball on the cloth is defined focusing on the lowest point of the ball. Next, the relationship between the input to the cloth anchor point and the position of the lowest point of the ball is expressed by a linear approximation. Then, the input to generate the desired rolling orbit is designed. Next, as an example of utilizing the rolling orbits, a manipulation method to rotate the ball around a vertical axis is developed. Finally, a multi-fingered hand with a piece of cloth attached to the fingertips is developed, and the effectiveness of the proposed system is experimentally verified. keywords: {Geometry;Analytical models;Shape;Force;Linear approximation;Focusing;Orbits},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609867&isnumber=10609862

L. Tao, J. Zhang, Q. Zheng and X. Zhang, "Curriculum-based Sensing Reduction in Simulation to Real-World Transfer for In-hand Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6530-6536, doi: 10.1109/ICRA57147.2024.10610328.Abstract: Simulation to Real-World Transfer allows affordable and fast training of learning-based robots for manipulation tasks using Deep Reinforcement Learning methods. Currently, Asymmetric Actor-Critic approaches are used for Sim2Real to reduce the rich idealized features in simulation to the accessible ones in the real world. However, the feature reduction from the simulation to the real world is conducted through an empirically defined one-step curtail. Small feature reduction does not sufficiently remove the actor’s features, which may still cause difficulty setting up the physical system, while large feature reduction may cause difficulty and inefficiency in training. To address this issue, we proposed Curriculum-based Sensing Reduction to enable the actor to start with the same rich feature space as the critic and then get rid of the hard-to-extract features step-by-step for higher training performance and better adaptation for real-world feature space. The reduced features are replaced with random signals from a Deep Random Generator to remove the dependency between the output and the removed features and avoid creating new dependencies. The methods are evaluated on the Allegro robot hand in a real-world in-hand manipulation task. The results show that our methods have faster training and higher task performance than baselines and can solve real-world tasks when selected tactile features are reduced. keywords: {Training;Robot sensing systems;Deep reinforcement learning;Generators;Sensors;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610328&isnumber=10609862

K. Van Wyk, A. Handa, V. Makoviychuk, Y. Guo, A. Allshire and N. D. Ratliff, "Geometric Fabrics: a Safe Guiding Medium for Policy Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6537-6543, doi: 10.1109/ICRA57147.2024.10610235.Abstract: Robotics policies are always subjected to complex, second order dynamics that entangle their actions with resulting states. In reinforcement learning (RL) contexts, policies have the burden of deciphering these complicated interactions over massive amounts of experience and complex reward functions to learn how to accomplish tasks. Moreover, policies typically issue actions directly to controllers like Operational Space Control (OSC) or joint PD control, which induces straightline motion towards these action targets in task or joint space. However, straightline motion in these spaces for the most part do not capture the rich, nonlinear behavior our robots need to exhibit, shifting the burden of discovering these behaviors more completely to the agent. Unlike these simpler controllers, geometric fabrics capture a much richer and desirable set of behaviors via artificial, second order dynamics grounded in nonlinear geometry. These artificial dynamics shift the uncontrolled dynamics of a robot via an appropriate control law to form behavioral dynamics. Behavioral dynamics unlock a new action space and safe, guiding behavior over which RL policies are trained. Behavioral dynamics enable bang-bang-like RL policy actions that are still safe for real robots, simplify reward engineering, and help sequence real-world, high-performance policies. We describe the framework more generally and create a specific instantiation for the problem of dexterous, in-hand reorientation of a cube by a highly actuated robot hand. keywords: {Geometry;Reinforcement learning;Aerospace electronics;Fabrics;PD control;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610235&isnumber=10609862

B. Liang, K. Ota, M. Tomizuka and D. K. Jha, "Robust In-Hand Manipulation with Extrinsic Contacts," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6544-6550, doi: 10.1109/ICRA57147.2024.10611664.Abstract: We present in-hand manipulation tasks where a robot moves an object in grasp, maintains its external contact mode with the environment, and adjusts its in-hand pose simultaneously. The proposed manipulation task leads to complex contact interactions which can be very susceptible to uncertainties in kinematic and physical parameters. Therefore, we propose a robust in-hand manipulation method, which consists of two parts. First, an in-gripper mechanics model that computes a naïve motion cone assuming all parameters are precise. Then, a robust planning method refines the motion cone to maintain desired contact mode regardless of parametric errors. Real-world experiments were conducted to illustrate the accuracy of the mechanics model and the effectiveness of the robust planning framework in the presence of kinematics parameter errors. keywords: {Uncertainty;Accuracy;Computational modeling;Kinematics;Planning;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611664&isnumber=10609862

G. Khandate, C. P. Mehlman, X. Wei and M. Ciocarlie, "Dexterous In-hand Manipulation by Guiding Exploration with Simple Sub-skill Controllers," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6551-6557, doi: 10.1109/ICRA57147.2024.10611300.Abstract: Recently, reinforcement learning has led to dexterous manipulation skills of increasing complexity. Nonetheless, learning these skills in simulation still exhibits poor sample-efficiency which stems from the fact these skills are learned from scratch without the benefit of any domain expertise. In this work, we aim to improve the sample efficiency of learning dexterous in-hand manipulation skills using controllers available via domain knowledge. To this end, we design simple sub-skill controllers and demonstrate improved sample efficiency using a framework that guides exploration toward relevant state space by following actions from these controllers. We are the first to demonstrate learning hard-to-explore finger-gaiting in-hand manipulation skills without the use of an exploratory reset distribution. keywords: {Reinforcement learning;Switches;Aerospace electronics;Control systems;Trajectory;Complexity theory;Task analysis;Dexterous manipulation;Reinforcement learning;In-hand reorientation;In-hand manipulation;Guided exploration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611300&isnumber=10609862

Y. Yuan et al., "Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6558-6565, doi: 10.1109/ICRA57147.2024.10610532.Abstract: Executing contact-rich manipulation tasks necessitates the fusion of tactile and visual feedback. However, the distinct nature of these modalities poses significant challenges. In this paper, we introduce a system that leverages visual and tactile sensory inputs to enable dexterous in-hand manipulation. Specifically, we propose Robot Synesthesia, a novel point cloudbased tactile representation inspired by human tactile-visual synesthesia. This approach allows for the simultaneous and seamless integration of both sensory inputs, offering richer spatial information and facilitating better reasoning about robot actions. Comprehensive ablations are performed on how the integration of vision and touch can improve reinforcement learning and Sim2Real performance. Our project page is available at https://yingyuan0414.github.io/visuotactile/. keywords: {Training;Point cloud compression;Visualization;Optical feedback;Pipelines;Tactile sensors;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610532&isnumber=10609862

J. Deng, G. Chan, H. Zhong and C. X. Lu, "Robust 3D Object Detection from LiDAR-Radar Point Clouds via Cross-Modal Feature Augmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6585-6591, doi: 10.1109/ICRA57147.2024.10610775.Abstract: This paper presents a novel framework for robust 3D object detection from point clouds via cross-modal hallucination. Our proposed approach is agnostic to either hallucination direction between LiDAR and 4D radar. We introduce multiple alignments on both spatial and feature levels to achieve simultaneous backbone refinement and hallucination generation. Specifically, spatial alignment is proposed to deal with the geometry discrepancy for better instance matching between LiDAR and radar. The feature alignment step further bridges the intrinsic attribute gap between the sensing modalities and stabilizes the training. The trained object detection models can deal with difficult detection cases better, even though only single-modal data is used as the input during the inference stage. Extensive experiments on the View-of-Delft (VoD) dataset show that our proposed method outperforms the state-of-the-art (SOTA) methods for both radar and LiDAR object detection while maintaining competitive efficiency in runtime. keywords: {Point cloud compression;Training;Laser radar;Three-dimensional displays;Runtime;Radar detection;Object detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610775&isnumber=10609862

Q. Jiang and H. Sun, "LSSAttn: Towards Dense and Accurate View Transformation for Multi-modal 3D Object Detection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6600-6606, doi: 10.1109/ICRA57147.2024.10610830.Abstract: Fusing the camera and LiDAR information in the unified BEV representation serves as the elegant paradigm for the 3D detection tasks. Current multi-modal fusion methods in BEV can be categorized into LSS-based and Transformer-based in terms of their view transformation. The former leverages inaccurate depth prediction and massive pseudo points for perspective-to-BEV transformation while the latter only fetches sparse image features to the BEV representation. To overcome their shortcomings, an optimized view transformation is proposed, which can be easily modulated into the LSS-based methods. The proposed module capitalizes on the LSS mechanism to establish dense associations between perspective pixels and BEV grids. It utilizes the attention mechanism to compute similarity scores for each associated pair during feature aggregation. Starting from the BEVFusion baseline, we further introduce (1) cross-attention within the associated subsets to transfer image features into the BEV, and (2) a multi-scale feature fusion mechanism for LSS-based view transformation. Extensive experiments on nuScenes validate the effectiveness and efficiency of our proposed module, which achieves an increase of 1.3% in mAP compared to the baseline model. keywords: {Three-dimensional displays;Attention mechanisms;Accuracy;Laser radar;Object detection;Transformers;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610830&isnumber=10609862

S. Moon, H. Park, J. Lee and J. Kim, "Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6607-6613, doi: 10.1109/ICRA57147.2024.10610934.Abstract: In autonomous driving and robotics, there is a growing interest in utilizing short-term historical data to enhance multi-camera 3D object detection, leveraging the continuous and correlated nature of input video streams. Recent work has focused on spatially aligning BEV-based features over timesteps. However, this is often limited as its gain does not scale well with long-term past observations. To address this, we advocate for supervising a model to predict objects’ poses given past observations, thus explicitly guiding to learn objects’ temporal cues. To this end, we propose a model called DAP (Detection After Prediction), consisting of a two-branch network: (i) a branch responsible for forecasting the current objects’ poses given past observations and (ii) another branch that detects objects based on the current and past observations. The features predicting the current objects from branch (i) is fused into branch (ii) to transfer predictive knowledge. We conduct extensive experiments with the large-scale nuScenes datasets, and we observe that utilizing such predictive information significantly improves the overall detection performance. Our model can be used plug-and-play, showing consistent performance gain. keywords: {Solid modeling;Three-dimensional displays;Refining;Object detection;Predictive models;Streaming media;Performance gain},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610934&isnumber=10609862

S. Tang, S. Zhang and Y. Fang, "HIC-YOLOv5: Improved YOLOv5 For Small Object Detection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6614-6619, doi: 10.1109/ICRA57147.2024.10610273.Abstract: Small object detection has been a challenging problem in the field of object detection. There has been some works that proposes improvements for this task, such as adding several attention blocks or changing the whole structure of feature fusion networks. However, the computation cost of these models is large, which makes deploying a real-time object detection system unfeasible, while leaving room for improvement. To this end, an improved YOLOv5 model: HIC-YOLOv5 is proposed to address the aforementioned problems. Firstly, an additional prediction head specific to small objects is added to provide a higher-resolution feature map for better prediction. Secondly, an involution block is adopted between the backbone and neck to increase channel information of the feature map. Moreover, an attention mechanism named CBAM is applied at the end of the backbone, thus not only decreasing the computation cost compared with previous works but also emphasizing the important information in both channel and spatial domain. Our result shows that HIC-YOLOv5 has improved mAP@[.5:.95] by 6.42% and mAP@0.5 by 9.38% on VisDrone-2019-DET dataset. keywords: {YOLO;Costs;Attention mechanisms;Computational modeling;Data augmentation;Prediction algorithms;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610273&isnumber=10609862

C. Jiang, Y. Yang and M. Jagersand, "CLIPUNetr: Assisting Human-robot Interface for Uncalibrated Visual Servoing Control with CLIP-driven Referring Expression Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6620-6626, doi: 10.1109/ICRA57147.2024.10611647.Abstract: The classical human-robot interface in uncalibrated image-based visual servoing (UIBVS) relies on either human annotations or semantic segmentation with categorical labels. Both methods fail to match natural human communication and convey rich semantics in manipulation tasks as effectively as natural language expressions. In this paper, we tackle this problem by using referring expression segmentation, which is a prompt-based approach, to provide more in-depth information for robot perception. To generate high-quality segmentation predictions from referring expressions, we propose CLIPUNetr - a new CLIP-driven referring expression segmentation network. CLIPUNetr leverages CLIP’s strong vision-language representations to segment regions from referring expressions, while utilizing its "U-shaped" encoder-decoder architecture to generate predictions with sharper boundaries and finer structures. Furthermore, we propose a new pipeline to integrate CLIPUNetr into UIBVS and apply it to control robots in real-world environments. In experiments, our method improves boundary and structure measurements by an average of 120% and can successfully assist real-world UIBVS control in an unstructured manipulation environment. keywords: {Adaptation models;Annotations;Semantic segmentation;Pipelines;Semantics;Natural languages;Modulation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611647&isnumber=10609862

S. V. Rebbapragada, P. Panda and V. N. Balasubramanian, "C2FDrone: Coarse-to-Fine Drone-to-Drone Detection using Vision Transformer Networks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6627-6633, doi: 10.1109/ICRA57147.2024.10609997.Abstract: A vision-based drone-to-drone detection system is crucial for various applications like collision avoidance, countering hostile drones, and search-and-rescue operations. However, detecting drones presents unique challenges, including small object sizes, distortion, occlusion, and real-time processing requirements. Current methods integrating multi-scale feature fusion and temporal information have limitations in handling extreme blur and minuscule objects. To address this, we propose a novel coarse-to-fine detection strategy based on vision transformers. We evaluate our approach on three challenging drone- to-drone detection datasets, achieving F1 score enhancements of 7%, 3%, and 1% on the FL-Drones, AOT, and NPS-Drones datasets, respectively. Additionally, we demonstrate real-time processing capabilities by deploying our model on an edge-computing device. Our code will be made publicly available. keywords: {Computer vision;Codes;Image edge detection;Image representation;Transformers;Distortion;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609997&isnumber=10609862

Y. You et al., "Better Monocular 3D Detectors with LiDAR from the Past," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6634-6641, doi: 10.1109/ICRA57147.2024.10610444.Abstract: Accurate 3D object detection is crucial to autonomous driving. Though LiDAR-based detectors have achieved impressive performance, the high cost of LiDAR sensors precludes their widespread adoption in affordable vehicles. Camera-based detectors are cheaper alternatives but often suffer inferior performance compared to their LiDAR-based counterparts due to inherent depth ambiguities in images. In this work, we seek to improve monocular 3D detectors by leveraging unlabeled historical LiDAR data. Specifically, at inference time, we assume that the camera-based detectors have access to multiple unlabeled LiDAR scans from past traversals at locations of interest (potentially from other high-end vehicles equipped with LiDAR sensors). Under this setup, we proposed a novel, simple, and end-to-end trainable framework, termed AsyncDepth, to effectively extract relevant features from asynchronous LiDAR traversals of the same location for monocular 3D detectors. We show consistent and significant performance gain (up to 9 AP) across multiple state-of-the-art models and datasets with a negligible additional latency of 9.66 ms and a small storage cost. Our code can be found at https://github.com/YurongYou/AsyncDepth. keywords: {Three-dimensional displays;Laser radar;Costs;Detectors;Object detection;Performance gain;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610444&isnumber=10609862

M. Gummadi, C. Kent, K. Schmeckpeper and E. Eaton, "A Metacognitive Approach to Out-of-Distribution Detection for Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6642-6649, doi: 10.1109/ICRA57147.2024.10611287.Abstract: Despite outstanding semantic scene segmentation in closed-worlds, deep neural networks segment novel instances poorly, which is required for autonomous agents acting in an open world. To improve out-of-distribution (OOD) detection for segmentation, we introduce a metacognitive approach in the form of a lightweight module that leverages entropy measures, segmentation predictions, and spatial context to characterize the segmentation model’s uncertainty and detect pixel-wise OOD data in real-time. Additionally, our approach incorporates a novel method of generating synthetic OOD data in context with in-distribution data, which we use to fine-tune existing segmentation models with maximum entropy training. This further improves the metacognitive module’s performance without requiring access to OOD data while enabling compatibility with established pre-trained models. Our resulting approach can reliably detect OOD instances in a scene, as shown by state-of-the-art performance on OOD detection for semantic segmentation benchmarks. keywords: {Training;Uncertainty;Semantic segmentation;Measurement uncertainty;Semantics;Data models;Entropy},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611287&isnumber=10609862

K. Honda, R. Yonetani, M. Nishimura and T. Kozuno, "When to Replan? An Adaptive Replanning Strategy for Autonomous Navigation using Deep Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6650-6656, doi: 10.1109/ICRA57147.2024.10611474.Abstract: The hierarchy of global and local planners is one of the most commonly utilized system designs in autonomous robot navigation. While the global planner generates a reference path from the current to goal locations based on the pre-built map, the local planner produces a kinodynamic trajectory to follow the reference path while avoiding perceived obstacles. To account for unforeseen or dynamic obstacles not present on the pre-built map, "when to replan" the reference path is critical for the success of safe and efficient navigation. However, determining the ideal timing to execute replanning in such partially unknown environments still remains an open question. In this work, we first conduct an extensive simulation experiment to compare several common replanning strategies and confirm that effective strategies are highly dependent on the environment as well as the global and local planners. Based on this insight, we then derive a new adaptive replanning strategy based on deep reinforcement learning, which can learn from experience to decide appropriate replanning timings in the given environment and planning setups. Our experimental results show that the proposed replanner can perform on par or even better than the current best-performing strategies in multiple situations regarding navigation robustness and efficiency. keywords: {Navigation;Deep reinforcement learning;Robustness;Timing;Trajectory;Planning;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611474&isnumber=10609862

H. Li, S. Yu, S. Zhang and G. Tan, "Resolving Loop Closure Confusion in Repetitive Environments for Visual SLAM through AI Foundation Models Assistance," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6657-6663, doi: 10.1109/ICRA57147.2024.10610083.Abstract: In visual SLAM (VSLAM) systems, loop closure plays a crucial role in reducing accumulated errors. However, VSLAM systems relying on low-level visual features often suffer from the problem of perceptual confusion in repetitive environments, where scenes in different locations are incorrectly identified as the same. Existing work has attempted to introduce object-level features or artificial landmarks. The former approach struggles to distinguish visually similar but different objects, while the latter is both time-consuming and labor-intensive. This paper introduces a novel loop closure detection method that leverages pretrained AI foundation models to extract rich semantic information about specific types of objects (e.g., door numbers), referred to as semantic anchors, that help to distinguish similar scenes better. In settings such as office buildings, hotels, and warehouses, this approach helps to improve the robustness of loop closure detection. We validate the effectiveness of our method through experiments conducted in both simulated and real-world environments. keywords: {Location awareness;Visualization;Simultaneous localization and mapping;Accuracy;Semantics;Buildings;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610083&isnumber=10609862

P. Katara, Z. Xian and K. Fragkiadaki, "Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6672-6679, doi: 10.1109/ICRA57147.2024.10610566.Abstract: Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task descriptions, temporal decompositions, and corresponding python reward functions for reinforcement learning. We show Gen2Sim succeeds in learning policies for diverse long horizon tasks, where reinforcement learning with non temporally decomposed reward functions fails. Gen2Sim provides a viable path for scaling up reinforcement learning for robot manipulators in simulation, both by diversifying and expanding task and environment development, and by facilitating the discovery of reinforcement-learned behaviors through temporal task decomposition in RL. Our work contributes hundreds of simulated assets, tasks and demonstrations, taking a step towards fully autonomous robotic manipulation skill acquisition in simulation. keywords: {Training;Solid modeling;Three-dimensional displays;Training data;Reinforcement learning;Manipulators;Robot learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610566&isnumber=10609862

J. Zhang et al., "FLTRNN: Faithful Long-Horizon Task Planning for Robotics with Large Language Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6680-6686, doi: 10.1109/ICRA57147.2024.10611663.Abstract: Recent planning methods based on Large Language Models typically employ the In-Context Learning paradigm. Complex long-horizon planning tasks require more context(including instructions and demonstrations) to guarantee that the generated plan can be executed correctly. However, in such conditions, LLMs may overlook(unfaithful) the rules in the given context, resulting in the generated plans being invalid or even leading to dangerous actions. In this paper, we investigate the faithfulness of LLMs for complex long-horizon tasks. Inspired by human intelligence, we introduce a novel framework named FLTRNN. FLTRNN employs a language-based RNN structure to integrate task decomposition and memory management into LLM planning inference, which could effectively improve the faithfulness of LLMs and make the planner more reliable. We conducted experiments in VirtualHome household tasks. Results show that our model significantly improves faithfulness and success rates for complex long-horizon tasks. Website at https://tannl.github.io/FLTRNN.github.io/ keywords: {Knowledge engineering;Large language models;Human intelligence;Memory management;Cognition;Planning;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611663&isnumber=10609862

T. -H. Wang et al., "Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6687-6694, doi: 10.1109/ICRA57147.2024.10611590.Abstract: As autonomous driving technology matures, end-to-end methodologies have emerged as a leading strategy, promising seamless integration from perception to control via deep learning. However, existing systems grapple with challenges such as unexpected open set environments and the complexity of black-box models. At the same time, the evolution of deep learning introduces larger, multimodal foundational models, offering multi-modal visual and textual understanding. In this paper, we harness these multimodal foundation models to enhance the robustness and adaptability of autonomous driving systems. We introduce a method to extract nuanced spatial features from transformers and the incorporation of latent space simulation for improved training and policy debugging. We use pixel/patch-aligned feature descriptors to expand foundational model capabilities to create an end-to-end multimodal driving model, demonstrating unparalleled results in diverse tests. Our solution combines language with visual perception and achieves significantly greater robustness on out-of-distribution situations. keywords: {Training;Deep learning;Adaptation models;Visualization;Debugging;Feature extraction;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611590&isnumber=10609862

Y. Chen, J. Arkin, C. Dawson, Y. Zhang, N. Roy and C. Fan, "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6695-6702, doi: 10.1109/ICRA57147.2024.10611163.Abstract: For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to jointly solve the task and motion plan. To improve translation, we automatically detect and correct both syntactic and semantic errors via autoregressive re-prompting, resulting in significant improvements in task completion. We show that our approach outperforms several methods using LLMs as planners in complex task domains. See our project website§ for prompts, videos, and code. keywords: {Runtime;Semantics;Optimization methods;Syntactics;Market research;Planning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611163&isnumber=10609862

J. Plewnia, F. Peller-Konrad and T. Asfour, "Forgetting in Robotic Episodic Long-Term Memory," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6711-6717, doi: 10.1109/ICRA57147.2024.10610299.Abstract: Artificial cognitive architectures traditionally rely on complex memory models to encode, store, and retrieve information. However, the conventional practice of transferring all data from working memory (WM) to long-term memory (LTM) leads to high data volumes and challenges in efficient information processing and access. Deciding what information to retain or discard within a robot’s LTM is particularly challenging since knowledge about future data utilization is absent. Drawing inspiration from human forgetting this paper implements and evaluates novel forgetting techniques that allow consolidation in the robot’s LTM only when new information is encountered. The proposed approach combines fast filtering during data transfer to the robot’s LTM with slower yet more precise forgetting mechanisms that are periodically evaluated for offline data deletion inside the LTM. We compare different mechanisms, utilizing metrics such as data similarity, data age, and consolidation frequency. The efficacy of forgetting techniques is evaluated by comparing their performance in a task where two ARMAR robots search through their LTM for past object locations in episodic ego-centric images and robot state data. Experimental results show that our forgetting techniques significantly reduce the space requirements of a robot’s LTM while maintaining its capacity to successfully perform tasks relying on LTM information. Notably, similarity-based forgetting methods outperform frequency- and time-based approaches. The combination of online frequency-based, online similarity-based, offline similarity-based, and time-based decay methods shows superior performance compared to using individual forgetting strategies. keywords: {Measurement;Location awareness;Time-frequency analysis;Filtering;Memory management;Information processing;Search problems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610299&isnumber=10609862

R. L. Haugaard, Y. Kim and T. M. Iversen, "Fixture calibration with guaranteed bounds from a few correspondence-free surface points," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6718-6724, doi: 10.1109/ICRA57147.2024.10610632.Abstract: Calibration of fixtures in robotic work cells is essential but also time consuming and error-prone, and poor calibration can easily lead to wasted debugging time in down-stream tasks. Contact-based calibration methods let the user measure points on the fixture’s surface with a tool tip attached to the robot’s end effector. Most such methods require the user to manually annotate correspondences on the CAD model, however, this is error-prone and a cumbersome user experience. We propose a correspondence-free alternative: The user simply measures a few points from the fixture’s surface, and our method provides a tight superset of the poses which could explain the measured points. This naturally detects ambiguities related to symmetry and uninformative points and conveys this uncertainty to the user. Perhaps more importantly, it provides guaranteed bounds on the pose. The computation of such bounds is made tractable by the use of a hierarchical grid on SE(3). Our method is evaluated both in simulation and on a real collaborative robot, showing great potential for easier and less error-prone fixture calibration. sites.google.com/view/ttpose keywords: {Solid modeling;Uncertainty;Fixtures;Measurement uncertainty;Collaborative robots;Manuals;User experience},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610632&isnumber=10609862

F. Mirus, F. Pasch and K. -U. Scholl, "Towards fault-tolerant deployment of mobile robot navigation in the edge: an experimental study," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6791-6797, doi: 10.1109/ICRA57147.2024.10611013.Abstract: Modern algorithms allow robots to reach a greater level of autonomy and fulfill more challenging tasks. However, on-board limitations regarding computational and battery resources are hindering factors regarding the deployment of such algorithms particularly on mobile robots. Although offloading a majority of the algorithmic components to the edge or even cloud offers an attractive option to leverage massive computing power in robotics applications, safety and reliability remain critical issues. This paper presents a minimalistic safety fallback mechanism when offloading mobile robot navigation to the edge, that ensures safe and collision-free navigation even in the presence of failures in the connection between the on-board and edge-devices. We show the effectiveness of our approach through extensive testing in three different relevant scenarios in a simulated warehouse environment. Our experiments demonstrate the effects of different fallback strategies and show how our proposed approach is able to ensure safety while allowing the robot to continue its mission during an interrupted connection and thus avoiding unnecessary downtime. keywords: {Wireless communication;Fault tolerance;Navigation;Fault tolerant systems;Safety;Batteries;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611013&isnumber=10609862

Y. Duan, Q. Zhang and R. Xu, "Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6798-6805, doi: 10.1109/ICRA57147.2024.10611614.Abstract: The utilization of Large Language Models (LLMs) within the realm of reinforcement learning, particularly as planners, has garnered a significant degree of attention in recent scholarly literature. However, a substantial proportion of existing research predominantly focuses on planning models for robotics that transmute the outputs derived from perception models into linguistic forms, thus adopting a ‘pure-language’ strategy. In this research, we propose a hybrid End-to-End learning framework for autonomous driving by combining basic driving imitation learning with LLMs based on multi-modality prompt tokens. Instead of simply converting perception results from the separated train model into pure language input, our novelty lies in two aspects. 1) The end-to-end integration of visual and LiDAR sensory input into learnable multi-modality tokens, thereby intrinsically alleviating description bias by separated pre-trained perception models. 2) Instead of directly letting LLMs drive, this paper explores a hybrid setting of letting LLMs help the driving model correct mistakes and complicated scenarios. The results of our experiments suggest that the proposed methodology can attain driving scores of 49.21%, coupled with an impressive route completion rate of 91.34% in the offline evaluation conducted via CARLA. These performance metrics are comparable to the most advanced driving models. keywords: {Measurement;Visualization;Laser radar;Imitation learning;Wheels;Reinforcement learning;Linguistics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611614&isnumber=10609862

Z. Huang, P. Karkus, B. Ivanovic, Y. Chen, M. Pavone and C. Lv, "DTPP: Differentiable Joint Conditional Prediction and Cost Evaluation for Tree Policy Planning in Autonomous Driving," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6806-6812, doi: 10.1109/ICRA57147.2024.10610550.Abstract: Motion prediction and cost evaluation are vital components in the decision-making system of autonomous vehicles. However, existing methods often ignore the importance of cost learning and treat them as separate modules. In this study, we employ a tree-structured policy planner and propose a differentiable joint training framework for both ego-conditioned prediction and cost models, resulting in a direct improvement of the final planning performance. For conditional prediction, we introduce a query-centric Transformer model that performs efficient ego-conditioned motion prediction. For planning cost, we propose a learnable context-aware cost function with latent interaction features, facilitating differentiable joint learning. We validate our proposed approach using the real-world nuPlan dataset and its associated planning test platform. Our framework not only matches state-of-the-art planning methods but outperforms other learning-based methods in planning quality, while operating more efficiently in terms of runtime. We show that joint training delivers significantly better performance than separate training of the two modules. Additionally, we find that tree-structured policy planning outperforms the conventional single-stage planning approach. Code is available: https://github.com/MCZhi/DTPP. keywords: {Training;Costs;Runtime;Trajectory planning;Predictive models;Transformers;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610550&isnumber=10609862

V. K. Nivash and A. H. Qureshi, "SIMMF: Semantics-aware Interactive Multiagent Motion Forecasting for Autonomous Vehicle Driving," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6813-6819, doi: 10.1109/ICRA57147.2024.10611189.Abstract: Autonomous vehicles require motion forecasting of their surrounding multiagents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose the Semantics-aware Interactive Multiagent Motion Forecasting (SIMMF) method to capture semantics along with spatial information and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. These encodings along with agents’ local information, are passed through an encoder to obtain time-dependent latent variables for a motion policy predicting the future trajectories. Our results show that the proposed approach outperforms stateof-the-art baselines and provides more accurate and scene-consistent predictions. The demonstration video is available at: https://youtu.be/Wjla071BPBA keywords: {Measurement;Pedestrians;Navigation;Semantics;Encoding;Trajectory;Forecasting},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611189&isnumber=10609862

L. Sun et al., "CausalAgents: A Robustness Benchmark for Motion Forecasting," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6820-6827, doi: 10.1109/ICRA57147.2024.10610186.Abstract: As machine learning models become increasingly prevalent in motion forecasting for autonomous vehicles (AVs), it is critical to ensure that model predictions are safe and reliable. In this paper, we examine the robustness of motion forecasting to non-causal perturbations. We construct a new benchmark for evaluating and improving model robustness by applying perturbations to existing data. Specifically, we conduct an extensive labeling effort to identify causal agents, or agents whose presence influences human drivers’ behavior, in the Waymo Open Motion Dataset (WOMD), and we use these labels to perturb the data by deleting non-causal agents from the scene. We evaluate a diverse set of state-of-the-art deep-learning models on our proposed benchmark and find that all evaluated models exhibit large shifts under non-causal perturbation: we observe a surprising 25-38% relative change in minADE as compared to the original. In addition, we investigate techniques to improve model robustness, including increasing the training dataset size and using targeted data augmentations that randomly drop non-causal agents throughout training. Finally, we release the causal agent labels as an extension to WOMD and the robustness benchmarks to aid the community in building more reliable and safe deep-learning models for motion forecasting 1. keywords: {Training;Sensitivity;Perturbation methods;Machine learning;Predictive models;Benchmark testing;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610186&isnumber=10609862

T. Nyberg, J. van Haastregt and J. Tumova, "Highway-Driving with Safe Velocity Bounds on Occluded Traffic," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6828-6835, doi: 10.1109/ICRA57147.2024.10610904.Abstract: Limited visibility and sensor occlusions pose pressing safety challenges for advanced driver-assistance systems (ADAS) and autonomous vehicles (AVs). In this work, our pursuit was to strike a balance: a method that ensures safety in occluded scenarios while preventing overly cautious behavior. We argue that such approaches are crucial for AVs’ future, particularly when navigating alongside human drivers on highways at high speeds. To this end, we used reachability analysis to find safe velocity bounds on occluded traffic participants. Compared to state-of-the-art methods, we achieved velocity increases in more than 60% of the 230 cut-in scenarios from the highD dataset, without sacrificing safety. keywords: {Road transportation;Navigation;Pressing;Robot sensing systems;Safety;Robotics and automation;Reachability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610904&isnumber=10609862

V. Jayawardana, S. Li, C. Wu, Y. Farid and K. Oguchi, "Generalizing Cooperative Eco-driving via Multi-residual Task Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6836-6842, doi: 10.1109/ICRA57147.2024.10610586.Abstract: Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability. However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms. Model-free Deep Reinforcement Learning (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning. We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control. By analyzing the performance of MRTL across nearly 600 signalized intersections and 1200 traffic scenarios, we demonstrate that it emerges as a promising approach to synergize the strengths of DRL and conventional methods in generalizable control. keywords: {Markov decision processes;Multitasking;Deep reinforcement learning;Control systems;Planning;Reliability;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610586&isnumber=10609862

D. Garces, S. Bhattacharya, D. Bertsekas and S. Gil, "Approximate Multiagent Reinforcement Learning for On-Demand Urban Mobility Problem on a Large Map," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6843-6849, doi: 10.1109/ICRA57147.2024.10611063.Abstract: In this paper, we focus on the autonomous multiagent taxi routing problem for a large urban environment where the location and number of future ride requests are unknown a-priori, but can be estimated by an empirical distribution. Recent theory has shown that a rollout algorithm with a stable base policy produces a near-optimal stable policy. In the routing setting, a policy is stable if its execution keeps the number of outstanding requests uniformly bounded over time. Although, rollout-based approaches are well-suited for learning cooperative multiagent policies with considerations for future demand, applying such methods to a large urban environment can be computationally expensive due to the large number of taxis required for stability. In this paper, we aim to address the computational bottleneck of multiagent rollout by proposing an approximate multiagent rollout-based two phase algorithm that reduces computational costs, while still achieving a stable near-optimal policy. Our approach partitions the graph into sectors based on the predicted demand and the maximum number of taxis that can run sequentially given the user’s computational resources. The algorithm then applies instantaneous assignment (IA) for re-balancing taxis across sectors and a sector-wide multiagent rollout algorithm that is executed in parallel for each sector. We provide two main theoretical results: 1) characterize the number of taxis m that is sufficient for IA to be stable; 2) derive a necessary condition on m to maintain stability for IA as time goes to infinity. Our numerical results show that our approach achieves stability for an m that satisfies the theoretical conditions. We also empirically demonstrate that our proposed two phase algorithm has equivalent performance to the one-at-a-time rollout over the entire map, but with significantly lower runtimes. keywords: {Sufficient conditions;Urban areas;Approximation algorithms;Prediction algorithms;Routing;Stability analysis;Partitioning algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611063&isnumber=10609862

H. Niu, Y. Xu, X. Jiang and J. Hu, "Continual Driving Policy Optimization with Closed-Loop Individualized Curricula," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6850-6857, doi: 10.1109/ICRA57147.2024.10611578.Abstract: The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for flexible implementation choices: AV Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a collision prediction task, where it estimates the chance of AV failures in these scenarios at each iteration. Subsequently, by re-sampling from historical scenarios based on these failure probabilities, CLIC tailors individualized curricula for downstream training, aligning them with the evaluated capability of AV. Accordingly, CLIC not only maximizes the utilization of the vast pre-collected scenario library for closed-loop driving policy optimization but also facilitates AV improvement by individualizing its training with more challenging cases out of those poorly organized scenarios. Experimental results clearly indicate that CLIC surpasses other curriculum-based training strategies, showing substantial improvement in managing risky scenarios, while still maintaining proficiency in handling simpler cases. keywords: {Training;Information filters;Libraries;Safety;Task analysis;Surges;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611578&isnumber=10609862

Y. Shen, L. Zheng, T. Zhou and C. Lin, "Task-Driven Domain-Agnostic Learning with Information Bottleneck for Autonomous Steering," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6858-6865, doi: 10.1109/ICRA57147.2024.10610479.Abstract: Environments for autonomous driving can vary from place to place, leading to challenges in designing a learning model for a new scene. Transfer learning can leverage knowledge from a learned domain to a new domain with limited data. In this work, we focus on end-to-end autonomous driving as the target task, consisting of both perception and control. We first utilize information bottleneck analysis to build a causal graph that defines our framework and the loss function; then we propose a novel domain-agnostic learning method for autonomous steering based on our analysis of training data, network architecture, and training paradigm. Experiments show that our method outperforms other SOTA methods. keywords: {Training;Measurement;Learning systems;Transfer learning;Training data;Network architecture;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610479&isnumber=10609862

A. O’Neill et al., "Open X-Embodiment: Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration0," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6892-6903, doi: 10.1109/ICRA57147.2024.10611477.Abstract: Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train "generalist" X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. The project website is robotics-transformer-x.github.io. keywords: {Learning systems;Adaptation models;Computer vision;Computational modeling;Collaboration;Data models;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611477&isnumber=10609862

H. Bharadhwaj, A. Gupta, V. Kumar and S. Tulsiani, "Towards Generalizable Zero-Shot Manipulation via Translating Human Interaction Plans," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6904-6911, doi: 10.1109/ICRA57147.2024.10610288.Abstract: We pursue the goal of developing robots that can interact zero-shot with generic unseen objects via a diverse repertoire of manipulation skills and show how passive human videos can serve as a rich source of data for learning such generalist robots. Unlike typical robot learning approaches which directly learn how a robot should act from interaction data, we adopt a factorized approach that can leverage large-scale human videos to learn how a human would accomplish a desired task (a human ‘plan’), followed by ‘translating’ this plan to the robot’s embodiment. Specifically, we learn a human ‘plan predictor’ that, given a current image of a scene and a goal image, predicts the future hand and object configurations. We combine this with a ‘translation’ module that learns a plan-conditioned robot manipulation policy, and allows following humans plans for generic manipulation tasks in a zero-shot manner with no deployment-time training. Importantly, while the plan predictor can leverage large-scale human videos for learning, the translation module only requires a small amount of in-domain data, and can generalize to tasks not seen during training. We show that our learned system can perform over 16 manipulation skills that generalize to 40 objects, encompassing 100 real-world tasks for table-top manipulation and diverse in-the-wild manipulation. https://homangab.github.io/hopman/ keywords: {Training;Robot learning;Task analysis;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610288&isnumber=10609862

J. Mejia, V. Dean, T. Hellebrekers and A. Gupta, "Hearing Touch: Audio-Visual Pretraining for Contact-Rich Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6912-6919, doi: 10.1109/ICRA57147.2024.10611305.Abstract: Although pre-training on a large amount of data is beneficial for robot learning, current paradigms only perform large-scale pretraining for visual representations, whereas representations for other modalities are trained from scratch. In contrast to the abundance of visual data, it is unclear what relevant internet-scale data may be used for pretraining other modalities such as tactile sensing. Such pretraining becomes increasingly crucial in the low-data regimes common in robotics applications. In this paper, we address this gap by using contact microphones as an alternative tactile sensor. Our key insight is that contact microphones capture inherently audio-based information, allowing us to leverage large-scale audio-visual pretraining to obtain representations that boost the performance of robotic manipulation. To the best of our knowledge, our method is the first approach leveraging large-scale multisensory pre-training for robotic manipulation. For supplementary information including videos of real robot experiments, please see https://sites.google.com/view/hearing-touch. keywords: {Visualization;Soft sensors;Tactile sensors;Robot sensing systems;Robot learning;Safety;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611305&isnumber=10609862

I. Leal et al., "SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust Attention," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6920-6927, doi: 10.1109/ICRA57147.2024.10611597.Abstract: We present Self-Adaptive Robust Attention for Robotics Transformers (SARA-RT): a new paradigm for addressing the emerging challenge of scaling up Robotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new method of fine-tuning proposed by us, called up-training. It converts pre-trained or already fine-tuned Transformer-based robotic policies of quadratic time complexity (including massive billion-parameter vision-language-action models or VLAs), into their efficient linear-attention counterparts maintaining high quality. We demonstrate the effectiveness of SARA-RT by speeding up: (a) the class of recently introduced RT-2 models [1], the first VLA robotic policies pre-trained on internet-scale data, as well as (b) Point Cloud Transformer (PCT) robotic policies operating on large point clouds. We complement our results with the rigorous mathematical analysis providing deeper insight into the phenomenon of SARA. keywords: {Point cloud compression;Adaptation models;Mathematical analysis;Transformers;Data models;Time complexity;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611597&isnumber=10609862

W. K. Do, A. Kundan Dhawan, M. Kitzmann and M. Kennedy, "DenseTact-Mini: An Optical Tactile Sensor for Grasping Multi-Scale Objects From Flat Surfaces," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6928-6934, doi: 10.1109/ICRA57147.2024.10610583.Abstract: Dexterous manipulation, especially of small daily objects, continues to pose complex challenges in robotics. This paper introduces the DenseTact-Mini, an optical tactile sensor with a soft, rounded, smooth gel surface and compact design equipped with a synthetic fingernail. We propose three distinct grasping strategies: tap grasping using adhesion forces such as electrostatic and van der Waals, fingernail grasping leveraging rolling/sliding contact between the object and fingernail, and fingertip grasping with two soft fingertips. Through comprehensive evaluations, the DenseTact-Mini demonstrates a lifting success rate exceeding 90.2% when grasping various objects, including items such as 1mm basil seeds, thin paperclips, and items larger than 15mm such as bearings. This work demonstrates the potential of soft optical tactile sensors for dexterous manipulation and grasping. keywords: {Integrated optics;Force measurement;Optical design;Tactile sensors;Grasping;Length measurement;Optical imaging},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610583&isnumber=10609862

T. Cohn, S. Shaw, M. Simchowitz and R. Tedrake, "Constrained Bimanual Planning with Analytic Inverse Kinematics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6935-6942, doi: 10.1109/ICRA57147.2024.10610675.Abstract: In order for a bimanual robot to manipulate an object that is held by both hands, it must construct motion plans such that the transformation between its end effectors remains fixed. This amounts to complicated nonlinear equality constraints in the configuration space, which are difficult for trajectory optimizers. In addition, the set of feasible configurations becomes a measure zero set, which presents a challenge to sampling-based motion planners. We leverage an analytic solution to the inverse kinematics problem to parametrize the configuration space, resulting in a lower-dimensional representation where the set of valid configurations has positive measure. We describe how to use this parametrization with existing motion planning algorithms, including sampling-based approaches, trajectory optimizers, and techniques that plan through convex inner-approximations of collision-free space. keywords: {Kinematics;End effectors;Trajectory;Planning;Motion measurement;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610675&isnumber=10609862

S. Ancha, P. R. Osteen and N. Roy, "Deep Evidential Uncertainty Estimation for Semantic Segmentation under Out-Of-Distribution Obstacles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6943-6951, doi: 10.1109/ICRA57147.2024.10611342.Abstract: In order to navigate safely and reliably in novel environments, robots must estimate perceptual uncertainty when confronted with out-of-distribution (OOD) obstacles not seen in training data. We present a method to accurately estimate pixel-wise uncertainty in semantic segmentation without requiring real or synthetic OOD examples at training time. From a shared per-pixel latent feature representation, a classification network predicts a categorical distribution over semantic labels, while a normalizing flow estimates the probability density of features under the training distribution. The label distribution and density estimates are combined in a Dirichlet-based evidential uncertainty framework that efficiently computes epistemic and aleatoric uncertainty in a single neural network forward pass. Our method is enabled by three key contributions. First, we simplify the problem of learning a transformation to the training data density by starting from a fitted Gaussian mixture model instead of the conventional standard normal distribution. Second, we learn a richer and more expressive latent pixel representation to aid OOD detection by training a decoder to reconstruct input image patches. Third, we perform theoretical analysis of the loss function used in the evidential uncertainty framework and propose a principled objective that more accurately balances training the classification and density estimation networks. We demonstrate the accuracy of our uncertainty estimation approach under long-tail OOD obstacle classes for semantic segmentation in both off-road and urban driving environments. keywords: {Training;Analytical models;Uncertainty;Accuracy;Navigation;Semantic segmentation;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611342&isnumber=10609862

Y. Mao et al., "NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6952-6958, doi: 10.1109/ICRA57147.2024.10611269.Abstract: Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure. Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGB-D images, along with extracting dense and complete surfaces. Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency. keywords: {Geometry;Tracking loops;Simultaneous localization and mapping;Accuracy;Robot vision systems;Rendering (computer graphics);Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611269&isnumber=10609862

Y. Lin, Z. Li, Y. Cui and Z. Fang, "SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud Tracking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6959-6965, doi: 10.1109/ICRA57147.2024.10611238.Abstract: 3D single object tracking (SOT) is an important and challenging task for the autonomous driving and mobile robotics. Most existing methods perform tracking between two consecutive frames while ignoring the motion patterns of the target over a series of frames, which would cause performance degradation in the scenes with sparse points. To break through this limitation, we introduce "Sequence-to-Sequence" tracking paradigm and a tracker named SeqTrack3D to capture target motion across continuous frames. Unlike previous methods that primarily adopted three strategies: matching two consecutive point clouds, predicting relative motion, or utilizing sequential point clouds to address feature degradation, our SeqTrack3D combines both historical point clouds and bounding box sequences. This novel method ensures robust tracking by leveraging location priors from historical boxes, even in scenes with sparse points. Extensive experiments conducted on large-scale datasets show that SeqTrack3D achieves new state-of-the-art performances, improving by 6.00% on NuScenes and 14.13% on Waymo dataset. The code will be made public at https://github.com/aron-lin/seqtrack3d. keywords: {Point cloud compression;Degradation;Target tracking;Three-dimensional displays;Codes;Transformers;Object tracking},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611238&isnumber=10609862

Y. Peng, C. Chen and G. Huang, "Ultrafast Square-Root Filter-based VINS," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6966-6972, doi: 10.1109/ICRA57147.2024.10610916.Abstract: In this paper, we strongly advocate square-root covariance (instead of information) filtering for Visual-Inertial Navigation Systems (VINS), in particular on resource-constrained edge devices, because of its superior efficiency and numerical stability. Although VINS have made tremendous progress in recent years, they still face resource stringency and numerical instability on embedded systems when imposing limited word length. To overcome these challenges, we develop an ultrafast and numerically-stable square-root filter (SRF)-based VINS algorithm (i.e., SR-VINS). The numerical stability of the proposed SR-VINS is inherited from the adoption of square-root covariance while the remarkable efficiency is largely enabled by the novel SRF update method that is based on our new permuted-QR (P-QR), which fully utilizes and properly maintains the upper triangular structure of the square-root covariance matrix. Furthermore, we choose a special ordering of the state variables which is amenable for (P-)QR operations in the SRF propagation and update and prevents unnecessary computation. The proposed SR-VINS is validated extensively through numerical studies, demonstrating that when the state-of-the-art (SOTA) filters have numerical difficulties, our SR-VINS has superior numerical stability, and remarkably, achieves efficient and robust performance on 32-bit single-precision float at a speed nearly twice as fast as the SOTA methods. We also conduct comprehensive real-world experiments to validate the efficiency, accuracy, and robustness of the proposed SR-VINS. keywords: {Embedded systems;Accuracy;Navigation;Filtering algorithms;Information filters;Robustness;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610916&isnumber=10609862

Z. Zhang et al., "Universal Visual Decomposer: Long-Horizon Manipulation Made Easy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6973-6980, doi: 10.1109/ICRA57147.2024.10611125.Abstract: Real-world robotic tasks stretch over extended horizons and encompass multiple stages. Learning long-horizon manipulation tasks, however, is a long-standing challenge, and demands decomposing the overarching task into several manageable subtasks to facilitate policy learning and generalization to unseen tasks. Prior task decomposition methods require task-specific knowledge, are computationally intensive, and cannot readily be applied to new tasks. To address these shortcomings, we propose Universal Visual Decomposer (UVD), an off-the-shelf task decomposition method for visual long-horizon manipulation using pre-trained visual representations designed for robotic control. At a high level, UVD discovers subgoals by detecting phase shifts in the embedding space of the pre-trained representation. Operating purely on visual demonstrations without auxiliary information, UVD can effectively extract visual subgoals embedded in the videos, while incurring zero additional training cost on top of standard visuomotor policy training. Goal-conditioned policies learned with UVD-discovered subgoals exhibit significantly improved compositional generalization at test time to unseen tasks. Furthermore, UVD-discovered subgoals can be used to construct goal-based reward shaping that jump-starts temporally extended exploration for reinforcement learning. We extensively evaluate UVD on both simulation and real-world tasks, and in all cases, UVD substantially outperforms baselines across imitation and reinforcement learning settings on in-domain and out-of-domain task sequences alike, validating the clear advantage of automated visual task decomposition within the simple, compact UVD framework. keywords: {Training;Visualization;Costs;Reinforcement learning;Aerospace electronics;Data mining;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611125&isnumber=10609862

A. Misik, D. Salihu, X. Su, H. Brock and E. Steinbach, "HEGN: Hierarchical Equivariant Graph Neural Network for 9DoF Point Cloud Registration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6981-6988, doi: 10.1109/ICRA57147.2024.10610562.Abstract: Given its wide application in robotics, point cloud registration is a widely researched topic. Conventional methods aim to find a rotation and translation that align two point clouds in 6 degrees of freedom (DoF). However, certain tasks in robotics, such as category-level pose estimation, involve non-uniformly scaled point clouds, requiring a 9DoF transform for accurate alignment. We propose HEGN, a novel equivariant graph neural network for 9DoF point cloud registration. HEGN utilizes equivariance to rotation, translation, and scaling to estimate the transformation without relying on point correspondences. Based on graph representations for both point clouds, we extract equivariant node features aggregated in their local, cross-, and global context. In addition, we introduce a novel node pooling mechanism that leverages the cross-context importance of nodes to pool the graph representation. By repeating the feature extraction and node pooling, we obtain a graph hierarchy. Finally, we determine rotation and translation by aligning equivariant features aggregated over the graph hierarchy. To estimate scaling, we leverage scale information in the vector norm of the equivariant features. We evaluate the effectiveness of HEGN through experiments with the synthetic ModelNet40 dataset and the real-world ScanObjectNN dataset. The results show the superior performance of HEGN in 9DoF point cloud registration and its competitive performance in conventional 6DoF point cloud registration. keywords: {Point cloud compression;Three-dimensional displays;Pose estimation;Transforms;Feature extraction;Graph neural networks;Vectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610562&isnumber=10609862

J. J. Heon Lee, C. Yoo, S. Anstee and R. Fitch, "Multi-query TDSP for Path Planning in Time-varying Flow Fields," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7005-7011, doi: 10.1109/ICRA57147.2024.10611501.Abstract: Many applications of path planning in time-varying flow fields, particularly in areas such as marine robotics and ship routing, can be modelled as instances of the time-varying shortest path (TDSP) problem. Although there are no known polynomial-time solutions to TDSP in general, our recent work has identified a tractable case where the flow is modelled as piecewise constant. Extending this method to allow for computational reuse in larger multi-query problems, however, requires additional thought. This paper shows that the piecewise-linear form of the cost function employed in previously work can be used to build an analogy of a shortest path tree, thereby enabling optimal concatenation of sub-problem solutions in the absence of an optimal substructure, and without uniform time discretisation. We present a framework for multi-query TDSP that finds an optimal path that passes through a defined sequence of waypoints and is computationally efficient. Performance comparison is provided in simulation that shows large (up to 100x) speedup compared to a naive approach. This result is significant for applications such as ship routing, where route evaluation is a desirable capability. keywords: {Computational modeling;Oceans;Buildings;Routing;Cost function;Path planning;Computational efficiency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611501&isnumber=10609862

K. Honda et al., "Stein Variational Guided Model Predictive Path Integral Control: Proposal and Experiments with Fast Maneuvering Vehicles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7020-7026, doi: 10.1109/ICRA57147.2024.10611021.Abstract: This paper presents a novel Stochastic Optimal Control (SOC) method based on Model Predictive Path Integral control (MPPI), named Stein Variational Guided MPPI (SVG-MPPI), designed to handle rapidly shifting multimodal optimal action distributions. While MPPI can find a Gaussian-approximated optimal action distribution in closed form, i.e., without iterative solution updates, it struggles with the mul-timodality of the optimal distributions. This is due to the less representative nature of the Gaussian. To overcome this limitation, our method aims to identify a target mode of the optimal distribution and guide the solution to converge to fit it. In the proposed method, the target mode is roughly estimated using a modified Stein Variational Gradient Descent (SVGD) method and embedded into the MPPI algorithm to find a closed-form "mode-seeking" solution that covers only the target mode, thus preserving the fast convergence property of MPPI. Our simulation and real-world experimental results demonstrate that SVG-MPPI outperforms both the original MPPI and other state-of-the-art sampling-based SOC algorithms in terms of path-tracking and obstacle-avoidance capabilities. https://github.com/kohonda/proj-svg_mppi keywords: {Stochastic processes;Optimal control;Predictive models;Prediction algorithms;Proposals;Iterative methods;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611021&isnumber=10609862

I. Ibrahim, J. Gillis, W. Decré and J. Swevers, "An Efficient Solution to the 2D Visibility Problem in Cartesian Grid Maps and its Application in Heuristic Path Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7027-7033, doi: 10.1109/ICRA57147.2024.10611529.Abstract: This paper introduces a novel, lightweight method to solve the visibility problem for 2D grids. The proposed method evaluates the existence of lines-of-sight from a source point to all other grid cells in a single pass with no preprocessing and independently of the number and shape of obstacles. It has a compute and memory complexity of $\mathcal{O}(n)$, where n = nx ×ny is the size of the grid, and requires at most ten arithmetic operations per grid cell. In the proposed approach, we use a linear first-order hyperbolic partial differential equation to transport the visibility quantity in all directions. In order to accomplish that, we use an entropy-satisfying upwind scheme that converges to the true visibility polygon as the step size goes to zero. This dynamic-programming approach allows the evaluation of visibility for an entire grid orders of magnitude faster than typical ray-casting algorithms. We provide a practical application of our proposed algorithm by posing the visibility quantity as a heuristic and implementing a deterministic, local-minima-free path planner, setting apart the proposed planner from traditional methods. Lastly, we provide necessary algorithms and an open-source implementation of the proposed methods. keywords: {Shape;Heuristic algorithms;Partial differential equations;Memory management;Path planning;Complexity theory;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611529&isnumber=10609862

M. Lee and D. Lee, "Efficient Clothoid Tree-Based Local Path Planning for Self-Driving Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7034-7040, doi: 10.1109/ICRA57147.2024.10610306.Abstract: In this paper, we propose a real-time clothoid tree-based path planning for self-driving robots. Clothoids, curves that exhibit linear curvature profiles, play an important role in road design and path planning due to their appealing properties. Nevertheless, their real-time applications face considerable challenges, primarily stemming from the lack of a closed-form clothoid expression. To address these challenges, we introduce two innovative techniques: 1) an efficient and precise clothoid approximation using the Gauss-Legendre quadrature; and 2) a data-efficient decoder for interpolating clothoid splines that leverages the symmetry and similarity of clothoids. These techniques are demonstrated with numerical examples. The clothoid approximation ensures an accurate and smooth representation of the curve, and the clothoid spline decoder effectively accelerates the clothoid tree exploration by relaxing the problem constraints and reducing the problem size. Both techniques are integrated into our path planning algorithm and evaluated in various driving scenarios. keywords: {Interpolation;Accuracy;Shape;Roads;Path planning;Real-time systems;Decoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610306&isnumber=10609862

T. Guo and J. Yu, "Decentralized Lifelong Path Planning for Multiple Ackerman Car-Like Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7041-7047, doi: 10.1109/ICRA57147.2024.10610330.Abstract: Path planning for multiple non-holonomic robots in continuous domains constitutes a difficult robotics challenge with many applications. Despite significant recent progress on the topic, computationally efficient and high-quality solutions are lacking, especially in lifelong settings where robots must continuously take on new tasks. In this work, we make it possible to extend key ideas enabling state-of-the-art (SOTA) methods for multi-robot planning in discrete domains to the motion planning of multiple Ackerman (car-like) robots in lifelong settings, yielding high-performance centralized and decentralized planners. Our planners compute trajectories that allow the robots to reach precise SE(2) goal poses. The effectiveness of our methods is thoroughly evaluated and confirmed using both simulation and real-world experiments. keywords: {Planning;Trajectory;Computational efficiency;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610330&isnumber=10609862

A. Seewald, C. J. Lerch, M. Chancán, A. M. Dollar and I. Abraham, "Energy-Aware Ergodic Search: Continuous Exploration for Multi-Agent Systems with Battery Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7048-7054, doi: 10.1109/ICRA57147.2024.10609871.Abstract: Continuous exploration without interruption is important in scenarios such as search and rescue and precision agriculture, where consistent presence is needed to detect events over large areas. Ergodic search already derives continuous trajectories in these scenarios so that a robot spends more time in areas with high information density. However, existing literature on ergodic search does not consider the robot's energy constraints, limiting how long a robot can explore. In fact, if the robots are battery-powered, it is physically not possible to continuously explore on a single battery charge. Our paper tackles this challenge, integrating ergodic search methods with energy-aware coverage. We trade off battery usage and coverage quality, maintaining uninterrupted exploration by at least one agent. Our approach derives an abstract battery model for future state-of-charge estimation and extends canonical ergodic search to ergodic search under battery constraints. Empirical data from simulations and real-world experiments demonstrate the effectiveness of our energy-aware ergodic search, which ensures continuous exploration and guarantees spatial coverage. keywords: {Precision agriculture;Limiting;Search methods;Estimation;Data models;Batteries;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609871&isnumber=10609862

T. Wang, H. Wen, Z. Song, Z. Dong and C. Liu, "Development of Variable Transmission Series Elastic Actuator for Hip Exoskeletons," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7055-7061, doi: 10.1109/ICRA57147.2024.10611435.Abstract: Series Elastic Actuator-based exoskeleton can offer precise torque control and transparency when interacting with human wearers. Accurate control of SEA-produced torques ensures the wearer’s voluntary motion and supports the implementation of multiple assistive paradigms. In this paper, a novel variable transmission series elastic actuator (VTSEA) is developed to meet torque-speed requirements in different exoskeleton-assisted locomotion modes, such as running, walking, sit-to-stand, and stand-to-sit. The VTSEA features a SEA-coupled variable transmission ratio adjusting mechanism and works between three discrete levels of transmission ratio depending on the user’s initiative. The proposed prototype can also improve transparency in human-robot interaction. Also, an accurate torque controller with inertial compensation is developed for the VTSEA via the singular perturbation theory, and its stability is proved. The feasibility of the proposed VTSEA prototype and the precise output torque performance of VTSEA are verified by experiments. keywords: {Legged locomotion;Actuators;Torque;Accuracy;Perturbation methods;Exoskeletons;Torque control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611435&isnumber=10609862

H. Zhu and U. Thomas, "A Novel Compact Design of a Lever-Cam based Variable Stiffness Actuator: LC-VSA," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7070-7076, doi: 10.1109/ICRA57147.2024.10610124.Abstract: Ensuring safe interaction between humans and robots is an important challenge in robotics. In recent years, researchers have developed many different soft robots. One possibility to reach this goal is to integrate mechanical springs into their joints. The forthcoming generation of soft robots will be adaptable for joint stiffness to accommodate various tasks. Consequently, the development of variable stiffness joints (VSA) has become crucial. Among the prevalent approaches for stiffness adjustment, lever mechanisms have been implemented in numerous variable stiffness joints. Nonetheless, the integration of the lever technology into VSA often faces challenges in achieving a compact design. This paper introduces a new mechanically compact design for a novel lever-cam based variable stiffness joint, which has been patent under the grand by the german Patentamt. keywords: {Patents;Actuators;Soft robotics;Task analysis;Springs;Faces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610124&isnumber=10609862

S. Yi, S. Liu, J. Liao and Z. Guo, "Design and Modeling of A Compact Serial Variable Stiffness Actuator (SVSA-III) with Linear Stiffness Profile," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7077-7082, doi: 10.1109/ICRA57147.2024.10610331.Abstract: Variable stiffness actuator (VSA) can imitate natural muscles in their compliance capbility, which can provide flexible adaptability for robots, improving the safety of robots interacting with the environment or human. This paper presents a new compact serial variable stiffness actuator ((SVSA-III)) with linear stiffness profile based on symmetrical variable lever arm mechanism. The stiffness motor is used to regulate the position of the pivot located on the Archimedean Spiral Relocation Mechanism (ASRM), so that the stiffness of the actuator can be adjusted (softening or hardening). By designing the lever length, the range of stiffness adjustment can change from 0.3Nm/degree to therotical infinity. Moreover, the continuous linear stiffness profile of the actuator can be customized by solving the transcendental equation of the relationship between the actuator stiffness and the rotation angle of the stiffness motor. SVSA-III has the advantages of compact structure, wide-range stiffness regulation, reduced control difficulty, and linear stiffness profile. Two experiments of step response and stiffness tracking have proved the high accuracy and fast response for both theoretical stiffness and position adjustment. keywords: {Actuators;Accuracy;Spirals;Motors;Regulation;Safety;Softening},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610331&isnumber=10609862

W. Roozing and G. Roozing, "Experimental comparison of pinwheel and non-pinwheel designs of 3D-printed cycloidal gearing for robotics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7091-7098, doi: 10.1109/ICRA57147.2024.10610250.Abstract: Recent trends in robotic actuation have highlighted the need for low cost, high performance, and efficient gearing. We present an experimental study comparing pinwheel and non-pinwheel designs of cycloidal gearing. The open source designs are 3D-printable, combined with off-the-shelf components, achieving a high performance-to-cost ratio. Extensive experimental data is presented, that compares two prototypes on run-in behaviour and a number of quantitative metrics including transmission error, play, friction, and stiffness. Furthermore, we assess overall actuator performance through position control experiments, and a 10-hour endurance test. The results show strong performance characteristics, and crucially, suggest that non-pinwheel designs of cycloidal gearing can be a lower complexity and cost alternative to classical pinwheel designs, while offering similar performance. keywords: {Measurement;Actuators;Costs;Friction;Prototypes;Position control;Market research},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610250&isnumber=10609862

T. Portha, L. Barbé, F. Geiskopf, J. Vappou and P. Renaud, "A non-magnetic dual-mode linear pneumatic actuator: initial design and assessment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7107-7113, doi: 10.1109/ICRA57147.2024.10611707.Abstract: A pneumatic linear actuator is presented and evaluated. Designed to operate in demanding environments such as MRI, it is developed to be used with two motion control modes: 1) a step-by-step mode with tooth-based gripping to ensure precision, 2) a continuous mode available locally for fine positioning. The actuator can also be disengaged to enable direct handling by an operator, for example for comanipulation. The design is presented. A prototype, developed in the medical context, is implemented and characterized. A specific step-by-step control sequence is then elaborated based on its characterization. Testing of the dual-mode actuation is finally described. The complementarity between the two motion modes and possible adaptations of the original design are discussed. keywords: {Actuators;Transducers;Force;Prototypes;Robot sensing systems;Synchronization;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611707&isnumber=10609862

N. Wilhelm, S. Haddadin, R. Burgkart, P. Van Der Smagt and M. Karl, "Accurate Kinematic Modeling using Autoencoders on Differentiable Joints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7122-7128, doi: 10.1109/ICRA57147.2024.10611062.Abstract: In robotics and biomechanics, accurately determining joint parameters and computing the corresponding forward and inverse kinematics are critical yet often challenging tasks, especially when dealing with highly individualized and partly unknown systems. This paper unveils a cutting-edge kinematic optimizer, underpinned by an autoencoder-based architecture, to address these challenges. Utilizing a neural network, our approach simulates inverse kinematics, converting measurement data into joint-specific parameters during encoding, enabling a stable optimization process. These parameters are subsequently processed through a predefined, differentiable forward kinematics model, resulting in a decoded representation of the original data. Beyond offering a comprehensive solution to kinematics challenges, our method also unveils previously unidentified joint parameters. Real experimental data from knee and hand joints validate the optimizer’s efficacy. Additionally, our optimizer is multifunctional: it streamlines the modeling and automation of kinematics and enables a nuanced evaluation of diverse modeling techniques. By assessing the differences in reconstruction losses, we illuminate the merits of each approach. Collectively, this preliminary study signifies advancements in kinematic optimization, with potential applications spanning both biomechanics and robotics. keywords: {Biomechanics;Automation;Biological system modeling;Neural networks;Kinematics;Computer architecture;Encoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611062&isnumber=10609862

J. C. Kiemel and T. Kröger, "Jerk-limited Traversal of One-dimensional Paths and its Application to Multi-dimensional Path Tracking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7142-7148, doi: 10.1109/ICRA57147.2024.10611388.Abstract: In this paper, we present an iterative method to quickly traverse multi-dimensional paths considering jerk constraints. As a first step, we analyze the traversal of each individual path dimension. We derive a range of feasible target accelerations for each intermediate waypoint of a one-dimensional path using a binary search algorithm. Computing a trajectory from waypoint to waypoint leads to the fastest progress on the path when selecting the highest feasible target acceleration. Similarly, it is possible to calculate a trajectory that leads to minimum progress along the path. This insight allows us to control the traversal of a one-dimensional path in such a way that a reference path length of a multi-dimensional path is approximately tracked over time. In order to improve the tracking accuracy, we propose an iterative scheme to adjust the temporal course of the selected reference path length. More precisely, the temporal region causing the largest position deviation is identified and updated at each iteration. In our evaluation, we thoroughly analyze the performance of our method using seven-dimensional reference paths with different path characteristics. We show that our method manages to quickly traverse the reference paths and compare the required traversing time and the resulting path accuracy with other state-of-the-art approaches. keywords: {Accuracy;Target tracking;Trajectory;Iterative methods;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611388&isnumber=10609862

V. M. Baez, N. Navkar and A. T. Becker, "An Analytic Solution to the 3D CSC Dubins Path Problem," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7157-7163, doi: 10.1109/ICRA57147.2024.10611360.Abstract: We present an analytic solution to the 3D Dubins path problem for paths composed of an initial circular arc, a straight component, and a final circular arc. These are commonly called CSC paths. By modeling the start and goal configurations of the path as the base frame and final frame of an RRPRR manipulator, we treat this as an inverse kinematics problem. The kinematic features of the 3D Dubins path are built into the constraints of our manipulator model. Furthermore, we show that the number of solutions is not constant, with up to seven valid CSC path solutions even in non-singular regions. An implementation of solution is available at https: //github.com/aabecker/dubins3D. keywords: {Solid modeling;Concentric tube robots;Analytical models;Three-dimensional displays;Kinematics;Manipulators;Turning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611360&isnumber=10609862

I. Meir, A. Bechar and A. Sintov, "Kinematic Optimization of a Robotic Arm for Automation Tasks with Human Demonstration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7172-7178, doi: 10.1109/ICRA57147.2024.10610924.Abstract: Robotic arms are highly common in various automation processes such as manufacturing lines. However, these highly capable robots are usually degraded to simple repetitive tasks such as pick-and-place. On the other hand, designing an optimal robot for one specific task consumes large resources of engineering time and costs. In this paper, we propose a novel concept for optimizing the fitness of a robotic arm to perform a specific task based on human demonstration. Fitness of a robot arm is a measure of its ability to follow recorded human arm and hand paths. The optimization is conducted using a modified variant of the Particle Swarm Optimization for the robot design problem. In the proposed approach, we generate an optimal robot design along with the required path to complete the task. The approach could reduce the time-to-market of robotic arms and enable the standardization of modular robotic parts. Novice users could easily apply a minimal robot arm to various tasks. Two test cases of common manufacturing tasks are presented yielding optimal designs and reduced computational effort by up to 92%. keywords: {Automation;Welding;Kinematics;Manipulators;Particle measurements;Manufacturing;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610924&isnumber=10609862

A. Verduyn, M. Vochten and J. De Schutter, "Enhancing motion trajectory segmentation of rigid bodies using a novel screw-based trajectory-shape representation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7179-7185, doi: 10.1109/ICRA57147.2024.10610030.Abstract: Trajectory segmentation refers to dividing a trajectory into meaningful consecutive sub-trajectories. This paper focuses on trajectory segmentation for 3D rigid-body motions. Most segmentation approaches in the literature represent the body’s trajectory as a point trajectory, considering only its translation and neglecting its rotation. We propose a novel trajectory representation for rigid-body motions that incorporates both translation and rotation, and additionally exhibits several invariant properties. This representation consists of a geometric progress rate and a third-order trajectory-shape descriptor. Concepts from screw theory were used to make this representation time-invariant and also invariant to the choice of body reference point. This new representation is validated for a self-supervised segmentation approach, both in simulation and using real recordings of human-demonstrated pouring motions. The results show a more robust detection of consecutive sub-motions with distinct features and a more consistent segmentation compared to conventional representations. We believe that other existing segmentation methods may benefit from using this trajectory representation to improve their invariance. keywords: {Three-dimensional displays;Motion segmentation;Fasteners;Feature extraction;Trajectory;Recording;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610030&isnumber=10609862

K. de Vos, E. Torta, H. Bruyninckx, C. A. López Martínez and M. J. G. van de Molengraft, "Automatic Configuration of Multi-Agent Model Predictive Controllers based on Semantic Graph World Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7194-7200, doi: 10.1109/ICRA57147.2024.10610708.Abstract: We propose a shared semantic map architecture to construct and configure Model Predictive Controllers (MPC) dynamically, that solve navigation problems for multiple robotic agents sharing parts of the same environment. The navigation task is represented as a sequence of semantically labeled areas in the map, that must be traversed sequentially, i.e. a route. Each semantic label represents one or more constraints on the robots’ motion behaviour in that area. The advantages of this approach are: (i) an MPC-based motion controller in each individual robot can be (re-)configured, at runtime, with the locally and temporally relevant parameters; (ii) the application can influence, also at runtime, the navigation behaviour of the robots, just by adapting the semantic labels; and (iii) the robots can reason about their need for coordination, through analyzing over which horizon in time and space their routes overlap. The paper provides simulations of various representative situations, showing that the approach of runtime configuration of the MPC drastically decreases computation time, while retaining task execution performance similar to an approach in which each robot always includes all other robots in its MPC computations. keywords: {Runtime;Navigation;Processor scheduling;Robot kinematics;Computational modeling;Heuristic algorithms;Semantics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610708&isnumber=10609862

J. Chen, Y. Gao, J. Hu, F. Deng and T. L. Lam, "Meta-Reinforcement Learning Based Cooperative Surface Inspection of 3D Uncertain Structures using Multi-robot Systems," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7201-7207, doi: 10.1109/ICRA57147.2024.10610420.Abstract: This paper presents a decentralized cooperative motion planning approach for surface inspection of 3D structures which includes uncertainties like size, number, shape, position, using multi-robot systems (MRS). Given that most of existing works mainly focus on surface inspection of single and fully known 3D structures, our motivation is two-fold: first, 3D structures separately distributed in 3D environments are complex, therefore the use of MRS intuitively can facilitate an inspection by fully taking advantage of sensors with different capabilities. Second, performing the aforementioned tasks when considering uncertainties is a complicated and time-consuming process because we need to explore, figure out the size and shape of 3D structures and then plan surface-inspection path. To overcome these challenges, we present a meta-learning approach that provides a decentralized planner for each robot to improve the exploration and surface inspection capabilities. The experimental results demonstrate our method can outperform other methods by approximately 10.5%-27% on success rate and 70%-75% on inspection speed. keywords: {Three-dimensional displays;Uncertainty;Shape;Inspection;Sensors;Multi-robot systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610420&isnumber=10609862

S. Wu, G. Chen, M. Shi and J. Alonso-Mora, "Decentralized Multi-Agent Trajectory Planning in Dynamic Environments with Spatiotemporal Occupancy Grid Maps," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7208-7214, doi: 10.1109/ICRA57147.2024.10610670.Abstract: This paper proposes a decentralized trajectory planning framework for the collision avoidance problem of multiple micro aerial vehicles (MAVs) in environments with static and dynamic obstacles. The framework utilizes spatiotemporal occupancy grid maps (SOGM), which forecast the occupancy status of neighboring space in the near future, as the environment representation. Based on this representation, we extend the kinodynamic A* and the corridor-constrained trajectory optimization algorithms to efficiently tackle static and dynamic obstacles with arbitrary shapes. Collision avoidance between communicating robots is integrated by sharing planned trajectories and projecting them onto the SOGM. The simulation results show that our method achieves competitive performance against state-of-the-art methods in dynamic environments with different numbers and shapes of obstacles. Finally, the proposed method is validated in real experiments. keywords: {Trajectory planning;Shape;Heuristic algorithms;Simulation;System recovery;Spatiotemporal phenomena;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610670&isnumber=10609862

R. Hull, D. Moratuwage, E. Scheide, R. Fitch and G. Best, "Communicating Intent as Behaviour Trees for Decentralised Multi-Robot Coordination," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7215-7221, doi: 10.1109/ICRA57147.2024.10610441.Abstract: We propose a decentralised multi-robot coordination algorithm that features a rich representation for encoding and communicating each robot’s intent. This representation for “intent messages” enables improved coordination behaviour and communication efficiency in difficult scenarios, such as those where there are unknown points of contention that require negotiation between robots. Each intent message is an adaptive policy that conditions on identified points of contention that conflict with the intentions of other robots. These policies are concisely expressed as behaviour trees via algebraic logic simplification, and are interpretable by robot teammates and human operators. We propose this intent representation in the context of the Dec-MCTS online planning algorithm for decentralised coordination. We present results for a generalised multi-robot orienteering domain that show improved plan convergence and coordination performance over standard Dec-MCTS enabled by the intent representation’s ability to encode and facilitate negotiation over points of contention. keywords: {Robot kinematics;Encoding;Planning;Logic;Standards;Convergence},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610441&isnumber=10609862

K. Vakil, M. Coffey and A. Pierson, "Partial Belief Space Planning for Scaling Stochastic Dynamic Games," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7222-7228, doi: 10.1109/ICRA57147.2024.10610219.Abstract: This paper presents a method to reduce computations for stochastic dynamic games with game-theoretic belief space planning through partially propagating beliefs. Complex interactions in scenarios such as surveillance, herding, and racing can be modeled using game-theoretic frameworks in the belief space. Stochastic dynamic games can be solved to a local Nash Equilibrium using a game-theoretic belief space variant of an iterative Linear Quadratic Gaussian (iLQG). However, the scalability of this method suffers due to the large dimensionality of beliefs which the iLQG must propagate. We examine the utility of partial belief space propagation, which allows polynomial runtime to decrease. We validate our findings through simulations and hardware implementation. keywords: {Runtime;Surveillance;Scalability;Stochastic processes;Games;Nash equilibrium;Polynomials},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610219&isnumber=10609862

A. Banerjee and J. Schneider, "Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7229-7235, doi: 10.1109/ICRA57147.2024.10609977.Abstract: Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous interagent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the Probability Hypothesis Density filter for posterior inference combined with Thompson sampling for decentralized multi-agent decision making. We compare different action selection policies, focusing on scenarios where targets outnumber agents. In simulation, we demonstrate that DecSTER is robust to unreliable inter-agent communication and outperforms information-greedy baselines in terms of the Optimal Sub-Pattern Assignment (OSPA) metric for different numbers of targets and varying teamsizes. keywords: {Target tracking;Uncertainty;Surveillance;Robot kinematics;Wildlife;Object detection;Filtering algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609977&isnumber=10609862

Y. Huang, X. Lin and B. Englot, "Multi-Robot Autonomous Exploration and Mapping Under Localization Uncertainty with Expectation-Maximization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7236-7242, doi: 10.1109/ICRA57147.2024.10611495.Abstract: We propose an autonomous exploration algorithm designed for decentralized multi-robot teams, which takes into account map and localization uncertainties of range-sensing mobile robots. Virtual landmarks are used to quantify the combined impact of process noise and sensor noise on map uncertainty. Additionally, we employ an iterative expectation-maximization inspired algorithm to assess the potential out-comes of both a local robot’s and its neighbors’ next-step actions. To evaluate the effectiveness of our framework, we conduct a comparative analysis with state-of-the-art algorithms. The results of our experiments show the proposed algorithm’s capacity to strike a balance between curbing map uncertainty and achieving efficient task allocation among robots. keywords: {Location awareness;Uncertainty;Simultaneous localization and mapping;Sensitivity;Noise;Stars;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611495&isnumber=10609862

Á. Calvo and J. Capitán, "Optimal Task Allocation for Heterogeneous Multi-robot Teams with Battery Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7243-7249, doi: 10.1109/ICRA57147.2024.10611147.Abstract: This paper presents a novel approach to optimal multi-robot task allocation in heterogeneous teams of robots. When robots have heterogeneous capabilities and there are diverse objectives and constraints to comply with, computing optimal plans can become especially hard. Moreover, we increase the problem complexity by: 1) considering battery-limited robots that need to schedule recharges; 2) tasks that can be decomposed into multiple fragments; and 3) multi-robot tasks that need to be executed by a coalition synchronously. We define a new problem for heterogeneous multi-robot task allocation and formulate it as a Mixed-Integer Linear Program that includes all the aforementioned features. Then we use an off-the-shelf solver to show the type of optimal solutions that our planner can produce and assess its performance in random scenarios. Our method, which is released as open-source code, represents a first step to formalize and analyze a complex problem that has not been solved in the state of the art. keywords: {Schedules;Codes;Real-time systems;Complexity theory;Batteries;Resource management;Synchronization;Multi-robot task allocation;Optimal planning and scheduling;Heterogeneous teams},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611147&isnumber=10609862

S. Paul, N. Maurer and S. Chowdhury, "Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7250-7256, doi: 10.1109/ICRA57147.2024.10611094.Abstract: Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite graph matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a Graph Reinforcement Learning (GRL) framework to learn the heuristics or incentives for a bipartite graph matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite graph that connects the set of tasks to the set of robots. The original capsule attention network architecture is fundamentally modified by adding encoding of robots’ state graph, and two Multihead Attention based decoders whose output are used to construct a LogNormal distribution matrix from which positive bigraph weights can be drawn. The performance of this new bigraph matching approach augmented with a GRL-derived incentive is found to be at par with the original bigraph matching approach that used expert-specified heuristics, with the former offering notable robustness benefits. During training, the learned incentive policy is found to get initially closer to the expert-specified incentive and then slightly deviate from its trend. keywords: {Training;Systematics;Scalability;Robustness;Bipartite graph;Decoding;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611094&isnumber=10609862

S. Chen, M. Mahdizadeh, C. Yu, J. Fan and T. Chen, "Through the Real World Haze Scenes: Navigating the Synthetic-to-Real Gap in Challenging Image Dehazing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7265-7272, doi: 10.1109/ICRA57147.2024.10611709.Abstract: Dehazing real-world hazy images is challenging due to the complexity of natural haze, varying haze conditions, details preservation, and the risk of overexposure. Existing methods excel in synthetic hazy scenarios but struggle in the real world because they don’t use all available features. Classical dehazing techniques primarily focus on low-level dehazing enhancements, whereas deep learning-based methods extract more intricate weather-related features. However, both of these approaches exhibit limitations in effectively addressing the real-world dehazing. To address these challenges, we introduce an innovative approach that combines the strengths of both modalities to dehaze and enhance the visibility of real-world hazy scenes. Firstly, we extract both low-level and deep features and then employ a pre-trained vector quantization GAN to create well-detailed data patches. A decoder, with a normalized module, effectively utilizes these high-quality features. Additionally, we introduce a controllable operation to improve feature matching. To further enhance dehazing and generalizability, the decoder’s output undergoes a sequence of gamma-correction operations and generates a series of multi-exposure images that are combined to create a haze-free and higher-quality image. Our method effectively reduces haziness, enhances sharpness, preserves natural colors, and minimizes artifacts in challenging real-world scenarios. The approach surpasses five SOTA methods in both qualitative and quantitative evaluations across three key metrics, utilizing three synthetic and two real-world hazy datasets. Notably, it achieves a substantial improvement in real-world datasets over the second-best method, with 0.5702 and 0.129 in FADE metrics for the RTTS and Fattal datasets, respectively. keywords: {Measurement;Image quality;Image color analysis;Vector quantization;Surveillance;Refining;Pipelines},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611709&isnumber=10609862

X. Bian, W. Chen, X. Tian and D. Ran, "CopperTag: A Real-Time Occlusion-Resilient Fiducial Marker," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7273-7279, doi: 10.1109/ICRA57147.2024.10611260.Abstract: Fiducial markers, like AprilTag and ArUco, are extensively utilized in robotics applications within industrial environments, encompassing navigation, docking, and object grasping tasks. However, in contrast to controlled laboratory conditions, markers installed in factory grounds or equipment surfaces, often face challenges like damage or contamination. These issues can lead to compromised marker integrity, resulting in reduced detection reliability. To address this challenge, we propose a novel fiducial marker called CopperTag, which incorporates circular and square elements to create a robust occlusion-resistant pattern. The CopperTag detection process relies on three fundamental steps: firstly, extracting all lines from the image; secondly, identifying corners; and lastly, searching for quadrilateral candidate regions using ellipses and nearby corners. The Reed-Solomon (RS) algorithm is utilized for both encoding and decoding the information content. This algorithm possesses the ability to recover corrupted messages in situations where CopperTag data is incomplete. The experimental results illustrate that CopperTag exhibits superior robustness and accuracy in detection when compared to other state-of-the-art fiducial markers, even in scenarios with heavy occlusion. Moreover, CopperTag maintains an average processing time of 10ms per frame on a standard laptop, effectively meeting the real-time demands of robotics applications. keywords: {Service robots;Surface contamination;Fiducial markers;Real-time systems;Robustness;Encoding;Decoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611260&isnumber=10609862

Z. Lei et al., "Robust Collaborative Perception without External Localization and Clock Devices," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7280-7286, doi: 10.1109/ICRA57147.2024.10610635.Abstract: A consistent spatial-temporal coordination across multiple agents is fundamental for collaborative perception, which seeks to improve perception abilities through information exchange among agents. To achieve this spatial-temporal alignment, traditional methods depend on external devices to provide localization and clock signals. However, hardware-generated signals could be vulnerable to noise and potentially malicious attack, jeopardizing the precision of spatial-temporal alignment. Rather than relying on external hardwares, this work proposes a novel approach: aligning by recognizing the inherent geometric patterns within the perceptual data of various agents. Following this spirit, we propose a robust collaborative perception system that operates independently of external localization and clock devices. The key module of our system, FreeAlign, constructs a salient object graph for each agent based on its detected boxes and uses a graph neural network to identify common subgraphs between agents, leading to accurate relative pose and time. We validate FreeAlign on both real-world and simulated datasets. The results show that, the FreeAlign empowered robust collaborative perception system perform comparably to systems relying on precise localization and clock devices. ${\mathbf{Code}}$ will be released. keywords: {Location awareness;Performance evaluation;Accuracy;Noise;Collaboration;Pattern recognition;Synchronization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610635&isnumber=10609862

Y. Li, J. Wu, L. Zhao and P. Liu, "DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 2787-2793, doi: 10.1109/ICRA57147.2024.10609981.Abstract: When capturing images through the glass during rainy or snowy weather conditions, the resulting images often contain waterdrops adhered on the glass surface, and these waterdrops significantly degrade the image quality and performance of many computer vision algorithms. To tackle these limitations, we propose a method to reconstruct the clear 3D scene implicitly from multi-view images degraded by water-drops. Our method exploits an attention network to predict the location of waterdrops and then train a Neural Radiance Fields to recover the 3D scene implicitly. By leveraging the strong scene representation capabilities of NeRF, our method can render high-quality novel-view images with waterdrops removed. Extensive experimental results on both synthetic and real datasets show that our method is able to generate clear 3D scenes and outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal methods. keywords: {Surface reconstruction;Three-dimensional displays;Pipelines;Estimation;Glass;Neural radiance field;Prediction algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609981&isnumber=10609862

R. Chen, Y. Cong and Y. Ren, "Marrying NeRF with Feature Matching for One-step Pose Estimation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7302-7309, doi: 10.1109/ICRA57147.2024.10610766.Abstract: Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90×, achieving real-time prediction at 6 FPS. keywords: {Training;Solid modeling;Three-dimensional displays;Pose estimation;Buildings;Neural radiance field;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610766&isnumber=10609862

M. H. Kim, M. Ju Kim and S. B. Yoo, "Occluded Part-aware Graph Convolutional Networks for Skeleton-based Action Recognition," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7310-7317, doi: 10.1109/ICRA57147.2024.10610972.Abstract: Recognizing human action is one of the most critical factors in the visual perception of robots. Specifically, skeletonbased action recognition has been actively researched to enhance recognition performance at a lower cost. However, action recognition in occlusion situations, where body parts are not visible, is still challenging.We propose an occluded part-aware graph convolutional network (OP-GCN) to address this challenge using the optimal occluded body parts. The proposed model uses an occluded part detector to identify occluded body parts within a human skeleton. It is based on an autoencoder trained on a nonoccluded human skeleton and exploits the symmetry and angular information of the skeleton. Then, we select an optimal group constructed considering the occluded body parts. Each group comprises five sets of joint nodes, focusing on the body parts, excluding the occluded ones. Finally, to enhance interaction within the selected groups, we apply an interpart association module, considering the fusion of global and local elements. The experimental results reveal that the proposed model outperforms others on the occluded datasets. These comparative experiments demonstrate the effectiveness of the study in addressing the challenge of action recognition in occlusion situations. Our code is publicly available at https://github.com/MJ-Kor/OP-GCN. keywords: {Costs;Codes;Graph convolutional networks;Human-robot interaction;Focusing;Detectors;Skeleton},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610972&isnumber=10609862

Y. -J. Dong, F. -L. Zhang and S. -H. Zhang, "MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 7318-7324, doi: 10.1109/ICRA57147.2024.10610688.Abstract: Depth perception is crucial for a wide range of robotic applications. Multi-frame self-supervised depth estimation methods have gained research interest due to their ability to leverage large-scale, unlabeled real-world data. However, the self-supervised methods often rely on the assumption of a static scene and their performance tends to degrade in dynamic environments. To address this issue, we present Motion-Aware Loss, which leverages the temporal relation among consecutive input frames and a novel distillation scheme between the teacher and student networks in the multi-frame self-supervised depth estimation methods. Specifically, we associate the spatial locations of moving objects with the temporal order of input frames to eliminate errors induced by object motion. Meanwhile, we enhance the original distillation scheme in multi-frame methods to better exploit the knowledge from a teacher network. MAL is a novel, plug-and-play module designed for seamless integration into multi-frame self-supervised monocular depth estimation methods. Adding MAL into previous state-of-the-art methods leads to a reduction in depth estimation errors by up to 4.2% and 10.8% on KITTI and CityScapes benchmarks, respectively. keywords: {Knowledge engineering;Estimation error;Dynamics;Estimation;Benchmark testing;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610688&isnumber=10609862

