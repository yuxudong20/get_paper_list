X. Zhang, L. Cui and J. Yin, "Neural Radiance Fields for Unbounded Lunar Surface Scene," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16858-16864, doi: 10.1109/ICRA57147.2024.10611137.Abstract: Accurate understanding of lunar surface topography is vital for effective decision-making and remote control of lunar rovers during exploration missions. Conventional sensing methods often struggle to capture the intricate details of the lunar landscape. In response, we propose an innovative approach that leverages NeRF to synthesize new viewpoints within the expansive lunar environment. By blending 3D hash grids and 2D plane grids representations, our approach provides a comprehensive scene representation. We employ the technique of spiral sampling and feature rendering to enhance rendering quality while simultaneously reducing training time. Additionally, we leverage sparse point cloud to aid the model in better learning the geometric structure of the lunar environment. Through experimentation, we have demonstrated that our method is capable of synthesizing realistic images of lunar environments. keywords: {Training;Space vehicles;Point cloud compression;Three-dimensional displays;Accuracy;Moon;Rendering (computer graphics)},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611137&isnumber=10609862

B. Ling et al., "SocialGAIL: Faithful Crowd Simulation for Social Robot Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16873-16880, doi: 10.1109/ICRA57147.2024.10610371.Abstract: Navigation through crowded human environments is challenging for social robots. While reinforcement learning has been adopted for its capacity to capture complex interactions, the training process often relies on simulators to replicate realistic crowd behaviors, ensuring cost-efficiency. Existing crowd simulation methods typically rely on either handcrafted rules, which may lead to overly aggressive navigation, or learning from human trajectory demonstrations, which can be challenging to generalize effectively. In this paper, we introduce a data-driven crowd simulation method called SocialGAIL, which leverages Generative Adversarial Imitation Learning (GAIL) to emulate real pedestrian navigation in crowded environments. SocialGAIL utilizes an attention-based graph neural network to encode observations and employs a generator-discriminator architecture to closely mimic pedestrian behavior. We propose a set of metrics to evaluate the faithfulness of crowd simulation. Experimental results demonstrate that SocialGAIL outperforms baseline methods in terms of goal-reaching, intermediate state faithfulness, trajectory faithfulness, and adherence to global trajectory patterns. The code of our approach is available at https://github.com/William-island/SocialGAIL. keywords: {Measurement;Training;Pedestrians;Navigation;Imitation learning;Social robots;Graph neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610371&isnumber=10609862

C. Jestel et al., "MuRoSim – A Fast and Efficient Multi-Robot Simulation for Learning-based Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16881-16887, doi: 10.1109/ICRA57147.2024.10610375.Abstract: Multi-robot navigation and dynamic obstacle avoidance are challenging problems in robot learning. Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated great potential in this area. Nonetheless, they often face challenges related to low sample efficiency. To overcome this challenge, some research proposes simulators that incorporate hardware acceleration. Although these simulators improve efficiency, they often lack the flexibility to generate diverse learning scenarios as often needed in multi-robot scenarios, where the different environments have varying numbers of agents.In this paper, we introduce MuRoSim, a multi-robot simulation for lidar-based navigation specifically designed for DRL applications. Due to its high level of abstraction, complete implementation in C++, and rigorous thread pool utilization, MuRoSim achieves high computational performance. We apply MuRoSim for training navigation policies for omnidirectional mobile robots equipped with lidar sensors using DRL. Finally, we conduct extensive Sim-to-Real experiments to confirm the realism of the simulator, by deploying the learned policy for dynamic navigation with up to six robots in numerous of real- world experiments. keywords: {Training;Laser radar;Navigation;Instruction sets;Robot sensing systems;Robot learning;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610375&isnumber=10609862

J. A. Fernández-Fernández, R. Lange, S. Laible, K. O. Arras and J. Bender, "STARK: A Unified Framework for Strongly Coupled Simulation of Rigid and Deformable Bodies with Frictional Contact," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16888-16894, doi: 10.1109/ICRA57147.2024.10610574.Abstract: The use of simulation in robotics is increasingly widespread for the purpose of testing, synthetic data generation and skill learning. A relevant aspect of simulation for a variety of robot applications is physics-based simulation of robot-object interactions. This involves the challenge of accurately modeling and implementing different mechanical systems such as rigid and deformable bodies as well as their interactions via constraints, contact or friction. Most state-of-the-art physics engines commonly used in robotics either cannot couple deformable and rigid bodies in the same framework, lack important systems such as cloth or shells, have stability issues in complex friction-dominated setups or cannot robustly prevent penetrations. In this paper, we propose a framework for strongly coupled simulation of rigid and deformable bodies with focus on usability, stability, robustness and easy access to state-of-the-art deformation and frictional contact models. Our system uses the Finite Element Method (FEM) to model deformable solids, the Incremental Potential Contact (IPC) approach for frictional contact and a robust second order optimizer to ensure stable and penetration-free solutions to tight tolerances. It is a general purpose framework, not tied to a particular use case such as grasping or learning, it is written in C++ and comes with a Python interface. We demonstrate our system’s ability to reproduce complex real-world experiments where a mobile vacuum robot interacts with a towel on different floor types and towel geometries. Our system is able to reproduce 100% of the qualitative outcomes observed in the laboratory environment. The simulation pipeline, named Stark (the German word for strong, as in strong coupling) is made open-source. keywords: {Deformable models;Accuracy;Deformation;Friction;Stability analysis;Robustness;Finite element analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610574&isnumber=10609862

L. -M. Chao and L. Li, "Hydrodynamic Interactions in Schooling Fish: Prioritizing Real Fish Kinematics Over Travelling-wavy Undulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16895-16900, doi: 10.1109/ICRA57147.2024.10611390.Abstract: Hydrodynamic interactions are crucial for understanding fish movement, particularly within the realm of robotic applications. Traditionally, many studies have favoured simplified travelling-wavy undulations derived from observed real fish kinematics. This approach often neglects higher-order undulations, thereby missing the subtleties of authentic fish movements. In this study, we utilised Computational Fluid Dynamics (CFD) to investigate the implications of using real fish kinematics in hydrodynamic interactions among schooling fish. We analysed two scenarios: one driven by real fish kinematics in spatiotemporal formations, and the other by travelling-wavy undulations inferred from the same real fish kinematics. Our results highlight the advantages of using real fish body kinematics for a more accurate representation of hydrodynamics in fish swimming. In contrast, the idealised travelling-wavy undulations tend to apply excessive force, displacing real fish more than expected. Additionally, the vortices and corresponding flow fields generated by real fish kinematics were found to be more stable than those arising from simplified travelling-wavy undulations. Our study underscores the significance of integrating real fish kinematics into robotic fish design and hydrodynamic studies in schooling fish. keywords: {Accuracy;Computational fluid dynamics;Force;Kinematics;Fish;Hydrodynamics;Spatiotemporal phenomena},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611390&isnumber=10609862

A. Richard et al., "OmniLRS: A Photorealistic Simulator for Lunar Robotics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16901-16907, doi: 10.1109/ICRA57147.2024.10610026.Abstract: Developing algorithms for extra-terrestrial robotic exploration has always been challenging. Along with the complexity associated with these environments, one of the main issues remains the evaluation of said algorithms. With the regained interest in lunar exploration, there is also a demand for quality simulators that will enable the development of lunar robots. In this paper, we propose Omniverse Lunar Robotic-Sim (OmniLRS) that is a photorealistic Lunar simulator based on Nvidia’s robotic simulator. This simulation provides fast procedural environment generation, multi-robot capabilities, along with synthetic data pipeline for machine-learning applications. It comes with ROS1 and ROS2 bindings to control not only the robots, but also the environments. This work also performs sim-to-real rock instance segmentation to show the effectiveness of our simulator for image-based perception. Trained on our synthetic data, a yolov8 model achieves performance close to a model trained on real-world data, with 5% performance gap. When finetuned with real data, the model achieves 14% higher average precision than the model trained on real-world data, demonstrating our simulator’s photorealism. The code is fully open-source, accessible here: https://github.com/AntoineRichard/OmniLRS, and comes with demonstrations. keywords: {Instance segmentation;Training;Photorealism;Moon;Pipelines;Machine learning;Rocks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610026&isnumber=10609862

J. Lu, K. Wong, C. Zhang, S. Suo and R. Urtasun, "SceneControl: Diffusion for Controllable Traffic Scene Generation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16908-16914, doi: 10.1109/ICRA57147.2024.10610324.Abstract: We consider the task of traffic scene generation. A common approach in the self-driving industry is to use manual creation to generate scenes with specific characteristics and automatic generation to generate canonical scenes at scale. However, manual creation is not scalable, and automatic generation typically use rules-based algorithms that lack realism. In this paper, we propose SceneControl, a framework for controllable traffic scene generation. To capture the complexity of real traffic, SceneControl learns an expressive diffusion model from data. Then, using guided sampling, we can flexibly control the sampling process to generate scenes that exhibit desired characteristics. Our experiments show that SceneControl achieves greater realism and controllability than the existing state-of-the-art. We also illustrate how SceneControl can be used as a tool for interactive traffic scene generation. keywords: {Training;Industries;Process control;Manuals;Diffusion models;Controllability;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610324&isnumber=10609862

G. Yang, S. Luo, Y. Feng, Z. Sun, C. Tie and L. Shao, "Jade: A Differentiable Physics Engine for Articulated Rigid Bodies with Intersection-Free Frictional Contact," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16915-16922, doi: 10.1109/ICRA57147.2024.10610750.Abstract: We present Jade, a differentiable physics engine for articulated rigid bodies. Jade models contacts as the Linear Complementarity Problem (LCP). Compared to existing differentiable simulations, Jade offers features including intersection-free collision simulation and stable LCP solutions for multiple frictional contacts. We use continuous collision detection to detect the time of impact and adopt the backtracking strategy to prevent intersection between bodies with complex geometry shapes. We derive the gradient calculation to ensure the whole simulation process is differentiable under the backtracking mechanism. We modify the popular Dantzig’s algorithm to get valid solutions under multiple frictional contacts. We conduct extensive experiments to demonstrate the effectiveness of our differentiable physics simulation over a variety of contact-rich tasks. Supplemental materials and videos are available on our project webpage at https://sites.google.com/view/diffsim keywords: {Geometry;Backtracking;Shape;Task analysis;Collision avoidance;Physics;Engines},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610750&isnumber=10609862

M. Wiedemann, O. Ahmed, A. Dieckhöfer, R. Gasoto and S. Kerner, "Simulation Modeling of Highly Dynamic Omnidirectional Mobile Robots Based on Real-World Data," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16923-16929, doi: 10.1109/ICRA57147.2024.10611459.Abstract: Simulation is a key technology in robotics as it enables the generation of environmental data and testing scenarios for development and maintenance purposes. However, simulations are an imperfect representation of the real-world and the so-called sim-to-real gap between simulation and reality hinders the deployment of virtual developed solutions without additional effort. Modeling complex systems like highly dynamic and holonomic mobile robots presents additional complexities in simulation. This paper addresses these challenges through a case study on creating a model for a highly dynamic logistics robot. The study considers the modeling of the entire system down to creating suitable colliders for the rollers of a Mecanum wheel. Additionally, the impact of significant physics parameters is presented. To bridge the sim-to-real gap, a pipeline is developed that utilizes a Motion Capture system to compare the behavior of a real robot with its simulated counterpart across various motions. By leveraging expert knowledge gained from the real-world data, the simulation model is manually tuned to replicate complex system behaviors, such as sliding effects. keywords: {Computational modeling;Pipelines;Dynamics;Manuals;Data models;Mobile robots;Complex systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611459&isnumber=10609862

J. Dao, H. Duan and A. Fern, "Sim-to-Real Learning for Humanoid Box Loco-Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16930-16936, doi: 10.1109/ICRA57147.2024.10610977.Abstract: In this work we propose a learning-based approach to box loco-manipulation for a humanoid robot. This is a particularly challenging problem due to the need for whole-body coordination in order to lift boxes of varying weight, position, and orientation while maintaining balance. To address this challenge, we present a sim-to-real reinforcement learning approach for training general box pickup and carrying skills for the bipedal robot Digit. Our reward functions are designed to produce the desired interactions with the box while also valuing balance and gait quality. We combine the learned skills into a full system for box loco-manipulation to achieve the task of moving boxes from one table to another with a variety of sizes, weights, and initial configurations. In addition to quantitative simulation results, we demonstrate successful sim-to-real transfer on the humanoid robot Digit. To our knowledge this is the first demonstration of a learned controller for such a task on real world hardware. keywords: {Training;Navigation;Simulation;Robot kinematics;Humanoid robots;Reinforcement learning;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610977&isnumber=10609862

A. Altawaitan, J. Stanley, S. Ghosal, T. Duong and N. Atanasov, "Hamiltonian Dynamics Learning from Point Cloud Observations for Nonholonomic Mobile Robot Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16937-16944, doi: 10.1109/ICRA57147.2024.10610395.Abstract: Reliable autonomous navigation requires adapting the control policy of a mobile robot in response to dynamics changes in different operational conditions. Hand-designed dynamics models may struggle to capture model variations due to a limited set of parameters. Data-driven dynamics learning approaches offer higher model capacity and better generalization but require large amounts of state-labeled data. This paper develops an approach for learning robot dynamics directly from point-cloud observations, removing the need and associated errors of state estimation, while embedding Hamiltonian structure in the dynamics model to improve data efficiency. We design an observation-space loss that relates motion prediction from the dynamics model with motion prediction from point-cloud registration to train a Hamiltonian neural ordinary differential equation. The learned Hamiltonian model enables the design of an energy-shaping model-based tracking controller for rigid-body robots. We demonstrate dynamics learning and tracking control on a real nonholonomic wheeled robot. keywords: {Point cloud compression;Adaptation models;Trajectory tracking;Dynamics;Predictive models;Mathematical models;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610395&isnumber=10609862

J. Sacks, R. Rana, K. Huang, A. Spitzer, G. Shi and B. Boots, "Deep Model Predictive Optimization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16945-16953, doi: 10.1109/ICRA57147.2024.10611492.Abstract: A major challenge in robotics is to design robust policies which enable complex and agile behaviors in the real world. On one end of the spectrum, we have model-free reinforcement learning (MFRL), which is incredibly flexible and general but often results in brittle policies. In contrast, model predictive control (MPC) continually re-plans at each time step to remain robust to perturbations and model inaccuracies. However, despite its real-world successes, MPC often under-performs the optimal strategy. This is due to model quality, myopic behavior from short planning horizons, and approximations due to computational constraints. And even with a perfect model and enough compute, MPC can get stuck in bad local optima, depending heavily on the quality of the optimization algorithm. To this end, we propose Deep Model Predictive Optimization (DMPO), which learns the inner-loop of an MPC optimization algorithm directly via experience, specifically tailored to the needs of the control problem. We evaluate DMPO on a real quadrotor agile trajectory tracking task, on which it improves performance over a baseline MPC algorithm for a given computational budget. It can outperform the best MPC algorithm by up to 27% with fewer samples and an end-to-end policy trained with MFRL by 19%. Moreover, because DMPO requires fewer samples, it can also achieve these benefits with 4.3× less memory. When we subject the quadrotor to turbulent wind fields with an attached drag plate, DMPO can adapt zero-shot while still outperforming all baselines. Additional results can be found at https://tinyurl.com/mr2ywmnw. keywords: {Runtime;Trajectory tracking;Computational modeling;Predictive models;Prediction algorithms;Approximation algorithms;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611492&isnumber=10609862

S. J. Wang, H. Zhu and A. M. Johnson, "Pay Attention to How You Drive: Safe and Adaptive Model-Based Reinforcement Learning for Off-Road Driving," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16954-16960, doi: 10.1109/ICRA57147.2024.10611059.Abstract: Autonomous off-road driving is challenging as unsafe actions may lead to catastrophic damage. As such, developing controllers in simulation is often desirable. However, robot dynamics in unstructured off-road environments can be highly complex and difficult to simulate accurately. Domain randomization addresses this problem by randomizing simulation dynamics to train policies that are robust towards modeling errors. While these policies are robust across a range of dynamics, they are sub-optimal for any particular system dynamics. We introduce a novel model-based reinforcement learning approach that aims to balance robustness with adaptability. We train a System Identification Transformer (SIT) and an Adaptive Dynamics Model (ADM) under a variety of simulated dynamics. The SIT uses attention mechanisms to distill target system state-transition observations into a context vector, which provides an abstraction for the target dynamics. Conditioned on this, the ADM probabilistically models the system’s dynamics. Online, we use a Risk-Aware Model Predictive Path Integral controller to safely control the robot under its current understanding of dynamics. We demonstrate in simulation and in the real world that this approach enables safer behaviors upon initialization and becomes less conservative (i.e. faster) as its understanding of the target system dynamics improves with more observations. In particular, our approach results in an approximately 41% improvement in lap-time over the non-adaptive baseline while remaining safe across different environments. keywords: {Adaptation models;System dynamics;Reinforcement learning;Predictive models;Transformers;Probabilistic logic;Vectors;model-based reinforcement learning;robust control;adaptive control;sim2real},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611059&isnumber=10609862

J. Luo et al., "SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16961-16969, doi: 10.1109/ICRA57147.2024.10610040.Abstract: In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to the widespread adoption of robotic RL, as well as the further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely adopted robot, and a number of challenging example tasks. We provide this library as a resource for the community, describe its design choices, and present experimental results. Perhaps surprisingly, we find that our implementation can achieve very efficient learning, acquiring policies for PCB board assembly, cable routing, and object relocation between 25 to 50 minutes of training per policy on average, improving over state-of-the-art results reported for similar tasks in the literature. These policies achieve perfect or near-perfect success rates, extreme robustness even under perturbations, and exhibit emergent recovery and correction behaviors. We hope these promising results and our high-quality open-source implementation will provide a tool for the robotics community to facilitate further developments in robotic RL. Our code, documentation, and videos can be found at https://serl-robot.github.io/ keywords: {Training;Perturbation methods;Reinforcement learning;Routing;Libraries;Software;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610040&isnumber=10609862

Y. Lin, G. Chou and D. Berenson, "Improving Out-of-Distribution Generalization of Learned Dynamics by Learning Pseudometrics and Constraint Manifolds," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16970-16976, doi: 10.1109/ICRA57147.2024.10611657.Abstract: We propose a method for improving the prediction accuracy of learned robot dynamics models on out-of-distribution (OOD) states. We achieve this by leveraging two key sources of structure often present in robot dynamics: 1) sparsity, i.e., some components of the state may not affect the dynamics, and 2) physical limits on the set of possible motions, in the form of nonholonomic constraints. Crucially, we do not assume this structure is known a priori, and instead learn it from data. We use contrastive learning to obtain a distance pseudometric that uncovers the sparsity pattern in the dynamics, and use it to reduce the input space when learning the dynamics. We then learn the unknown constraint manifold by approximating the normal space of possible motions from the data, which we use to train a Gaussian process (GP) representation of the constraint manifold. We evaluate our approach on a physical differential-drive robot and a simulated quadrotor, showing improved prediction accuracy on OOD data relative to baselines. keywords: {Manifolds;Accuracy;Dynamics;Gaussian processes;Contrastive learning;Predictive models;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611657&isnumber=10609862

C. Bhateja et al., "Robotic Offline RL from Internet Videos via Value-Function Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16977-16984, doi: 10.1109/ICRA57147.2024.10611575.Abstract: Pre-training on Internet data has proven to be a key ingredient for broad generalization in many modern ML systems. What would it take to enable such capabilities in robotic reinforcement learning (RL)? Offline RL methods, which learn from datasets of robot experience, offer one way to leverage prior data into the robotic learning pipeline. However, these methods have a "type mismatch" with video data (such as Ego4D), which are the largest prior datasets available for robotics, since video offers observation-only experience without the action or reward annotations needed for RL methods. In this paper, we develop a system for leveraging large-scale human video datasets in robotic offline RL, based entirely on learning value functions via temporal-difference learning. We show that value learning on video datasets learns representations that are more conducive to downstream robotic offline RL than other approaches for learning from video data. Our system, called V-PTR, combines the benefits of pre-training on video data with robotic offline RL approaches that train on diverse robot data, resulting in value functions and policies for manipulation tasks that perform better, act robustly, and generalize broadly. On several manipulation tasks on a real WidowX robot and in simulated settings, our framework produces policies that greatly improve over other prior methods. Our video and additional details can be found at https://dibyaghosh.com/vptr/. keywords: {Annotations;Temporal difference learning;Pipelines;Internet;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611575&isnumber=10609862

D. Kerimoglu et al., "Learning manipulation of steep granular slopes for fast Mini Rover turning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16985-16990, doi: 10.1109/ICRA57147.2024.10611700.Abstract: Future planetary exploration missions will require reaching challenging regions such as craters and steep slopes. Such regions are ubiquitous and present science-rich targets potentially containing information regarding the planet’s internal structure. Steep slopes consisting of low-cohesion regolith are prone to flow downward under small disturbances, making it challenging for autonomous rovers to traverse. Moreover, the navigation trajectories of rovers are heavily limited by the terrain topology and future systems will need to maneuver on flowable surfaces without getting trapped, allowing them to further expand their reach and increase mission efficiency.In this work, we used a robophysical rover model and performed maneuvering experiments on a steep granular slope of poppy seeds to explore the rover’s turning capabilities. The rover is capable of lifting, sweeping, and spinning its wheels, allowing it to execute leg-like gait patterns. The high-dimensional actuation capabilities of the rover facilitate effective manipulation of the underlying granular surface. We used Bayesian Optimization (BO) to gain insight into successful turning gaits in high dimensional search space and found strategies such as differential wheel spinning and pivoting around a single sweeping wheel. We then used these insights to further fine-tune the turning gait, enabling the rover to turn nearly 90 degrees at just above 4 seconds with minimal downhill slip. Combining gait optimization and human-tuning approaches, we found that fast turning is empowered by creating anisotropic torques with the sweeping wheel. keywords: {Systematics;Navigation;Wheels;Turning;Bayes methods;Trajectory;Topology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611700&isnumber=10609862

S. Pannek et al., "Exploring the Needle Tip Interaction Force with Retinal Tissue Deformation in Vitreoretinal Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 16999-17005, doi: 10.1109/ICRA57147.2024.10610807.Abstract: Recent advancements in age-related macular degeneration treatments necessitate precision delivery into the subretinal space, emphasizing minimally invasive procedures targeting the retinal pigment epithelium (RPE)-Bruch’s membrane complex without causing trauma. Even for skilled surgeons, the inherent hand tremors during manual surgery can jeopardize the safety of these critical interventions.This has fostered the evolution of robotic systems designed to prevent such tremors. These robots are enhanced by FBG sensors, which sense the small force interactions between the surgical instruments and retinal tissue. To enable the community to design algorithms taking advantage of such force feedback data, this paper focuses on the need to provide a specialized dataset, integrating optical coherence tomography (OCT) imaging together with the aforementioned force data.We introduce a unique dataset, integrating force sensing data synchronized with OCT B-scan images, derived from a sophisticated setup involving robotic assistance and OCT integrated microscopes. Furthermore, we present a neural network model for image-based force estimation to demonstrate the dataset’s applicability. keywords: {Force;Estimation;Pigments;Robot sensing systems;Retina;Needles;Robot learning;Data Sets for Robotic Vision;Medical Robots and Systems;Data Sets for Robot Learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610807&isnumber=10609862

A. Bacchin, L. Barcellona, S. Shamsizadeh, E. Olivastri, A. Pretto and E. Menegatti, "PanNote: an Automatic Tool for Panoramic Image Annotation of People’s Positions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17006-17012, doi: 10.1109/ICRA57147.2024.10610347.Abstract: Panoramic cameras offer a 4π steradian field of view, which is desirable for tasks like people detection and tracking since nobody can exit the field of view. Despite the recent diffusion of low-cost panoramic cameras, their usage in robotics remains constrained by the limited availability of datasets featuring annotations in the robot space, including people’s 2D or 3D positions. To tackle this issue, we introduce PanNote, an automatic annotation tool for people’s positions in panoramic videos. Our tool is designed to be cost-effective and straightforward to use without requiring human intervention during the labeling process and enabling the training of machine learning models with low effort. The proposed method introduces a calibration model and a data association algorithm to fuse data from panoramic images and 2D LiDAR readings. We validate the capabilities of PanNote by collecting a real-world dataset. On these data, we compared manual labels, automatic labels and the predictions of a baseline deep neural network. Results clearly show the advantage of using our method, with a 15-fold speed up in labeling time and a considerable gain in performance while training deep neural models on automatically labelled data. keywords: {Training;Laser radar;Navigation;Annotations;Robot vision systems;Cameras;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610347&isnumber=10609862

S. Thoduka, N. Hochgeschwender, J. Gall and P. G. Plöger, "A Multimodal Handover Failure Detection Dataset and Baselines," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17013-17019, doi: 10.1109/ICRA57147.2024.10610143.Abstract: An object handover between a robot and a human is a coordinated action which is prone to failure for reasons such as miscommunication, incorrect actions and unexpected object properties. Existing works on handover failure detection and prevention focus on preventing failures due to object slip or external disturbances. However, there is a lack of datasets and evaluation methods that consider unpreventable failures caused by the human participant. To address this deficit, we present the multimodal Handover Failure Detection dataset, which consists of failures induced by the human participant, such as ignoring the robot or not releasing the object. We also present two baseline methods for handover failure detection: (i) a video classification method using 3D CNNs and (ii) a temporal action segmentation approach which jointly classifies the human action, robot action and overall outcome of the action. The results show that video is an important modality, but using force-torque data and gripper position help improve failure detection and action segmentation accuracy. keywords: {Three-dimensional displays;Accuracy;Robot kinematics;Prevention and mitigation;Handover;Robot sensing systems;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610143&isnumber=10609862

B. Meden, P. Vega, F. M. De Chamisso and S. Bourgeois, "Introducing CEA-IMSOLD: an Industrial Multi-Scale Object Localization Dataset," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17020-17026, doi: 10.1109/ICRA57147.2024.10609999.Abstract: We introduce the CEA Industrial Multi-Scale Object Localization Dataset (CEA-IMSOLD), a new BOP format dataset for 6-DoF object localization, crucial for robotics. This dataset aims to evaluate the current localization methods with respect to a new difficulty: large variations in observation distance and, consequently, large variations in image appearance. Compared to the other publicly available datasets, our dataset provides both images with objects small and completely visible in the image, and images where objects are observed close enough so they appear larger than the field of view of the camera. We also propose to consider the observation distance in the evaluation process and introduce new metrics to do so. Finally, our dataset contains a large variety of industrial objects, from small and simple objects such as bolts to sizable and complex ones such as large car parts. We provide baseline results and the dataset is made publicly available to support the community at https://cea-list.github.io/CEA-IMSOLD/. keywords: {Location awareness;Measurement;Three-dimensional displays;Service robots;Object detection;Fasteners;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609999&isnumber=10609862

Y. Yan, B. Liu, J. Ai, Q. Li, R. Wan and J. Pu, "PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17027-17034, doi: 10.1109/ICRA57147.2024.10610043.Abstract: Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Semantic Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navigation. The code and datasets are available at https://github.com/yyxssm/PointSSC. keywords: {Point cloud compression;Three-dimensional displays;Navigation;Semantics;Pipelines;Benchmark testing;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610043&isnumber=10609862

K. Bai, L. Zhang, Z. Chen, F. Wan and J. Zhang, "Close the Sim2real Gap via Physically-based Structured Light Synthetic Data Simulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17035-17041, doi: 10.1109/ICRA57147.2024.10611401.Abstract: Despite the substantial progress in deep learning, its adoption in industrial robotics projects remains limited, primarily due to challenges in data acquisition and labeling. Previous sim2real approaches using domain randomization require extensive scene and model optimization. To address these issues, we introduce an innovative physically-based structured light simulation system, generating both RGB and physically realistic depth images, surpassing previous dataset generation tools. We create an RGBD dataset tailored for robotic industrial grasping scenarios and evaluate it across various tasks, including object detection, instance segmentation, and embedding sim2real visual perception in industrial robotic grasping. By reducing the sim2real gap and enhancing deep learning training, we facilitate the application of deep learning models in industrial settings. Project details are available at https://baikaixin-public.github.io/structured_light_3D_synthesizer/ keywords: {Deep learning;Training;Adaptation models;Three-dimensional displays;Service robots;Annotations;Transfer learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611401&isnumber=10609862

M. Vaidis, M. H. Shahraji, E. Daum, W. Dubois, P. Giguère and F. Pomerleau, "RTS-GT: Robotic Total Stations Ground Truthing dataset," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17050-17056, doi: 10.1109/ICRA57147.2024.10610998.Abstract: Numerous datasets and benchmarks exist to assess and compare Simultaneous Localization and Mapping (SLAM) algorithms. Nevertheless, their precision must follow the rate at which SLAM algorithms improved in recent years. Moreover, current datasets fall short of comprehensive data-collection protocol for reproducibility and the evaluation of the precision or accuracy of the recorded trajectories. With this objective in mind, we proposed the Robotic Total Stations Ground Truthing dataset (RTS-GT) dataset to support localization research with the generation of six-Degrees Of Freedom (DOF) ground truth trajectories. This novel dataset includes six-DOF ground truth trajectories generated using a system of three Robotic Total Stations (RTSs) tracking moving robotic platforms. Furthermore, we compare the performance of the RTS-based system to a Global Navigation Satellite System (GNSS)-based setup. The dataset comprises around sixty experiments conducted in various conditions over a period of 17 months, and encompasses over 49 kilometers of trajectories, making it the most extensive dataset of RTS-based measurements to date. Additionally, we provide the precision of all poses for each experiment, a feature not found in the current state-of-the-art datasets. Our results demonstrate that RTSs provide measurements that are 22 times more stable than GNSS in various environmental settings, making them a valuable resource for SLAM benchmark development. keywords: {Global navigation satellite system;Simultaneous localization and mapping;Protocols;Current measurement;Robot vision systems;Benchmark testing;Reproducibility of results},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610998&isnumber=10609862

X. Liu, C. Zhang, G. Wang, R. Zhang and X. Ji, "RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17057-17064, doi: 10.1109/ICRA57147.2024.10611208.Abstract: In robotic vision, a de-facto paradigm is to learn in simulated environments and then transfer to real-world applications, which poses an essential challenge in bridging the sim-to-real domain gap. While mainstream works tackle this problem in the RGB domain, we focus on depth data synthesis and develop a Range-aware RGB-D data Simulation pipeline (RaSim). In particular, high-fidelity depth data is generated by imitating the imaging principle of real-world sensors. A range-aware rendering strategy is further introduced to enrich data diversity. Extensive experiments show that models trained with RaSim can be directly applied to real-world scenarios without any finetuning and excel at downstream RGB-D perception tasks. Data and code are available at https://github.com/shanice-l/RaSim. keywords: {Image sensors;Bridges;Solid modeling;Three-dimensional displays;Codes;Pipelines;Rendering (computer graphics)},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611208&isnumber=10609862

E. Saccon, A. Tikna, D. De Martini, E. Lamon, L. Palopoli and M. Roveri, "When Prolog Meets Generative Models: a New Approach for Managing Knowledge and Planning in Robotic Applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17065-17071, doi: 10.1109/ICRA57147.2024.10610800.Abstract: In this paper, we propose a robot oriented knowledge representation system based on the use of the Prolog language. Our framework hinges on a special organisation of Knowledge Base (KB) that enables: 1) its efficient population from natural language texts using semi-automated procedures based on Large Language Models (LLMs); 2) the seamless generation of temporal parallel plans for multi-robot systems through a sequence of transformations; 3) the automated translation of the plan into an executable formalism. The framework is supported by a set of open source tools and its functionality is shown with a realistic application. keywords: {Large language models;Sociology;Knowledge based systems;Knowledge representation;Fasteners;Planning;Multi-robot systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610800&isnumber=10609862

S. Jeon, S. Shin and B. -T. Zhang, "HAPFI: History-Aware Planning based on Fused Information," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17072-17078, doi: 10.1109/ICRA57147.2024.10610571.Abstract: Embodied Instruction Following (EIF) is a task of planning a long sequence of sub-goals given high-level natural language instructions, such as "Rinse a slice of lettuce and place on the white table next to the fork". To successfully execute these long-term horizon tasks, we argue that an agent must consider its past, i.e., historical data, when making decisions in each step. Nevertheless, recent approaches in EIF often neglects the knowledge from historical data and also do not effectively utilize information across the modalities. To this end, we propose History-Aware Planning based on Fused Information(HAPFI), effectively leveraging the historical data from diverse modalities that agents collect while interacting with the environment. Specifically, HAPFI integrates multiple modalities, including historical RGB observations, bounding boxes, sub-goals, and high-level instructions, by effectively fusing modalities via our Mutually Attentive Fusion method. Through experiments with diverse comparisons, we show that an agent utilizing historical multi-modal information surpasses all the compared methods that neglect the historical data in terms of action planning capability, enabling the generation of well-informed action plans for the next step. Moreover, we provided qualitative evidence highlighting the significance of leveraging historical multi-modal data, particularly in scenarios where the agent encounters intermediate failures, showcasing its robust re-planning capabilities. keywords: {Navigation;Natural languages;Planning;Task analysis;Robots;Capacity planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610571&isnumber=10609862

P. Hammer, P. Isaev, L. Feng, R. Johansson and J. Tumova, "Non-Axiomatic Reasoning for an Autonomous Mobile Robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17079-17085, doi: 10.1109/ICRA57147.2024.10611411.Abstract: We present the integration of a Non-Axiomatic Reasoning System (NARS) with mobile robots for planning and decision making. NARS enables robots to effectively handle uncertainty in real-time with complete sensor and actuator integration, thereby ensuring adaptability to evolving scenarios. We discuss essential parts of the logic, the architecture and working principles of NARS, and the integration of NARS as a ROS node. A case study is provided demonstrating the system’s proficiency to carry out a garbage collection task in an open-air environment by operating a mobile robot with manipulator arm, and we demonstrate its ability to learn about the place-dependent accumulation of garbage items. Case study also reveals that our approach performs more effectively on the overall task than the Belief-Desire-Intention model we compared with. keywords: {Actuators;Uncertainty;Object detection;Switches;Robot sensing systems;Manipulators;Cognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611411&isnumber=10609862

Y. Sung, R. Shome and P. Stone, "Asynchronous Task Plan Refinement for Multi-Robot Task and Motion Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17086-17092, doi: 10.1109/ICRA57147.2024.10610503.Abstract: This paper explores general multi-robot task and motion planning, where multiple robots in close proximity manipulate objects while satisfying constraints and a given goal. In particular, we formulate the plan refinement problem—which, given a task plan, finds valid assignments of variables corresponding to solution trajectories—as a hybrid constraint satisfaction problem. The proposed algorithm follows several design principles that yield the following features: (1) efficient solution finding due to sequential heuristics and implicit time and roadmap representations, and (2) maximized feasible solution space obtained by introducing minimally necessary coordination-induced constraints and not relying on prevalent simplifications that exist in the literature. The evaluation results demonstrate the planning efficiency of the proposed algorithm, outperforming the synchronous approach in terms of makespan. keywords: {Robot kinematics;Heuristic algorithms;Bidirectional control;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610503&isnumber=10609862

K. Watanabe, G. Fainekos, B. Hoxha, M. Lahijanian, H. Okamoto and S. Sankaranarayanan, "Optimal Planning for Timed Partial Order Specifications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17093-17099, doi: 10.1109/ICRA57147.2024.10611547.Abstract: This paper addresses the challenge of planning a sequence of tasks to be performed by multiple robots while minimizing the overall completion time subject to timing and precedence constraints. Our approach uses the Timed Partial Orders (TPO) model to specify these constraints. We translate this problem into a Traveling Salesman Problem (TSP) variant with timing and precedent constraints, and we solve it as a Mixed Integer Linear Programming (MILP) problem. Our contributions include a general planning framework for TPO specifications, a MILP formulation accommodating time windows and precedent constraints, its extension to multi-robot scenarios, and a method to quantify plan robustness. We demonstrate our framework on several case studies, including an aircraft turnaround task involving three Jackal robots, highlighting the approach’s potential applicability to important real-world problems. Our benchmark results show that our MILP method outperforms state-of-the-art open-source TSP solvers OR-Tools. keywords: {Heuristic algorithms;Contingency management;Traveling salesman problems;Robustness;Planning;Timing;Mixed integer linear programming},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611547&isnumber=10609862

M. Li, Y. H. Zhou, T. Li and Y. Jiang, "Incipient Slip-Based Rotation Measurement via Visuotactile Sensing During In-Hand Object Pivoting," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17132-17138, doi: 10.1109/ICRA57147.2024.10610988.Abstract: In typical in-hand manipulation tasks represented by object pivoting, the real-time perception of rotational slippage has been proven beneficial for improving the dexterity and stability of robotic hands. An effective strategy is to obtain the contact properties for measuring rotation angle through visuotactile sensing. However, existing methods for rotation estimation did not consider the impact of the incipient slip during the pivoting process, which introduces measurement errors and makes it hard to determine the boundary between stable contact and macro slip. This paper describes a generalized 2-d contact model under pivoting, and proposes a rotation measurement method based on the line-features in the stick region. The proposed method was applied to the Tac3D vision-based tactile sensors using continuous marker patterns. Experiments show that the rotation measurement system could achieve an average static measurement error of 0.17°±0.15° and an average dynamic measurement error of 1.34°±0.48°. Besides, the proposed method requires no training data and can achieve real-time sensing during the in-hand object pivoting. keywords: {Training;Measurement errors;Training data;Tactile sensors;Machine learning;Real-time systems;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610988&isnumber=10609862

L. Y. E. R. Cheret, V. P. Da Fonseca and T. E. A. de Oliveira, "Leveraging Compliant Tactile Perception for Haptic Blind Surface Reconstruction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17139-17145, doi: 10.1109/ICRA57147.2024.10610162.Abstract: Non-flat surfaces pose difficulties for robots operating in unstructured environments. Reconstructions of uneven surfaces may only be partially possible due to non-compliant end-effectors and limitations on vision systems such as transparency, reflections, and occlusions. This study achieves blind surface reconstruction by harnessing the robotic manipulator’s kinematic data and a compliant tactile sensing module, which incorporates inertial, magnetic, and pressure sensors. The module’s flexibility enables us to estimate contact positions and surface normals by analyzing its deformation during interactions with unknown objects. While previous works collect only positional information, we include the local normals in a geometrical approach to estimate curvatures between adjacent contact points. These parameters then guide a spline-based patch generation, which allows us to recreate larger surfaces without an increase in complexity while reducing the time-consuming step of probing the surface. Experimental validation demonstrates that this approach outperforms an off-the-shelf vision system in estimation accuracy. Moreover, this compliant haptic method works effectively even when the manipulator’s approach angle is not aligned with the surface normals, which is ideal for unknown non-flat surfaces. keywords: {Pressure sensors;Surface reconstruction;Deformation;Machine vision;Kinematics;Robot sensing systems;Reflection;Contact Modeling;Soft Sensors and Actuators;Haptics and Haptic Interfaces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610162&isnumber=10609862

K. Haninger, K. Samuel, F. Rozzi, S. Oh and L. Roveda, "Differentiable Compliant Contact Primitives for Estimation and Model Predictive Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17146-17152, doi: 10.1109/ICRA57147.2024.10611406.Abstract: Control techniques like MPC can realize contact-rich manipulation which exploits dynamic information, maintaining friction limits and safety constraints. However, contact geometry and dynamics are required to be known. This information is often extracted from CAD, limiting scalability and the ability to handle tasks with varying geometry. To reduce the need for a priori models, we propose a framework for estimating contact models online based on torque and position measurements. To do this, compliant contact models are used, connected in parallel to model multi-point contact and constraints such as a hinge. They are parameterized to be differentiable with respect to all of their parameters (rest position, stiffness, contact location), allowing the coupled robot/environment dynamics to be linearized or efficiently used in gradient-based optimization. These models are then applied for: offline gradient-based parameter fitting, online estimation via an extended Kalman filter, and online gradient-based MPC. The proposed approach is validated on two robots, showing the efficacy of sensorless contact estimation and the effects of online estimation on MPC performance. Video results can be seen at https://youtu.be/CuCTcmn3H-o. keywords: {Geometry;Solid modeling;Torque;Uncertainty;Scalability;Fitting;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611406&isnumber=10609862

Z. Lu et al., "TacShade: A New 3D-printed Soft Optical Tactile Sensor Based on Light, Shadow and Greyscale for Shape Reconstruction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17153-17159, doi: 10.1109/ICRA57147.2024.10610508.Abstract: In this paper, we present the TacShade: a newly designed 3D-printed soft optical tactile sensor. The sensor is developed for shape reconstruction under the inspiration of sketch drawing that uses the density of sketch lines to draw light and shadow, resulting in the creation of a 3D-view effect. TacShade, building upon the strengths of the TacTip, a single-camera tactile sensor of large in-depth deformation and being sensitive to edge and surface following, improves the structure in that the markers are distributed within the gap of papillae pins. Variations in light, dark and grey effects can be generated inside the sensor under the external contact interactions. The contours of the contacting objects are outlined by white markers, while the contact depth characteristics can be indirectly obtained from the distribution of black pins and white markers, creating a 2.5D visualization. Based on the imaging effect, we improve the Shape from Shading (SFS) algorithm to process tactile images, enabling a coarse but fast reconstruction for the contact objects. Two experiments are performed. The first verifies TacShade’s ability to reconstruct the shape of the contact objects through one image for object distinction. The second experiment shows the shape reconstruction capability of TacShade for a large panel with ridged patterns based on the location of robots and image splicing technology. keywords: {Surface reconstruction;Three-dimensional displays;Shape;Tactile sensors;Optical imaging;Three-dimensional printing;Pins},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610508&isnumber=10609862

M. Saleh, M. Sommersperger, N. Navab and F. Tombari, "Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17160-17166, doi: 10.1109/ICRA57147.2024.10610366.Abstract: In robotics, it’s crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries. We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We’ve made our code and dataset public to advance research in robotic simulation and grasping.† keywords: {Deformable models;Deformation;Service robots;Shape;Computational modeling;Grasping;Predictive models;Contact Modelling;Simulation and Animation;Manipulation and Grasping},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610366&isnumber=10609862

M. Schonger et al., "Learning Barrier-Certified Polynomial Dynamical Systems for Obstacle Avoidance with Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17201-17207, doi: 10.1109/ICRA57147.2024.10610828.Abstract: Established techniques that enable robots to learn from demonstrations are based on learning a stable dynamical system (DS). To increase the robots’ resilience to perturbations during tasks that involve static obstacle avoidance, we propose incorporating barrier certificates into an optimization problem to learn a stable and barrier-certified DS. Such optimization problem can be very complex or extremely conservative when the traditional linear parameter-varying formulation is used. Thus, different from previous approaches in the literature, we propose to use polynomial representations for DSs, which yields an optimization problem that can be tackled by sum-of-squares techniques. Finally, our approach can handle obstacle shapes that fall outside the scope of assumptions typically found in the literature concerning obstacle avoidance within the DS learning framework. Supplementary material can be found at the project webpage: https://martinschonger.github.io/abc-ds keywords: {Shape;Scalability;Perturbation methods;Polynomials;Collision avoidance;Dynamical systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610828&isnumber=10609862

W. Wang and G. D. Hager, "Domain Adaptation of Visual Policies with a Single Demonstration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17208-17215, doi: 10.1109/ICRA57147.2024.10610569.Abstract: Deploying machine learning algorithms for robot tasks in real-world applications presents a core challenge: overcoming the domain gap between the training and the deployment environment. This is particularly difficult for visuomotor policies that utilize high-dimensional images as input, particularly when those images are generated via simulation. A common method to tackle this issue is through domain randomization, which aims to broaden the span of the training distribution to cover the test-time distribution. However, this approach is only effective when the domain randomization encompasses the actual shifts in the test-time distribution. We take a different approach, where we make use of a single demonstration (a prompt) to learn policy that adapts to the testing target environment. Our proposed framework, PromptAdapt, leverages the Transformer architecture’s capacity to model sequential data to learn demonstration-conditioned visual policies, allowing for in-context adaptation to a target domain that is distinct from training. Our experiments in both simulation and real-world settings show that PromptAdapt is a strong domain-adapting policy that outperforms baseline methods by a large margin under a range of domain shifts, including variations in lighting, color, texture, and camera pose. Videos and more information can be viewed at project webpage: https://sites.google.com/view/promptadapt. keywords: {Training;Visualization;Machine learning algorithms;Lighting;Transformers;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610569&isnumber=10609862

F. Nawaz, T. Li, N. Matni and N. Figueroa, "Learning Complex Motion Plans using Neural ODEs with Safety and Stability Guarantees," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17216-17222, doi: 10.1109/ICRA57147.2024.10611584.Abstract: We propose a Dynamical System (DS) approach to learn complex, possibly periodic motion plans from kinesthetic demonstrations using Neural Ordinary Differential Equations (NODE). To ensure reactivity and robustness to disturbances, we propose a novel approach that selects a target point at each time step for the robot to follow, by combining tools from control theory and the target trajectory generated by the learned NODE. A correction term to the NODE model is computed online by solving a quadratic program that guarantees stability and safety using control Lyapunov functions and control barrier functions, respectively. Our approach outperforms baseline DS learning techniques on the LASA handwriting dataset and complex periodic trajectories. It is also validated on the Franka Emika robot arm to produce stable motions for wiping and stirring tasks that do not have a single attractor, while being robust to perturbations and safe around humans and obstacles. The project’s web-page is https://sites.google.com/view/lfd-neural-ode/home. keywords: {Perturbation methods;Ordinary differential equations;Manipulators;Stability analysis;Robustness;Mathematical models;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611584&isnumber=10609862

Y. Zhang, Y. Zou, H. Li, H. Zhang and L. Cheng, "Learning a Stable Dynamic System with a Lyapunov Energy Function for Demonstratives Using Neural Networks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17223-17228, doi: 10.1109/ICRA57147.2024.10610633.Abstract: Autonomous Dynamic System (DS)-based algorithms hold a pivotal and foundational role in the field of Learning from Demonstration (LfD). Nevertheless, they confront the formidable challenge of striking a delicate balance between achieving precision in learning and ensuring the overall stability of the system. In response to this substantial challenge, this paper introduces a novel DS algorithm rooted in neural network technology. This algorithm not only possesses the capability to extract critical insights from demonstration data but also demonstrates the capacity to learn a candidate Lyapunov energy function that is consistent with the provided demonstrations. The model presented in this paper employs a simplistic neural network architecture that excels in fulfilling a dual objective: optimizing accuracy while simultaneously preserving global stability. To comprehensively evaluate the effectiveness of the proposed algorithm, rigorous assessments are conducted using the LASA dataset, further reinforced by empirical validation through a robotic experiment. keywords: {Training;Accuracy;Heuristic algorithms;Neural networks;Stability analysis;Vectors;Data mining},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610633&isnumber=10609862

E. V. Mascaro, Y. Yan and D. Lee, "Robot Interaction Behavior Generation based on Social Motion Forecasting for Human-Robot Interaction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17264-17271, doi: 10.1109/ICRA57147.2024.10610682.Abstract: Integrating robots into populated environments is a complex challenge that requires an understanding of human social dynamics. In this work, we propose to model social motion forecasting in a shared human-robot representation space, which facilitates us to synthesize robot motions that interact with humans in social scenarios despite not observing any robot in the motion training. We develop a transformer-based architecture called ECHO, which operates in the aforementioned shared space to predict the future motions of the agents encountered in social scenarios. Contrary to prior works, we reformulate the social motion problem as the refinement of the predicted individual motions based on the surrounding agents, which facilitates the training while allowing for single-motion forecasting when only one human is in the scene. We evaluate our model in multi-person and human-robot motion forecasting tasks and obtain state-of-the-art performance by a large margin while being efficient and performing in real-time. Additionally, our qualitative results showcase the effectiveness of our approach in generating human-robot interaction behaviors that can be controlled via text commands. keywords: {Robot motion;Training;Semantics;Human-robot interaction;Predictive models;Transformers;Skeleton},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610682&isnumber=10609862

L. Xie, W. Gao, H. Zheng and G. Li, "SPCGC: Scalable Point Cloud Geometry Compression for Machine Vision," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17272-17278, doi: 10.1109/ICRA57147.2024.10610894.Abstract: With the proliferation of sensor devices, the extensive utilization of three-dimensional data in multimedia continues to grow. Point clouds are widely adopted within this domain because they are one of the most intuitive representations of three-dimensional data. However, the substantial volume of point cloud data poses significant challenges for storage and transmission. Moreover, a considerable portion of the data loses its semantic information during transmission. Consequently, how can we ensure both the perceptual quality for the human and the performance of downstream tasks during the transmission? To address this issue, we propose a scalable point cloud geometry compression framework (SPCGC) for machine perception. This framework tackles the fidelity issues associated with point cloud compression and preserves more semantic information, enhancing the performance of machine vision tasks. Our solution consists of a base layer bitstream and an enhancement layer bitstream. The base layer bitstream contains geometry data, while the enhancement layer bitstream utilizes semantic-guided residual data. Additionally, we introduce two modules for extracting and coding residual features. And incorporate classification and segmentation losses from downstream tasks into the Rate-Distortion (RD) optimization. Our approach outperforms existing learning-based lossy point cloud coding methods through empirical validation in downstream tasks without sacrificing point cloud compression performance. keywords: {Point cloud compression;Geometry;Semantics;Rate-distortion;Feature extraction;Encoding;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610894&isnumber=10609862

J. Morgan, D. Millard and G. S. Sukhatme, "CppFlow: Generative Inverse Kinematics for Efficient and Robust Cartesian Path Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12279-12785, doi: 10.1109/ICRA57147.2024.10611724.Abstract: In this work we present CppFlow - a novel and performant planner for the Cartesian Path Planning problem, which finds valid trajectories up to 129x faster than current methods, while also succeeding on more difficult problems where others fail. At the core of the proposed algorithm is the use of a learned, generative Inverse Kinematics solver, which is able to efficiently produce promising entire candidate solution trajectories on the GPU. Precise, valid solutions are then found through classical approaches such as differentiable programming, global search, and optimization. In combining approaches from these two paradigms we get the best of both worlds - efficient approximate solutions from generative AI which are made exact using the guarantees of traditional planning and optimization. We evaluate our system against other state of the art methods on a set of established baselines as well as new ones introduced in this work and find that our method significantly outperforms others in terms of the time to find a valid solution and planning success rate, and performs comparably in terms of trajectory length over time. Additional results and an open source implementation is available at https://jstmn.github.io/cppflow-website/. keywords: {Adaptation models;Generative AI;Graphics processing units;Kinematics;Programming;Trajectory;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611724&isnumber=10609862

W. Xiao, T. He, J. Dolan and G. Shi, "Safe Deep Policy Adaptation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17286-17292, doi: 10.1109/ICRA57147.2024.10611340.Abstract: A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors and extra perturbations. Comprehensive experiments on (1) classic control problems (Inverted Pendulum), (2) simulation benchmarks (Safety Gym), and (3) a real-world agile robotics platform (RC Car) demonstrate great superiority of SafeDPA in both safety and task performance, over state-of-the-art baselines. Particularly, SafeDPA demonstrates notable generalizability, achieving a 300% increase in safety rate compared to the baselines, under unseen disturbances in real-world experiments. keywords: {Adaptation models;Perturbation methods;Reinforcement learning;Predictive models;Benchmark testing;Robustness;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611340&isnumber=10609862

M. Bensch, T. -D. Job, T. -L. Habich, T. Seel and M. Schappler, "Physics-Informed Neural Networks for Continuum Robots: Towards Fast Approximation of Static Cosserat Rod Theory," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17293-17299, doi: 10.1109/ICRA57147.2024.10610742.Abstract: Sophisticated models can accurately describe deformations of continuum robots while being computationally demanding, which limits their application. Especially when considering sampling-based path planning, the model has to be evaluated frequently, which can lead to substantially increased computation times. We present a new approach to compute the entire shape of a tendon-driven continuum robot by a physics-informed neural network (PINN). The underlying physics is modelled with the Cosserat rod theory and incorporated into the PINN’s loss function. The boundary values for the training are obtained from a reference model, solved by the shooting method. Our approach allows for a computation of the learned Cosserat rod model multiple orders of magnitude faster than a publicly available reference model. The median position deviation from the reference model lies below 1mm (0.5% of the simulated robot length) for each of the robot’s 20 disks. keywords: {Training;Shape;Computational modeling;Neural networks;Predictive models;Routing;Path planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610742&isnumber=10609862

K. K. Babarahmati, M. Kasaei, C. Tiseo, M. Mistry and S. Vijayakumar, "Robust and Dexterous Dual-arm Tele-Cooperation using Adaptable Impedance Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17337-17343, doi: 10.1109/ICRA57147.2024.10610410.Abstract: In recent years, the need for robots to transition from isolated industrial tasks to shared environments, including human-robot collaboration and teleoperation, has become increasingly evident. Building on the foundation of Fractal Impedance Control (FIC) introduced in our previous work, this paper presents a novel extension to dualarm tele-cooperation, leveraging the non-linear stiffness and passivity of FIC to adapt to diverse cooperative scenarios. Unlike traditional impedance controllers, our approach ensures stability without relying on energy tanks, as demonstrated in our prior research. In this paper, we further extend the FIC framework to bimanual operations, allowing for stable and smooth switching between different dynamic tasks without gain tuning. We also introduce a telemanipulation architecture that offers higher transparency and dexterity, addressing the challenges of signal latency and low-bandwidth communication. Through extensive experiments, we validate the robustness of our method and the results confirm the advantages of the FIC approach over traditional impedance controllers, showcasing its potential for applications in planetary exploration and other scenarios requiring dexterous telemanipulation. This paper’s contributions include the seamless integration of FIC into multi-arm systems, the ability to perform robust interactions in highly variable environments, and the provision of a comprehensive comparison with competing approaches, thereby significantly enhancing the robustness and adaptability of robotic systems. keywords: {Training;Accuracy;Service robots;Refining;Switches;Robustness;Impedance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610410&isnumber=10609862

S. Izquierdo-Badiola, G. Canal, C. Rizzo and G. Alenyà, "PlanCollabNL: Leveraging Large Language Models for Adaptive Plan Generation in Human-Robot Collaboration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17344-17350, doi: 10.1109/ICRA57147.2024.10610055.Abstract: "Hey, robot. Let’s tidy up the kitchen. By the way, I have back pain today". How can a robotic system devise a shared plan with an appropriate task allocation from this abstract goal and agent condition? Classical AI task planning has been explored for this purpose, but it involves a tedious definition of an inflexible planning problem. Large Language Models (LLMs) have shown promising generalisation capabilities in robotics decision-making through knowledge extraction from Natural Language (NL). However, the translation of NL information into constrained robotics domains remains a challenge. In this paper, we use LLMs as translators between NL information and a structured AI task planning problem, targeting human-robot collaborative plans. The LLM generates information that is encoded in the planning problem, including specific subgoals derived from an NL abstract goal, as well as recommendations for subgoal allocation based on NL agent conditions. The framework, PlanCollabNL, is evaluated for a number of goals and agent conditions, and the results show that correct and executable plans are found in most cases. With this framework, we intend to add flexibility and generalisation to HRC plan generation, eliminating the need for a manual and laborious definition of restricted planning problems and agent models. keywords: {Pain;Large language models;Decision making;Collaboration;Manuals;Planning;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610055&isnumber=10609862

R. Pandya, M. Zhao, C. Liu, R. Simmons and H. Admoni, "Multi-Agent Strategy Explanations for Human-Robot Collaboration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17351-17357, doi: 10.1109/ICRA57147.2024.10610720.Abstract: As robots are deployed in human spaces, it is important that they are able to coordinate their actions with the people around them. Part of such coordination involves ensuring that people have a good understanding of how a robot will act in the environment. This can be achieved through explanations of the robot’s policy. Much prior work in explainable AI and RL focuses on generating explanations for single-agent policies, but little has been explored in generating explanations for collaborative policies. In this work, we investigate how to generate multi-agent strategy explanations for human-robot collaboration. We formulate the problem using a generic multi-agent planner, show how to generate visual explanations through strategy-conditioned landmark states and generate textual explanations by giving the landmarks to an LLM. Through a user study, we find that when presented with explanations from our proposed framework, users are able to better explore the full space of strategies and collaborate more efficiently with new robot partners. keywords: {Visualization;Explainable AI;Robot kinematics;Collaboration;Space exploration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610720&isnumber=10609862

A. Pupa and C. Secchi, "Efficient ISO/TS 15066 Compliance through Model Predictive Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17358-17364, doi: 10.1109/ICRA57147.2024.10610039.Abstract: In the actual industrial scenarios, human operators and robots work together sharing the workspace. Such proximity requires special attention in ensuring safety for the human operator, which is often translated in collision avoidance behaviour or high speed reduction. Adhering safety however is not the only aspect that must be taken into account. For many tasks, such as welding, it is crucial to ensure that the robot performs exactly the planned path. To optimize robot performance while complying with safety regulations, this work introduces a novel optimal nonlinear control problem. It prioritizes path preservation, exploiting redundancy to minimize task execution time, while explicitly adhering to the constraints imposed by ISO/TS 15066. To achieve high-performance outcomes, the control problem is addressed using the Model Predictive Control (MPC) approach. The proposed strategy has been experimentally validated in both simulations and a real-world industrial task involving a Kuka LWR4+ robot. keywords: {Service robots;Welding;Redundancy;Regulation;Safety;Collision avoidance;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610039&isnumber=10609862

A. Pupa, M. Minelli and C. Secchi, "A Time-Optimal Energy Planner for Safe Human-Robot Collaboration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17373-17379, doi: 10.1109/ICRA57147.2024.10611118.Abstract: The human-robot collaboration scenarios are characterized by the presence of human operators and robots that work in close contact with each other. As a consequence, the safety regulations have been updated in order to provide guidelines on how to asses safety in these new scenarios. In particular, Power and Force Limiting (PFL) collaborative mode describes how the energy should be regulated during the collaboration. Based on these guidelines, we propose a new optimal trajectory planner which, by exploiting the variability of the robot’s inertia as a function of its configuration, is able to return trajectories that can be travelled at greater speed and in less time, while guaranteeing the safety limits according to the standard. The proposed planner was validated first in simulation, comparing completion times with other state-of-the-art planning algorithms, and then experimentally, demonstrating the performance of the planned trajectories during physical interaction with the environment. Both validations confirm the effectiveness of the proposed planner, which returns shorter completion times while ensuring safe interaction. keywords: {Limiting;Force;Collaboration;Regulation;Trajectory;Safety;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611118&isnumber=10609862

Y. Long, X. Li, W. Cai and H. Dong, "Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17380-17387, doi: 10.1109/ICRA57147.2024.10611565.Abstract: Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model’s own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and sifting through in-consistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking. Our project web can be seen at the https://sites.google.com/view/discussnav. keywords: {Measurement;Visualization;Navigation;Large language models;Estimation;Predictive models;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611565&isnumber=10609862

A. Nardelli, A. Sgorbissa and C. T. Recchiuto, "Personality- and Memory-Based Software Framework for Human-Robot Interaction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17388-17394, doi: 10.1109/ICRA57147.2024.10611168.Abstract: The synergic orchestration of the cognitive and psychological dimensions characterizes human intelligence. Accordingly, carefully designing this mechanism in artificial intelligence can be a successful strategy to increase human likeness in a robot, enhancing mutual understanding and building a more natural and intuitive interaction. For this purpose, the main contribution of this work is a psychological and cognitive architecture tailored for HRI based on the interplay between robotic personality and memory-based cognitive processes. Indeed, the artificial personality manifests itself not only in various aspects of the behavior but also within the action selection process, which is closely intertwined with personality-dependent hedonic experiences linked to memories. Within this paper, we propose a task- and platform-independent framework, evaluated in a multiparty collaborative scenario. Obtained results show that a robot connected to our proposed framework is perceived as a cognitive agent capable of manifesting perceivable and distinguishable personality traits. keywords: {Software architecture;Taxonomy;Buildings;Psychology;Human-robot interaction;Collaboration;Vectors;Artificial personality;Human-Robot Interaction;Personality-adaptive architecture;Social Robotics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611168&isnumber=10609862

M. Shirasaka et al., "Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17395-17402, doi: 10.1109/ICRA57147.2024.10611640.Abstract: A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases. https://sites.google.com/view/srgpsr keywords: {Adaptation models;Adaptive systems;Service robots;Pipelines;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611640&isnumber=10609862

Y. Guo, Z. Lu, Y. Zhou and X. Jiang, "Autonomous Quilt Spreading for Caregiving Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17403-17409, doi: 10.1109/ICRA57147.2024.10610898.Abstract: In this work, we propose a novel strategy to ensure infants, who inadvertently displace their quilts during sleep, are promptly and accurately re-covered. Our approach is formulated into two subsequent steps: interference resolution and quilt spreading. By leveraging the DWPose human skeletal detection and the Segment Anything instance segmentation models, the proposed method can accurately recognize the states of the infant and the quilt over her, which involves addressing the interferences resulted from an infant’s limbs laid on part of the quilt. Building upon prior research, the EM*D deep learning model is employed to forecast quilt state transitions before and after quilt spreading actions. To improve the sensitivity of the network in distinguishing state variation of the handled quilt, we introduce an enhanced loss function that translates the voxelized quilt state into a more representative one. Both simulation and real-world experiments validate the efficacy of our method, in spreading and recover a quilt over an infant. keywords: {Instance segmentation;Image resolution;Sensitivity;Image recognition;Interference;Predictive models;Planning;Deformable Object Manipulation;Deep Learning in Robotics;Caregiving Robot},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610898&isnumber=10609862

A. Chen, H. Yu, Y. Wang and R. Xiong, "CNS: Correspondence Encoded Neural Image Servo Policy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17410-17416, doi: 10.1109/ICRA57147.2024.10611185.Abstract: Image servo is an indispensable technique in robotic applications that helps to achieve high precision positioning. The intermediate representation of image servo policy is important to sensor input abstraction and policy output guidance. Classical approaches achieve high precision but require clean keypoint correspondence, and suffer from limited convergence basin or weak feature error robustness. Recent learning-based methods achieve moderate precision and large convergence basin on specific scenes but face issues when generalizing to novel environments. In this paper, we encode keypoints and correspondence into a graph and use graph neural network as architecture of controller. This design utilizes both advantages: generalizable intermediate representation from keypoint correspondence and strong modeling ability from neural network. Other techniques including realistic data generation, feature clustering and distance decoupling are proposed to further improve efficiency, precision and generalization. Experiments in simulation and real-world verify the effectiveness of our method in speed (maximum 40fps along with observer), precision (<0.3° and sub-millimeter accuracy) and generalization (sim-to-real without fine-tuning). Project homepage (full paper with supplementary text, video and code): https://hhcaz.github.io/CNS-home. keywords: {Learning systems;Training;Three-dimensional displays;Robot sensing systems;Graph neural networks;Visual servoing;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611185&isnumber=10609862

H. Yu et al., "Adapting for Calibration Disturbances: A Neural Uncalibrated Visual Servoing Policy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17417-17423, doi: 10.1109/ICRA57147.2024.10610364.Abstract: Visual servoing (VS) is a widely used technique in industries where there are hundreds of robots, but it requires accurate camera calibration including camera intrinsic and extrinsic parameters. However, it is labour-intensive to calibrate robots one-by-one in practical use. In this paper, we propose a neural uncalibrated VS policy (NUVS) that can adapt to calibration disturbances with an adaption mechanism and a control-oriented guidance. It bridges the disturbance adaption of classical VS methods and the large convergence of learning-based VS methods. NUVS estimates the calibration embedding from past observations and servos to the desired pose under the supervision of a PBVS that can access the ground truth in simulation. With this adaption mechanism, NUVS outperforms the classical IBUVS algorithm when facing large initial camera pose offsets under the calibration disturbance. Supplementary material in: https://sites.google.com/view/neural-uncalibrated-vs keywords: {Bridges;Industries;Visualization;Service robots;Cameras;Visual servoing;Calibration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610364&isnumber=10609862

D. M. A. P. Dunuwila et al., "ARIS 1.0: An Autonomous Multitasking Medical Service Robot for Hospital Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17424-17430, doi: 10.1109/ICRA57147.2024.10610423.Abstract: Introducing robotics in the healthcare sector revolutionizes medical services by providing advanced treatments, medication management, and robotic assistance while overcoming resource limitations. In the current healthcare domain, an intermediate robotic communication platform is essential for distributing equal medical services, facilitating remote consultations, and maintaining the integrity of medical education, especially in rural areas and during pandemics. This work introduces ARIS, a multitasking medical service robot designed for telemedicine aspects and to facilitate remote medical education activities such as ward rounds. The prototype called ARIS 1.0 was developed, including a three-wheeled omnidirectional mobile platform, a torso and a novel movable neck mechanism with a face. The prototype robot can generate an online summarized report using its integrated language interaction and IoT-based vital sign extraction modules. The ROS-based semi-autonomous navigation facilitates the robot to be an assistive agent, allowing it to either accompany doctors or visit patients individually. Ultimately, ARIS 1.0 serves telepresence and novel regional language capabilities, specifically Sinhala-based self-communication features. This enables inter-party communication among doctors, medical students, and patients. The functionalities of ARIS 1.0 were validated in an emulated indoor environment to evaluate their feasibility. The results indicate that ARIS 1.0 is feasible for providing remote medical services. Furthermore, the paper discusses several promising research directions related to the proposed concept. keywords: {Torso;Hospitals;Telemedicine;Education;Prototypes;Feature extraction;Multitasking},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610423&isnumber=10609862

Z. Gong, C. Ning, J. Liang and T. Zhang, "Design, Modeling and Analysis of a Spherical Parallel Continuum Manipulator for Nursing Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17431-17437, doi: 10.1109/ICRA57147.2024.10610046.Abstract: In the healthcare industry, nursing robots have made great contributions, assisting in the delivery of food and medicine as well as the movement and transfer of patients. However, the traditional continuum manipulator often has the problems of limited workspace and weak carrying capacity. Compared with traditional manipulator, the continuum manipulator has the advantages of a small moment of inertia and high dexterity. This paper proposes a original cable-driven parallel continuum manipulator with a spherical parallel mechanism as the continuous segments. Due to the spherical parallel mechanisms’ characteristics, the proposed cable-driven spherical parallel continuum manipulator offers many inherent advantages for nursing robots. The prototype is tested and analyzed, and the kinematics and statics are verified. The results show that the cable-driven spherical parallel continuum manipulator for nursing robots has low requirements for workspace, suitable for complex spaces and can have a large carrying capacity. keywords: {Solid modeling;Medical robotics;Service robots;Prototypes;Medical services;Kinematics;Manipulators;Continuum manipulator;Cable-driven Continuum Manipulator;Spherical Parallel Continuum Manipulator;Nursing robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610046&isnumber=10609862

H. Tanaka and K. Ogata, "LeagTag: An Elongated High-Accuracy Fiducial Marker for Tight Spaces *," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17438-17444, doi: 10.1109/ICRA57147.2024.10611391.Abstract: Fiducial markers enable reliable service robot control. In human-robot coexistence environments, efficient placement of square or circular markers can be challenging due to limited space. In this study, we developed a world-first, elongated fiducial marker, capable of high-accuracy 6-DoF measurements, designed to be installable in tight spaces. We introduced two types of lenticular angle gauges to enhance pose estimation and developed new marker patterns and measurement algorithms to maintain recognition distance and accuracy. The proposed marker achieved a measurement accuracy of 0.1% position error and 0.5° orientation error. This technology will enhance the practicality and applicability of fiducial markers, contributing to the creation of robot-friendly space for future service robots. keywords: {Accuracy;Service robots;Pose estimation;Measurement uncertainty;Aerospace electronics;Position measurement;Extraterrestrial measurements},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611391&isnumber=10609862

P. Mania, S. Stelter, G. Kazhoyan and M. Beetz, "An Open and Flexible Robot Perception Framework for Mobile Manipulation Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17445-17451, doi: 10.1109/ICRA57147.2024.10610743.Abstract: Over the last years, powerful methods for solving specific perception problems such as object detection, pose estimation or scene understanding have been developed. While performing mobile manipulation actions, a robot’s perception framework needs to execute a series of these methods in a specific sequence each time it receives a new perception task. Generating proficient combinations of vision methods to solve individual perception tasks remains a challenge, as the combination depends on the requirements of the task and the capabilities of the robot’s hardware.In this paper, we propose RoboKudo, an open-source knowledge-enabled perception framework that leverages the strengths of the Unstructured Information Management (UIM) principle and the flexibility of Behavior Trees to model task-specific perception processes. The framework can combine state-of-the-art computer vision methods to satisfy the requirements of each perception task and scales to different robot platforms. The generality and effectiveness of the framework are evaluated in real world experiments where it solves various perception tasks in the context of mobile manipulation actions in a household domain. Code and additional material are available at https://robokudo.ai.uni-bremen.de/rkop. keywords: {Computer vision;Codes;Computational modeling;Pose estimation;Object detection;Data models;Information management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610743&isnumber=10609862

M. A. V. J. Muthugala, S. M. B. P. Samarakoon, R. E. Abdulkader and M. Rajesh Elara, "Toward Mass Customization of a Robot’s Morphology Design for Improving Area Coverage," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17452-17458, doi: 10.1109/ICRA57147.2024.10610203.Abstract: Floor cleaning robots have been developed to cater to building maintenance needs. Complete area coverage is crucial for a floor cleaning robot, and its morphology design plays a vital role in realizing complete area coverage. However, floor cleaning robots with fixed morphologies have difficulty in achieving a high area coverage performance. Mass customization of a robot’s morphology would improve its productivity in terms of area coverage. This paper proposes a novel system that can be used for mass customizing the morphology of a robot to improve area coverage performance in an environment of interest. The customized morphology is determined through an optimization technique by considering an environment of interest and design constraints. The area coverage of a candidate morphology design is evaluated by simulating the robot navigation in an environment of interest. Generalized pattern search, particle swarm optimization, and surrogate optimization are independently considered optimization techniques. Experiments have been conducted considering the cases of robot deployments. The statistical conclusions on experimental results validate that the proposed system can synthesize a morphology that significantly improves the area coverage performance in an environment of interest. keywords: {Productivity;Measurement;Mass customization;Navigation;Morphology;Cleaning;Particle swarm optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610203&isnumber=10609862

T. Kim, E. Song, S. An, H. Choi and K. Kong, "Leaf-Inspired FSR Array and Insole-Type Sensor Module for Mobile Three-Dimensional Ground Reaction Force Estimation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17459-17464, doi: 10.1109/ICRA57147.2024.10610058.Abstract: This paper presents an insole-type sensor module with a novel leaf-inspired force-sensitive resistor (FSR) array for accurate three-dimensional ground reaction force (GRF) estimation during human’s various motions. Joint torque analysis, essential for numerous applications in biomechanics and wearable robotics, necessitates the measurement of three-dimensional GRF vector information, traditionally achieved in indoor environments using costly force plates. To overcome these limitations, this study proposes an alternative method by incorporating FSRs on three inclined planes within the insole. A vector scaling process transforms the force values from the FSRs into the three-dimensional force vector, enabling continuous and user-independent estimation of GRF. The sensor module is integrated with machine learning, demonstrating its accuracy and usability in various motion scenarios. The results confirm the effectiveness of the leaf-inspired FSR array, giving the possibilities for portable and cost-effective motion analysis systems. keywords: {Training;Force measurement;Accuracy;Protocols;Force;Estimation;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610058&isnumber=10609862

H. Li et al., "Human-Exoskeleton Locomotion Interaction Experience Transfer: Speeding up and Improving the Performance of Preference-based Optimizations of Exoskeleton Assistance During Walking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17465-17471, doi: 10.1109/ICRA57147.2024.10611497.Abstract: Preference-based optimizing methods have shown their advantages and potential in exploring individual, comfortable, and effective control strategies and assistance parameters of exoskeletons during locomotion. Research indicates that compared with naive wearers, knowledgeable wearers with abundant exoskeleton assistance experience have obvious advantages in speeding up the parameters exploration process and improving the assistant performance. However, there is no existing method that could utilize the human-exoskeleton locomotion interaction experience (HELIE) to assist naive wearers during the exploration process. In this work, we propose a novel preference-based human-exoskeleton locomotion interaction experience transfer (LIET) framework, which could speed up the exploration of human-preferred parameters and acquire more satisfying results for naive wearers via the HELIE acquired from knowledgeable wearers. In addition, based on the proposed LIET framework, we establish the mathematical expression of the HELIE transfer during exoskeleton assistance. This will promote the research that concerns utilizing HELIE for exoskeleton control parameters optimizations in the future. Finally, experiments demonstrate the proposed LIET framework could speed up the exploration process and acquire more satisfying optimized results for naive wearers. keywords: {Legged locomotion;Exoskeletons;Process control;Optimization methods;Robotics and automation;Preference-based optimization;lower-limb exoskeleton;Interaction Experience Transfer},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611497&isnumber=10609862

S. Sengupta and J. -H. Ryu, "Design of a Knee-joint Exoskeleton to Reduce Misalignment in Both the Sagittal and Coronal Planes," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17472-17478, doi: 10.1109/ICRA57147.2024.10610400.Abstract: Many individuals experience knee dysfunctions attributed to the natural aging process and degenerative conditions. To aid individuals in regaining knee functionality, supportive exoskeletons were designed to be affixed to both the shin and thigh. However, a common issue encountered in knee exoskeletons involves the misalignment of joints between the exoskeleton and the user, resulting in discomfort and potential injuries. To reduce misalignment with the knee joint, it is essential for the thigh and shin harnesses of the exoskeleton to replicate the natural trajectories of the knee. However, achieving this is a complex task due to the shifting center of rotation of the knee in both the Sagittal and Coronal planes. Previous knee exoskeletons primarily focus on aligning the joint in the Sagittal plane, neglecting alignment in the other dimension due to inherent design constraints. For the first time, this study introduces a knee-joint exoskeleton capable of conforming to the natural movement of the knee in both the Sagittal and Coronal planes, with the aim of minimizing joint misalignment without the use of inherently soft materials. A spherical scissor linkage mechanism (SSLM) was utilized in conjunction with a customized guide rail to adjust the center of rotation of the SSLM. This configuration facilitates knee flexion/extension while accommodating the knee joint’s center of rotation in both the Sagittal and Coronal planes. The experimental outcomes demonstrated a substantial reduction in misalignment with the knee when compared to a commercial knee-support brace with a one-degree-of-freedom revolute joint. keywords: {Rails;Couplings;Exoskeletons;Thigh;Aging;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610400&isnumber=10609862

O. Bey et al., "A Novel Funnel-Based L1 Adaptive Fuzzy Approach for the Control Of An Actuated Ankle Foot Orthosis," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17487-17493, doi: 10.1109/ICRA57147.2024.10610584.Abstract: This paper introduces a novel funnel-based adaptive ${{\mathcal{L}}_1}$ fuzzy control strategy for assisting ankle joint movement during walking with the use of an actuated ankle foot orthosis (AAFO). A projection-based adaptation mechanism employing a fuzzy system is used to estimate the unknown time-varying parameters of the ${{\mathcal{L}}_1}$ control law, ensuring precise tracking of the AAFO-wearer system by the state estimator. The projection operator guarantees the convergence of the parameters while offering a limited amount of assistance torque. Funnel-based feedback control is used to mitigate the typical time lag seen when using ${{\mathcal{L}}_1}$-based approaches due to the presence of a low-pass filter commonly used in this type of approach. The effectiveness of the proposed control strategy is demonstrated through real-time experiments involving five healthy subjects. keywords: {Ankle;Legged locomotion;Fuzzy control;Torque;Trajectory tracking;Stability analysis;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610584&isnumber=10609862

Y. Chen, M. Wang and Y. Wang, "Pneumatic Back Exoskeleton for Lifting Posture Detection and Correction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17494-17500, doi: 10.1109/ICRA57147.2024.10611702.Abstract: Low back pain is a widespread issue that affects people worldwide and can lead to serious conditions such as herniated discs, spinal stenosis, or lumbar radiculopathy. Improper posture while lifting heavy weights is a common cause of back pain, especially among laborers. However, current back exoskeletons are often bulky and require electric motors, making them challenging to use and consuming significant power. Some passive exoskeletons don’t require power, but their fixed stiffness constrains normal motion. This paper presents a novel solution: a pneumatic back exoskeleton made of structured fabrics that can adjust stiffness under various air pressures. Additionally, it includes IMU sensors to detect lifting posture and correct it in real time. The exoskeleton’s effectiveness was tested through lifting experiments, demonstrating that it significantly corrects lifting posture, reduces stress on the lumbar spine, and mitigates back muscle stress. This pneumatic back exoskeleton offers a promising solution to prevent low back pain during weight-lifting tasks and provides guidance for future back exoskeleton designs. keywords: {Pain;Exoskeletons;Spine;Back;Fabrics;Real-time systems;Optical sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611702&isnumber=10609862

P. So et al., "CITR: A Coordinate-Invariant Task Representation for Robotic Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17501-17507, doi: 10.1109/ICRA57147.2024.10611312.Abstract: The basis for robotics skill learning is an adequate representation of manipulation tasks based on their physical properties. As manipulation tasks are inherently invariant to the choice of reference frame, an ideal task representation would also exhibit this property. Nevertheless, most robotic learning approaches use unprocessed, coordinate-dependent robot state data for learning new skills, thus inducing challenges regarding the interpretability and transferability of the learned models.In this paper, we propose a transformation from spatial measurements to a coordinate-invariant feature space, based on the pairwise inner product of the input measurements. We describe and mathematically deduce the concept, establish the task fingerprints as an intuitive image-based representation, experimentally collect task fingerprints, and demonstrate the usage of the representation for task classification. This representation motivates further research on data-efficient and transferable learning methods for online manipulation task classification and task-level perception. keywords: {Learning systems;Coordinate measuring machines;Robot kinematics;Taxonomy;Fingerprint recognition;Mathematical models;Libraries},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611312&isnumber=10609862

A. Rezazadeh, A. Badithela, K. Desingh and C. Choi, "SlotGNN: Unsupervised Discovery of Multi-Object Representations and Visual Dynamics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17508-17514, doi: 10.1109/ICRA57147.2024.10611588.Abstract: Learning multi-object dynamics from visual data using unsupervised techniques is challenging due to the need for robust, object representations that can be learned through robot interactions. This paper presents a novel framework with two new architectures: SlotTransport for discovering object representations from RGB images and SlotGNN for predicting their collective dynamics from RGB images and robot interactions. Our SlotTransport architecture is based on slot attention for unsupervised object discovery and uses a feature transport mechanism to maintain temporal alignment in object-centric representations. This enables the discovery of slots that consistently reflect the composition of multi-object scenes. These slots robustly bind to distinct objects, even under heavy occlusion or absence. Our SlotGNN, a novel unsupervised graph-based dynamics model, predicts the future state of multi-object scenes. SlotGNN learns a graph representation of the scene using the discovered slots from SlotTransport and performs relational and spatial reasoning to predict the future appearance of each slot conditioned on robot actions. We demonstrate the effectiveness of SlotTransport in learning object-centric features that accurately encode both visual and positional information. Further, we highlight the accuracy of SlotGNN in downstream robotic tasks, including challenging multi-object rearrangement and long-horizon prediction. Finally, our unsupervised approach proves effective in the real world. With only minimal additional data, our framework robustly predicts slots and their corresponding dynamics in real-world control tasks. Our project webpage: bit.ly/slotgnn. keywords: {Visualization;Adaptation models;Accuracy;Predictive models;Cognition;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611588&isnumber=10609862

S. Silwal et al., "What Do We Learn from a Large-Scale Study of Pre-Trained Visual Representations in Sim and Real Environments?," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17515-17521, doi: 10.1109/ICRA57147.2024.10610218.Abstract: We present a large empirical investigation on the use of pre-trained visual representations (PVRs) for training downstream policies that execute real-world tasks. Our study involves five different PVRs, each trained for five distinct manipulation or indoor navigation tasks. We performed this evaluation using three different robots and two different policy learning paradigms. From this e ort, we can arrive at three insights: 1) the performance trends of PVRs in the simulation are generally indicative of their trends in the real world, 2) the use of PVRs enables a first-of-its-kind result with indoor ImageNav (zero-shot transfer to a held-out scene in the real world), and 3) the benefits from variations in PVRs, primarily data-augmentation and fine-tuning, also transfer to the real-world performance. See project website1 for additional details and visuals. keywords: {Training;Visualization;Indoor navigation;Market research;Data augmentation;Robot learning;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610218&isnumber=10609862

K. Singh, C. Adhivarahan and K. Dantu, "L-DYNO: Framework to Learn Consistent Visual Features Using Robot’s Motion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17522-17528, doi: 10.1109/ICRA57147.2024.10611097.Abstract: Historically, feature-based approaches have been used extensively for camera-based robot perception tasks such as localization, mapping, tracking, and others. Several of these approaches also combine other sensors (inertial sensing, for example) to perform combined state estimation. Our work rethinks this approach; we present a representation learning mechanism that identifies visual features that best correspond to robot motion as estimated by an external signal. Specifically, we utilize the robot’s transformations through an external signal (inertial sensing, for example) and give attention to image space that is most consistent with the external signal. We use a pairwise consistency metric as a representation to keep the visual features consistent through a sequence with the robot’s relative pose transformations. This approach enables us to incorporate information from the robot’s perspective instead of solely relying on the image attributes. We evaluate our approach on real-world datasets such as KITTI & EuRoC and compare the refined features with existing feature descriptors. We also evaluate our method using our real robot experiment. We notice an average of 49% reduction in the image search space without compromising the trajectory estimation accuracy. Our method reduces the execution time of visual odometry by 4.3% and also reduces reprojection errors. We demonstrate the need to select only the most important features and show the competitiveness using various feature detection baselines. keywords: {Representation learning;Location awareness;Visualization;Tracking;Robot sensing systems;Feature extraction;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611097&isnumber=10609862

S. Peri, I. Lee, C. Kim, L. Fuxin, T. Hermans and S. Lee, "Point Cloud Models Improve Visual Robustness in Robotic Learners," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17529-17536, doi: 10.1109/ICRA57147.2024.10610710.Abstract: Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training – often exhibiting sharp declines in capability even for minor differences. In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies. To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies. Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts. Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training. Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners. Code: https://github.com/pvskand/pcwm keywords: {Point cloud compression;Training;Degradation;Visualization;Three-dimensional displays;Lighting;Robustness;point cloud world model;model-based reinforcement learning;vision-based robot control;robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610710&isnumber=10609862

V. Vasilopoulos, S. Garg, J. Huh, B. Lee and V. Isler, "HIO-SDF: Hierarchical Incremental Online Signed Distance Fields," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17537-17543, doi: 10.1109/ICRA57147.2024.10610367.Abstract: A good representation of a large, complex mobile robot workspace must be space-efficient yet capable of encoding relevant geometric details. When exploring unknown environments, it needs to be updatable incrementally in an online fashion. We introduce HIO-SDF, a new method that represents the environment as a Signed Distance Field (SDF). State of the art representations of SDFs are based on either neural networks or voxel grids. Neural networks are capable of representing the SDF continuously. However, they are hard to update incrementally as neural networks tend to forget previously observed parts of the environment unless an extensive sensor history is stored for training. Voxel-based representations do not have this problem but they are not space-efficient especially in large environments with fine details. HIO-SDF combines the advantages of these representations using a hierarchical approach which employs a coarse voxel grid that captures the observed parts of the environment together with high-resolution local information to train a neural network. HIO-SDF achieves a 46% lower mean global SDF error across all test scenes than a state of the art continuous representation, and a 30% lower error than a discrete representation at the same resolution as our coarse global SDF grid. Videos and code are available at: https://samsunglabs.github.io/HIO-SDF-project-page/ keywords: {Training;Motion planning;Three-dimensional displays;Neural networks;Robot sensing systems;Planning;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610367&isnumber=10609862

J. Qian, A. Panagopoulos and D. Jayaraman, "Recasting Generic Pretrained Vision Transformers As Object-Centric Scene Encoders For Manipulation Policies," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17544-17552, doi: 10.1109/ICRA57147.2024.10610131.Abstract: Generic re-usable pre-trained image representation encoders have become a standard component of methods for many computer vision tasks. As visual representations for robots however, their utility has been limited, leading to a recent wave of efforts to pre-train robotics-specific image encoders that are better suited to robotic tasks than their generic counterparts. We propose Scene Objects From Transformers, abbreviated as SOFT(•), a wrapper around pre-trained vision transformer (PVT) models that bridges this gap without any further training. Rather than construct representations out of only the final layer activations, SOFT(•) individuates and locates object-like entities from PVT attentions, and describes them with PVT activations, producing an object-centric embedding. Across standard choices of generic pre-trained vision transformers PVT, we demonstrate in each case that policies trained on SOFT(PVT) far outstrip standard PVT representations for manipulation tasks in simulated and real settings, approaching the state-of-the-art robotics-aware representations. Code, appendix and videos: https://sites.google.com/view/robot-soft/ keywords: {Training;Computer vision;Visualization;Codes;Performance gain;Image representation;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610131&isnumber=10609862

J. Song, K. Yang, Z. Zhang, M. Li, T. Cao and M. Ghaffari, "Iterative PnP and its application in 3D-2D vascular image registration for robot navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17560-17566, doi: 10.1109/ICRA57147.2024.10610392.Abstract: This paper reports on a new real-time robotcentered 3D-2D vascular image alignment algorithm, which is robust to outliers and can align nonrigid shapes. Few works have managed to achieve both real-time and accurate performance for vascular intervention robots. This work bridges high-accuracy 3D-2D registration techniques and computational efficiency requirements in intervention robot applications. We categorize centerline-based vascular 3D-2D image registration problems as an iterative Perspective-n-Point (PnP) problem and propose using the Levenberg-Marquardt solver on the Lie manifold. Then, the recently developed Reproducing Kernel Hilbert Space (RKHS) algorithm is introduced to overcome the "big-to-small" problem in typical robotic scenarios. Finally, an iterative reweighted least squares is applied to solve RKHSbased formulation efficiently. Experiments indicate that the proposed algorithm processes registration over 50 Hz (rigid) and 20 Hz (nonrigid) and obtains competing registration accuracy similar to other works. Results indicate that our Iterative PnP is suitable for future vascular intervention robot applications. keywords: {Image registration;Accuracy;Portable computers;Navigation;Shape;Real-time systems;Iterative algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610392&isnumber=10609862

J. Francoeur, D. Lezcano, Y. Zhetpissov, R. Kashyap, I. Iordachita and S. Kadoury, "Fully Distributed Shape Sensing of a Flexible Surgical Needle Using Optical Frequency Domain Reflectometry for Prostate Interventions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17594-17601, doi: 10.1109/ICRA57147.2024.10610256.Abstract: In minimally invasive procedures such as biopsies and prostate cancer brachytherapy, accurate needle placement remains challenging due to limitations in current tracking methods related to interference, reliability, resolution or image contrast. This often leads to frequent needle adjustments and reinsertions. To address these shortcomings, we introduce an optimized needle shape-sensing method using a fully distributed grating-based sensor. The proposed method uses simple trigonometric and geometric modeling of the fiber using optical frequency domain reflectometry (OFDR), without requiring prior knowledge of tissue properties or needle deflection shape and amplitude. Our optimization process includes a reproducible calibration process and a novel tip curvature compensation method. We validate our approach through experiments in artificial isotropic and inhomogeneous animal tissues, establishing ground truth using 3D stereo vision and cone beam computed tomography (CBCT) acquisitions, respectively. Our results yield an average RMSE ranging from 0.58 ± 0.21 mm to 0.66 ± 0.20 mm depending on the chosen spatial resolution, achieving the submillimeter accuracy required for interventional procedures. keywords: {Optical fiber sensors;Accuracy;Three-dimensional displays;Shape;Reflectometry;Needles;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610256&isnumber=10609862

C. Oldemeyer, M. Hellerer, M. Reiner, B. Thiele, P. Weber and T. Bellmann, "MoRC—A Modular Robot Controller," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17643-17649, doi: 10.1109/ICRA57147.2024.10611553.Abstract: MoRC is a high-performance modular robot controller based on the Functional Mock-up Interface (FMI) standard. The goal is to control any (industrial) robot with electrical drives using a customizable vendor-agnostic control cabinet and an innovative, self-developed software architecture based on exchangeable multi-rate real-time control components with standardized interfaces. On the hardware side, the use of EtherCAT (Ethernet for Control Automation Technology) allows connecting a freely selectable number of COTS (commercial off-the-shelf) electrical drives and sensors. On the software side, this is matched with exchangeable control software modules based on the FMI standard. Those can be interconnected for forming user-defined multi-rate control structures which can be executed as synchronized real-time threads on a central Linux-based multi-core computing unit. That unlocks additional computational potential for advanced high-frequency control algorithms. Control structures can be switched at runtime to handle highly diverse control tasks. This paper presents the architectural concepts as well as first experiments on an industrial robot testbed. keywords: {Service robots;Software architecture;Switches;Industrial robots;Control systems;Software;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611553&isnumber=10609862

J. -P. Busch, L. Reiher and L. Eckstein, "Enabling the Deployment of Any-Scale Robotic Applications in Microservice Architectures through Automated Containerization*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17650-17656, doi: 10.1109/ICRA57147.2024.10611586.Abstract: In an increasingly automated world – from ware-house robots to self-driving cars – streamlining the development and deployment process and operations of robotic applications becomes ever more important. Automated DevOps processes and microservice architectures have already proven successful in other domains such as large-scale customer-oriented web services (e.g., Netflix). We recommend to employ similar microservice architectures for the deployment of small- to large-scale robotic applications in order to accelerate development cycles, loosen functional dependence, and improve resiliency and elasticity. In order to facilitate involved DevOps processes, we present and release a tooling suite for automating the development of microservices for robotic applications based on the Robot Operating System (ROS). Our tooling suite covers the automated minimal containerization of ROS applications, a collection of useful machine learning-enabled base container images, as well as a CLI tool for simplified interaction with container images during the development phase. Within the scope of this paper, we embed our tooling suite into the overall context of streamlined robotics deployment and compare it to alternative solutions. We release our tools as open-source software at github.com/ika-rwth-aachen/dorotos. keywords: {DevOps;Operating systems;Microservice architectures;Computer architecture;Containers;Streaming media;Stakeholders},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611586&isnumber=10609862

M. Mayer, J. Külz and M. Althoff, "CoBRA: A Composable Benchmark for Robotics Applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17665-17671, doi: 10.1109/ICRA57147.2024.10610776.Abstract: Selecting an optimal robot, its base pose, and trajectory for a given task is currently mainly done by human expertise or trial and error. To evaluate automatic approaches to this combined optimization problem, we introduce a benchmark suite encompassing a unified format for robots, environments, and task descriptions. Our benchmark suite is especially useful for modular robots, where the multitude of robots that can be assembled creates a host of additional parameters to optimize. We include tasks such as machine tending and welding in synthetic environments and 3D scans of real-world machine shops. All benchmarks are accessible through cobra.cps.cit.tum.de, a platform to conveniently share, reference, and compare tasks, robot models, and solutions. keywords: {Three-dimensional displays;Service robots;Welding;Benchmark testing;Planning;Trajectory;Machine shops},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610776&isnumber=10609862

H. H. Erwich, B. P. Duisterhof and G. C. H. E. de Croon, "GSL-Bench: High Fidelity Gas Source Localization Benchmarking Tool," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17672-17678, doi: 10.1109/ICRA57147.2024.10610755.Abstract: Gas Source Localization (GSL) is a challenging field of research within the robotics community, with high-stakes search-and-rescue applications. Existing methods vary widely and each has its strengths and weaknesses. Comparisons of different methods are limited due to the lack of a broadly adopted and standardized testing methodology. Existing GSL evaluations vary in environment size, wind conditions, and gas simulation fidelity. They also lack photo-realistic rendering for the integration of obstacle avoidance. In this paper, we propose GSL-Bench, a benchmarking tool that can evaluate the performance of existing GSL algorithms. GSL-Bench features high-fidelity graphics and gas simulation, featuring NVIDIA’s® Isaac Sim and OpenFOAM computational fluid dynamics software (CFD). Realism is further increased by simulating relevant gas and wind sensors. Scene generation is simplified with the introduction of AutoGDM+, capable of procedural environment generation, CFD and particle-based gas dispersion simulation. To illustrate GSL-Bench’s capabilities, three algorithms are compared in six warehouse settings of increasing complexity: E. Coli, dung beetle, and a random walker. Our results demonstrate GSL-Bench’s ability to provide valuable insights into algorithm performance.Site: https://sites.google.com/view/gslbench/ keywords: {Location awareness;Visualization;Computational fluid dynamics;Heuristic algorithms;Software algorithms;Escherichia coli;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610755&isnumber=10609862

A. Mavrogiannis, C. Mavrogiannis and Y. Aloimonos, "Cook2LTL: Translating Cooking Recipes to LTL Formulae using Large Language Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17679-17686, doi: 10.1109/ICRA57147.2024.10611086.Abstract: Cooking recipes are challenging to translate to robot plans as they feature rich linguistic complexity, temporally-extended interconnected tasks, and an almost infinite space of possible actions. Our key insight is that combining a source of cooking domain knowledge with a formalism that captures the temporal richness of cooking recipes could enable the extraction of unambiguous, robot-executable plans. In this work, we use Linear Temporal Logic (LTL) as a formal language expressive enough to model the temporal nature of cooking recipes. Leveraging a pretrained Large Language Model (LLM), we present Cook2LTL, a system that translates instruction steps from an arbitrary cooking recipe found on the internet to a set of LTL formulae, grounding high-level cooking actions to a set of primitive actions that are executable by a manipulator in a kitchen environment. Cook2LTL makes use of a caching scheme that dynamically builds a queryable action library at runtime. We instantiate Cook2LTL in a realistic simulation environment (AI2-THOR), and evaluate its performance across a series of cooking recipes. We demonstrate that our system significantly decreases LLM API calls (−51%), latency (−59%), and cost (−42%) compared to a baseline that queries the LLM for every newly encountered action at runtime. keywords: {Runtime;Grounding;Large language models;Formal languages;Linguistics;Feature extraction;Libraries},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611086&isnumber=10609862

A. Macaluso, N. Cote and S. Chitta, "Toward Automated Programming for Robotic Assembly Using ChatGPT," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17687-17693, doi: 10.1109/ICRA57147.2024.10610554.Abstract: Despite significant technological advancements, the process of programming robots for adaptive assembly remains labor-intensive, demanding expertise in multiple domains and often resulting in task-specific, inflexible code. This work explores the potential of Large Language Models (LLMs), like ChatGPT, to automate this process, leveraging their ability to understand natural language instructions, generalize examples to new tasks, and write code. In this paper, we suggest how these abilities can be harnessed and applied to real-world challenges in the manufacturing industry. We present a novel system that uses ChatGPT to automate the process of programming robots for adaptive assembly by decomposing complex tasks into simpler subtasks, generating robot control code, executing the code in a simulated workcell, and debugging syntax and control errors, such as collisions. We outline the architecture of this system and strategies for task decomposition and code generation. Finally, we demonstrate how our system can autonomously program robots for various assembly tasks in a real-world project. keywords: {Robotic assembly;Codes;Debugging;Chatbots;Encoding;Cognition;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610554&isnumber=10609862

P. Stoop, T. Ratnayake and G. Toffetti, "A Method for Multi-Robot Asynchronous Trajectory Execution in MoveIt2," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17694-17700, doi: 10.1109/ICRA57147.2024.10611498.Abstract: This paper introduces a method that enables the parallel independent execution of trajectories for multi-robot / multi-arm systems in a shared workspace in MoveIt2. The proposed method leverages a centralized scheduler in a distributed set up to prevent collisions while the robots move independently. We argue that this approach is better suited than the state of the art (i.e., synchronous execution) for flexible/adaptive robotic tasks where the actions to be performed may vary in planning and execution time depending on sensor data (e.g., pick and place with inspection, assembly) as it is able to reduce the total execution time w.r.t. current approaches leveraging a single arm or multiple arms with synchronous motion planning. keywords: {Inspection;Robot sensing systems;Manipulators;Trajectory;Planning;Collision avoidance;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611498&isnumber=10609862

E. Sani, A. Sgorbissa and S. Carpin, "Improving the ROS 2 Navigation Stack with Real-Time Local Costmap Updates for Agricultural Applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17701-17707, doi: 10.1109/ICRA57147.2024.10610984.Abstract: The ROS 2 Navigation Stack (Nav2) has emerged as a widely used software component providing the underlying basis to develop a variety of high-level functionalities. However, when used in outdoor environments such as orchards and vineyards, its functionality is notably limited by the presence of obstacles and/or situations not commonly found in indoor settings. One such example is given by tall grass and weeds that can be safely traversed by a robot, but that can be perceived as obstacles by LiDAR sensors, and then force the robot to take longer paths to avoid them, or abort navigation altogether. To overcome these limitations, domain specific extensions must be developed and integrated into the software pipeline. This paper presents a new, lightweight approach to address this challenge and improve outdoor robot navigation. Leveraging the multi-scale nature of the costmaps supporting Nav2, we developed a system that using a depth camera performs pixel level classification on the images, and in real time injects corrections into the local cost map, thus enabling the robot to traverse areas that would otherwise be avoided by the Nav2. Our approach has been implemented and validated on a Clearpath Husky and we demonstrate that with this extension the robot is able to perform navigation tasks that would be otherwise not practical with the standard components. keywords: {Training;Image segmentation;Visualization;Systematics;Navigation;Pipelines;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610984&isnumber=10609862

W. Dai et al., "Automated Non-invasive Analysis of Motile Sperms Using Cross-scale Guidance Network," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17708-17714, doi: 10.1109/ICRA57147.2024.10611526.Abstract: Unbiased measurement of sperm morphometric and motility parameters is essential for assessing fertility potential and guiding visual feedback for microrobotic manipulation. Automated analysis of multiple sperms and selection of an optimal sperm is crucial for in vitro fertilisation treatment such as robotic intracytoplasmic sperm injection. However, conventional image processing methods have limitations in analysing small sperm objects under microscopic imaging. The emergence of convolutional neural networks (CNNs) has offered promising advancements in microscopic image analysis. However, previous CNN methods have struggled to accurately segment tiny objects, requiring staining or fluorescence techniques to enhance visual contrast between sperm and culture medium, leading to clinical impracticality. To address these limitations, we introduce a novel segmentation network named the cross-scale guidance (CSG) network for accurate and efficient segmentation of minute sperm objects. The CSG network employs innovative modules, including collateral multi-scale convolution, cross-scale feature map guide, and multi-scale feature fusion, to preserve essential sperm details despite their small size. Experimental results indicate that the CSG network surpassed the state-of-the-art models designed for small object segmentation, achieving a significant increase up to 18.62% higher mean intersection over union (mIoU). Additionally, the CSG network excelled in sperm morphometric analysis, achieving errors below 20%. Moreover, sperm motility parameters were further derived from the segmentation results for comprehensive sperm fertility analysis. keywords: {Visualization;Image segmentation;Accuracy;Image analysis;Microscopy;Morphology;Object segmentation;Automation at micro/nano scale;microrobotics;deep learning;sperm analysis;in vitro fertilisation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611526&isnumber=10609862

K. Otani, H. Sugiura, S. Watanabe, T. Bilal, S. Amaya and F. Arai, "Robotic capillary insertion to the Xenopus oocyte using microscopic image analysis and QCR force sensor," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17723-17728, doi: 10.1109/ICRA57147.2024.10610363.Abstract: This paper presented the three-dimensional oocyte manipulation system for the two-electrode voltage clamp (TEVC) experiment under stereomicroscopy. We firstly developed a sequential calibration method to correlate the workspace of the stereomicroscopy with the image and the micromanipulator. Even though the focal depth of the microscopy was limited, the proposed method functioned the three-dimensional position detection and calculated the homogeneous transformation matrix. We secondly employed hybrid use of the image-based manipulation and the quartz crystal resonator (QCR) force sensor. The imaging technique was used to detect the tip of the glass capillary and the contact to the cell membrane, whereas the QCR force sensor was incorporated to detect the force interaction between the sample and the glass capillary. Using the system and proposed technique, we demonstrated the automatic capillary insertion for TEVC experiment, at which the low insertion depth was preferable. The results indicated that the coordination calibration technique provided the positioning accuracy of the capillary tip on the order of 10 μm. The imaging technique could detect the contact to the elastic objects and cell membrane. QCR force sensor achieved quite small force measurement and feedback control at the control frequency of 100 Hz without latency. keywords: {Image analysis;Robot kinematics;Microscopy;Force;Glass;Robot sensing systems;Biomembranes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610363&isnumber=10609862

F. R. Leiro, S. Régnier, F. Delarue and M. Boudaoud, "Robotic Mosaic Atomic Force Microscopy Through Sequential Imaging and Multiview Iterative Closest Points Method," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17729-17735, doi: 10.1109/ICRA57147.2024.10610473.Abstract: This paper presents a functionality that has been developed for the home-made AFM-in-SEM robotic system at the ISIR laboratory. The method allows extending the range of an Atomic Force Microscope (AFM) and dealing with drift issues by fusing multiple individually AFM topography patches. The merging of the patches into a single image is done through a Generalized Procrustes Analysis Iterative Closest Point (GPA-ICP) algorithm. To validate the effectiveness of the approach, an AFM image of a TGX1 calibration grid and a 3.4billion-year-old organic-walled microfossil are reconstructed by automatically merging 50 AFM elementary topography patches of dimension 0.9 µm × 1.2 µm based on feature matching. The overlap between two adjacent patches is 50 % and 33 % in the X and Y axes respectively. The result is a coherent 3.2 µm × 3.0 µm drift-free long range AFM topography without significant artifacts. The method is tested using an AFM-in-SEM system based on a 3-DOF cartesian robot equipped with inertial piezoelectric actuators. This method can be used to extend the range of any type of AFM with a dual XY stage setup. Thus, it opens the door for high-resolution long-range AFM by adding a long-range coarse resolution stage to a preexisting AFM system all without needing to actuate both stages simultaneously. keywords: {Three-dimensional displays;Merging;Force;LoRa;Surfaces;Surface topography;Calibration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610473&isnumber=10609862

H. Song et al., "Automated Sperm Immobilization with a Clinically-Compatible and Compact XYZ Stage," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17736-17742, doi: 10.1109/ICRA57147.2024.10611426.Abstract: Automated positioning systems play a pivotal role in micro-scale cell manipulation. In clinical intracytoplasmic sperm injection (ICSI) of in vitro fertilization (IVF) treatment, a motile sperm needs to be immobilized by glass micropipette tapping for subsequent surgical steps. The process requires accurate tracking of the target sperm and precise alignment between the sperm tail and the micropipette. Manual sperm immobilization suffers from inconsistent success rates, and current robotic systems developed for the task fail to comply with the standard clinical setup. Instead of using a motorized micromanipulator as in existing robotic systems, this paper presents an automated, compact three-dimensional positioning stage for sperm immobilization that can be seamlessly integrated into standard clinical platforms. Based on the analysis of the sperm head orientation, an adaptive tail tapping planning strategy is established to avoid the risk of touching the sperm head where DNA is contained. A visual servo controller equipped with a dynamic sperm motion observer is employed to achieve precise tracking and positioning of the target sperm three-dimensionally. Experimental results revealed the system achieved a success rate of 93.5% and a time cost of 5.5 s for automated sperm immobilization. keywords: {Visualization;Target tracking;Costs;Micromanipulators;Tail;Observers;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611426&isnumber=10609862

W. Chen et al., "Automated Sperm Morphology Analysis Based on Instance-Aware Part Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17743-17749, doi: 10.1109/ICRA57147.2024.10611339.Abstract: Traditional sperm morphology analysis is based on tedious manual annotation. Automated morphology analysis of a high number of sperm requires accurate segmentation of each sperm part and quantitative morphology evaluation. State-of-the-art instance-aware part segmentation networks follow a "detect-then-segment" paradigm. However, due to sperm’s slim shape, their segmentation suffers from large context loss and feature distortion due to bounding box cropping and resizing during ROI Align. Moreover, morphology measurement of sperm tail is demanding because of the long and curved shape and its uneven width. This paper presents automated techniques to measure sperm morphology parameters automatically and quantitatively. A novel attention-based instance-aware part segmentation network is designed to reconstruct lost contexts outside bounding boxes and to fix distorted features, by refining preliminary segmented masks through merging features extracted by feature pyramid network. An automated centerline-based tail morphology measurement method is also proposed, in which an outlier filtering method and endpoint detection algorithm are designed to accurately reconstruct tail endpoints. Experimental results demonstrate that the proposed network outperformed the state-of-the-art top-down RP-R-CNN by 9.2% ${\mathbf{AP}}_{vol}^p$, and the proposed automated tail morphology measurement method achieved high measurement accuracies of 95.34%,96.39%,91.20% for length, width and curvature, respectively. keywords: {Accuracy;Shape;Shape measurement;Morphology;Tail;Manuals;Length measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611339&isnumber=10609862

Y. Luo, Y. Liu, J. Zhou, S. -L. Chen, Y. Guo and G. -Z. Yang, "Fast Photoacoustic Microscopy with Robot Controlled Microtrajectory Optimization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17750-17756, doi: 10.1109/ICRA57147.2024.10610085.Abstract: Photoacoustic Microscopy (PAM) is a relatively new imaging modality in biomedicine. However, point-by-point raster scanning in PAM suffers from low imaging speed. Sparse sampling has been studied in recent years and with the development of deep learning algorithms, extensive efforts have been devoted to sparse image reconstruction while little attention has been paid to sparse sampling trajectory design required for actual implementation. The use of real-time adaptive robotically controlled sampling with micro-scale accuracy with due consideration of physical constraints can pave the way for using PAM for robot-assisted microsurgery. This work proposes a fast PAM scheme with robot-controlled microtrajectory optimization. The proposed method is adaptive to imaging details of different regions of interest (ROI) and detailed experiments have been conducted on both simulation and in-vivo settings. Results show that our proposed method can achieve faster scanning speed than traditional raster scanning and improved image quality in ROI than the standard spiral trajectory, which demonstrates the effectiveness of our proposed method and its potential to be deployed in other point-by-point scanning systems. keywords: {Image quality;Deep learning;Spirals;Microscopy;Microsurgery;Real-time systems;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610085&isnumber=10609862

Z. Zuo et al., "Acoustically Driven Micropipette for Hydrodynamic Manipulation of Mouse Oocytes," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17757-17762, doi: 10.1109/ICRA57147.2024.10610242.Abstract: Micromanipulation techniques that can achieve controlled fine operations at the micro scale play an important role in biomedical fields including embryo engineering, gene engineering, drug screening, and cell analysis. However, micromanipulation of biological micro-objects, such as cells and micro tissues, suffers from mechanical damage and low efficiency. Several techniques have been introduced to manipulate cells more easily, but most of them are restricted by expensive devices, limited work area, and potential damage to cellular structure. Here we develop a hydrodynamic manipulation method to rotate and transport mouse oocytes, which utilizes acoustic waves and micropipette to generate acoustic radiation force and excite microstreaming. This method can accomplish rotational and translational operations precisely and controllably. We tested the process of trapping, rotation, and transportation of the mouse oocytes, and measured rotational and translational speed with a range of applied voltage. The method was able to shorten the cost time of delivery and posture adjustment before oocyte injection. Our study provides an easy-to-use technique for oocyte manipulation without contact, and it has the potential to be universally applied in many cellular studies. keywords: {Visualization;Voltage measurement;Three-dimensional displays;Force;Transportation;Hydrodynamics;Biology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610242&isnumber=10609862

L. -J. W. Ligtenberg et al., "Remote Control of Untethered Magnetic Robots within a Lumen using X-Ray-Guided Robotic Platform," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17763-17769, doi: 10.1109/ICRA57147.2024.10611161.Abstract: Until now, the potential of untethered magnetic robots (UMRs), propelled by external time-periodic magnetic fields, has been hindered by the limitations of wireless manipulation systems or noninvasive imaging techniques combined. The need for simultaneous actuation and noninvasive localization imposes a strict constraint on both functionalities. This study addresses this challenge by substantiating the feasibility through experimental validation, showcasing the direct teleoperation of UMRs within a fluid-filled lumen. This teleoperation capability is facilitated by a scalable X-ray-guided robotic platform, extendable to match the dimensions required for in vivo applications, marking a noteworthy advancement. Our methodology is demonstrated by teleoperating a 12-mm-long screw-shaped UMR (5 mm in diameter) within a bifurcated lumen, filled with blood. This navigation is achieved using controlled rotating magnetic fields, guided by real-time X-ray Fluoroscopy images. Incorporating a two-degree-of-freedom control system, we demonstrate the operator’s capability to use X-ray Fluoroscopy images to keep the UMR coupled with the external field during wireless teleoperations, resulting in a success rate of 76.6% when moving along the intended pathways, with a mean absolute position error of 1.6 ± 2.1 mm. keywords: {Wireless communication;Navigation;Magnetic resonance imaging;Lumen;Control systems;Real-time systems;Magnetic fields},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611161&isnumber=10609862

F. P. Audonnet, J. Grizou, A. Hamilton and G. Aragon-Camarasa, "TELESIM: A Modular and Plug-and-Play Framework for Robotic Arm Teleoperation using a Digital Twin," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17770-17777, doi: 10.1109/ICRA57147.2024.10610935.Abstract: Teleoperating robotic arms can be a challenging task for non-experts, particularly when using complex control devices or interfaces. To address the limitations and challenges of existing teleoperation frameworks, such as cognitive strain, control complexity, robot compatibility, and user evaluation, we propose TELESIM, a modular and plug-and-play framework that enables direct teleoperation of any robotic arm using a digital twin as the interface between users and the robotic system. Due to TELESIM’s modular design, it is possible to control the digital twin using any device that outputs a 3D pose, such as a virtual reality controller or a finger-mapping hardware controller. To evaluate the efficacy and user-friendliness of TELESIM, we conducted a user study with 37 participants. The study involved a simple pick-and-place task, which was performed using two different robots equipped with two different control modalities. Our experimental results show that most users were able to succeed by building at least a tower of 3 cubes in 10 minutes, with only 5 minutes of training beforehand, regardless of the control modality or robot used, demonstrating the usability and user-friendliness of TELESIM. keywords: {Training;Performance evaluation;Three-dimensional displays;Poles and towers;Virtual reality;Manipulators;Digital twins},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610935&isnumber=10609862

J. Lee, S. Park, J. Park, K. Lee and S. Choi, "SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17786-17792, doi: 10.1109/ICRA57147.2024.10611613.Abstract: Pick-and-place is one of the fundamental tasks in robotics research. However, the attention has been mostly focused on the "pick" task, leaving the "place" task relatively unexplored. In this paper, we address the problem of placing objects in the context of a teleoperation framework. Particularly, we focus on two aspects of the place task: stability robustness and contextual reasonableness of object placements. Our proposed method combines simulation-driven physical stability verification via real-to-sim and the semantic reasoning capability of large language models. In other words, given place context information (e.g., user preferences, object to place, and current scene information), our proposed method outputs a probability distribution over the possible placement candidates, considering the robustness and reasonableness of the place task. Our proposed method is extensively evaluated in two simulation and one real world environments and we show that our method can greatly increase the physical plausibility of the placement as well as contextual soundness while considering user preferences. Code, video, and details are available at: https://joonhyunglee.github.io/spots/ keywords: {Feedback loop;Large language models;Semantics;Stability analysis;Cognition;Robustness;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611613&isnumber=10609862

G. Cortigiani, B. Brogi, A. Villani, T. L. Baldi, N. D’Aurizio and D. Prattichizzo, "Online Minimization of the Robot Silhouette Viewed From Eye-to-Hand Camera," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17793-17799, doi: 10.1109/ICRA57147.2024.10610007.Abstract: Redundant robots have the potential to perform internal joints motion without modifying the pose of the end-effector by exploiting the null-space of the Jacobian matrix. Capitalizing on that feature, we developed a control technique for minimizing the robot visual appearance when observed from an eye-to-hand camera. Such algorithm is instrumental in contexts where quickly adjusting the perspective to see objects obstructed by the robot is impractical (e.g., teleoperation in narrow environment). Diminished reality techniques are frequently employed in these cases to mitigate the robot intrusion into the environment, although these techniques may sometimes compromise the perceived realism. The experimental evaluation confirmed the effectiveness of our control algorithm, demonstrating an average reduction of 4.67% of the area covered by the robot within the frame when compared to the case without the optimization action. keywords: {Jacobian matrices;Visualization;Instruments;Robot vision systems;Cameras;Minimization;End effectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610007&isnumber=10609862

F. C. Weigend, X. Liu, S. Sonawani, N. Kumar, V. Vasudevan and H. Ben Amor, "iRoCo: Intuitive Robot Control From Anywhere Using a Smartwatch," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17800-17806, doi: 10.1109/ICRA57147.2024.10610805.Abstract: This paper introduces iRoCo (intuitive Robot Control) – a framework for ubiquitous human-robot collaboration using a single smartwatch and smartphone. By integrating probabilistic differentiable filters, iRoCo optimizes a combination of precise robot control and unrestricted user movement from ubiquitous devices. We demonstrate and evaluate the effectiveness of iRoCo in practical teleoperation and drone piloting applications. Comparative analysis shows no significant difference between task performance with iRoCo and gold-standard control systems in teleoperation tasks. Additionally, iRoCo users complete drone piloting tasks 32% faster than with a traditional remote control and report less frustration in a subjective load index questionnaire. Our findings strongly suggest that iRoCo is a promising new approach for intuitive robot control through smartwatches and smart-phones from anywhere, at any time. The code is available at www.github.com/wearable-motion-capture keywords: {Performance evaluation;Wearable Health Monitoring Systems;Robot control;Collaboration;Probabilistic logic;Motion capture;Kalman filters},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610805&isnumber=10609862

P. Naughton, J. S. Nam, A. Stratton and K. Hauser, "Integrating Open-World Shared Control in Immersive Avatars," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17807-17813, doi: 10.1109/ICRA57147.2024.10611618.Abstract: Teleoperated avatar robots allow people to transport their manipulation skills to environments that may be difficult or dangerous to work in. Current systems are able to give operators direct control of many components of the robot to immerse them in the remote environment, but operators still struggle to complete tasks as competently as they could in person. We present a framework for incorporating open-world shared control into avatar robots to combine the benefits of direct and shared control. This framework preserves the fluency of our avatar interface by minimizing obstructions to the operator’s view and using the same interface for direct, shared, and fully autonomous control. In a human subjects study (N=19), we find that operators using this framework complete a range of tasks significantly more quickly and reliably than those that do not. keywords: {Avatars;Affordances;Grasping;Reliability;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611618&isnumber=10609862

M. Cai, K. Patel, S. Iba and S. Li, "Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17814-17820, doi: 10.1109/ICRA57147.2024.10610388.Abstract: In human-robot collaboration, shared control presents an opportunity to teleoperate robotic manipulation to improve the efficiency of manufacturing and assembly processes. Robots are expected to assist in executing the user’s intentions. To this end, robust and prompt intention estimation is needed, relying on behavioral observations. The framework presents an intention estimation technique at hierarchical levels i.e., low-level actions and high-level tasks, by incorporating multi-scale hierarchical information in neural networks. Technically, we employ hierarchical dependency loss to boost overall accuracy. Furthermore, we propose a multi-window method that assigns proper hierarchical prediction windows of input data. An analysis of the predictive power with various inputs demonstrates the predominance of the deep hierarchical model in the sense of prediction accuracy and early intention identification. We implement the algorithm on a virtual reality (VR) setup to teleoperate robotic hands in a simulation with various assembly tasks to show the effectiveness of online estimation. Video demonstration is available at: https://youtu.be/CMYDgcI4j1g. keywords: {Solid modeling;Accuracy;Estimation;Process control;Virtual reality;Predictive models;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610388&isnumber=10609862

B. Ibrahim, I. H. Elhajj and D. Asmar, "3D Autocomplete: Enhancing UAV Teleoperation with AI in the Loop," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17829-17835, doi: 10.1109/ICRA57147.2024.10610932.Abstract: Manually teleoperating a flying robot can be a demanding task, especially for users with limited levels of experience. This is primarily due to the non-linear properties of such robots in addition to the difficulty of controlling various degrees of freedom at the same time. 3D Autocomplete helps mitigate such limitations by assisting the users in teleoperation. It aids in teleoperating 3D motions, such as helical motions, which are more challenging to the users. The proposed framework uses Artificial Intelligence (AI) to predict just-in-time the user’s intended motion and then, if the user accepts, completes it autonomously in 3D. The AI component of 3D Autocomplete was presented in our previous work, where we introduced a deep learning model and an algorithm to predict as early as possible the user’s desired motion. Moving forward in this work, we focus on synthesizing and completing the user-intended motion autonomously. Also, we introduce a Mixed Reality (MR) user interface for better human-robot interaction. Finally, we evaluate our system subjectively and objectively through human-subject experiments. Autocomplete outperformed traditional method on all criteria with at least 30% improvement in all objective measures. keywords: {Solid modeling;Three-dimensional displays;Mixed reality;Virtual reality;Predictive models;Prediction algorithms;Artificial intelligence},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610932&isnumber=10609862

S. Zhou, S. Papatheodorou, S. Leutenegger and A. P. Schoellig, "Control-Barrier-Aided Teleoperation with Visual-Inertial SLAM for Safe MAV Navigation in Complex Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17836-17842, doi: 10.1109/ICRA57147.2024.10611280.Abstract: In this paper, we consider a Micro Aerial Vehicle (MAV) system teleoperated by a non-expert and introduce a perceptive safety filter that leverages Control Barrier Functions (CBFs) in conjunction with Visual-Inertial Simultaneous Localization and Mapping (VI-SLAM) and dense 3D occupancy mapping to guarantee safe navigation in complex and unstructured environments. Our system relies solely on onboard IMU measurements, stereo infrared images, and depth images and autonomously corrects teleoperated inputs when they are deemed unsafe. We define a point in 3D space as unsafe if it satisfies either of two conditions: (i) it is occupied by an obstacle, or (ii) it remains unmapped. At each time step, an occupancy map of the environment is updated by the VI-SLAM by fusing the onboard measurements, and a CBF is constructed to parameterize the (un)safe region in the 3D space. Given the CBF and state feedback from the VI-SLAM module, a safety filter computes a certified reference that best matches the teleoperation input while satisfying the safety constraint encoded by the CBF. In contrast to existing perception-based safe control frameworks, we directly close the perception-action loop and demonstrate the full capability of safe control in combination with real-time VI-SLAM without any external infrastructure or prior knowledge of the environment. We verify the efficacy of the perceptive safety filter in real-time MAV experiments using exclusively onboard sensing and computation and show that the teleoperated MAV is able to safely navigate through unknown environments despite arbitrary inputs sent by the teleoperator. keywords: {Teleoperators;State feedback;Three-dimensional displays;Simultaneous localization and mapping;Navigation;Real-time systems;Time measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611280&isnumber=10609862

T. A. Kent and S. Bergbreiter, "Flow Shadowing: A Method to Detect Multiple Flow Headings using an Array of Densely Packed Whisker-inspired Sensors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17843-17849, doi: 10.1109/ICRA57147.2024.10610885.Abstract: Understanding airflow around a drone is critical for performing advanced maneuvers while maintaining flight stability. Recent research has worked to understand this flow by employing 2D and 3D flow sensors to measure flow from a single source like wind or the drone’s relative motion. Our current work advances flow detection by introducing a strategy to distinguish between two flow sources applied simultaneously from different directions. By densely packing an array of flow sensors (or whiskers), we alter the path of airflow as it moves through the array. We have named this technique “flow shadowing” because we take advantage of the fact that a downstream whisker shadowed (or occluded) by an upstream whisker receives less incident flow. We show that this relationship is predictable for two whiskers based on the percent of occlusion. We then show that a 2x2 spatial array of whiskers responds asymmetrically when multiple flow sources from different headings are applied to the array. This asymmetry is direction-dependent, allowing us to predict the headings of flow from two different sources, like wind and a drone’s relative motion. keywords: {Three-dimensional displays;Sensor phenomena and characterization;Prediction algorithms;Stability analysis;Motion measurement;Wind forecasting;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610885&isnumber=10609862

X. Chen, Y. Zhang, B. Zhou and S. Shen, "APACE: Agile and Perception-aware Trajectory Generation for Quadrotor Flights," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 17858-17864, doi: 10.1109/ICRA57147.2024.10610150.Abstract: Various perception-aware planning approaches have attempted to enhance the state estimation accuracy during maneuvers, while the feature matchability among frames, a crucial factor influencing estimation accuracy, has often been overlooked. In this paper, we present APACE, an Agile and Perception-Aware trajeCtory gEneration framework for quadrotors aggressive flight, that takes into account feature matchability during trajectory planning. We seek to generate a perception-aware trajectory that reduces the error of visual-based estimator while satisfying the constraints on smoothness, safety, agility and the quadrotor dynamics. The perception objective is achieved by maximizing the number of covisible features while ensuring small enough parallax angles. Additionally, we propose a differentiable and accurate visibility model that allows decomposition of the trajectory planning problem for efficient optimization resolution. Through validations conducted in both a photorealistic simulator and real-world experiments, we demonstrate that the trajectories generated by our method significantly improve state estimation accuracy, with root mean square error (RMSE) reduced by up to an order of magnitude. The source code will be released to benefit the community 1. keywords: {Accuracy;Trajectory planning;Source coding;Trajectory;Safety;Planning;State estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610150&isnumber=10609862

