M. Sivaprakasam et al., "TartanDrive 2.0: More Modalities and Better Infrastructure to Further Self-Supervised Learning Research in Off-Road Driving Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12606-12606, doi: 10.1109/ICRA57147.2024.10611265.Abstract: We present TartanDrive 2.0, a large-scale off-road driving dataset for self-supervised learning tasks. In 2021 we released TartanDrive 1.0, which is one of the largest datasets for off-road terrain. As a follow-up to our original dataset, we collected seven hours of data at speeds of up to 15m/s with the addition of three new LiDAR sensors alongside the original camera, inertial, GPS, and proprioceptive sensors. We also release the tools we use for collecting, processing, and querying the data, including our metadata system designed to further the utility of our data. Custom infrastructure allows end users to reconfigure the data to cater to their own platforms. These tools and infrastructure alongside the dataset are useful for a variety of tasks in the field of off-road autonomy and, by releasing them, we encourage collaborative data aggregation. These resources lower the barrier to entry to utilizing large-scale datasets, thereby helping facilitate the advancement of robotics in areas such as self-supervised learning, multi-modal perception, inverse reinforcement learning, and representation learning. The dataset is available at https://theairlab.org/TartanDrive2. keywords: {Representation learning;Laser radar;Propioception;Self-supervised learning;Reinforcement learning;Metadata;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611265&isnumber=10609862

J. Wen et al., "EnYOLO: A Real-Time Framework for Domain-Adaptive Underwater Object Detection with Image Enhancement," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12613-12619, doi: 10.1109/ICRA57147.2024.10610639.Abstract: In recent years, significant progress has been made in the field of underwater image enhancement (UIE). However, its practical utility for high-level vision tasks, such as underwater object detection (UOD) in Autonomous Underwater Vehicles (AUVs), remains relatively unexplored. It may be attributed to several factors: (1) Existing methods typically employ UIE as a pre-processing step, which inevitably introduces considerable computational overhead and latency. (2) The process of enhancing images prior to training object detectors may not necessarily yield performance improvements. (3) The complex underwater environments can induce significant domain shifts across different scenarios, seriously deteriorating the UOD performance. To address these challenges, we introduce EnYOLO, an integrated real-time framework designed for simultaneous UIE and UOD with domain-adaptation capability. Specifically, both the UIE and UOD task heads share the same network backbone and utilize a lightweight design. Furthermore, to ensure balanced training for both tasks, we present a multi-stage training strategy aimed at consistently enhancing their performance. Additionally, we propose a novel domain-adaptation strategy to align feature embeddings originating from diverse underwater environments. Comprehensive experiments demonstrate that our framework not only achieves state-of-the-art (SOTA) performance in both UIE and UOD tasks, but also shows superior adaptability when applied to different underwater scenarios. Our efficiency analysis further highlights the substantial potential of our framework for onboard deployment. keywords: {Training;Autonomous underwater vehicles;Head;Object detection;Detectors;Real-time systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610639&isnumber=10609862

S. Triest, D. D. Fan, S. Scherer and A. -A. Agha-Mohammadi, "UNRealNet: Learning Uncertainty-Aware Navigation Features from High-Fidelity Scans of Real Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12627-12634, doi: 10.1109/ICRA57147.2024.10610724.Abstract: Traversability estimation in rugged, unstructured environments remains a challenging problem in field robotics. Often, the need for precise, accurate traversability estimation is in direct opposition to the limited sensing and compute capability present on affordable, small-scale mobile robots. To address this issue, we present a novel method to learn [u]ncertainty-aware [n]avigation features from high-fidelity scans of [real]-world environments (UNRealNet). This network can be deployed on-robot to predict these high-fidelity features using input from lower-quality sensors. UNRealNet predicts dense, metric-space features directly from single-frame lidar scans, thus reducing the effects of occlusion and odometry error. Our approach is label-free, and is able to produce traversability estimates that are robot-agnostic. Additionally, we can leverage UNRealNet’s predictive uncertainty to both produce risk-aware traversability estimates, and refine our feature predictions over time. We find that our method outperforms traditional local mapping and inpainting baselines by up to 40%, and demonstrate its efficacy on multiple legged platforms. keywords: {Uncertainty;Laser radar;Accuracy;Navigation;Estimation;Sensor phenomena and characterization;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610724&isnumber=10609862

M. Eder and G. Steinbauer-Wagner, "Robot-Dependent Traversability Estimation for Outdoor Environments using Deep Multimodal Variational Autoencoders," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12635-12642, doi: 10.1109/ICRA57147.2024.10609988.Abstract: Efficient and reliable navigation in off-road environments poses a significant challenge for robotics, especially when factoring in the varying capabilities of robots across different terrains. To achieve this, the robot system’s traversability is usually estimated to plan traversable routes through an environment. This paper presents a new approach that utilizes Deep Multimodal Variational Autoencoders (DMVAEs) for estimating the traversability of different robots in complex offroad terrains. Our method utilizes DMVAEs to capture essential environmental information and robot properties, effectively modeling factors that influence robotic traversability. The key contribution of this research is a two-stage traversability estimation framework for various robots in diverse off-road conditions that integrates robot properties in addition to environmental information to predict the traversability for various robots in a single model. We validate our method through real-world experiments involving four ground robots navigating an alpine environment. Comparative evaluations against state-of-the-art traversability estimation methods demonstrate the superior accuracy and robustness of our approach. Additionally, we investigate the transfer of trained models to new robots, enhancing their traversability estimation and extending the applicability of our framework. keywords: {Training;Correlation;Accuracy;Navigation;Estimation;Training data;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609988&isnumber=10609862

A. Yang, W. Li and Y. Hu, "F3DMP: Foresighted 3D Motion Planning of Mobile Robots in Wild Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12643-12649, doi: 10.1109/ICRA57147.2024.10611055.Abstract: In wild environments, motion planning for mobile robots faces the challenge of local optimal path traps due to limited sensor perception range and lack of spatial awareness. Existing approaches that avoid local optimum by designing heuristic functions or high-quality global paths in wild environments are time-consuming and unstable. This work proposes F3DMP, which consists of two parts to alleviate the local optimum solution and better utilize distant terrain information. First, the entire planning framework is adapted to the three-dimensional space so that the planning result conforms to the geometric characteristics of the terrain. Second, a time allocation function based on offline reinforcement learning is proposed. This function can anticipate potential challenges or opportunities based on semantic information for the image and proactively determine a time allocation. Our planner is integrated into a complete mobile robot system and deployed to a real robot. Experiments in simulation and the real world demonstrate that our method can improve the success rate by 28% and the trajectory smoothness by 27% compared with traditional methods. keywords: {Visualization;Three-dimensional displays;Semantics;Reinforcement learning;Robot sensing systems;Planning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611055&isnumber=10609862

Z. Xu, R. Zhou, Y. Yin, H. Gao, M. Tomizuka and J. Li, "MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12650-12657, doi: 10.1109/ICRA57147.2024.10610944.Abstract: Data-driven methods have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications. However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios. On the other hand, algorithmic data generation methods are usually limited by their model capacities, making them unable to offer realistic and diverse data needed by various application users. In this work, we study trajectory-level data generation for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of generating interactive human behaviors in realistic diverse contexts. We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motions based on diverse destinations and heterogeneous behaviors. We carried out extensive comparison and ablation studies to illustrate the effectiveness of our approach across various metrics. We also presented experiments that demonstrate the capability of MATRIX to serve as data augmentation for imitation-based motion planning. keywords: {Measurement;Heuristic algorithms;Human-robot interaction;Data models;Behavioral sciences;Trajectory;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610944&isnumber=10609862

A. Shamshirgaran, S. Manjanna and S. Carpin, "Distributed Multi-robot Online Sampling with Budget Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12658-12664, doi: 10.1109/ICRA57147.2024.10611289.Abstract: In multi-robot informative path planning the problem is to find a route for each robot in a team to visit a set of locations that can provide the most useful data to reconstruct an unknown scalar field. In the budgeted version, each robot is subject to a travel budget limiting the distance it can travel. Our interest in this problem is motivated by applications in precision agriculture, where robots are used to collect measurements to estimate domain-relevant scalar parameters such as soil moisture or nitrates concentrations. In this paper, we propose an online, distributed multi-robot sampling algorithm based on Monte Carlo Tree Search (MCTS) where each robot iteratively selects the next sampling location through communication with other robots and considering its remaining budget.We evaluate our proposed method for varying team sizes and in different environments, and we compare our solution with four different baseline methods. Our experiments show that our solution outperforms the baselines when the budget is tight by collecting measurements leading to smaller reconstruction errors. keywords: {Precision agriculture;Monte Carlo methods;Limiting;Soil measurements;Moisture measurement;Soil moisture;Measurement uncertainty},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611289&isnumber=10609862

S. Xie, C. Hu, D. Wang, J. Johnson, M. Bagavathiannan and D. Song, "Coupled Active Perception and Manipulation Planning for a Mobile Manipulator in Precision Agriculture Applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12665-12671, doi: 10.1109/ICRA57147.2024.10610753.Abstract: A mobile manipulator often finds itself in an application where it needs to take a close-up view before performing a manipulation task. Named this as a coupled active perception and manipulation (CAPM) problem, we model the uncertainty in the perception process and devise a key state/task planning algorithm that considers reachability conditions jointly established from perception and manipulation task constraints. By minimizing expected energy usage in body key state planning while satisfying task constraints, our algorithm is able to find an energy-efficient trajectory with less body repositioning motion while ensuring the success of the task. We have implemented the algorithm and tested it in both simulation and physical experiments. The results have confirmed that our algorithm has a lower energy consumption compared to a two-stage decoupled approach, while still maintaining a success rate of 100% for the task. keywords: {Precision agriculture;Energy consumption;Uncertainty;Active perception;Manipulators;Energy efficiency;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610753&isnumber=10609862

R. J. Kirschner et al., "Towards Safe Robot Use with Edged or Pointed Objects: A Surrogate Study Assembling a Human Hand Injury Protection Database," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12680-12687, doi: 10.1109/ICRA57147.2024.10610422.Abstract: The use of pointed or edged tools or objects is one of the most challenging aspects of today’s application of physical human-robot interaction (pHRI). One reason for this is that the severity of harm caused by such edged or pointed impactors is less well studied than for blunt impactors. Consequently, the standards specify well-reasoned force and pressure thresholds for blunt impactors and advise avoiding any edges and corners in contacts. Nevertheless, pointed or edged impactor geometries cannot be completely ruled out in real pHRI applications. For example, to allow edged or pointed tools such as screwdrivers near human operators, the knowledge of injury severity needs to be extended so that robot integrators can perform well-reasoned, time-efficient risk assessments. In this paper, we provide the initial datasets on injury prevention for the human hand based on drop tests with surrogates for the human hand, namely pig claws and chicken drumsticks. We then demonstrate the ease and efficiency of robot use using the dataset for contact on two examples. Finally, our experiments provide a set of injuries that may also be expected for human subjects under certain robot mass-velocity constellations in collisions. To extend this work, testing on human samples and a collaborative effort from research institutes worldwide is needed to create a comprehensive human injury avoidance database for any pHRI scenario and thus for safe pHRI applications including edged and pointed geometries. keywords: {Geometry;Databases;Collaboration;Skin;Collision avoidance;Protection;Injuries},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610422&isnumber=10609862

K. Elimelech, Z. Kingston, W. Thomason, M. Y. Vardi and L. E. Kavraki, "Accelerating Long-Horizon Planning with Affordance-Directed Dynamic Grounding of Abstract Strategies," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12688-12695, doi: 10.1109/ICRA57147.2024.10610486.Abstract: Long-horizon task planning is important for robot autonomy, especially as a subroutine for frameworks such as Integrated Task and Motion Planning. However, task planning is computationally challenging and struggles to scale to realistic problem settings. We propose to accelerate task planning over an agent’s lifetime by integrating abstract strategies: a generalizable planning experience encoding introduced in earlier work. In this work, we contribute a practical approach to planning with strategies by introducing a novel formalism of planning in a strategy-augmented domain. We also introduce and formulate the notion of a strategy’s affordance, which indicates its predicted benefit to the solution, and use it to guide the planning and strategy grounding processes. Together, our observations yield an affordance-directed, lazy-search planning algorithm, which can seamlessly compose strategies and actions to solve long-horizon planning problems. We evaluate our planner in an object rearrangement domain, where we demonstrate performance benefits relative to a state-of-the-art task planner. keywords: {Grounding;Heuristic algorithms;Affordances;Accelerated aging;Prediction algorithms;Real-time systems;Encoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610486&isnumber=10609862

M. S. Sakib and Y. Sun, "From Cooking Recipes to Robot Task Trees – Improving Planning Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12704-12711, doi: 10.1109/ICRA57147.2024.10611369.Abstract: Task planning for robotic cooking involves generating a sequence of actions for a robot to prepare a meal successfully. This paper introduces a novel task tree generation pipeline producing correct planning and efficient execution for cooking tasks. Our method first uses a large language model (LLM) to retrieve recipe instructions and then utilizes a fine-tuned GPT-3 to convert them into a task tree, capturing sequential and parallel dependencies among subtasks. The pipeline then mitigates the uncertainty and unreliable features of LLM outputs using task tree retrieval. We combine multiple LLM task tree outputs into a graph and perform a task tree retrieval to avoid questionable nodes and high-cost nodes to improve planning correctness and execution efficiency. Our evaluation results show its superior performance in task planning accuracy and efficiency compared to previous works. keywords: {Knowledge engineering;Accuracy;Uncertainty;Costs;Large language models;Pipelines;Chatbots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611369&isnumber=10609862

S. Wu, C. Wang, J. Pan, D. Han and Z. Zhao, "Bayesian-Guided Evolutionary Strategy with RRT for Multi-Robot Exploration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12720-12726, doi: 10.1109/ICRA57147.2024.10610963.Abstract: With the increasing demand for multi-robot exploration of unknown environments, how to accomplish this problem efficiently has become a focus of research. However, in this kind of task, the formulation of strategies for frontier point detection and task allocation largely determines the overall efficiency of the system. In the task of multi-robot exploration of unknown environments, the strategies of frontier point detection and task assignment determine the overall efficiency of the system. Most of the existing methods implement frontier point detection based on the Rapidly-Exploring Random Tree (RRT) and use greedy algorithms for task allocation. However, the classical RRT algorithm is a fixed growth step, which leads to the difficulty of growing branches in narrow environments, making the efficiency and correctness of detecting frontier points lower. Meanwhile, the allocation strategy of the greedy algorithm causes each robot to consider only the exploration area with the largest gain for itself, which easily leads to repeated exploration and reduces the overall efficiency of the system. To solve these problems, we propose an adaptive RRT tree growth strategy for frontier point detection, which can adjust the step size according to the known map information and thus improve the efficiency and accuracy of detection; and introduce a Bayesian-guided evolutionary strategy(BGE) for efficient task allocation, which can utilize the current and historical information to find the optimal allocation scheme in a global perspective. We conduct a comprehensive test of the proposed strategy in the ROS system as well as in the real world, which proves the efficiency of our strategy. Our code is open-sourced and can be provided under request. keywords: {Greedy algorithms;Solid modeling;Three-dimensional displays;Solids;Bayes methods;Resource management;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610963&isnumber=10609862

B. Yi, Y. Fan and D. Liu, "A Novel Model for Layer Jamming-based Continuum Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12727-12733, doi: 10.1109/ICRA57147.2024.10610912.Abstract: Continuum robots with variable stiffness have gained wide popularity in the last decade. Layer jamming (LJ) has emerged as a simple and efficient technique to achieve tunable stiffness for continuum robots. Despite its merits, the development of a control-oriented dynamical model 1 tailored for this specific class of robots remains an open problem in the literature. This paper aims to present the first solution, to the best of our knowledge, to close the gap. We propose an energy-based model that is integrated with the LuGre frictional model for LJ-based continuum robots. Then, we take a comprehensive theoretical analysis for this model, focusing on two fundamental characteristics of LJ-based continuum robots: shape locking and adjustable stiffness. To validate the modeling approach and theoretical results, a series of experiments using our OctRobot-I continuum robotic platform was conducted. The results show that the proposed model is capable of interpreting and predicting the dynamical behaviors in LJ-based continuum robots. keywords: {Analytical models;Accuracy;Shape;Force;Fitting;Focusing;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610912&isnumber=10609862

P. Vartholomeos, Z. Wu, S. M. H. Sadati and C. Bergeles, "Lumped Parameter Dynamic Model of an Eversion Growing Robot: Analysis, Simulation and Experimental Validation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12734-12740, doi: 10.1109/ICRA57147.2024.10611157.Abstract: This paper presents a lumped-parameter dynamic model of a pressure driven eversion robot carrying a catheter through its hollow core. A simulation framework based on the model is developed in MATLAB and is used for understanding the underlying physics, for identifying the regions of operation, and for demonstrating that, for a range of input commands, the catheter can be used as an actuation mechanism for propelling eversion; an approach especially useful for miniaturised systems. Simulations are experimentally validated on the MAMMOBOT system, which is a miniature steerable soft growing robot for early breast cancer detection. It was demonstrated that for most regions of operation experimental results compare well with simulation exhibiting an error less than 4%. Only one region of operation demonstrated larger deviations due possibly to unmodeled dynamics, which will be investigated in future work. keywords: {Analytical models;Propulsion;Mathematical models;Breast cancer;Robots;Catheters;Physics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611157&isnumber=10609862

Z. Wang, G. Wang, X. Chen and N. M. Freris, "Kinematic Modeling and Control of a Soft Robotic Arm with Non-constant Curvature Deformation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12749-12755, doi: 10.1109/ICRA57147.2024.10611049.Abstract: The passive compliance of soft robotic arms renders the development of accurate kinematic models and model-based controllers challenging. The most widely used model in soft robotic kinematics assumes Piecewise Constant Curvature (PCC). However, PCC introduces errors when the robot is subject to external forces or even gravity. In this paper, we establish a three-dimensional (3D) kinematic representation of a soft robotic arm with pseudo universal and prismatic joints that are capable of capturing non-constant curvature deformations of the soft segments. We theoretically demonstrate that this constitutes a more general methodology than PCC. Simulations and experiments on the real robot attest to the superior modeling accuracy of our approach in 3D motions with unknown loads. The maximum position/rotation error of the proposed model is verified 6.7×/4.6× lower than the PCC model considering gravity and external forces. Furthermore, we devise an inverse kinematic controller that is capable of positioning the tip, tracking trajectories, as well as performing interactive tasks in the 3D space. keywords: {Deformable models;Solid modeling;Three-dimensional displays;Accuracy;Deformation;Kinematics;Soft robotics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611049&isnumber=10609862

Z. Zhang, M. Koch and D. Ahmed, "Soft Acoustic End-effector," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12772-12778, doi: 10.1109/ICRA57147.2024.10611503.Abstract: Acoustic techniques have been developed as multifunctional tools for various microscale manipulations. In prevalent design paradigms, a position-fixed piezoelectric transducer (PZT) is utilized to generate ultrasound waves. However, the immobility of the PZT restricts the modulation of the acoustic field's position and orientation, consequently diminishing the adaptability and effectiveness of subsequent acoustic micromanipulation tasks. Here, we proposed a miniaturized soft acoustic end-effector and demonstrated acoustic field modulation and microparticle manipulation by adjusting PZT position and orientation. The PZT is mounted on the end of a soft robotic arm that has three individual degrees of freedom and can be deformed in 3D space by inflating or deflating each chamber. Experiments showed that the soft acoustic end-effector can change the traveling direction of microparticles and modulate the location of a standing wave field. Our approach is simple, flexible, and controllable. We envision that the soft acoustic end-effector will facilitate multiscale acoustic manipulation in interdisciplinary applications, especially, for in vivo acoustic therapies. keywords: {Three-dimensional displays;Ultrasonic imaging;Modulation;Medical treatment;Soft robotics;Acoustics;End effectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611503&isnumber=10609862

M. Y. Aoyama, J. Moura, N. Saito and S. Vijayakumar, "Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12839-12845, doi: 10.1109/ICRA57147.2024.10610502.Abstract: In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD approach decouples the learnt model into a haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using a large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wiping motions for unseen sponges, outperforming the LfD method without pre-training. We validate the motion generated by our semi-supervised LfD model on the physical robot hardware using the KUKA iiwa robot arm. We also validate that the haptic representation encoder, pre-trained in simulation, captures the properties of real objects, explaining its contribution to improving the generalisation of the downstream task. See our accompanying video: https://youtu.be/zP4JvHaCWHk. keywords: {Friction;Force;Robot sensing systems;Manipulators;Hardware;Haptic interfaces;Decoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610502&isnumber=10609862

Y. Meng, S. Vemprela, R. Bonatti, C. Fan and A. Kapoor, "ConBaT: Control Barrier Transformer for Safe Robot Learning from Demonstrations," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12857-12864, doi: 10.1109/ICRA57147.2024.10611109.Abstract: Large-scale self-supervised models have recently revolutionized our ability to perform a variety of tasks within the vision and language domains. However, using such models for autonomous systems is challenging because of safety requirements: besides executing correct actions, an autonomous agent must also avoid the high cost and potentially fatal critical mistakes. Traditionally, self-supervised training mainly focuses on imitating previously observed behaviors, and the training demonstrations carry no notion of which behaviors should be explicitly avoided. In this work, we propose Control Barrier Transformer (ConBaT), an approach that learns safe behaviors from demonstrations in a self-supervised fashion. ConBaT is inspired by the concept of control barrier functions in control theory and uses a causal transformer that learns to predict safe robot actions autoregressively using a critic that requires minimal safety data labeling. During deployment, we employ a lightweight online optimization to find actions that ensure future states lie within the learned safe set. We apply our approach to different simulated control tasks and show that our method results in safer control policies compared to other classical and learning-based methods such as imitation learning, reinforcement learning, and model predictive control. keywords: {Training;Learning systems;Reinforcement learning;Transformers;Robot learning;Safety;Labeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611109&isnumber=10609862

S. Sharma, S. Tuli and R. Paul, "Unsupervised Learning of Neuro-symbolic Rules for Generalizable Context-aware Planning in Object Arrangement Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12865-12872, doi: 10.1109/ICRA57147.2024.10610696.Abstract: As robots tackle complex object arrangement tasks, it becomes imperative for them to be able to generalize to complex worlds and scale with number of objects. This work postulates that extracting action primitives, such as push operations, their pre-conditions and effects would enable strong generalization to unseen worlds. Hence, we factorize policy learning as inference of such generic rules, which act as strong priors for predicting actions given the world state. Learnt rules act as propositional knowledge and enable robots to reach goals in a zero-shot method by applying the rules independently and incrementally. However, obtaining hand-engineered rules, such as PDDL descriptions is hard, especially for unseen worlds. This work aims to learn generic, sparse, and context-aware rules that govern action primitives in robotic worlds through human demonstrations in simple domains. We demonstrate that our approach, namely RLAP, is able to extract rules without explicit supervision of rule labels and generate goal-reaching plans in complex Sokoban styled domains that scale with number of objects. RLAP furnishes significantly higher goal reaching rate and shorter planning times compared to the state-of-the-art techniques. The code, dataset, and videos are hosted at https://rule-learning-rlap.github.io/. keywords: {Uncertainty;Protocols;Monte Carlo methods;Stacking;Search engines;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610696&isnumber=10609862

S. Afshar et al., "PBP: Path-based Trajectory Prediction for Autonomous Driving," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12927-12934, doi: 10.1109/ICRA57147.2024.10610610.Abstract: Trajectory prediction plays a crucial role in the autonomous driving stack by enabling autonomous vehicles to anticipate the motion of surrounding agents. Goal-based prediction models have gained traction in recent years for addressing the multimodal nature of future trajectories. Goal-based prediction models simplify multimodal prediction by first predicting 2D goal locations of agents and then predicting trajectories conditioned on each goal. However, a single 2D goal location serves as a weak inductive bias for predicting the whole trajectory, often leading to poor map compliance, i.e., part of the trajectory going off-road or breaking traffic rules. In this paper, we improve upon goal-based prediction by proposing the Path-based prediction (PBP) approach. PBP predicts a discrete probability distribution over reference paths in the HD map using the path features and predicts trajectories in the path-relative Frenet frame. We applied the PBP trajectory decoder on top of the HiVT scene encoder and report results on the Argoverse dataset. Our experiments show that PBP achieves competitive performance on the standard trajectory prediction metrics, while significantly outperforming state-of-the-art baselines in terms of map compliance. keywords: {Measurement;Accuracy;Predictive models;Probability distribution;Trajectory;Decoding;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610610&isnumber=10609862

S. Shan and Q. -C. Pham, "Sensorless Estimation of Contact Using Deep-Learning for Human-Robot Interaction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12935-12941, doi: 10.1109/ICRA57147.2024.10609990.Abstract: Physical human-robot interaction has been an area of interest for decades. Collaborative tasks, such as joint compliance, demand high-quality joint torque sensing. While external torque sensors are reliable, they come with the drawbacks of being expensive and vulnerable to impacts. To address these issues, studies have been conducted to estimate external torques using only internal signals, such as joint states and current measurements. However, insufficient attention has been given to friction hysteresis approximation, which is crucial for tasks involving extensive dynamic to static state transitions. In this paper, we propose a deep-learning-based method that leverages a novel long-term memory scheme to achieve dynamics identification, accurately approximating the static hysteresis. We also introduce modifications to the well-known Residual Learning architecture, retaining high accuracy while reducing inference time. The robustness of the proposed method is illustrated through a joint compliance and task compliance experiment. keywords: {Adaptation models;Torque;Friction;Current measurement;Estimation;Human-robot interaction;Robot sensing systems;Deep Learning Methods;Physical Human-robot Interaction;Industrial Robots;Dynamics Identification},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609990&isnumber=10609862

D. Lawson and A. H. Qureshi, "Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12942-12948, doi: 10.1109/ICRA57147.2024.10610919.Abstract: Recent work has shown the promise of creating generalist, transformer-based, models for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in parameter space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also demonstrate the importance of various methodological choices when merging policies, such as utilizing common pre-trained initializations, increasing model capacity, and utilizing Fisher information for weighting parameter importance. In general, we believe research in this direction could help democratize and distribute the process that forms multi-task robotics policies. Our implementation is available at https://github.com/daniellawson9999/merging-decision-transformer. keywords: {Training;Computational modeling;Merging;Decision making;Transformers;Multitasking;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610919&isnumber=10609862

P. Yu, S. Dong, S. Sheng, L. Feng and M. Kwiatkowska, "Trust-Aware Motion Planning for Human-Robot Collaboration under Distribution Temporal Logic Specifications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12949-12955, doi: 10.1109/ICRA57147.2024.10610874.Abstract: Recent work has considered trust-aware decision making for human-robot collaboration (HRC) with a focus on model learning. In this paper, we are interested in enabling the HRC system to complete complex tasks specified using temporal logic formulas that involve human trust. Since accurately observing human trust in robots is challenging, we adopt the widely used partially observable Markov decision process (POMDP) framework for modelling the interactions between humans and robots. To specify the desired behaviour, we propose to use syntactically co-safe linear distribution temporal logic (scLDTL), a logic that is defined over predicates of states as well as belief states of partially observable systems. The incorporation of belief predicates in scLDTL enhances its expressiveness while simultaneously introducing added complexity. This also presents a new challenge as the belief predicates must be evaluated over the continuous (infinite) belief space. To address this challenge, we present an algorithm for solving the optimal policy synthesis problem. First, we enhance the belief MDP (derived by reformulating the POMDP) with a probabilistic labelling function. Then a product belief MDP is constructed between the probabilistically labelled belief MDP and the automaton translation of the scLDTL formula. Finally, we show that the optimal policy can be obtained by leveraging existing point-based value iteration algorithms with essential modifications. Human subject experiments with 21 participants on a driving simulator demonstrate the effectiveness of the proposed approach. keywords: {Markov decision processes;Heuristic algorithms;Decision making;Collaboration;Automata;Probabilistic logic;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610874&isnumber=10609862

R. Pandya, Z. Wang, Y. Nakahira and C. Liu, "Towards Proactive Safe Human-Robot Collaborations via Data-Efficient Conditional Behavior Prediction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12956-12963, doi: 10.1109/ICRA57147.2024.10610860.Abstract: We focus on the problem of how we can enable a robot to collaborate seamlessly with a human partner, specifically in scenarios where preexisting data is sparse. Much prior work in human-robot collaboration uses observational models of humans (i.e. models that treat the robot purely as an observer) to choose the robot’s behavior, but such models do not account for the influence the robot has on the human’s actions, which may lead to inefficient interactions. We instead formulate the problem of optimally choosing a collaborative robot’s behavior based on a conditional model of the human that depends on the robot’s future behavior. First, we propose a novel model-based formulation of conditional behavior prediction that allows the robot to infer the human’s intentions based on its future plan in data-sparse environments. We then show how to utilize a conditional model for proactive goal selection and safe trajectory generation around human collaborators. Finally, we use our proposed proactive controller in a collaborative task with real users to show that it can improve users’ interactions with a robot collaborator quantitatively and qualitatively. keywords: {Collaboration;Predictive models;Observers;Behavioral sciences;Trajectory;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610860&isnumber=10609862

Y. Zhang, P. Robertson, T. Shu, S. Hong and B. C. Williams, "Risk-Bounded Online Team Interventions via Theory of Mind," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12964-12970, doi: 10.1109/ICRA57147.2024.10609865.Abstract: Despite advancements in human-robot teamwork, limited progress was made in developing AI assistants capable of advising teams online during task time, due to the challenges of modeling both individual and collective beliefs of the team members. Dynamic epistemic logic has proved to be a viable tool for representing a machine Theory of Mind and for modeling communication in epistemic planning, with applications to human-robot teamwork. However, this approach has yet to be applied in an online teaming assistance context and fails to account for the real-life probabilities of potential team beliefs. We propose a novel blend of epistemic planning and POMDP techniques to create a risk-bounded AI team assistant, that intervenes only when the team’s expected likelihood of failure exceeds a predefined risk threshold or in the case of potential execution deadlocks. Our experiments and simulated demonstration on the Virtualhome testbed show that the assistant can effectively improve team performance. keywords: {System recovery;Teamwork;Planning;Logic;Artificial intelligence;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609865&isnumber=10609862

S. Cao and J. Xiao, "Human-Robot Complementary Collaboration for Flexible and Precision Assembly," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12971-12977, doi: 10.1109/ICRA57147.2024.10610825.Abstract: This paper addresses human-robot collaborative (HRC) precision assembly that complements natural human ability and the strength of an autonomous robot system. Our approach enables both flexibility and efficiency of tight-clearance assembly of various complex-shaped parts in the presence of uncertainty without requiring assembly skills and knowledge of robotics from the human operator. We demonstrated the effectiveness of our approach in a variety of experiments and comparisons with other HRC assembly approaches. keywords: {Training;Robotic assembly;Productivity;Human computer interaction;Uncertainty;Collaboration;Manipulators;Human-robot collaboration;compliant assembly;human-centered automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610825&isnumber=10609862

M. Sayour et al., "HAC-SLAM: Human Assisted Collaborative 3D-SLAM Through Augmented Reality," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12978-12984, doi: 10.1109/ICRA57147.2024.10610377.Abstract: Simultaneous Localization and Mapping (SLAM) has emerged as a prime autonomous mobile agent localization algorithm. Despite the global research effort to improve SLAM, its mapping component remains limited and serves little more than to satisfy the coupled localization problem. We present a collaborative 3D SLAM approach leveraging the power of augmented reality (AR). The system introduces a trio of diverse agents, each with its unique capability to become an active member in the mapping process: mobile robots, human operators, and AR head-mounted display (AR-HMD). A 3D complementary mapping pipeline is developed to utilize the built-in SLAM capabilities of the AR-HMD as shareable data. Our system aligns and merges the AR-HMD and the robot’s local map automatically, triggered by a human-dictated initial guess. The created merged map proves advantageous in scenarios where the robot is restricted from navigating in certain areas. To correct map imperfections resulting from problematic objects such as transparent or reflective surfaces, the fused map is overlayed onto the environment, and hand gestures are used to add or delete 3D map features in real-time. Our system is implemented in both a lab and a real industrial warehouse setup. The results show a significant improvement in the map quality and mapping duration. keywords: {Location awareness;Point cloud compression;Three-dimensional displays;Simultaneous localization and mapping;Navigation;Pipelines;Collaboration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610377&isnumber=10609862

A. Jain, P. Long, V. Villani, J. D. Kelleher and M. Chiara Leva, "CoBT: Collaborative Programming of Behaviour Trees from One Demonstration for Robot Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 12993-12999, doi: 10.1109/ICRA57147.2024.10611654.Abstract: Mass customization and shorter manufacturing cycles are becoming more important among small and medium-sized companies. However, classical industrial robots struggle to cope with product variation and dynamic environments. In this paper, we present CoBT, a collaborative programming by demonstration framework for generating reactive and modular behavior trees. CoBT relies on a single demonstration and a combination of data-driven machine learning methods with logic-based declarative learning to learn a task, thus eliminating the need for programming expertise or long development times. The proposed framework is experimentally validated on 7 manipulation tasks and we show that CoBT achieves ≈ 93% success rate overall with an average of 7.5s programming time. We conduct a pilot study with non-expert users to provide feedback regarding the usability of CoBT. More videos and generated behavior trees are available at: https://github.com/jainaayush2006/CoBT.git. keywords: {Mass customization;Collaboration;Machine learning;Companies;Programming;Industrial robots;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611654&isnumber=10609862

H. Karnan, E. Yang, G. Warnell, J. Biswas and P. Stone, "Wait, That Feels Familiar: Learning to Extrapolate Human Preferences for Preference-Aligned Path Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13008-13014, doi: 10.1109/ICRA57147.2024.10611475.Abstract: Autonomous mobility tasks such as last-mile delivery require reasoning about operator-indicated preferences over terrains on which the robot should navigate to ensure both robot safety and mission success. However, coping with out of distribution data from novel terrains or appearance changes due to lighting variations remains a fundamental problem in visual terrain-adaptive navigation. Existing solutions either require labor-intensive manual data re-collection and labeling or use hand-coded reward functions that may not align with operator preferences. In this work, we posit that operator preferences for visually novel terrains, which the robot should adhere to, can often be extrapolated from established terrain preferences within the inertial-proprioceptive-tactile domain. Leveraging this insight, we introduce Preference extrApolation for Terrain-awarE Robot Navigation (PATERN), a novel framework for extrapolating operator terrain preferences for visual navigation. PATERN learns to map inertial-proprioceptive-tactile measurements from the robot’s observations to a representation space and performs nearest-neighbor search in this space to estimate operator preferences over novel terrains. Through physical robot experiments in outdoor environments, we assess PATERN’s capability to extrapolate preferences and generalize to novel terrains and challenging lighting conditions. Compared to baseline approaches, our findings indicate that PATERN 1 robustly generalizes to diverse terrains and varied lighting conditions, while navigating in a preference-aligned manner. keywords: {Visualization;Extrapolation;Navigation;Lighting;Nearest neighbor methods;Extraterrestrial measurements;Path planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611475&isnumber=10609862

A. Meixner, M. Carl, F. Krebs, N. Jaquier and T. Asfour, "Towards Unifying Human Likeness: Evaluating Metrics for Human-Like Motion Retargeting on Bimanual Manipulation Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13015-13022, doi: 10.1109/ICRA57147.2024.10611024.Abstract: Generating human-like robot motions is pivotal for achieving smooth human-robot interactions. Such motions contribute to better predictions of robot motions by humans, thus leading to more intuitive interaction and increased acceptability. Human likeness in robot motions has been conventionally measured and realized via the optimization of human-likeness metrics. However, the abundance of such metrics and the absence of standardized criteria impede their usage in novel contexts. In this work, we introduce a unified human-likeness metric built from a hierarchically weighted sum of individual metrics. The proposed metric is derived from a thorough analysis of eleven existing human-likeness criteria and is applicable across various tasks and robot models. We evaluate its performance in the context of motion retargeting of bimanual tasks with three different humanoid robots. keywords: {Measurement;Robot motion;Analytical models;Humanoid robots;Human-robot interaction;Task analysis;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611024&isnumber=10609862

A. Mylaeus et al., "MiBOT: A head-worn robot that modulates cardiovascular responses through human-like soft massage," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13023-13029, doi: 10.1109/ICRA57147.2024.10610867.Abstract: Massage therapy is helpful for the rehabilitation of various diseases, such as headaches caused by migraines and stress. Existing robotic systems have focused on massage therapy on the torso and limbs, but performing massage motions through suitable actuation on a person’s head has been a challenge. In this paper, we present MiBOT, a head-worn massage robot that actuates two soft tactors to produce touch motions mimicking human massage. A key design principle behind MiBOT is its silent actuation, which we achieve through pneumatic artificial muscles in conjunction with a controller loop to respond to contact pressure. We evaluated the effectiveness of MiBOT in a controlled study and assessed subjects’ blood pressure and heart rate levels while applying MiBOT. We found that our mechanical system generated positive and conclusive quantitative outcomes that are similar to the human-administered massage, decreasing participants’ mean systolic and diastolic blood pressure by 2.8 mmHg and 1.7 mmHg, respectively, as well as calming their heart rate by 8–10% on average. keywords: {Heart rate;Torso;Atmospheric measurements;Migraine;Medical treatment;Particle measurements;Blood pressure},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610867&isnumber=10609862

D. Wang et al., "ESP: Extro-Spective Prediction for Long-term Behavior Reasoning in Emergency Scenarios," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13030-13037, doi: 10.1109/ICRA57147.2024.10610002.Abstract: Emergent-scene safety is the key milestone for fully autonomous driving, and reliable on-time prediction is essential to maintain safety in emergency scenarios. However, these emergency scenarios are long-tailed and hard to collect, which restricts the system from getting reliable predictions. In this paper, we build a new dataset, which aims at the longterm prediction with the inconspicuous state variation in history for the emergency event, named the Extro-Spective Prediction (ESP) problem. Based on the proposed dataset, a flexible feature encoder for ESP is introduced to various prediction methods as a seamless plug-in, and its consistent performance improvement underscores its efficacy. Furthermore, a new metric named clamped temporal error (CTE) is proposed to give a more comprehensive evaluation of prediction performance, especially in time-sensitive emergency events of subseconds. Interestingly, as our ESP features can be described in human-readable language naturally, the application of integrating into ChatGPT also shows huge potential. The ESP-dataset and all benchmarks are released at https://dingrui-wang.github.io/ESP-Dataset/. keywords: {Measurement;Prediction methods;Benchmark testing;Chatbots;Cognition;Safety;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610002&isnumber=10609862

D. Schneider et al., "SynthAct: Towards Generalizable Human Action Recognition based on Synthetic Data," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13038-13045, doi: 10.1109/ICRA57147.2024.10611486.Abstract: Synthetic data generation is a proven method for augmenting training sets without the need for extensive setups, yet its application in human activity recognition is underexplored. This is particularly crucial for human-robot collaboration in household settings, where data collection is often privacy-sensitive. In this paper, we introduce SynthAct, a synthetic data generation pipeline designed to significantly minimize the reliance on real-world data. Leveraging modern 3D pose estimation techniques, SynthAct can be applied to arbitrary 2D or 3D video action recordings, making it applicable for uncontrolled in-the-field recordings by robotic agents or smarthome monitoring systems. We present two SynthAct datasets: AMARV, a large synthetic collection with over 800k multi-view action clips, and Synthetic Smarthome, mirroring the Toyota Smarthome dataset. SynthAct generates a rich set of data, including RGB videos and depth maps from four synchronized views, 3D body poses, normal maps, segmentation masks and bounding boxes. We validate the efficacy of our datasets through extensive synthetic-to-real experiments on NTU RGB+D and Toyota Smarthome. SynthAct is available on our project page4. keywords: {Training;Three-dimensional displays;Pose estimation;Training data;Data collection;Generators;Recording},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611486&isnumber=10609862

C. Zuo, K. He, J. Shao and Y. Sui, "Self Model for Embodied Intelligence: Modeling Full-Body Human Musculoskeletal System and Locomotion Control with Hierarchical Low-Dimensional Representation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13062-13069, doi: 10.1109/ICRA57147.2024.10610081.Abstract: Modeling and control of the human musculoskele-tal system is important for understanding human motor functions, developing embodied intelligence, and optimizing human-robot interaction systems. However, current human musculoskeletal models are restricted to a limited range of body parts and often with a reduced number of muscles. There is also a lack of algorithms capable of controlling over 600 muscles to generate reasonable human movements. To fill this gap, we build a musculoskeletal model (MS-HUMAN-700) with 90 body segments, 206 joints, and 700 muscle-tendon units, allowing simulation of full-body dynamics and interaction with various devices. We develop a new algorithm using low-dimensional representation and hierarchical deep reinforcement learning to achieve state-of-the-art full-body control. We validate the effectiveness of our model and algorithm in simulations with real human locomotion data. The musculoskeletal model, along with its control algorithm, will be made available to the research community to promote a deeper understanding of human motion control and better design of interactive robots.Project page: https://lnsgroup.cc/research/MS-Human-700 keywords: {Torso;Adaptation models;Heuristic algorithms;Biological system modeling;Muscles;Motors;Deep reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610081&isnumber=10609862

Y. Chen and C. Wang, "Keypoints-guided Lightweight Network for Single-view 3D Human Reconstruction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13070-13076, doi: 10.1109/ICRA57147.2024.10610020.Abstract: Single-view 3D human reconstruction has been a hot topic due to the potential of wide applications. To achieve high accuracy, existing works usually take computationally intensive models as backbone for exhaustive underlying features and then directly estimate human mesh vertices. These factors lead to redundant parameters, large calculations and low efficiency, while lightweight solutions to address these challenges are relatively scarce. In this work, based on the problems studied above, we propose a keypoints-guided lightweight network with an encoding-decoding framework. As the input is an image, a lightweight backbone named multi-stage and global feature enhanced network is designed for 2D encoding, where some operations of multi-scale fusion and frequency domain filtering are performed to extract more informative but low-resolution features. As the output is mesh of human body, we construct a keypoints-based 3D human template, with which the 2D low-resolution features can be mapped to 3D space to guide the 3D decoding with high efficiency and high accuracy. Extensive experiments on popular benchmarks 3DPW and Human3.6M illustrate the favorable trade-off between the accuracy and complexity of our method. Our code is publicly available at https://github.com/ChrisChenYh/EfficientHuman.git. keywords: {Three-dimensional displays;Accuracy;Image coding;Frequency-domain analysis;Estimation;Benchmark testing;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610020&isnumber=10609862

X. Chen, A. Anikode, J. Yi and T. Liu, "Foot Shape-Dependent Resistive Force Model for Bipedal Walkers on Granular Terrains," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13093-13099, doi: 10.1109/ICRA57147.2024.10610190.Abstract: Legged robots have demonstrated high efficiency and effectiveness in unstructured and dynamic environments. However, it is still challenging for legged robots to achieve rapid and efficient locomotion on deformable, yielding substrates, such as granular terrains. We present an enhanced resistive force model for bipedal walkers on soft granular terrains by introducing effective intrusion depth correction. The enhanced force model captures fundamental kinetic results considering the robot foot shape, walking gait speed variation, and energy expense. The model is validated by extensive foot intrusion experiments with a bipedal robot. The results confirm the model accuracy on the given type of granular terrains. The model can be further integrated with the motion control of bipedal robotic walkers. keywords: {Legged locomotion;Accuracy;Shape;Drag;Force;Dynamics;Kinetic theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610190&isnumber=10609862

L. Pu, Y. Liu, A. Zheng, B. Qi and C. Xu, "Adaptive Passive Biped Dynamic Walking on Unknown Uneven Terrain," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13100-13106, doi: 10.1109/ICRA57147.2024.10610661.Abstract: In this paper, we propose an adaptive controller for virtual passive biped dynamic walking on unknown uneven terrain. The adaptive controller consists of a trajectory tracking control law developed via backstepping method to mimic reference passive gait, and a slope estimator for the inclination angle of the terrain. In addition, a re-planning approach is introduced to correct the robot state off-track from the reference gait due to the terrain changes. The controller is validated through the simulations on the mixed uneven terrain consisting of varying slopes and steps. The results suggest that the controller shows comparable cost of transport and greater adaptability to terrain changes compared with certain existing methods. keywords: {Legged locomotion;Adaptation models;Costs;Backstepping;Trajectory tracking;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610661&isnumber=10609862

A. Tang et al., "HumanMimic: Learning Natural Locomotion and Transitions for Humanoid Robot via Wasserstein Adversarial Imitation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13107-13114, doi: 10.1109/ICRA57147.2024.10610449.Abstract: Transferring human motion skills to humanoid robots remains a significant challenge. In this study, we introduce a Wasserstein adversarial imitation learning system, allowing humanoid robots to replicate natural whole-body locomotion patterns and execute seamless transitions by mimicking human motions. First, we present a unified primitive-skeleton motion retargeting to mitigate morphological differences between arbitrary human demonstrators and humanoid robots. An adversarial critic component is integrated with Reinforcement Learning (RL) to guide the control policy to produce behaviors aligned with the data distribution of mixed reference motions. Additionally, we employ a specific Integral Probabilistic Metric (IPM), namely the Wasserstein-1 distance with a novel soft boundary constraint to stabilize the training process and prevent model collapse. Our system is evaluated on a full-sized humanoid JAXON in the simulator. The resulting control policy demonstrates a wide range of locomotion patterns, including standing, push-recovery, squat walking, humanlike straight-leg walking, and dynamic running. Notably, even in the absence of transition motions in the demonstration dataset, the robot showcases an emerging ability to transit naturally between distinct locomotion patterns as desired speed changes. keywords: {Training;Legged locomotion;Measurement;Imitation learning;Integral equations;Dynamics;Humanoid robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610449&isnumber=10609862

M. E. Mungai, G. Prabhakaran and J. W. Grizzle, "Fall Prediction for Bipedal Robots: The Standing Phase," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13135-13141, doi: 10.1109/ICRA57147.2024.10611387.Abstract: This paper presents a novel approach to fall prediction for bipedal robots, specifically targeting the detection of potential falls while standing caused by abrupt, incipient, and intermittent faults. Leveraging a 1D convolutional neural network (CNN), our method aims to maximize lead time for fall prediction while minimizing false positive rates. The proposed algorithm uniquely integrates the detection of various fault types and estimates the lead time for potential falls. Our contributions include the development of an algorithm capable of detecting abrupt, incipient, and intermittent faults in full-sized robots, its implementation using both simulation and hardware data for a humanoid robot, and a method for estimating lead time. Evaluation metrics, including false positive rate, lead time, and response time, demonstrate the efficacy of our approach. Particularly, our model achieves impressive lead times and response times across different fault scenarios with a false positive rate of 0. The findings of this study hold significant implications for enhancing the safety and reliability of bipedal robotic systems. keywords: {Torso;Reviews;Prediction algorithms;Hardware;Classification algorithms;Safety;Time factors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611387&isnumber=10609862

I. Sorrentino, G. Romualdi and D. Pucci, "UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13150-13156, doi: 10.1109/ICRA57147.2024.10610951.Abstract: This paper proposes a novel sensor fusion based on Unscented Kalman Filtering for the online estimation of joint-torques of humanoid robots without joint-torque sensors. At the feature level, the proposed approach considers multimodal measurements (e.g. currents, accelerations, etc.) and non-directly measurable effects, such as external contacts, thus leading to joint torques readily usable in control architectures for human-robot interaction. The proposed sensor fusion can also integrate distributed, non-collocated force/torque sensors, thus being a flexible framework with respect to the underlying robot sensor suit. To validate the approach, we show how the proposed sensor fusion can be integrated into a two-level torque control architecture aiming at task-space torque-control. The performances of the proposed approach are shown through extensive tests on the new humanoid robot ergoCub, currently being developed at Istituto Italiano di Tecnologia. We also compare our strategy with the existing state-of-the-art approach based on the recursive Newton-Euler algorithm. Results demonstrate that our method achieves low root mean square errors in torque tracking, ranging from 0.05 Nm to 2.5 Nm, even in the presence of external contacts. keywords: {Torque;Current measurement;Torque control;Humanoid robots;Estimation;Sensor fusion;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610951&isnumber=10609862

V. Reijgwart, M. Pantic, R. Siegwart and L. Ott, "Waverider: Leveraging Hierarchical, Multi-Resolution Maps for Efficient and Reactive Obstacle Avoidance," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13157-13163, doi: 10.1109/ICRA57147.2024.10610580.Abstract: Fast and reliable obstacle avoidance is an important task for mobile robots. In this work, we propose an efficient reactive system that provides high-quality obstacle avoidance while running at hundreds of hertz with minimal resource usage. Our approach combines wavemap, a hierarchical volumetric map representation, with a novel hierarchical and parallelizable obstacle avoidance algorithm formulated through Riemannian Motion Policies (RMP). Leveraging multi-resolution obstacle avoidance policies, the proposed navigation system facilitates precise, low-latency (36ms), and extremely efficient obstacle avoidance with a very large perceptive radius (30m). We perform extensive statistical evaluations on indoor and outdoor maps, verifying that the proposed system compares favorably to fixed-resolution RMP variants and CHOMP. Finally, the RMP formulation allows the seamless fusion of obstacle avoidance with additional objectives, such as goal-seeking, to obtain a fully-fledged navigation system that is versatile and robust. We deploy the system on a Micro Aerial Vehicle and show how it navigates through an indoor obstacle course. Our complete implementation, called waverider, is made available as open source 1. keywords: {Three-dimensional displays;Navigation;Trajectory tracking;Numerical analysis;Planning;Reliability;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610580&isnumber=10609862

T. Kossas, W. Remmas, R. Gkliva, A. Ristolainen and M. Kruusmaa, "Whisker-Based Tactile Navigation Algorithm For Underground Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13164-13170, doi: 10.1109/ICRA57147.2024.10610762.Abstract: This work explores the use of artificial whiskers as tactile sensors for enhancing the perception and navigation capabilities of mobile robots in challenging settings such as caves and underground mines. These environments exhibit inconsistent lighting conditions, locally self-similar textures, and general poor visibility conditions, that can cause the performance of state-of-the-art vision-based methods to decline. In order to evaluate the efficacy of tactile sensing in this context, three algorithms were developed and tested with simulated and physical experiments: a wall-follower, a navigation algorithm based on Theta*, and a hybrid approach that combines the two. The obtained results highlight the efficacy of tactile sensing for wall-following in intricate environments. When paired with an external method for pose estimation, it further aids in navigating unknown environments. Moreover, by integrating navigation with wall-following, the third, hybrid algorithm enhanced the map traversal speed by roughly 26−43% compared to standard navigation methods without wall-following. keywords: {Simultaneous localization and mapping;Navigation;Service robots;Pose estimation;Tactile sensors;Prototypes;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610762&isnumber=10609862

T. Miura, N. Akai, K. Honda and S. Hara, "Spline-Interpolated Model Predictive Path Integral Control with Stein Variational Inference for Reactive Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13171-13177, doi: 10.1109/ICRA57147.2024.10610501.Abstract: This paper presents a reactive navigation method that leverages a Model Predictive Path Integral (MPPI) control enhanced with spline interpolation for the control input sequence and Stein Variational Gradient Descent (SVGD). The MPPI framework addresses a nonlinear optimization problem by determining an optimal sequence of control inputs through a sampling-based approach. The efficacy of MPPI is significantly influenced by the sampling noise. To rapidly identify routes that circumvent large and/or newly detected obstacles, it is essential to employ high levels of sampling noise. However, such high noise levels result in jerky control input sequences, leading to non-smooth trajectories. To mitigate this issue, we propose the integration of spline interpolation within the MPPI process, enabling the generation of smooth control input sequences despite the utilization of substantial sampling noises. Nonetheless, the standard MPPI algorithm struggles in scenarios featuring multiple optimal or near-optimal solutions, such as environments with several viable obstacle avoidance paths, due to its assumption that the distribution over an optimal control input sequence can be closely approximated by a Gaussian distribution. To address this limitation, we extend our method by incorporating SVGD into the MPPI framework with spline interpolation. SVGD, rooted in the optimal transportation algorithm, possesses the unique ability to cluster samples around an optimal solution. Consequently, our approach facilitates robust reactive navigation by swiftly identifying obstacle avoidance paths while maintaining the smoothness of the control input sequences. The efficacy of our proposed method is validated on simulations with a quadrotor, demonstrating superior performance over existing baseline techniques. keywords: {Interpolation;Navigation;Noise;Transportation;Predictive models;Approximation algorithms;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610501&isnumber=10609862

F. Eberle, R. Laha, H. Yao, A. Naceri, L. F. C. Figueredo and S. Haddadin, "RETOM: Leveraging Maneuverability for Reactive Tool Manipulation using Wrench-Fields," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13178-13184, doi: 10.1109/ICRA57147.2024.10611140.Abstract: This paper investigates the problem of effective tool manipulation for motion planning in complex human-like scenarios. Vector-field-based real-time strategies, although widely used, usually do not account for unwieldy tools or incorporate systematic methods to handle these extra maneuvers needed. Instead, we formalize the problem and propose a novel field-based reactive planner that explicitly accounts for rotational forces for seamless maneuvers based on the tool’s geometry and featured points. Furthermore, we capture and encode robot performance through capability metrics and improve the same using an additional quality distribution method. This enables seamless integration of the robot’s embodiment with the reactive force-torque (wrench) field giving rise to flexible tool usage in non-stationary environments. Extensive simulation analysis on a 7 DoF collaborative robot manipulating a common tool in an unorganized table-top layout reinforces our claim of robustness in stationary and non-stationary scenarios. keywords: {Measurement;Geometry;Systematics;Layout;Collaborative robots;Robustness;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611140&isnumber=10609862

S. Xu, W. Zhang, C. P. Ho and L. Zhu, "Optimal Prescribed-Time Control based Reactive Planning System for Quadruped Robot Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13185-13191, doi: 10.1109/ICRA57147.2024.10610336.Abstract: In this paper, we propose a reactive planning system for quadruped robots based on prescribed-time control. The navigation of the quadruped robot is fundamentally depicted as omnidirectional movements, while a feedback control law is formulated to address any deviations the robot may encounter. In particular, our proposed feedback control system is theoretically proven to achieve convergence within a predefined finite time that is specified by the user. To further compute the optimal convergent time and the local goal state, we present a high-level planning node encompassing terrain-aware kinodynamic search and spatiotemporal trajectory optimization, which can generate collision-free, smooth, and efficient trajectories. The effectiveness of our proposed framework is validated through both numerical simulation and real-robot experiments in indoor and outdoor environments, including scenarios with cluttered obstacles, slopes, and external disturbances. keywords: {Navigation;Numerical simulation;Control systems;Planning;Feedback control;Spatiotemporal phenomena;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610336&isnumber=10609862

L. Ai, Y. Liu, M. Armand, A. Kheradmand and A. Martin-Gomez, "On the Fly Robotic-Assisted Medical Instrument Planning and Execution Using Mixed Reality," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13192-13199, doi: 10.1109/ICRA57147.2024.10611515.Abstract: Robotic-assisted medical systems (RAMS) have gained significant attention for their advantages in alleviating surgeons’ fatigue and improving patients’ outcomes. These systems comprise a range of human-computer interactions, including medical scene monitoring, anatomical target planning, and robot manipulation. However, despite its versatility and effectiveness, RAMS demands expertise in robotics, leading to a high learning cost for the operator. In this work, we introduce a novel framework using mixed reality technologies to ease the use of RAMS. The proposed framework achieves real-time planning and execution of medical instruments by providing 3D anatomical image overlay, human-robot collision detection, and robot programming interface. These features, integrated with an easy-to-use calibration method for head-mounted display, improve the effectiveness of human-robot interactions. To assess the feasibility of the framework, two medical applications are presented in this work: 1) coil placement during transcranial magnetic stimulation and 2) drill and injector device positioning during femoroplasty. Results from these use cases demonstrate its potential to extend to a wider range of medical scenarios. keywords: {Transcranial magnetic stimulation;Random access memory;Mixed reality;Human-robot interaction;Medical instruments;Virtual reality;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611515&isnumber=10609862

F. Zeug, M. Becker and M. A. Müller, "Circular Field Motion Planning for Highly-Dynamic Multi-Robot Systems with Application to Robot Soccer," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13220-13226, doi: 10.1109/ICRA57147.2024.10611138.Abstract: The rise of autonomous driving in everyday life makes efficient and collision-free motion planning more important than ever. However, multi robot applications in highly dynamic environments still pose hard challenges for state-of-the-art motion planners. In this paper, we present a new iteration of a reactive circular fields motion planner with the focus on simultaneous control of multiple robots in robotic soccer games, which is able to operate omnidirectional robots safely and efficiently despite high measurement delays and inaccuracies. Our extension enables the definition and effective execution of complex tasks in soccer specific problems. We extensively evaluated our planner in several complex simulation environments and experimentally verified the approach in realistic scenarios on real soccer robots. Furthermore, we demonstrated the capabilities of our motion planner during the successful participation in the RoboCup 2022 and 2023. keywords: {Shape;Games;Planning;Delays;Multi-robot systems;Motion measurement;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611138&isnumber=10609862

H. Jardali, M. Ali and L. Liu, "Autonomous Mapless Navigation on Uneven Terrains," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13227-13233, doi: 10.1109/ICRA57147.2024.10611669.Abstract: We propose a new method for autonomous navigation in uneven terrains by utilizing a sparse Gaussian Process (SGP) based local perception model. The SGP local perception model is trained on local ranging observation (pointcloud) to learn the terrain elevation profile and extract the feasible navigation subgoals around the robot. Subsequently, a cost function, which prioritizes the safety of the robot in terms of keeping the robot’s roll and pitch angles bounded within a specified range, is used to select a safety-aware subgoal that leads the robot to its final destination. The algorithm is designed to run in real-time and is intensively evaluated in simulation and real-world experiments. The results compellingly demonstrate that our proposed algorithm consistently navigates uneven terrains with high efficiency and surpasses the performance of other planners. The implementation of our method, including the supplementary video showing the experimental and real-world results, is available at https://rb.gy/3ov2r8. keywords: {Navigation;Gaussian processes;Streaming media;Cost function;Real-time systems;Distance measurement;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611669&isnumber=10609862

Y. Aoyama, A. Haeri and E. A. Theodorou, "Optimal Control of Granular Material," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13234-13241, doi: 10.1109/ICRA57147.2024.10611171.Abstract: The control of granular materials, which are found in many industrial applications, is a challenging open research problem. Granular material systems are complex-behavior (as they could have solid-, fluid-, and gas-like behaviors) and high-dimensional (as they could have many grains/particles with at least 3 DOF in 3D) systems. Recently, a machine learning-based Graph Neural Network (GNN) simulator has been proposed to learn the underlying dynamics. In this paper, we perform optimal control of a rigid body-driven granular material system whose dynamics is learned by a GNN model trained by reduced data generated via a physics-based simulator and Principal Component Analysis (PCA). We use Differential Dynamic Programming (DDP) to obtain optimal control commands that can form granular particles into a target shape. The model and results are shown to be relatively fast and accurate. The control commands are also applied to the ground truth model, i.e., physics-based simulator, to further validate the approach. keywords: {Accuracy;Three-dimensional displays;Shape;Computational modeling;Optimal control;Graph neural networks;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611171&isnumber=10609862

H. Xue, E. L. Zhu, J. M. Dolan and F. Borrelli, "Learning Model Predictive Control with Error Dynamics Regression for Autonomous Racing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13250-13256, doi: 10.1109/ICRA57147.2024.10611628.Abstract: This work presents a novel Learning Model Predictive Control (LMPC) strategy for autonomous racing at the handling limit that can iteratively explore and learn unknown dynamics in high-speed operational domains. We start from existing LMPC formulations and modify the system dynamics learning method. In particular, our approach uses a nominal, global, nonlinear, physics-based model with a local, linear, data-driven learning of the error dynamics. We conducted experiments in simulation and on 1/10th scale hardware, and deployed the proposed LMPC on a full-scale autonomous race car used in the Indy Autonomous Challenge (IAC) with closed loop experiments at the Putnam Park Road Course in Indiana, USA. The results show that the proposed control policy exhibits improved robustness to parameter tuning and data scarcity. Incremental and safety-aware exploration toward the limit of handling and iterative learning of the vehicle dynamics in high-speed domains is observed both in simulations and experiments. keywords: {System dynamics;Roads;Robustness;Hardware;Nonlinear dynamical systems;Safety;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611628&isnumber=10609862

H. Y. Park and J. Hoon Kim, "Robust Balancing Control of Biped Robots for External Forces," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13257-13262, doi: 10.1109/ICRA57147.2024.10611281.Abstract: This paper develops a controller synthesis method for ensuring an admissible bound of external forces on biped robots in a desired level. We first introduce the authors’ preceding results on the norm-based stability criterion for a biped walking constructed on its linear inverted pendulum model (LIPM). More precisely, an induced norm can be taken to formulate the fact that the balance for a biped robot is achieved if its zero moment point (ZMP) always stays in the supporting region at each step. Based on this norm-based criterion, we aim at making the maximum energy of external forces admissible for balancing the biped robot be a pregiven desired bound γ(> 0). To achieve this objective, a robust controller is designed through the linear matrix inequality (LMI)-based approach. More importantly, a necessary and sufficient condition for the existence of a robust controller leading to the desired bound is characterized by some LMI conditions. The effectiveness of the overall arguments is validated through some comparative simulation results of a biped walking robot with external forces. keywords: {Legged locomotion;Sufficient conditions;Upper bound;Trajectory tracking;Simulation;Stability criteria;Linear matrix inequalities},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611281&isnumber=10609862

F. Girlanda, L. Shala, S. Kumar and F. Kirchner, "Robust Co-Design of Canonical Underactuated Systems for Increased Certifiable Stability," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13271-13277, doi: 10.1109/ICRA57147.2024.10611645.Abstract: Optimal behaviours of a system to perform a specific task can be achieved by leveraging the coupling between trajectory optimization, stabilization, and design optimization. This approach is particularly advantageous for underactuated systems, which are systems that have fewer actuators than degrees of freedom and thus require for more elaborate control systems. This paper proposes a novel co-design algorithm, namely Robust Trajectory Control with Design optimization (RTC-D). An inner optimization layer (RTC) simultaneously performs direct transcription (DIRTRAN) to find a nominal trajectory while computing optimal hyperparameters for a stabilizing time-varying linear quadratic regulator (TVLQR). RTC-D augments RTC with a design optimization layer, maximizing the system’s robustness through a time-varying Lyapunov-based region of attraction (ROA) analysis. This analysis provides a formal guarantee of stability for a set of off-nominal states. The proposed algorithm has been tested on two different underactuated systems: the torque-limited simple pendulum and the cart-pole. Extensive simulations of off-nominal initial conditions demonstrate improved robustness, while real-system experiments show increased insensitivity to torque disturbances. keywords: {Couplings;Torque;Regulators;Stability analysis;Robustness;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611645&isnumber=10609862

A. L. Bishop, J. Z. Zhang, S. Gurumurthy, K. Tracy and Z. Manchester, "ReLU-QP: A GPU-Accelerated Quadratic Programming Solver for Model-Predictive Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13285-13292, doi: 10.1109/ICRA57147.2024.10611249.Abstract: We present ReLU-QP, a GPU-accelerated solver for quadratic programs (QPs) that is capable of solving high-dimensional control problems at real-time rates. ReLU-QP is derived by exactly reformulating the Alternating Direction Method of Multipliers (ADMM) algorithm for solving QPs as a deep, weight-tied neural network with rectified linear unit (ReLU) activations. This reformulation enables the deployment of ReLU-QP on GPUs using standard machine-learning toolboxes. We evaluate the performance of ReLU-QP across three model-predictive control (MPC) benchmarks: stabilizing random linear dynamical systems with control limits, balancing an Atlas humanoid robot on a single foot, and performing a whole-body pick-up motion on a quadruped equipped with a six-degree-of-freedom arm. These benchmarks indicate that ReLU-QP is competitive with state-of-the-art CPU-based solvers for small-to-medium-scale problems and offers order-of-magnitude speed improvements for larger-scale problems. keywords: {Neural networks;Machine learning;Benchmark testing;Manipulators;Real-time systems;Quadrupedal robots;Quadratic programming},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611249&isnumber=10609862

M. M. Kalantari et al., "Markerless Ultrasound Probe Pose Estimation in Mini-Invasive Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13293-13299, doi: 10.1109/ICRA57147.2024.10610837.Abstract: In mini-invasive surgery, the laparoscopic ultra-sound probe is visible in the laparoscopic image. We address the problem of estimating the probe pose with respect to the laparoscope without using markers and additional sensors. We propose the first method using a single standard laparoscopic monocular RGB image. It is robust, initialization-free and runs at 10 fps, thus forming a promising tool to improve robotic and augmented reality-based surgery. keywords: {Laparoscopes;Image segmentation;Visualization;Runtime;Surgery;Robot sensing systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610837&isnumber=10609862

K. Weerasinghe, S. H. Reza Roodabeh, K. Hutchinson and H. Alemzadeh, "Multimodal Transformers for Real-Time Surgical Activity Prediction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13323-13330, doi: 10.1109/ICRA57147.2024.10611048.Abstract: Real-time recognition and prediction of surgical activities are fundamental to advancing safety and autonomy in robot-assisted surgery. This paper presents a multimodal transformer architecture for real-time recognition and prediction of surgical gestures and trajectories based on short segments of kinematic and video data. We conduct an ablation study to evaluate the impact of fusing different input modalities and their representations on gesture recognition and prediction performance. We perform an end-to-end assessment of the proposed architecture using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) dataset. Our model outperforms the state-of-the-art (SOTA) with 89.5% accuracy for gesture prediction through effective fusion of kinematic features with spatial and contextual video features. It achieves the real-time performance of 1.1-1.3ms for processing a 1-second input window by relying on a computationally efficient model. keywords: {Computational modeling;Computer architecture;Kinematics;Streaming media;Predictive models;Transformers;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611048&isnumber=10609862

J. A. Barragan, J. Zhang, H. Zhou, A. Munawar and P. Kazanzides, "Realistic Data Generation for 6D Pose Estimation of Surgical Instruments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13347-13353, doi: 10.1109/ICRA57147.2024.10611638.Abstract: Automation in surgical robotics has the potential to improve patient safety and surgical efficiency, but it is difficult to achieve due to the need for robust perception algorithms. In particular, 6D pose estimation of surgical instruments is critical to enable the automatic execution of surgical maneuvers based on visual feedback. In recent years, supervised deep learning algorithms have shown increasingly better performance at 6D pose estimation tasks; yet, their success depends on the availability of large amounts of annotated data. In household and industrial settings, synthetic data, generated with 3D computer graphics software, has been shown as an alternative to minimize annotation costs of 6D pose datasets. However, this strategy does not translate well to surgical domains as commercial graphics software have limited tools to generate images depicting realistic instrument-tissue interactions. To address these limitations, we propose an improved simulation environment for surgical robotics that enables the automatic generation of large and diverse datasets for 6D pose estimation of surgical instruments. Among the improvements, we developed an automated data generation pipeline and an improved surgical scene. To show the applicability of our system, we generated a dataset of 7.5k images with pose annotations of a surgical needle that was used to evaluate a state-of-the-art pose estimation network. The trained model obtained a mean translational error of 2.59mm on a challenging dataset that presented varying levels of occlusion. These results highlight our pipeline’s success in training and evaluating novel vision algorithms for surgical robotics applications. keywords: {Training;Visualization;Medical robotics;Three-dimensional displays;Annotations;Instruments;Pose estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611638&isnumber=10609862

S. Schmidgall, A. Krieger and J. Eshraghian, "Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13354-13361, doi: 10.1109/ICRA57147.2024.10610448.Abstract: Recent advances in robot-assisted surgery have resulted in progressively more precise, efficient, and minimally invasive procedures, sparking a new era of robotic surgical intervention. This enables doctors, in collaborative interaction with robots, to perform traditional or minimally invasive surgeries with improved outcomes through smaller incisions. Recent efforts are working toward making robotic surgery more autonomous which has the potential to reduce variability of surgical outcomes and reduce complication rates. Deep reinforcement learning methodologies offer scalable solutions for surgical automation, but their effectiveness relies on extensive data acquisition due to the absence of prior knowledge in successfully accomplishing tasks. Due to the intensive nature of simulated data collection, previous works have focused on making existing algorithms more efficient. In this work, we focus on making the simulator more efficient, making training data much more accessible than previously possible. We introduce Surgical Gym, an open-source high performance platform for surgical robot learning where both the physics simulation and reinforcement learning occur directly on the GPU. We demonstrate between 100-5000× faster training times compared with previous surgical learning platforms. The code is available at: https://github.com/SamuelSchmidgall/SurgicalGym. keywords: {Training;Medical robotics;Minimally invasive surgery;Torque;Training data;Hardware;PD control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610448&isnumber=10609862

J. Fu, Y. Long, K. Chen, W. Wei and Q. Dou, "Multi-objective Cross-task Learning via Goal-conditioned GPT-based Decision Transformers for Surgical Robot Task Automation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13362-13368, doi: 10.1109/ICRA57147.2024.10611051.Abstract: Surgical robot task automation has been a promising research topic for improving surgical efficiency and quality. Learning-based methods have been recognized as an interesting paradigm and been increasingly investigated. However, existing approaches encounter difficulties in long-horizon goal-conditioned tasks due to the intricate compositional structure, which requires decision-making for a sequence of sub-steps and understanding of inherent dynamics of goal-reaching tasks. In this paper, we propose a new learning-based framework by leveraging the strong reasoning capability of the GPT-based architecture to automate surgical robotic tasks. The key to our approach is developing a goal-conditioned decision transformer to achieve sequential representations with goal-aware future indicators in order to enhance temporal reasoning. Moreover, considering to exploit a general understanding of dynamics inherent in manipulations, thus making the model’s reasoning ability to be task-agnostic, we also design a cross-task pretraining paradigm that uses multiple training objectives associated with data from diverse tasks. We have conducted extensive experiments on 10 tasks using the surgical robot learning simulator SurRoL [1]. The results show that our new approach achieves promising performance and task versatility compared to existing methods. The learned trajectories can be deployed on the da Vinci Research Kit (dVRK) for validating its practicality in real surgical robot settings. Our project website is at: https://med-air.github.io/SurRoL. keywords: {Training;Learning systems;Medical robotics;Automation;Decision making;Transformers;Cognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611051&isnumber=10609862

J. Škerlj, M. Hamad, J. Elsner, A. Naceri and S. Haddadin, "Safe-By-Design Digital Twins for Human-Robot Interaction: A Use Case for Humanoid Service Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13369-13375, doi: 10.1109/ICRA57147.2024.10611178.Abstract: Integrating humanoid service mobile robots into human environments presents numerous challenges, primarily concerning the safety of interactions between robots and humans. To address these safety concerns, we propose a novel approach that leverages the capabilities of digital twin technology by tailoring it to incorporate comprehensive and robust safety concepts. This paper introduces a "safe-by-design" digital twin that operates alongside the real twin robot in the loop, engaging real-time safety framework during physical interactions with the surrounding environment, including humans.To validate the effectiveness of our proposed safe-by-design digital twin framework, we conducted experiments using a humanoid service mobile robot alongside simulated human counterparts. Our results demonstrate the capability of the integrated impact safety module within the proposed digital twin approach to limit the velocities of both the robot’s base and arms, adhering to injury biomechanics-based safety thresholds. These findings emphasize the promise of our proposed approach for ensuring the physical safety of humanoid service mobile robots operating in dynamic human environments. It enables the digital twin to preemptively identify potential safety hazards and formulate safe intervention actions to ensure the robot’s compliance with safety regulations, paving the way for safer and more widespread adoption of robotic systems in various service domains. keywords: {Service robots;Tracking;Humanoid robots;Human-robot interaction;Robot sensing systems;Safety;Digital twins},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611178&isnumber=10609862

Z. Shen, M. Saveriano, F. J. Abu-Dakka and S. Haddadin, "Safe Execution of Learned Orientation Skills with Conic Control Barrier Functions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13376-13382, doi: 10.1109/ICRA57147.2024.10611325.Abstract: In the field of Learning from Demonstration (LfD), Dynamical Systems (DSs) have gained significant attention due to their ability to generate real-time motions and reach predefined targets. However, the conventional convergence-centric behavior exhibited by DSs may fall short in safety-critical tasks, specifically, those requiring precise replication of demonstrated trajectories or strict adherence to constrained regions even in the presence of perturbations or human intervention. Moreover, existing DS research often assumes demonstrations solely in Euclidean space, overlooking the crucial aspect of orientation in various applications. To alleviate these shortcomings, we present an innovative approach geared toward ensuring the safe execution of learned orientation skills within constrained regions surrounding a reference trajectory. This involves learning a stable DS on SO(3), extracting time-varying conic constraints from the variability observed in expert demonstrations, and bounding the evolution of the DS with Conic Control Barrier Function (CCBF) to fulfill the constraints. We validated our approach through extensive evaluation in simulation and showcased its effectiveness for a cutting skill in the context of assisted teleoperation. keywords: {Learning systems;Perturbation methods;Real-time systems;Trajectory;Behavioral sciences;Usability;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611325&isnumber=10609862

S. Fujii and Q. -C. Pham, "Real-time Batched Distance Computation for Time-Optimal Safe Path Tracking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13383-13389, doi: 10.1109/ICRA57147.2024.10610196.Abstract: In human-robot collaboration, there has been a trade-off relationship between the speed of collaborative robots and the safety of human workers. In our previous paper, we introduced a time-optimal path tracking algorithm designed to maximize speed while ensuring safety for human workers [1]. This algorithm runs in real-time and provides the safe and fastest control input for every cycle with respect to ISO standards [2]. However, true optimality has not been achieved due to inaccurate distance computation resulting from conservative model simplification. To attain true optimality, we require a method that can compute distances 1. at many robot configurations to examine along a trajectory 2. in realtime for online robot control 3. as precisely as possible for optimal control. In this paper, we propose a batched, fast and precise distance checking method based on precomputed link-local SDFs. Our method can check distances for 500 waypoints along a trajectory within less than 1 millisecond using a GPU at runtime, making it suited for time-critical robotic control. Additionally, a neural approximation has been proposed to accelerate preprocessing by a factor of 2. Finally, we experimentally demonstrate that our method can navigate a 6-DoF robot earlier than a geometric-primitives-based distance checker in a dynamic and collaborative environment. keywords: {Runtime;Navigation;Scalability;Memory management;Robot control;Graphics processing units;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610196&isnumber=10609862

C. Lollett, A. Sriram, M. Kamezaki and S. Sugano, "Overcoming Hand and Arm Occlusion in Human-to-Robot Handovers: Predicting Safe Poses with a Multimodal DNN Regression Model," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13390-13396, doi: 10.1109/ICRA57147.2024.10610777.Abstract: Handovers play a key role in human-robot interactions. However, current research focuses on visible-hand handovers, thereby heavily relying on hand detection. Large objects in human-robot interactions present a unique challenge: they inherently block the person’s hands and arms from the robot's view. This occlusion raises the robot’s risk of unintended physical contact with the person, leading to discomfort and safety concerns. This study aims to develop a model that can determine a pose for the robot that ensures a handover that avoids physical contact with the person, especially in scenarios when hands and arms are occluded. Toward this goal, a three-branch multimodal Deep Neural Network (DNN) regression model was implemented. First, a robust human-pose keypoints detection to calculate shoulder-elbow angles is applied. Secondly, we extract the refined object’s segmented mask. Thirdly, we compute two intrinsic object properties. The concatenated outputs from these branches pass through extra dense layers, resulting in the prediction of the robot's 14 arms-joint angles. Compared to an only keypoint data processed-based model, our multimodal approach made a 17.7% accuracy improvement. The experiments highlight each pipeline step’s significance, showing important results even when hands and arms were heavily occluded, adjusting to different variations. keywords: {Accuracy;Computational modeling;Pipelines;Human-robot interaction;Artificial neural networks;Handover;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610777&isnumber=10609862

J. Geldenbott and K. Leung, "Legible and Proactive Robot Planning for Prosocial Human-Robot Interactions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13397-13403, doi: 10.1109/ICRA57147.2024.10611294.Abstract: Humans have a remarkable ability to fluently engage in joint collision avoidance in crowded navigation tasks despite the complexities and uncertainties inherent in human behavior. Underlying these interactions is a mutual understanding that (i) individuals are prosocial, that is, there is equitable responsibility in avoiding collisions, and (ii) individuals should behave legibly, that is, move in a way that clearly conveys their intent to reduce ambiguity in how they intend to avoid others. Toward building robots that can safely and seamlessly interact with humans, we propose a general robot trajectory planning framework for synthesizing legible and proactive behaviors and demonstrate that our robot planner naturally leads to prosocial interactions. Specifically, we introduce the notion of a markup factor to incentivize legible and proactive behaviors and an inconvenience budget constraint to ensure equitable collision avoidance responsibility. We evaluate our approach against well-established multi-agent planning algorithms and show that using our approach produces safe, fluent, and prosocial interactions. We demonstrate the real-time feasibility of our approach with human-in-the-loop simulations. Project page can be found at https://uw-ctrl.github.io/phri/. keywords: {Uncertainty;Trajectory planning;Navigation;Human-robot interaction;Real-time systems;Human in the loop;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611294&isnumber=10609862

Y. Nam and C. Kwon, "Integrated Data-driven Inference and Planning-based Human Motion Prediction for Safe Human-Robot Interaction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13404-13410, doi: 10.1109/ICRA57147.2024.10611239.Abstract: This paper presents a unified prediction and planning algorithm for an autonomous vehicle to interact with an uncertain human-driven vehicle. Predicting human motion is challenging due to inherent uncertainties in diverse human internal states, i.e., driving styles and rationality. To address these complexities, we propose a hierarchical prediction strategy that combines data-driven internal state inference and planning-based human motion prediction. First, we employ Long Short Term Memory Networks (LSTM) based inference modules to capture both driving styles and rationality from the observed motion of human driver. With these inferred internal states, we predict the future trajectories of human-driven vehicle by formulating a human planning model as an optimization problem. Lastly, we present a Stochastic Model Predictive Control (SMPC) for the autonomous vehicle to safely interact with the human-driven vehicle while actively inferring human internal states. The simulation results, demonstrating the lane change scenarios, indicate the proposed method outperforms the existing work in both predicting the human motion and achieving the robot’s goal. keywords: {Training;Uncertainty;Stochastic processes;Prediction algorithms;Inference algorithms;Planning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611239&isnumber=10609862

X. Zhang, J. Chong and K. Youcef-Toumi, "How Does Perception Affect Safety: New Metrics and Strategy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13411-13417, doi: 10.1109/ICRA57147.2024.10610657.Abstract: Perception plays a pivotal role in enhancing the functionality of autonomous agents. However, the intricate relationship between robotic perception metrics and actuation metrics remains unclear, leading to ambiguity in the development and fine-tuning of perception algorithms. In this paper, we introduce a methodology for quantifying this relationship, taking into account factors such as detection rate, detection quality, and latency. Furthermore, we introduce two novel perception metrics for Human-Robot Collaboration safety predicated upon basic perception metrics: Critical Collision Probability (CCP) and Average Collision Probability (ACP). To validate the utility of these metrics in facilitating algorithm development and tuning, we develop an attentive processing strategy that focuses exclusively on key input features. This approach significantly reduces computational time while preserving a similar level of accuracy. Experimental findings demonstrate that integrating this strategy into an object detector results in a notable maximum reduction of 30.09% in inference time and 26.53% in total time per frame. Additionally, the strategy lowers the CCP and ACP in a baseline model by 11.25% and 13.50%, respectively. keywords: {Measurement;Accuracy;Computational modeling;Collaboration;Detectors;Real-time systems;Inference algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610657&isnumber=10609862

Z. Zhang, T. Li and N. Figueroa, "Constrained Passive Interaction Control: Leveraging Passivity and Safety for Robot Manipulators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13418-13424, doi: 10.1109/ICRA57147.2024.10610232.Abstract: Passivity is necessary for robots to fluidly collaborate and interact with humans physically. Nevertheless, due to the unconstrained nature of passivity-based impedance control laws, the robot is vulnerable to infeasible and unsafe configurations upon physical perturbations. In this paper, we propose a novel control architecture that allows a torque-controlled robot to guarantee safety constraints such as kinematic limits, self-collisions, external collisions and singularities and is passive only when feasible. This is achieved by constraining a dynamical system based impedance control law with a relaxed hierarchical control barrier function quadratic program subject to multiple concurrent, possibly contradicting, constraints. Joint space constraints are formulated from efficient data-driven self- and external ${\mathcal{C}^2}$ collision boundary functions. We theoretically prove constraint satisfaction and show that the robot is passive when feasible. Our approach is validated in simulation and real robot experiments on a 7DoF Franka Research 3 manipulator. keywords: {Perturbation methods;Kinematics;Aerospace electronics;Manipulators;Control systems;Safety;Impedance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610232&isnumber=10609862

Y. Zhao et al., "Automated Assembly by Two-Fingered Microhand for Fabrication of Soft Magnetic Microrobots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13433-13438, doi: 10.1109/ICRA57147.2024.10611042.Abstract: Micro-assembly is an emerging method to fabricate microrobots with multiple modules or particles. However, there is always a lack of a flexible and efficient method to freely create the desired magnetic soft microrobots. In this paper, an automated assembly system based on a two-fingered microhand is presented for fabricating magnetic soft microrobots. Our proposed system can automatically pick and place components to assemble microrobots with a two-fingered micromanipulator, and orient these components through an external magnetic field. The automated assembly has the advantages of high accuracy, high speed, and high success rate. It can endow magnetic microrobots with flexible material selection, arbitrary geometry design, and programable magnetization profile. We can make full use of this system to fabricate multiple magnetic soft microrobots. The experiment results demonstrate that this system can efficiently fabricate microrobots with excellent mechanical properties, which have application potential in robotics, biomedical engineering, and environmental governance. keywords: {Three-dimensional displays;Shape;Micromanipulators;Magnetization;Manuals;Three-dimensional printing;Soft magnetic materials},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611042&isnumber=10609862

S. Kim and S. Bergbreiter, "Thin-film NiTi Microactuator With A Magnetic Spring For A Tiny Launcher Mechanism," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13439-13445, doi: 10.1109/ICRA57147.2024.10610159.Abstract: In this work, we present a thin-film shape memory alloy microactuator with a magnetic spring. This novel actuator design utilizes two permanent magnets and 3D-printed magnet holders to effectively apply a tensile strain on the NiTi thin-film. This actuator is expected to generate 8.7 mN of blocking force, and a free displacement of 30 µm is experimentally characterized. The actuator leverages bare NiTi film (∼ 1 µm thick) for actuation, enabling a high actuator bandwidth up to 50 Hz. A comprehensive analytical model is also studied, which was then validated by comparing to the experimental results. A launcher mechanism was designed and integrated with the NiTi actuator, and this mechanism was used to launch a microscale projectile (a salt grain) thereby demonstrating the relative high power actuation achievable with thin-film NiTi. keywords: {Analytical models;Strips;Tensile strain;Projectiles;Force;Bandwidth;Microactuators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610159&isnumber=10609862

Z. Xu, Q. Xu and H. H. Yu, "Nature-Inspired Bubble Magnetic Microrobots for Multimode Locomotion, Cargo delivery, Imaging, and Biosensing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13446-13451, doi: 10.1109/ICRA57147.2024.10611619.Abstract: Wirelessly actuated magnetic microrobots are promising tools in medical applications due to their tiny sizes and attractive robotic properties. However, it remains a huge challenge to integrate sufficient functionalities in a limited volume. Microscopic natural phenomenon is a great reference for current microrobot design, where the underlying intelligence and subtlety spurs related modern artificial systems. Inspired by air bubbles in nature, herein, we report a kind of novel magnetic air bubble microrobots. The air bubble-based structure enables multiple functionalities including cargo delivery, multimode locomotion, micromanipulation, medical imaging, and biosensing. The proposed microrobot is essentially Pickering bubbles composed of magnetic particles and air bubbles. Their hollow structures help produce lighter microrobots with density less than 1 g/cm3, enabling buoyancy-based self-propulsion. Buoyancy and magnetic forces actuation enables flexible 3D locomotion in fluidic environments. Experimental results show that the microrobots can be controlled properly for designated assignments. Furthermore, the introduction of air bubble enhances ultrasound imaging, facilitating further in vivo applications. These findings offer a significant microrobot design paradigm by exploiting natural physical intelligence at the small scale. keywords: {In vivo;Visualization;Ultrasonic imaging;Three-dimensional displays;Magnetic particles;Magnetic resonance imaging;Biosensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611619&isnumber=10609862

A. C. Davis, E. Z. Freeman and D. J. Cappelleri, "Magnetic Mobile Micro-Gripping MicroRobots (MMµGRs) with Two Independent Magnetic Actuation Modes," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13452-13457, doi: 10.1109/ICRA57147.2024.10609974.Abstract: In this paper, we introduce magnetic mobile micro-gripping microrobots with two independent actuation modes. By aligning two magnets with slight variations in magnetic moment orientations, we create a net magnetic moment for precise position and orientation control through external fields, while harnessing opposing torques on the magnets to induce internal stresses needed for gripping. Our microrobot design features a compliant spring-like structure for significant deflection, enabling a gripping motion under specific magnetic field conditions. Magnet rotation allows precise control over gripper actions, returning to a default state (normally open or closed) when the magnetic field diminishes. This work advances magnetic field-controlled microrobotics, bridging the millimeter-to-micrometer gap. It holds promise for applications in microsurgery, micro-assembly, and microscale exploration. keywords: {Wireless communication;Technological innovation;Magnetomechanical effects;Microsurgery;Magnetic moments;Three-dimensional printing;Magnetic fields},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609974&isnumber=10609862

L. Noseda, A. Liu, L. Pancaldi and M. S. Sakar, "A Flat Tendon-Driven Continuum Microrobot for Brain Interventions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13466-13471, doi: 10.1109/ICRA57147.2024.10611399.Abstract: Navigating biomedical instruments inside the brain remains challenging and high-risk. The delicate nature of the tissues involved requires the development of cutting-edge robotic technologies to enhance precision and safety. In response to these demands, this paper presents a novel ribbon-shaped, tendon-driven continuum microrobot designed explicitly to navigate through brain tissues. The microrobot has a cross-sectional area of 1 mm2, and its design is readily compatible with conventional microfabrication techniques for further miniaturization. The flat geometry aims to provide superior maneuverability and opens up new challenges for modeling and control. We detail the design methodology and fabrication, followed by in vitro characterization and testing within brain tissue phantoms. keywords: {Geometry;Navigation;Design methodology;Microfabrication;Phantoms;Brain modeling;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611399&isnumber=10609862

G. Rekleitis and E. Papadopoulos, "System Identification of Space Manipulator Systems and its Implications on Robust Control Performance*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13480-13486, doi: 10.1109/ICRA57147.2024.10610181.Abstract: Space manipulator system (SMS) maneuvers can excite flexible appendages, while fuel sloshing effects impact its dynamics and performance. To predict this behavior and control such systems, sloshing and flexible appendages are modeled. A novel system identification scheme is developed, which identifies all parameters required for the reconstruction of system dynamics despite unmeasurable sloshing and modal states. This is achieved by two identification experiments. In Exp.1 all unmeasurable states are eliminated, while in Exp.2 the unmeasurable sloshing states are eliminated, and a novel estimator is used for the unmeasurable modal states. The significance of accurate SYSID in controller design and performance is demonstrated by simulating a 3D SMS controlled by model-based and robust controllers. In both cases, using the identified parameters results in significant robust control performance enhancement. keywords: {Robust control;Three-dimensional displays;Accuracy;Uncertainty;System dynamics;System performance;Aerospace electronics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610181&isnumber=10609862

J. Zhao, Z. Wang, Z. Liu, L. Zhao, Q. Duan and H. Liu, "Model Design and Concept of Operations of Standard Interface for On-orbit Construction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13487-13493, doi: 10.1109/ICRA57147.2024.10609869.Abstract: The construction of large-scale space facilities requires the use of on-orbit construction technology. However, several of its key components, such as standard interface design, compliant control methods, and path planning for multi-branch robots, still need improvement before practical application. This paper presents a comprehensive solution for on-orbit construction tasks, encompassing a novel standard interface, docking control method, and path planning method for space multi-branch robots. Firstly, a novel standard interface is introduced, which features multiple mating modes and a lightweight design. Additionally, a compliant docking method is provided to generate lower contact forces along the Z-direction. Furthermore, for four-armed space robots, a hierarchical planning method is proposed, which innovates in environment map construction and locomotion planning. Specifically, the closed-form Minkowski sum method is employed to solve the robot’s free space, and a concise locomotion method is elucidated based on transition support points. Finally, simulations and experiments are conducted. keywords: {Visualization;Machine vision;Prototypes;Aerospace electronics;Path planning;Planning;Task analysis;On-orbit construction;space robot;standard interface;docking control;path planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609869&isnumber=10609862

M. Oevermann, D. Pravecek, G. Jibrail, R. Jangale and R. O. Ambrose, "RoboBall: An All-Terrain Spherical Robot with a Pressurized Shell," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13502-13508, doi: 10.1109/ICRA57147.2024.10610555.Abstract: Spherical robots are a different type of mobility platform. A spherical robot is self-contained within its shell rather than relying on a chassis with wheels to navigate. In this shell, it is completely shielded from dust and the environment. This benefit of geometric simplicity has led to the spherical robot becoming an advantageous option for all-terrain exploration and surveying. This paper focuses on a novel iteration of such a robot with a pressurized pneumatic shell design. A soft robot of this type brings benefits of a passive, compliant contact surface that can affect its performance. However, the added softness of its shell adds new unmodeled dynamics into the system that impair commonly used control schemes. This paper outlines the design and manufacture of a soft, inflatable, spherical shell designed for a robot driven by an internal 2-DOF pendulum. In addition, it presents models for controlling the pendulum and understanding the shell dynamics. The paper concludes with experimental validations of these models and field tests of the system on slopes, gravel, rough grass, and on water. keywords: {Navigation;Wheels;2-DOF;Pneumatic systems;Soft robotics;Mobile robots;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610555&isnumber=10609862

M. Yüksel, W. Brinkmann, M. Jankovic, H. D. Küçüker and F. Kirchner, "Enhanced multifunctional interface for reconfigurability of robotic teams in planetary applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13509-13515, doi: 10.1109/ICRA57147.2024.10610642.Abstract: Exploration missions on extra terrestrial celestial bodies are to date performed by complex and heavy robotic systems. The trend is towards lighter modular systems that can be (re)configured in situ according to mission specific requirements. To facilitate flexible configurability, a multifunctional interconnect is used to mechanically couple the involved systems while providing electrical power and data transmission. The paper presents the further development of the reliable electromechanical interface (EMI) from the TransTerrA project, which has been proven in several field tests and reached TRL 4. Docking under loads of up to 550 N has been successfully tested with the new design. The experiments presented include undocking at various inclinations with different loads expected for the application scenario. The maximum determined static load that can be carried by the further developed EMI is 2000 N. In further experiments, new contact blocks responsible for the transfer of electrical power and data were tested for water resistance and resilience to environmental factors, as well as power and data transfer. The obtained results will be helpful in the development of a multi-functional interface suitable for lunar applications and missions having similar challenging environmental conditions. keywords: {Water;Resistance;Contacts;Electromagnetic interference;Moon;Data transfer;Pins;Planetary robotics;interface;modularity;docking;lunar environment},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610642&isnumber=10609862

D. Hirano, N. Tanishima and T. G. Chen, "Autonomous Perching on Flat Surfaces for Free-Flying Robots with Gecko Adhesive Gripper," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13516-13521, doi: 10.1109/ICRA57147.2024.10610685.Abstract: Gecko-inspired adhesives have the advantage of being able to grasp and release flat surfaces in a vacuum using their microwedge structures. This makes them an especially attractive solution for perching on and grasping flat objects in space for free-flying robots. To grasp and anchor onto these flat surfaces, the gripper must ensure contact between the gecko adhesives and the surface before applying the appropriate forces to activate their adhesion. However, in the case of a free-flying robot in microgravity, physical contact with the surface induces reaction forces, causing the robot to quickly bounce away from the surface. To solve this issue, we propose a simple passive mechanism and a control method of a robotic arm on a free-flying robot with a gecko adhesive gripper. The gripper utilizes a single-motor controlled tendon-driven mechanism mounted at the end of a robotic arm equipped with controllable stiffness joints and a linear spring-damper system. A free-flying robot on an air-bearing platform can successfully perch on a flat surface with a velocity of up to 72.5mm/s and with an approach angle misalignment of up to 33.0 degrees. keywords: {Velocity control;Grasping;Arms;Robot sensing systems;Manipulators;Motors;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610685&isnumber=10609862

Z. Qin, J. Song, X. Gong and C. Liu, "VWDER:A Variable Wheel-Diameter Ellipsoidal Robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13522-13528, doi: 10.1109/ICRA57147.2024.10610985.Abstract: In recent years, many researchers have conducted extensive research on spherical robots due to their high flexibility and anti-overturning capabilities. Nevertheless, compared with legged and traditional wheeled robots, spherical robots face certain limitations. The spherical robot is composed of a closed spherical shell structure, which makes the capacity of carrying workloads weak. At the same time, the single point contact with the ground cause the contact friction force with the ground is small, so it is hard to climb obstacles such as steps and doorsill. Therefore, we propose a new solution: the variable wheel diameter ellipsoidal robot (VWDER), which combines the characteristics of two-wheel differential driven robot and spherical robot driven by equivalent pendulum. VWDER is equipped with six retractable shell-shaped legs on each side and this innovative design allows both wheels to independently change diameter while rolling. The main frame of the VWDER can keep the top of the frame facing up under the action of equivalent pendulum during the locomotion, which makes it possible to carry workloads such as manipulator arms, cameras, IMU etc. The VWDER robot can climb steps or doorsill using its two adjacent shell-shaped legs. This paper introduces the design of the VWDER and analyzes the kinematics and dynamics of the VWDER. The experimental results verified the performance of the VWDER, including its autonomous opening and closing, obstacle crossing, automatic reorientation and slope climbing etc. keywords: {Legged locomotion;Robot vision systems;Wheels;Kinematics;Inspection;Hazards;Indexes;Index Terms - Spherical Robot;Variable Wheel Diameter;Ellipsoidal Robot;Trafficability;Retractable Mechanism},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610985&isnumber=10609862

K. Nanos, E. Chachamis and E. Papadopoulos, "On Robust Control Laws Trade-off Analysis for Space Manipulators with Uncertain Parameters and Flexible Appendages*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13529-13535, doi: 10.1109/ICRA57147.2024.10611335.Abstract: To accurately accomplish on-orbit tasks using Space Manipulator Systems (SMS), advanced model-based controllers, dependent on the knowledge of SMS parameters, can be employed. However, these parameters may change on orbit for several reasons. Also, during an SMS task, excitation of flexible appendages, such as solar panels, or fuel sloshing may introduce significant end-effector errors. Therefore, controllers robust to parametric uncertainty and disturbances are needed. A robust controller attractive due to its small computational effort is the Linear Parameter Varying (LPV) gain-scheduled controller. However, its design for spatial SMS is not trivial and has not been studied yet. Therefore, the aim of this work is to study and compare robust controllers and examine their applicability to SMS. An LPV plus H∞ controller is compared with a Model-Based PD, and a Model-Based PD plus H∞ controller, in the presence of parametric uncertainty, noisy measurements and disturbances, using a planar example. The criteria considered include: (i) Design Complexity, (ii) Trajectory Errors, (iii) Required Torques, and (iv) Computational Effort. keywords: {Space vehicles;Uncertainty;Computational modeling;Measurement uncertainty;Extraterrestrial measurements;Trajectory;Solar panels},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611335&isnumber=10609862

M. Jacquet and K. Alexis, "N-MPC for Deep Neural Network-Based Collision Avoidance exploiting Depth Images," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13536-13542, doi: 10.1109/ICRA57147.2024.10610572.Abstract: This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs. The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view. Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon. The N-MPC achieves real time control of a UAV with a control frequency of 100Hz. The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo simulations and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments. The associated code is released open-source. keywords: {Training;Time-frequency analysis;Three-dimensional displays;Statistical analysis;Artificial neural networks;Real-time systems;Mathematical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610572&isnumber=10609862

V. D. Hoang, F. Falk Nyboe, N. H. Malle and E. Ebeid, "Autonomous Overhead Powerline Recharging for Uninterrupted Drone Operations," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13551-13557, doi: 10.1109/ICRA57147.2024.10611522.Abstract: We present a fully autonomous self-recharging drone system capable of long-duration sustained operations near powerlines. The drone is equipped with a robust onboard perception and navigation system that enables it to locate powerlines and approach them for landing. A passively actuated gripping mechanism grasps the powerline cable during landing after which a control circuit regulates the magnetic field inside a split-core current transformer to provide sufficient holding force as well as battery recharging.The system is evaluated in an active outdoor three-phase powerline environment. We demonstrate multiple contiguous hours of fully autonomous uninterrupted drone operations composed of several cycles of flying, landing, recharging, and takeoff, validating the capability of extended, essentially unlimited, operational endurance. keywords: {Navigation;Force;Inspection;Robustness;Magnetic fields;Robotics and automation;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611522&isnumber=10609862

I. D. Changoluisa Caiza, A. Milas, M. A. Montes Grova, F. Javier Pérez-Grau and T. Petrovic, "Autonomous Exploration of Unknown 3D Environments Using a Frontier-Based Collector Strategy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13566-13572, doi: 10.1109/ICRA57147.2024.10611139.Abstract: Autonomous exploration using unmanned aerial vehicles (UAVs) is essential for various tasks such as building inspections, rescue operations, deliveries, and warehousing. However, there are two main limitations to previous approaches: they may not be able to provide a complete map of the environment and assume that the map built during exploration is accurate enough for safe navigation, which is usually not the case. To address these limitations, a novel exploration method is proposed that combines frontier-based exploration with a collector strategy that achieves global exploration and complete map creation. In each iteration, the collector strategy stores and validates frontiers detected during exploration and selects the next best frontier to navigate to. The collector strategy ensures global exploration by balancing the exploitation of a known map with the exploration of unknown areas. In addition, the online path replanning ensures safe navigation through the map created during motion. The performance of the proposed method is verified by exploring 3D simulation environments in comparison with the state-of-the-art methods. Finally, the proposed approach is validated in a real-world experiment. keywords: {Three-dimensional displays;Video on demand;Navigation;Warehousing;Buildings;Inspection;Web sites},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611139&isnumber=10609862

Q. Saadiyean, S. P. Samprithi and S. Sundaram, "Learning Multi-Scale Context Mask-RCNN Network for Slant Angled Aerial Imagery in Instance Segmentation in a Sim2Real setup," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13573-13580, doi: 10.1109/ICRA57147.2024.10610358.Abstract: While instance segmentation models excel at object detection in satellite imagery, their performance drops when applied to slant-angled aerial images due to occlusion and scale variation. This is mainly caused by a lack of training data for such diverse viewpoints and scales. To address this limitation, we propose the Sim2Real-based Multi-Scale Context Mask-RCNN (MSC-RCNN) network, specifically designed for slant-angled aerial imagery. Sim2Real-based transfer learning is adapted to compensate for the limited availability of real-world slant-angle training data. A synthetic dataset is generated using Unreal Engine, detailing the methodology of replicating the real-world scene, for producing diverse slant-angle drone datasets with various weather conditions and backgrounds. The model leverages two distinct feature pyramid backbones, with one incorporating dilated convolutions to address large-scale objects and the other optimized for regular convolutions. Their outputs are fused to effectively detect objects across various scales and angles. Through experiments, it was demonstrated that incorporating this synthetic data significantly reduces reliance on real data while maintaining high mean Average Precision (mAP) scores. Compared to the baseline Mask R-CNN, the proposed approach with Sim2Real adaptation and the MSC-RCNN architecture achieves a remarkable 7.6% performance improvement in instance segmentation accuracy with only a 6% increase in model size. Code can be found at: https://github.com/MSC-RCNN keywords: {Instance segmentation;Convolutional codes;Adaptation models;Accuracy;Surveillance;Transfer learning;Training data},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610358&isnumber=10609862

L. -C. Wang, Y. -C. Chu, Y. Huang and F. -L. Lian, "Enhancement on Target-Gripper Alignment: A Tomato Harvesting Robot with Dual-Camera Image-Based Visual Servoing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13611-13617, doi: 10.1109/ICRA57147.2024.10610772.Abstract: Automation application in crop harvesting has increased in the past decades. Various types of harvesting robots are emerging in both commercial and research areas. One of the main challenges is the precision alignment of the gripper and the target crop. An undesired dislocation can harm both the gripper and the crop, which is mainly caused by uncertainties from the sensors and the manipulator. To solve the problem, the dual-camera setup is designed and implemented on a self-built robot. The perception of the tomato is done by a fixed depth camera and a camera without depth on the gripper. The proposed dual-camera image-based visual servoing (IBVS) controller is designed to deal with the image feedback from both cameras and the proof of asymptotically convergence is provided. Furthermore, the cumulative error compensation reduces the time for the harvesting process. The experiments were conducted in the greenhouse and tested under various conditions. The time cost is formulated as a function and the success picking rate of tomatoes is 68.4%. keywords: {Uncertainty;Navigation;Green products;Crops;Error compensation;Cameras;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610772&isnumber=10609862

J. A. James, H. K. Manching, A. M. Hulse-Kemp and W. J. Beksi, "Few-Shot Fruit Segmentation via Transfer Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13618-13624, doi: 10.1109/ICRA57147.2024.10610003.Abstract: Advancements in machine learning, computer vision, and robotics have paved the way for transformative solutions in various domains, particularly in agriculture. For example, accurate identification and segmentation of fruits from field images plays a crucial role in automating jobs such as harvesting, disease detection, and yield estimation. However, achieving robust and precise infield fruit segmentation remains a challenging task since large amounts of labeled data are required to handle variations in fruit size, shape, color, and occlusion. In this paper, we develop a few-shot semantic segmentation framework for infield fruits using transfer learning. Concretely, our work is aimed at addressing agricultural domains that lack publicly available labeled data. Motivated by similar success in urban scene parsing, we propose specialized pre-training using a public benchmark dataset for fruit transfer learning. By leveraging pre-trained neural networks, accurate semantic segmentation of fruit in the field is achieved with only a few labeled images. Furthermore, we show that models with pre-training learn to distinguish between fruit still on the trees and fruit that have fallen on the ground, and they can effectively transfer the knowledge to the target fruit dataset. keywords: {Computer vision;Accuracy;Shape;Image color analysis;Semantic segmentation;Transfer learning;Neural networks;Agricultural Automation;Computer Vision for Automation;Object Detection;Segmentation and Categorization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610003&isnumber=10609862

S. B. Pandey, D. Colliaux and A. Chaudhury, "Spatio-Temporal Correspondence Estimation of Growing Plants by Hausdorff Distance based Skeletonization for Organ Tracking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13625-13631, doi: 10.1109/ICRA57147.2024.10610490.Abstract: Tracking of plant organs over spatio-temporal sequence of point cloud data is one of the demanding tasks of agricultural robotics for automated plant monitoring and growth analysis. Due to the complex geometry of plants, it is extremely difficult to identify and track the individual organs in different growth stages of plants. In this paper, we present an approach to perform correspondence estimation of different plant organs over a series of spatio-temporal data. The approach is based on two stages. In the first stage we develop a robust skeleton extraction method from unstructured plant point cloud data by adopting Hausdorff distance metric and modified breadth first search algorithm. The proposed skeletonization method is shown to be performing better than state-of-the-art, especially in handling very thin and delicate branches. We also address an overlooked problem of connecting skeleton points in the form of a graph, and demonstrate that different types of plant phenotype parameters can be obtained in a fully automatic manner from the skeleton graph. In the second stage, we exploit the skeleton graphs in developing an algorithm to perform correspondence estimation among the skeleton nodes using a cosine similarity based approach. We demonstrate the effectiveness of the proposed skeletonization technique in tracking different organs of the plant by finding good quality correspondences. Experiments are performed on three datasets on real and synthetic sequence of spatio-temporal plant point cloud data to demonstrate the effectiveness of the proposed method. keywords: {Point cloud compression;Target tracking;Phenotypes;Time series analysis;Estimation;Skeleton;Data mining},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610490&isnumber=10609862

V. N. Sankaranarayanan, A. Saradagi, S. Satpute and G. Nikolakopoulos, "A CBF-Adaptive Control Architecture for Visual Navigation for UAV in the Presence of Uncertainties," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13659-13665, doi: 10.1109/ICRA57147.2024.10611530.Abstract: In this article, we propose a control solution for the safe transfer of a quadrotor UAV between two surface robots positioning itself only using the visual features on the surface robots, which enforces safety constraints for precise landing and visual locking, in the presence of modeling uncertainties and external disturbances. The controller handles the ascending and descending phases of the navigation using a visual locking control barrier function (VCBF) and a parametrizable switching descending CBF (DCBF) respectively, eliminating the need for an external planner. The control scheme has a backstepping approach for the position controller with the CBF filter acting on the position kinematics to produce a filtered virtual velocity control input, which an adaptive controller tracks to overcome modeling uncertainties and external disturbances. The experimental validation is carried out with a UAV that navigates from the base to the target using an RGB camera. keywords: {Visualization;Uncertainty;Target tracking;Navigation;Velocity control;Robot vision systems;Switches},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611530&isnumber=10609862

Z. Yi et al., "Multimodal Indoor Localization Using Crowdsourced Radio Maps," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13666-13672, doi: 10.1109/ICRA57147.2024.10610683.Abstract: Indoor Positioning Systems (IPS) traditionally rely on odometry and building infrastructures like WiFi, often supplemented by building floor plans for increased accuracy. However, the limitation of floor plans in terms of availability and timeliness of updates challenges their wide applicability. In contrast, the proliferation of smartphones and WiFi-enabled robots has made crowdsourced radio maps – databases pairing locations with their corresponding Received Signal Strengths (RSS) – increasingly accessible. These radio maps not only provide WiFi fingerprint-location pairs but encode movement regularities akin to the constraints imposed by floor plans. This work investigates the possibility of leveraging these radio maps as a substitute for floor plans in multimodal IPS. We introduce a new framework to address the challenges of radio map inaccuracies and sparse coverage. Our proposed system integrates an uncertainty-aware neural network model for WiFi localization and a bespoken Bayesian fusion technique for optimal fusion. Extensive evaluations on multiple real-world sites indicate a significant performance enhancement, with results showing ∼ 25% improvement over the best baseline. keywords: {Location awareness;Uncertainty;Neural networks;Buildings;Bayes methods;IP networks;Floors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610683&isnumber=10609862

S. Matsuzaki et al., "CLIP-Loc: Multi-modal Landmark Association for Global Localization in Object-based Maps," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13673-13679, doi: 10.1109/ICRA57147.2024.10611393.Abstract: This paper describes a multi-modal data association method for global localization using object-based maps and camera images. In global localization, or relocalization, using object-based maps, existing methods typically resort to matching all possible combinations of detected objects and landmarks with the same object category, followed by inlier extraction using RANSAC or brute-force search. This approach becomes infeasible as the number of landmarks increases due to the exponential growth of correspondence candidates. In this paper, we propose labeling landmarks with natural language descriptions and extracting correspondences based on conceptual similarity with image observations using a Vision Language Model (VLM). By leveraging detailed text information, our approach efficiently extracts correspondences compared to methods using only object categories. Through experiments, we demonstrate that the proposed method enables more accurate global localization with fewer iterations compared to baseline methods, exhibiting its efficiency. keywords: {Location awareness;Accuracy;Natural languages;Search problems;Cameras;Labeling;Data mining},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611393&isnumber=10609862

T. Suzuki, "Multiple Update Particle Filter: Position Estimation by Combining GNSS Pseudorange and Carrier Phase Observations," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13680-13686, doi: 10.1109/ICRA57147.2024.10610209.Abstract: This paper presents an efficient method for updating particles in a particle filter (PF) to address the position estimation problem when dealing with sharp-peaked likelihood functions derived from multiple observations. Sharp-peaked likelihood functions commonly arise from millimeter-accurate distance observations of carrier phases in the global navigation satellite system (GNSS). However, when such likelihood functions are used for particle weight updates, the absence of particles within the peaks leads to all particle weights becoming zero. To overcome this problem, in this study, a straightforward and effective approach is introduced for updating particles when dealing with sharp-peaked likelihood functions obtained from multiple observations. The proposed method, termed as the multiple update PF, leverages prior knowledge regarding the spread of distribution for each likelihood function and conducts weight updates and resampling iteratively in the particle update process, prioritizing the likelihood function spreads. Experimental results demonstrate the efficacy of our proposed method, particularly when applied to position estimation utilizing GNSS pseudorange and carrier phase observations. The multiple update PF exhibits faster convergence with fewer particles when compared to the conventional PF. Moreover, vehicle position estimation experiments conducted in urban environments reveal that the proposed method outperforms conventional GNSS positioning techniques, yielding more accurate position estimates. keywords: {Global navigation satellite system;Estimation error;Accuracy;Three-dimensional displays;Urban areas;Estimation;Kinematics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610209&isnumber=10609862

Q. Xiao, Z. Zaidi and M. Gombolay, "Multi-Camera Asynchronous Ball Localization and Trajectory Prediction with Factor Graphs and Human Poses," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13695-13702, doi: 10.1109/ICRA57147.2024.10610631.Abstract: The rapid and precise localization and prediction of a ball are critical for developing agile robots in ball sports, particularly in sports like tennis characterized by high-speed ball movements and powerful spins. The Magnus effect induced by spin adds complexity to trajectory prediction during flight and bounce dynamics upon contact with the ground. In this study, we introduce an innovative approach that combines a multi-camera system with factor graphs for real-time and asynchronous 3D tennis ball localization. Additionally, we estimate hidden states like velocity and spin for trajectory prediction. Furthermore, to enhance spin inference early in the ball’s flight, where limited observations are available, we integrate human pose data using a temporal convolutional network (TCN) to compute spin priors within the factor graph. This refinement provides more accurate spin priors at the beginning of the factor graph, leading to improved early-stage hidden state inference for prediction. Our results show the trained TCN can predict the spin priors with RMSE of 5.27 Hz. Integrating TCN into the factor graph reduces the prediction error of landing positions by over 63.6% compared to a baseline method that utilized an adaptive extended Kalman filter. keywords: {Location awareness;Three-dimensional displays;Sports equipment;Neural networks;Prediction algorithms;Real-time systems;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610631&isnumber=10609862

A. Galeote-Luque, V. Kubelka, M. Magnusson, J. -R. Ruiz-Sarmiento and J. Gonzalez-Jimenez, "Doppler-only Single-scan 3D Vehicle Odometry," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13703-13709, doi: 10.1109/ICRA57147.2024.10611199.Abstract: We present a novel 3D odometry method that recovers the full motion of a vehicle only from a Doppler-capable range sensor. It leverages the radial velocities measured from the scene, estimating the sensor’s velocity from a single scan. The vehicle’s 3D motion, defined by its linear and angular velocities, is calculated taking into consideration its kinematic model which provides a constraint between the velocity measured at the sensor frame and the vehicle frame.Experiments carried out prove the viability of our single-sensor method compared to mounting an additional IMU. Our method provides a more reliable translation of the sensor, compared to the errors linked to IMUs due to noise and biases. Its short-term accuracy and fast operation (∼5ms) make it a proper candidate to supply the initialization to more complex localization algorithms or mapping pipelines. Not only does it reduce the error of the mapper, but it does so at a comparable level of accuracy as an IMU would. All without the need to mount and calibrate an extra sensor on the vehicle. keywords: {Accuracy;Three-dimensional displays;Roads;Heuristic algorithms;Pipelines;Kinematics;Odometry;Localization;Range Sensing;Autonomous Vehicle Navigation;Range Odometry;Radar;Doppler},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611199&isnumber=10609862

V. Kubelka, E. Fritz and M. Magnusson, "Do we need scan-matching in radar odometry?," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13710-13716, doi: 10.1109/ICRA57147.2024.10610666.Abstract: There is a current increase in the development of "4D" Doppler-capable radar and lidar range sensors that produce 3D point clouds where all points also have information about the radial velocity relative to the sensor. 4D radars in particular are interesting for object perception and navigation in low-visibility conditions (dust, smoke) where lidars and cameras typically fail. With the advent of high-resolution Doppler-capable radars comes the possibility of estimating odometry from single point clouds, foregoing the need for scan registration which is error-prone in feature-sparse field environments. We compare several odometry estimation methods, from direct integration of Doppler/IMU data and Kalman filter sensor fusion to 3D scan-to-scan and scan-to-map registration, on three datasets with data from two recent 4D radars and two IMUs. Surprisingly, our results show that the odometry from Doppler and IMU data alone give similar or better results than 3D point cloud registration. In our experiments, the position drift can be as low as 0.9% over 1.8 and 4.5km trajectories. That allows accurate estimation of 6-DOF ego-motion over long distances also in feature-sparse mine environments. These results are useful not least for applications of navigation with resource-constrained robot platforms in feature-sparse and low-visibility conditions such as mining, construction, and search & rescue operations. keywords: {Point cloud compression;Accuracy;Three-dimensional displays;Laser radar;Estimation;Radar;Radar imaging;4D Radar;Radar Odometry;Mobile robot;Localization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610666&isnumber=10609862

P. Yin et al., "Outram: One-shot Global Localization via Triangulated Scene Graph and Global Outlier Pruning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13717-13723, doi: 10.1109/ICRA57147.2024.10610206.Abstract: One-shot LiDAR localization refers to the ability to estimate the robot pose from one single point cloud, which yields significant advantages in initialization and relocalization processes. In the point cloud domain, the topic has been extensively studied as a global descriptor retrieval (i.e., loop closure detection) and pose refinement (i.e., point cloud registration) problem both in isolation or combined. However, few have explicitly considered the relationship between candidate retrieval and correspondence generation in pose estimation, leaving them brittle to substructure ambiguities. To this end, we propose a hierarchical one-shot localization algorithm called Outram that leverages substructures of 3D scene graphs for locally consistent correspondence searching and global substructure-wise outlier pruning. Such a hierarchical process couples the feature retrieval and the correspondence extraction to resolve the substructure ambiguities by conducting a local-to-global consistency refinement. We demonstrate the capability of Outram in a variety of scenarios in multiple large-scale outdoor datasets. Our implementation is open-sourced: https://github.com/Pamphlett/Outram. keywords: {Location awareness;Point cloud compression;Three-dimensional displays;Laser radar;Costs;Pose estimation;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610206&isnumber=10609862

C. Chen, Y. Peng and G. Huang, "Fast and Consistent Covariance Recovery for Sliding-window Optimization-based VINS," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13724-13731, doi: 10.1109/ICRA57147.2024.10610360.Abstract: In this paper, we introduce a novel and efficient technique for consistent covariance recovery in nonlinear optimization-based Visual-Inertial Navigation Systems (VINS). Estimating uncertainty in real-time is crucial for evaluating system performance and enhancing downstream operations such as data association. However accessing the marginal covariance of the state variables of interest in optimization-based VINS presents a significant challenge – a computational bottleneck due to the need to invert the high-dimensional information (Hessian) matrix. In our recent work [1], the First-Estimates Jacobian (FEJ) methodology was used to properly fix state linearization points in the optimization-based VINS, which seems counter-intuitive but improves the estimation performance in both consistency and accuracy. Capitalizing on this unique aspect of the FEJ strategy, in this work we carefully design the covariance recovery algorithm to improve efficiency by avoiding redundant computation. Remarkably, our approach achieves a computational speed that is 4-10 times faster than the existing methods. Through comprehensive numerical evaluations across four state-of-the-art marginalization archetypes, we not only affirm the consistency of our covariance estimates but underscore its superior computational efficiency. keywords: {Jacobian matrices;Uncertainty;Accuracy;Navigation;System performance;Estimation;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610360&isnumber=10609862

Y. Wu, T. Guadagnino, L. Wiesmann, L. Klingbeil, C. Stachniss and H. Kuhlmann, "LIO-EKF: High Frequency LiDAR-Inertial Odometry using Extended Kalman Filters," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13741-13747, doi: 10.1109/ICRA57147.2024.10610667.Abstract: Odometry estimation is crucial for every autonomous system requiring navigation in an unknown environment. In modern mobile robots, 3D LiDAR-inertial systems are often used for this task. By fusing LiDAR scans and IMU measurements, these systems can reduce the accumulated drift caused by sequentially registering individual LiDAR scans and provide a robust pose estimate. Although effective, LiDAR-inertial odometry systems require proper parameter tuning to be deployed. In this paper, we propose LIO-EKF, a tightly-coupled LiDAR-inertial odometry system based on point-to-point registration and the classical extended Kalman filter scheme. We propose an adaptive data association that considers the relative pose uncertainty, the map discretization errors, and the LiDAR noise. In this way, we can substantially reduce the parameters to tune for a given type of environment. The experimental evaluation suggests that the proposed system performs on par with the state-of-the-art LiDAR-inertial odometry pipelines but is significantly faster in computing the odometry. The source code of our implementation is publicly available (https://github.com/YibinWu/LIO-EKF). keywords: {Laser radar;Uncertainty;Three-dimensional displays;Source coding;Noise;Robot sensing systems;Odometry},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610667&isnumber=10609862

Q. Chen, G. Li, X. Xue and J. Pu, "Multi-LIO: A Lightweight Multiple LiDAR-Inertial Odometry System," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13748-13754, doi: 10.1109/ICRA57147.2024.10611257.Abstract: The integration of multiple LiDAR sensors has the potential to significantly enhance odometry systems by providing comprehensive environmental measurements. However, current multiple LiDAR-inertial odometry frameworks face challenges in real-time processing due to the voluminous data generated. This paper introduces a real-time, computationally efficient multiple LiDAR-inertial odometry system (Multi-LIO) that outperforms existing state-of-the-art solutions in accuracy and scalability. Utilizing a novel parallel strategy for state updates and a voxelized map format, Multi-LIO optimizes computational efficiency. Furthermore, we introduce a point-wise uncertainty estimation method to augment the accuracy of scan-to-map registration, particularly in large-scale and complex scenarios. We validate our system’s performance through extensive experiments on various challenging sequences. Multi-LIO emerges as a robust, scalable, and extensible solution, adaptable to various LiDAR configurations. keywords: {Accuracy;Uncertainty;Laser radar;Scalability;Urban areas;Estimation;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611257&isnumber=10609862

J. Morris, Y. Wang and V. Ila, "The Importance of Coordinate Frames in Dynamic SLAM," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13755-13761, doi: 10.1109/ICRA57147.2024.10610840.Abstract: Most Simultaneous localisation and mapping (SLAM) systems have traditionally assumed a static world, which does not align with real-world scenarios. To enable robots to safely navigate and plan in dynamic environments, it is essential to employ representations capable of handling moving objects. Dynamic SLAM is an emerging field in SLAM research as it improves the overall system accuracy while providing additional estimation of object motions. State-of-the-art literature informs two main formulations for Dynamic SLAM, representing dynamic object points in either the world or object coordinate frame. While expressing object points in their local reference frame may seem intuitive, it does not necessarily lead to the most accurate and robust solutions. This paper conducts and presents a thorough analysis of various Dynamic SLAM formulations, identifying the best approach to address the problem. To this end, we introduce a front-end agnostic framework using GTSAM [1] that can be used to evaluate various Dynamic SLAM formulations.1 keywords: {Simultaneous localization and mapping;Accuracy;Navigation;Robot kinematics;Dynamics;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610840&isnumber=10609862

P. Kaveti, M. Giamou, H. Singh and D. M. Rosen, "OASIS: Optimal Arrangements for Sensing in SLAM," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13818-13824, doi: 10.1109/ICRA57147.2024.10611644.Abstract: The number and arrangement of sensors on mobile robot dramatically influence its perception capabilities. Ensuring that sensors are mounted in a manner that enables accurate detection, localization, and mapping is essential for the success of downstream control tasks. However, when designing a new robotic platform, researchers and practitioners alike usually mimic standard configurations or maximize simple heuristics like field-of-view (FOV) coverage to decide where to place exteroceptive sensors. In this work, we conduct an information-theoretic investigation of this overlooked element of robotic perception in the context of simultaneous localization and mapping (SLAM). We show how to formalize the sensor arrangement problem as a form of subset selection under the E-optimality performance criterion. While this formulation is NP-hard in general, we show that a combination of greedy sensor selection and fast convex relaxation-based post-hoc verification enables the efficient recovery of certifiably optimal sensor designs in practice. Results from synthetic experiments reveal that sensors placed with OASIS outperform benchmarks in terms of mean squared error of visual SLAM estimates. keywords: {Location awareness;Visualization;Simultaneous localization and mapping;Robot vision systems;Sensors;Mobile robots;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611644&isnumber=10609862

I. Guzey, Y. Dai, B. Evans, S. Chintala and L. Pinto, "See to Touch: Learning Tactile Dexterity through Visual Incentives," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13825-13832, doi: 10.1109/ICRA57147.2024.10611407.Abstract: Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects’ spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI achieves a success rate of 73% using our four-fingered Allegro robot hand. The increase in performance is 108% higher than policies using tactile and vision-based rewards and 135% higher than policies without tactile observational input. Robot videos are best viewed on our project website: https://see-to-touch.github.io/. keywords: {Visualization;Limiting;Reinforcement learning;Robot sensing systems;Cognition;Sensors;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611407&isnumber=10609862

K. Chen et al., "Real-time Contact State Estimation in Shape Control of Deformable Linear Objects under Small Environmental Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 13833-13839, doi: 10.1109/ICRA57147.2024.10611558.Abstract: Controlling the shape of deformable linear objects using robots and constraints provided by environmental fixtures has diverse industrial applications. In order to establish robust contacts with these fixtures, accurate estimation of the contact state is essential for preventing and rectifying potential anomalies. However, this task is challenging due to the small sizes of fixtures, the requirement for real-time performances, and the infinite degrees of freedom of the deformable linear objects. In this paper, we propose a real-time approach for estimating both contact establishment and subsequent changes by leveraging the dependency between the applied and detected contact force on the deformable linear objects. We seamlessly integrate this method into the robot control loop and achieve an adaptive shape control framework which avoids, detects and corrects anomalies automatically. Real-world experiments validate the robustness and effectiveness of our contact estimation approach across various scenarios, significantly increasing the success rate of shape control processes. keywords: {Visualization;Shape control;Shape;Service robots;Fixtures;Robot control;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611558&isnumber=10609862

