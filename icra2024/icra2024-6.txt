N. Xu et al., "Aligning Knowledge Graph with Visual Perception for Object-goal Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5214-5220, doi: 10.1109/ICRA57147.2024.10610980.Abstract: Object-goal navigation is a challenging task that requires guiding an agent to specific objects based on first-person visual observations. The ability of agent to comprehend its surroundings plays a crucial role in achieving successful object finding. However, existing knowledge-graph-based navigators often rely on discrete categorical one-hot vectors and vote counting strategy to construct graph representation of the scenes, which results in misalignment with visual images. To provide more accurate and coherent scene descriptions and address this misalignment issue, we propose the Aligning Knowledge Graph with Visual Perception (AKGVP) method for object-goal navigation. Technically, our approach introduces continuous modeling of the hierarchical scene architecture and leverages visual-language pre-training to align natural language description with visual perception. The integration of a continuous knowledge graph architecture and multimodal feature alignment empowers the navigator with a remarkable zero-shot navigation capability. We extensively evaluate our method using the AI2-THOR simulator and conduct a series of experiments to demonstrate the effectiveness and efficiency of our navigator. keywords: {Visualization;Accuracy;Navigation;Natural languages;Knowledge graphs;Vectors;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610980&isnumber=10609862

J. Wang and H. Soh, "Probable Object Location (POLo) Score Estimation for Efficient Object Goal Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5221-5227, doi: 10.1109/ICRA57147.2024.10610671.Abstract: In this work, we focus on object search tasks within unexplored environments. We introduce a framework centered around the Probable Object Location (POLo) score. Utilizing a 3D object probability map, the POLo score allows the agent to make data-driven decisions for efficient object search. We further enhance the framework’s practicality by introducing POLoNet, a neural network trained to approximate the computationally-intensive POLo score. Our approach addresses critical limitations of both end-to-end reinforcement learning methods, which suffer from memory decay over long-horizon tasks, and traditional map-based methods that neglect visibility constraints. Our experiments, involving the first phase of the Open-Vocabulary Mobile Manipulation (OVMM) 2023 challenge, demonstrate that an agent equipped with POLoNet significantly outperforms a range of baseline methods, including end-to-end RL techniques and prior map-based strategies. To provide a comprehensive evaluation, we introduce new performance metrics that offer insights into the efficiency and effectiveness of various agents in object goal navigation. keywords: {Measurement;Three-dimensional displays;Navigation;Neural networks;Noise;Reinforcement learning;Search problems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610671&isnumber=10609862

W. Cai et al., "Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5228-5234, doi: 10.1109/ICRA57147.2024.10610499.Abstract: Zero-shot object navigation is a challenging task for home-assistance robots. This task emphasizes visual grounding, commonsense inference and locomotion abilities, where the first two are inherent in foundation models. But for the locomotion part, most works still depend on map-based planning approaches. The gap between RGB space and map space makes it difficult to directly transfer the knowledge from foundation models to navigation tasks. In this work, we propose a Pixel-guided Navigation skill (PixNav), which bridges the gap between the foundation models and the embodied navigation task. It is straightforward for recent foundation models to indicate an object by pixels, and with pixels as the goal specification, our method becomes a versatile navigation policy towards all different kinds of objects. Besides, our PixNav is a pure RGB-based policy that can reduce the cost of homeassistance robots. Experiments demonstrate the robustness of the PixNav which achieves 80+% success rate in the local path-planning task. To perform long-horizon object navigation, we design an LLM-based planner to utilize the commonsense knowledge between objects and rooms to select the best waypoint. Evaluations across both photorealistic indoor simulators and real-world environments validate the effectiveness of our proposed navigation strategy. More details are accessible via our project website https://sites.google.com/view/pixnav/. keywords: {Visualization;Costs;Navigation;Grounding;Large language models;Robustness;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610499&isnumber=10609862

P. Roth, J. Nubert, F. Yang, M. Mittal and M. Hutter, "ViPlanner: Visual Semantic Imperative Learning for Local Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5243-5249, doi: 10.1109/ICRA57147.2024.10610025.Abstract: Real-time path planning in outdoor environments still challenges modern robotic systems due to differences in terrain traversability, diverse obstacles, and the necessity for fast decision-making. Established approaches have primarily focused on geometric navigation solutions, which work well for structured geometric obstacles but have limitations regarding the semantic interpretation of different terrain types and their affordances. Moreover, these methods fail to identify traversable geometric occurrences, such as stairs. To overcome these issues, we introduce ViPlanner, a learned local path planning approach that generates local plans based on geometric and semantic information. The system is trained using the Imperative Learning paradigm, for which the network weights are optimized end-to-end based on the planning task objective. This optimization uses a differentiable formulation of a semantic costmap, which enables the planner to distinguish between the traversability of different terrains and accurately identify obstacles. The semantic information is represented in 30 classes using an RGB colorspace that can effectively encode the multiple levels of traversability. We show that the planner can adapt to diverse real-world environments without requiring any real-world training. In fact, the planner is trained purely in simulation, enabling a highly scalable training data generation. Experimental results demonstrate resistance to noise, zero-shot sim-to-real transfer, and a decrease of 38.02% in terms of traversability cost compared to purely geometric-based approaches. Code and models are made publicly available: https://github.com/leggedrobotics/viplanner. keywords: {Training;Resistance;Visualization;Navigation;Semantics;Training data;Stairs},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610025&isnumber=10609862

X. Lin et al., "UIVNAV: Underwater Information-driven Vision-based Navigation via Imitation Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5250-5256, doi: 10.1109/ICRA57147.2024.10611203.Abstract: Autonomous navigation in the underwater environment is challenging due to limited visibility, dynamic changes, and the lack of a cost-efficient, accurate localization system. We introduce UIVNAV, a novel end-to-end underwater navigation solution designed to navigate robots over Objects of Interest (OOI) while avoiding obstacles, all without relying on localization. UIVNAV utilizes imitation learning and draws inspiration from the navigation strategies employed by human divers, who do not rely on localization. UIVNAV consists of the following phases: (1) generating an intermediate representation (IR) and (2) training the navigation policy based on human-labeled IR. By training the navigation policy on IR instead of raw data, the second phase is domain-invariant — the navigation policy does not need to be retrained if the domain or the OOI changes. We demonstrate this within simulation by deploying the same navigation policy to survey two distinct Objects of Interest (OOIs): oyster and rock reefs. We compared our method with complete coverage and random walk methods, showing that our approach is more efficient in gathering information for OOIs while avoiding obstacles. The results show that UIVNAV chooses to visit the areas with larger area sizes of oysters or rocks with no prior information about the environment or localization. Moreover, a robot using UIVNAV compared to complete coverage method surveys on average 36% more oysters when traveling the same distances. We also demonstrate the feasibility of real-time deployment of UIVNAV in pool experiments with BlueROV underwater robot for surveying a bed of oyster shells. keywords: {Location awareness;Training;Surveys;Autonomous underwater vehicles;Navigation;Imitation learning;Rocks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611203&isnumber=10609862

C. Ye, G. De Croon and S. Hamaza, "A Biomorphic Whisker Sensor for Aerial Tactile Applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5257-5263, doi: 10.1109/ICRA57147.2024.10610850.Abstract: Unmanned air vehicles (UAVs) have traditionally been considered as "eyes in the sky", that can move in three dimensions and need to avoid any contact with their environment. On the contrary, contact should not be considered as a problem, but as an opportunity to expand the range of UAVs applications. In this paper, we designed, fabricated, and characterized a whisker sensor unit based on MEMS barometers suitable for tactile localization on UAVs, featuring lightweight, low stiffness, high sensitivity, a broad sensing range, and scalability. Then, for the challenging task of contact point localization, we propose a Recurrent Multi-output Network (RMN) for predicting 3D contact points under continuous contact conditions to address the problems of non-linearity, hysteresis, and non-injective mapping between signals and contact points by considering time series. In addition, we propose an azimuth prediction loss function which reduces the RMSE by 3.24◦ compared to L1 loss. Finally, we conduct experiments on a linear stage to validate the 3D contact point localization capability of the proposed whisker system and model. The results show that our localization can achieve excellent performance, with an inference time of 1.4 ms and a mean error of only 9.18 mm in Euclidean distance within 3D space, laying a robust foundation for future implementation of tactile localization on UAVs. The design files, dataset, and source code are available on: https://github.com/BioMorphic-Intelligence-Lab/Whisker-3D-Localization. keywords: {Location awareness;Three-dimensional displays;Sensitivity;Azimuth;Scalability;Euclidean distance;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610850&isnumber=10609862

A. Zhang, L. Chin, D. L. Tong and D. Rus, "Embedded air channels transform soft lattices into sensorized grippers," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5264-5270, doi: 10.1109/ICRA57147.2024.10610187.Abstract: Sensing plays a pivotal role in robotic manipulation, dictating the accuracy and versatility with which objects are handled. Vision-based sensing methods often suffer from fabrication complexity and low durability, while approaches that rely on direct measurements on the gripper often have limited resolution and are difficult to scale. Here, we present a soft robotic gripper made out of two cubic lattices that are sensorized by embedding air channels within the structure. The lattices are 3D printed from a single build material, simplifying the fabrication process. The flexibility of this approach offers significant control over sensor and lattice design, while the pressure-based internal sensing provides measurements with minimal disruption to the grasping surface. With only 12 sensors, 6 per lattice, this gripper can estimate an object’s weight and location and offer new insights into grasp parameters like friction coefficients and grasp force. keywords: {Fabrication;Three-dimensional displays;Lattices;Transforms;Grasping;Soft robotics;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610187&isnumber=10609862

H. Li, L. Jiang, R. Zhen, M. Cheng and K. Ding, "Design of A Rigid-soft Hybrid Robotic Glove with Force Sensing Function," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5279-5285, doi: 10.1109/ICRA57147.2024.10610415.Abstract: Soft robotic gloves can not only provide timely, effective, safe and cheap rehabilitation training for patients with impaired movement function of hand, but also assist in completing daily grasping activities. However, most soft robotic gloves are completely composed of flexible structures. Although they have high flexibility and safety, there are problems such as poor fit and low output force. In order to solve these problems, this paper refers to the structure of the human hand and designs an articulated rigid-soft hybrid robotic glove, which combines the advantages of rigid robotic gloves and soft robotic gloves, and has high flexibility, high output force and good fit. In addition, soft robotic gloves generally lack the ability to sense the force between the human hand and the glove. Therefore, this paper designed an array flexible force sensor, and studied the structure, signal acquisition and preparation process of the sensor. Finally, a complete test platform was built to test the performance of the rigid-soft hybrid robotic glove with force sensing function. The test results show that the robotic glove has good fit and high output force, can effectively assist training and assist grasping, and can perceive the contact force. keywords: {Training;Sensitivity;Force;Grasping;Soft robotics;Robot sensing systems;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610415&isnumber=10609862

D. Van Lewen, C. Wang, H. C. Lee, A. Devaiah, U. Upadhyay and S. Russo, "Capacitive Origami Sensing Modules for Measuring Force in a Neurosurgical, Soft Robotic Retractor," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5302-5308, doi: 10.1109/ICRA57147.2024.10610863.Abstract: In neurosurgery, soft robots have the potential to introduce significant benefits over traditional metal tools for their ability to safely interact with delicate tissues. In this paper, we introduce a proof-of-concept soft, capacitive origami sensing module (OSM) that can measure forces during neurosurgical retraction. Using origami-inspired design and fabrication principles, the OSM is easily folded and integrated within a soft robotic retractor that interacts with brain tissue to generate a surgical workspace upon actuation. We demonstrate the individual OSM signal response to forces and folding. We further characterize the OSM response within a fully-assembled soft robotic retractor to both folding and the application of forces over 0-5 N showing a 0.38 N average prediction error and resolution of 0.25 N. The sensing capability of the retractor is validated on an in-vitro model to demonstrate prediction errors of 0.06 N and the proposed operation during neurosurgery. keywords: {Fabrication;Force measurement;Force;Soft robotics;Predictive models;Robot sensing systems;Capacitance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610863&isnumber=10609862

V. Del Bono, M. McCandless, F. J. Wise and S. Russo, "A soft miniaturized continuum robot with 3D shape sensing via functionalized soft optical waveguides," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5309-5316, doi: 10.1109/ICRA57147.2024.10611549.Abstract: In this paper, we present a fully soft miniaturized continuum robot that integrates 3D optical shape sensing through functionalized tubing used as soft optical waveguides. The sensor is fabricated by laser patterning an off-the-shelf medical tubing, allowing for bidirectional responses to large curvatures in two bending directions, enabling 3D shape sensing and tip tracking of the continuum robot. The robot is able to bend and sense its own shape up to a curvature of 44.7 m-1, corresponding to a bending angle of 102°, having high-accuracy tracking capabilities, resulting in an average tracking error of 3.08 mm, that is 7.7 % of the robot length. The robot’s functionality was shown in validation experiments, including a real-time shape prediction through a graphical user interface. keywords: {Three-dimensional displays;Biomedical optical imaging;Shape;Optical device fabrication;Waveguide lasers;Robot sensing systems;Optical sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611549&isnumber=10609862

L. Ju, H. Jia, Y. Shi, X. Ding, Y. Feng and W. Zhang, "Continuously Estimate and Control Prosthetic Grip Force by an Optical Waveguide Sensor," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5317-5322, doi: 10.1109/ICRA57147.2024.10610613.Abstract: The emergence of intelligent prostheses has facilitated the life and work of disabled patients. The interaction aspect of prostheses has become a highlight research topic in the field of rehabilitation robotics. However, most of the existing prosthetic interaction methods focus on the use of myoelectricity to classify finite gestures, rather than continuous (infinite) force detection, which greatly limits the use of prosthetic scenarios. In this study, a novel optical waveguide sensor was used to collect muscle deformation information from the human arm for continuous control of the prosthetic grip force. The optical waveguide sensor was embedded with carbon fiber to limit the stretching of the waveguide, which led to the optical waveguide sensor being sensitive to bending deformation. Compared with EMGs, the accuracy of continuous grip force control based on the optical waveguide sensor is higher. The R-Square for prosthetic grip force and hand grip force were 0.867 and 0.9724 in the periodic and sustaining grip force experiments, respectively. The results suggested that the proposed method could provide a new approach to the interaction of prostheses. keywords: {Optical fibers;Integrated optics;Optical fiber sensors;Deformation;Force;Muscles;Robot sensing systems;Prosthetic interaction;grip force;muscle deformation information;optiacl waveguide},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610613&isnumber=10609862

C. Maxey, J. Choi, H. Lee, D. Manocha and H. Kwon, "UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5323-5329, doi: 10.1109/ICRA57147.2024.10611523.Abstract: Tremendous variations coupled with large degrees of freedom in UAV-based imaging conditions lead to a significant lack of data in adequately learning UAV-based perception models. Using various synthetic renderers in conjunction with perception models is prevalent to create synthetic data to augment the learning in the ground-based imaging domain. However, severe challenges in the austere UAV-based domain require distinctive solutions to image synthesis for data augmentation. In this work, we leverage recent advancements in neural rendering to improve static and dynamic novel-view UAV-based image synthesis, especially from high altitudes, capturing salient scene attributes. Finally, we demonstrate a considerable performance boost is achieved when a state-of-the-art detection model is optimized primarily on hybrid sets of real and synthetic data instead of the real or synthetic data separately. keywords: {Image synthesis;Imaging;Rendering (computer graphics);Data augmentation;Data models;Robotics and automation;Synthetic data},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611523&isnumber=10609862

J. Xing, L. Bauersfeld, Y. Song, C. Xing and D. Scaramuzza, "Contrastive Learning for Enhancing Robust Scene Transfer in Vision-based Agile Flight," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5330-5337, doi: 10.1109/ICRA57147.2024.10610095.Abstract: Scene transfer for vision-based mobile robotics applications is a highly relevant and challenging problem. The utility of a robot greatly depends on its ability to perform a task in the real world, outside of a well-controlled lab environment. Existing scene transfer end-to-end policy learning approaches often suffer from poor sample efficiency or limited generalization capabilities, making them unsuitable for mobile robotics applications. This work proposes an adaptive multi-pair contrastive learning strategy for visual representation learning that enables zero-shot scene transfer and real-world deployment. Control policies relying on the embedding are able to operate in unseen environments without the need for finetuning in the deployment environment. We demonstrate the performance of our approach on the task of agile, vision-based quadrotor flight. Extensive simulation and real-world experiments demonstrate that our approach successfully generalizes beyond the training domain and outperforms all baselines. Video: https://youtu.be/4A4YyPgEWD8 keywords: {Training;Representation learning;Visualization;Contrastive learning;Robustness;Safety;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610095&isnumber=10609862

F. Achermann et al., "Watching the Air Rise: Learning-Based Single-Frame Schlieren Detection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5338-5344, doi: 10.1109/ICRA57147.2024.10611416.Abstract: Detecting air flows caused by phenomena such as heat convection is valuable in multiple scenarios, including leak identification and locating thermal updrafts for extending UAV flight duration. Unfortunately, the heat signature of these flows is often too subtle to be seen by a thermal camera. While convection also leads to fluctuations in air density and hence causes so-called schlieren – intensity and color variations in images – existing techniques such as Background-oriented schlieren (BOS) allow detecting them only against a known background and from a static camera, making these approaches unsuitable for moving vehicles. In this work we demonstrate the feasibility of visualizing air movement by predicting the corresponding schlieren-induced optical flow from a single greyscale image captured by a moving camera against an unfamiliar background. We first record and label a set of optical flows in an indoor setup using standard BOS techniques. We then train a convolutional neural network (CNN) by applying the previously collected optical flow distortions to a dataset containing a mixture of real and synthetically generated images to predict the two-dimensional optical flow from a single image. Finally, we evaluate our approach on the task of extracting the optical flow caused by schlieren from both a static and moving camera on previously unseen flow patterns and background images. keywords: {Heating systems;Training;Training data;Optical distortion;Cameras;Distortion;Convolutional neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611416&isnumber=10609862

L. Crupi, A. Giusti and D. Palossi, "High-throughput Visual Nano-drone to Nano-drone Relative Localization using Onboard Fully Convolutional Networks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5345-5351, doi: 10.1109/ICRA57147.2024.10611455.Abstract: Relative drone-to-drone localization is a fundamental building block for any swarm operations. We address this task in the context of miniaturized nano-drones, i.e., ∼10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor. The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms. A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones. This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard. We present a vertically integrated system based on a novel vision-based fully convolutional neural network (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems. Considering the best-performing SoA approach, our model results in a R2 improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of ∼30k images. Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4min. keywords: {Location awareness;Visualization;Target tracking;Pose estimation;Batteries;System-on-chip;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611455&isnumber=10609862

L. Li and N. Zhao, "End-to-End Semi-Supervised 3D Instance Segmentation with PCTeacher," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5352-5358, doi: 10.1109/ICRA57147.2024.10610145.Abstract: 3D instance segmentation is a fundamental and critical task for enabling robots to operate effectively in unstructured 3D environments. In order to address the challenges posed by the high demand for large-scale annotated data and the limited availability of such data in the context of 3D instance segmentation, we study semi-supervised 3D instance segmentation problem and propose a novel end-to-end framework based on the mean teacher paradigm, named PCTeacher. Our PCTeacher generates both point-level and cluster-level pseudo labels to harness knowledge from unlabeled data. It notably enhances the training stability through end-to-end training and improves pseudo-label quality. Specifically, for point-level pseudo labels, PCTeacher employs a multi-view fusion strategy to achieve higher precision and recall. Regarding cluster-level pseudo labels, it introduces a hybrid grouping strategy to generate more potential proposals and utilizes a point-cluster agreement-based thresholding (PCAT) mechanism to fully exploit cluster-level pseudo labels. By combining and strengthening both point-level and cluster-level pseudo labels, our PCTeacher achieves state-of-the-art performance on two benchmark datasets across multiple labeled data ratios with a more compact network compared to the existing method. keywords: {Instance segmentation;Training;Three-dimensional displays;Accuracy;Benchmark testing;Hybrid power systems;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610145&isnumber=10609862

A. Cheng, Z. Yang, H. Zhu and K. Mao, "GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5367-5374, doi: 10.1109/ICRA57147.2024.10610653.Abstract: Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes. The incorporation of semantic constraints for indoor self-supervised depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model. Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation. Our code will be available at https://github.com/AnqiCheng1234/GAM-Depth. keywords: {Training;Adaptation models;Codes;Semantics;Pipelines;Estimation;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610653&isnumber=10609862

Y. Tian et al., "RoboKeyGen: Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5375-5381, doi: 10.1109/ICRA57147.2024.10611423.Abstract: Estimating robot pose and joint angles is significant in advanced robotics, enabling applications like robot collaboration and online hand-eye calibration. However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality. Previous methods either regress 3D keypoints directly or utilise a render&compare strategy. These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem. This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 2D keypoints detection and lifting 2D keypoints to 3D. This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques. A vital component of our method is the lifting of 2D keypoints to 3D keypoints. Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions. Leveraging the robust modeling potential of diffusion models, we reframe this issue as a conditional 3D keypoints generation task. To bolster cross-camera adaptability, we introduce the Normalised Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics. Experimental results demonstrate that the proposed method outperforms the state-of-the-art render&compare method and achieves higher inference speed. Furthermore, the tests accentuate our method’s robust cross-camera generalisation capabilities. We intend to release both the dataset and code in https://nimolty.github.io/Robokeygen/. keywords: {Solid modeling;Three-dimensional displays;Uncertainty;Robot kinematics;Robot vision systems;Pose estimation;Collaboration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611423&isnumber=10609862

R. Zhao et al., "Advancements in 3D Lane Detection Using LiDAR Point Clouds: From Data Collection to Model Development," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5382-5388, doi: 10.1109/ICRA57147.2024.10610087.Abstract: Advanced Driver-Assistance Systems (ADAS) have successfully integrated learning-based techniques into vehicle perception and decision-making. However, their application in 3D lane detection for effective driving environment perception is hindered by the lack of comprehensive LiDAR datasets. The sparse nature of LiDAR point cloud data prevents an efficient manual annotation process. To solve this problem, we present LiSV-3DLane, a large-scale 3D lane dataset that comprises 20k frames of surround-view LiDAR point clouds with enriched semantic annotation. Unlike existing datasets confined to a frontal perspective, LiSV-3DLane provides a full 360-degree spatial panorama around the ego vehicle, capturing complex lane patterns in both urban and highway environments. We leverage the geometric traits of lane lines and the intrinsic spatial attributes of LiDAR data to design a simple yet effective automatic annotation pipeline for generating finer lane labels. To propel future research, we propose a novel LiDAR-based 3D lane detection model, LiLaDet, incorporating the spatial geometry learning of the LiDAR point cloud into Bird’s Eye View (BEV) based lane identification. Experimental results indicate that LiLaDet outperforms existing camera- and LiDAR-based approaches in the 3D lane detection task on the K-Lane dataset and our LiSV-3DLane. The project code will be available at https://github.com/RunkaiZhao/LiLaDet. keywords: {Point cloud compression;Solid modeling;Three-dimensional displays;Laser radar;Lane detection;Annotations;Semantics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610087&isnumber=10609862

Y. Kuang et al., "STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects on Production Lines," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5389-5396, doi: 10.1109/ICRA57147.2024.10611585.Abstract: In this work, we present STOPNet, a framework for 6-DoF object suction detection on production lines, with a focus on but not limited to transparent objects, which is an important and challenging problem in robotic systems and modern industry. Current methods requiring depth input fail on transparent objects due to depth cameras’ deficiency in sensing their geometry, while we proposed a novel framework to reconstruct the scene on the production line depending only on RGB input, based on multiview stereo. Compared to existing works, our method not only reconstructs the whole 3D scene in order to obtain high-quality 6-DoF suction poses in real time but also generalizes to novel environments, novel arrangements and novel objects, including challenging transparent objects, both in simulation and the real world. Extensive experiments in simulation and the real world show that our method significantly surpasses the baselines and has better generalizability, which caters to practical industrial needs. keywords: {Industries;Three-dimensional displays;Service robots;Pipelines;Production;Robot sensing systems;6-DOF},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611585&isnumber=10609862

M. Vecerik et al., "RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5397-5403, doi: 10.1109/ICRA57147.2024.10611409.Abstract: For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes. keywords: {Training;Visualization;Tracking;Stacking;Production facilities;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611409&isnumber=10609862

S. -M. Yang, M. Magnusson, J. A. Stork and T. Stoyanov, "Learning Extrinsic Dexterity with Parameterized Manipulation Primitives," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5404-5410, doi: 10.1109/ICRA57147.2024.10611431.Abstract: Many practically relevant robot grasping problems feature a target object for which all grasps are occluded, e.g., by the environment. Single-shot grasp planning invariably fails in such scenarios. Instead, it is necessary to first manipulate the object into a configuration that affords a grasp. We solve this problem by learning a sequence of actions that utilize the environment to change the object’s pose. Concretely, we employ hierarchical reinforcement learning to combine a sequence of learned parameterized manipulation primitives. By learning the low-level manipulation policies, our approach can control the object’s state through exploiting interactions between the object, the gripper, and the environment. Designing such a complex behavior analytically would be infeasible under uncontrolled conditions, as an analytic approach requires accurate physical modeling of the interaction and contact dynamics. In contrast, we learn a hierarchical policy model that operates directly on depth perception data, without the need for object detection, pose estimation, or manual design of controllers. We evaluate our approach on picking box-shaped objects of various weight, shape, and friction properties from a constrained table-top workspace. Our method transfers to a real robot and is able to successfully complete the object picking task in 98% of experimental trials. keywords: {Training;Shape;Pose estimation;Grasping;Reinforcement learning;Object detection;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611431&isnumber=10609862

M. Sivertsvik, K. Sumskiy and E. Misimi, "Learning active manipulation to target shapes with model-free, long-horizon deep reinforcement learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5411-5418, doi: 10.1109/ICRA57147.2024.10610033.Abstract: We investigate the active manipulation of objects using model-free and long-horizon DRL (Deep Reinforcement Learning) to achieve target shapes. Our proposed approach uses visual observations consisting of segmented images, to mitigate the sim-to-real gap. We address a long-horizon manipulation task requiring a sequence of accurate actions to achieve the target shapes using a robot arm with an RGB-D camera in eye-in-hand configuration, and an elongated, volumetric, elastoplastic object. We find similar objects in food, marine, and manufacturing domains. The aim is to actively manipulate the object into an arbitrary target shape using image observations. We trained a DRL agent using PPO (Proximal Policy Optimization) by running 768 parallel actors in simulation, for a total of 1,2M environment interactions, and tested this on 200 unseen target deformations. In three attempts, 82% of the trials achieved a greater than 90% overlap with the 200 target shapes. By relying on segmentation images as a visual observation space, we successfully transferred the agent to the real world without supplementary training. Our approach does not need any real-world manipulation examples nor fine-tuning in the real world. The robustness of our approach was demonstrated in simulation, and experimentally validated in the real world for specific manipulation tasks, achieving a 94.2% mean zero-shot overlap success rate on previously unseen target shapes. keywords: {Training;Image segmentation;Visualization;Accuracy;Shape;Robot vision systems;Deep reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610033&isnumber=10609862

Q. Yu et al., "GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5419-5426, doi: 10.1109/ICRA57147.2024.10610652.Abstract: Articulated objects like cabinets and doors are widespread in daily life. However, directly manipulating 3D articulated objects is challenging because they have diverse geometrical shapes, semantic categories, and kinetic constraints. Prior works mostly focused on recognizing and manipulating articulated objects with specific joint types. They can either estimate the joint parameters or distinguish suitable grasp poses to facilitate trajectory planning. Although these approaches have succeeded in certain types of articulated objects, they lack generalizability to unseen objects, which significantly impedes their application in broader scenarios. In this paper, we propose a novel framework of Generalizable Articulation Modeling and Manipulating for Articulated Objects (GAMMA), which learns both articulation modeling and grasp pose affordance from diverse articulated objects with different categories. In addition, GAMMA adopts adaptive manipulation to iteratively reduce the modeling errors and enhance manipulation performance. We train GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive experiments in SAPIEN simulation and real-world Franka robot. Results show that GAMMA significantly outperforms SOTA articulation modeling and manipulation algorithms in unseen and cross-category articulated objects. Images, videos and codes are published on the project website at: sites.google.com/view/gamma-articulation. keywords: {Adaptation models;Three-dimensional displays;Trajectory planning;Shape;Affordances;Semantics;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610652&isnumber=10609862

Y. Liu et al., "Efficient End-to-End Detection of 6-DoF Grasps for Robotic Bin Picking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5427-5433, doi: 10.1109/ICRA57147.2024.10611417.Abstract: Bin picking is an important building block for many robotic systems, in logistics, production or in household use-cases. In recent years, machine learning methods for the prediction of 6-DoF grasps on diverse and unknown objects have shown promising progress. However, existing approaches only consider a single ground truth grasp orientation at a grasp location during training and therefore can only predict limited grasp orientations which leads to a reduced number of feasible grasps in bin picking with restricted reachability. In this paper, we propose a novel approach for learning dense and diverse 6-DoF grasps for parallel-jaw grippers in robotic bin picking. We introduce a parameterized grasp distribution model based on Power-Spherical distributions that enables a training based on all possible ground truth samples. Thereby, we also consider the grasp uncertainty enhancing the model’s robustness to noisy inputs. As a result, given a single top-down view depth image, our model can generate diverse grasps with multiple collision-free grasp orientations. Experimental evaluations in simulation and on a real robotic bin picking setup demonstrate the model’s ability to generalize across various object categories achieving an object clearing rate of around 90% in simulation and real-world experiments. We also outperform state of the art approaches. Moreover, the proposed approach exhibits its usability in real robot experiments without any refinement steps, even when only trained on a synthetic dataset, due to the probabilistic grasp distribution modeling. keywords: {Training;Uncertainty;Network architecture;6-DOF;Collision avoidance;Grippers;Usability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611417&isnumber=10609862

E. Sayar, Z. Bing, C. D’Eramo, O. S. Oguz and A. Knoll, "Contact Energy Based Hindsight Experience Prioritization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5434-5440, doi: 10.1109/ICRA57147.2024.10610910.Abstract: Multi-goal robot manipulation tasks with sparse rewards are difficult for reinforcement learning (RL) algorithms due to the inefficiency in collecting successful experiences. Recent algorithms such as Hindsight Experience Replay (HER) expedite learning by taking advantage of failed trajectories and replacing the desired goal with one of the achieved states so that any failed trajectory can be utilized as a contribution to learning. However, HER uniformly chooses failed trajectories, without taking into account which ones might be the most valuable for learning. In this paper, we address this problem and propose a novel approach Contact Energy Based Prioritization (CEBP) to select the samples from the replay buffer based on rich information due to contact, leveraging the touch sensors in the gripper of the robot and object displacement. Our prioritization scheme favors sampling of contact-rich experiences, which are arguably the ones providing the largest amount of information. We evaluate our proposed approach on various sparse reward robotic tasks and compare it with the state-of-the-art methods. We show that our method surpasses or performs on par with those methods on robot manipulation tasks. Finally, we deploy the trained policy from our method to a real Franka robot for a pick-and-place task. We observe that the robot can solve the task successfully. The videos and code are publicly available at: https://erdiphd.github.io/HER_force/. keywords: {Training;Codes;Friction;Catalysts;Tactile sensors;Reinforcement learning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610910&isnumber=10609862

J. Shi et al., "ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5441-5447, doi: 10.1109/ICRA57147.2024.10611152.Abstract: In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera. ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments. In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction. We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion. Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer. Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs. Project page: https://pku-epic.github.io/ASGrasp keywords: {Geometry;Upper bound;Three-dimensional displays;Robot vision systems;Grasping;Cameras;6-DOF},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611152&isnumber=10609862

Q. Dong, T. Kaneko and M. Sugiyama, "An offline learning of behavior correction policy for vision-based robotic manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5448-5454, doi: 10.1109/ICRA57147.2024.10610177.Abstract: Offline learning usually requires a large dataset for training. In this paper, we focus on vision-based robotic manipulation tasks and utilize certain task properties to achieve offline learning with a small dataset. We propose a two-stage agent consisting of a tentative decision stage and a correction stage, where the tentative decision stage determines a tentative action from the original camera image, and the correction stage determines a correction to the tentative action based on the cropped image according to the tentative action. The correction stage utilizes task properties to obtain the cropped image with task-relevant features, enabling efficient correction. In particular, the training of the two stages can be performed individually, which enables a straightforward application of general offline learning algorithms. We conduct experiments by combining the two-stage agent with conventional offline reinforcement learning and imitation learning algorithms. In both cases, we benchmark the proposed method using RLBench and demonstrate that the task performance is significantly improved by the correction stage. keywords: {Training;Imitation learning;Robot vision systems;Crops;Reinforcement learning;Benchmark testing;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610177&isnumber=10609862

M. Kwon, H. Hu, V. Myers, S. Karamcheti, A. Dragan and D. Sadigh, "Toward Grounded Commonsense Reasoning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5463-5470, doi: 10.1109/ICRA57147.2024.10611218.Abstract: Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the "tidying." How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded commonsense reasoning. To evaluate our framework at scale, we release the MessySurfaces dataset which contains images of 70 real-world surfaces that need to be cleaned. We additionally illustrate our approach with a robot on 2 carefully designed surfaces. We find an average 12.9% improvement on the MessySurfaces benchmark and an average 15% improvement on the robot experiments over baselines that do not use active perception. The dataset, code, and videos of our approach can be found at https://minaek.github.io/grounded_commonsense_reasoning/. keywords: {Pediatrics;Grounding;Large language models;Toy manufacturing industry;Automobiles;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611218&isnumber=10609862

M. Rehm, A. L. Krummheuer and C. G. Cubero, "The effect of rejection strategy on trust and shopping choices in robot-assisted shopping *," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5471-5477, doi: 10.1109/ICRA57147.2024.10611366.Abstract: In this paper, we investigate how a customer-facing service robot can support decision making in shopping interactions. In this role, a robot needs sometimes to reject a customer’s choice. Thus, we investigate different rejection strategies with the goal of changing customer behavior. The implemented strategies have been developed based on an ethnographic study on assisted shopping and tested in a lab experiment with 31 participants. The experiment showed significant differences in trust ratings and decision-making depending on the employed strategy. keywords: {Service robots;Decision making;Focusing;Context modeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611366&isnumber=10609862

A. Halilovic and S. Krivic, "Planning of Explanations for Robot Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5478-5484, doi: 10.1109/ICRA57147.2024.10611000.Abstract: The choices made by autonomous robots in social settings bear consequences for humans and their presumptions of robot behavior. Explanations can serve to alleviate detrimental impacts on humans and amplify their comprehension of robot decisions. We model the process of explanation generation for robot navigation as an automated planning problem considering different possible explanation attributes. Our visual and textual explanations of a robot’s navigation are influenced by the robot’s personality. Moreover, they account for different contextual, environmental, and spatial characteristics. We present the results of a user study demonstrating that users are more satisfied with multimodal than unimodal explanations. Additionally, our findings reveal low user satisfaction with explanations of a robot with extreme personality traits. In conclusion, we deliberate on potential future research directions and the associated constraints. Our work advocates for fostering socially adept and safe autonomous robot navigation. keywords: {Visualization;Navigation;Planning;Autonomous robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611000&isnumber=10609862

Y. Zhou and J. Garcke, "Learning Crowd Behaviors in Navigation with Attention-based Spatial-Temporal Graphs," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5485-5491, doi: 10.1109/ICRA57147.2024.10610279.Abstract: Safe and efficient navigation in dynamic environments shared with humans remains an open and challenging task for mobile robots. Previous works have shown the efficacy of using reinforcement learning frameworks to train policies for efficient navigation. However, their performance deteriorates when crowd configurations change, i.e. become larger or more complex. Thus, it is crucial to fully understand the complex, dynamic, and sophisticated interactions of the crowd resulting in proactive and foresighted behaviors for robot navigation. In this paper, a novel deep graph learning architecture based on attention mechanisms is proposed, which leverages the spatial-temporal graph to enhance robot navigation. We employ spatial graphs to capture the current spatial interactions, and through the integration with RNN, the temporal graphs utilize past trajectory information to infer the future intentions of each agent. The spatial-temporal graph reasoning ability allows the robot to better understand and interpret the relationships between agents over time and space, thereby making more informed decisions. Compared to previous state-of-the-art methods, our method demonstrates superior robustness in terms of safety, efficiency, and generalization in various challenging scenarios. keywords: {Navigation;Computational modeling;Decision making;Reinforcement learning;Robustness;Trajectory;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610279&isnumber=10609862

L. Grassi, Z. Hong, C. T. Recchiuto and A. Sgorbissa, "Grounding Conversational Robots on Vision Through Dense Captioning and Large Language Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5492-5498, doi: 10.1109/ICRA57147.2024.10611232.Abstract: This work explores a novel approach to empowering robots with visual perception capabilities using textual descriptions. Our approach involves the integration of GPT-4 with dense captioning, enabling robots to perceive and interpret the visual world through detailed text-based descriptions. To assess both user experience and the technical feasibility of this approach, experiments were conducted with human participants interacting with a Pepper robot equipped with visual capabilities. The results affirm the viability of the proposed approach, allowing to perform vision-based conversations effectively, despite processing time limitations. keywords: {Visualization;Face recognition;Oral communication;User experience;Delays;Face detection;Time factors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611232&isnumber=10609862

I. Bakhoda, P. Shahverdi, K. Rousso, J. Klotz and W. -Y. G. Louie, "Exploring the Impact of Narrator Type on Response Latency and Utterance Length During Interactive Storytelling," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5499-5504, doi: 10.1109/ICRA57147.2024.10610817.Abstract: The inexorable progress of technology brought forth an era where robots increasingly integrate into human life which necessitates the understanding of human-robot interactions (HRI). This study unravels the details of HRI within interactive storytelling contexts. Through a between-subject experiment with 28 participants, we assessed response latency and utterance lengths to interactive story narrations delivered by either a human or a robot. Findings indicated that participants displayed longer response latency interacting with the robot narrator while articulating shorter utterances compared to the human condition where participants displayed longer utterances and shorter response latency. These observations suggest significant differences in cognitive and communicative strategies in human-human versus human-robot interactions. The results underscore the challenges and potential of designing social robots that are time-sensitive in interacting with humans. Future explorations should focus on the cognitive and emotional drivers behind these interactions. keywords: {Social robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610817&isnumber=10609862

R. Gomez et al., "Design of Embodied Mediator Haru for Remote Cross Cultural Communication," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5505-5511, doi: 10.1109/ICRA57147.2024.10611253.Abstract: Social robots for children have focused mainly on conventional education domains such as teaching language, science, and math, while applications focusing on the enhancement of cultural competency are quite scarce. In this paper, we present a prototype of a robot-mediation framework for cross-cultural communication. This framework paves the way for a social robot to act as a mediator between groups of schoolchildren from different countries. First, we conducted a participatory design activity by an interdisciplinary team, resulting in the extraction of the design, robot’s roles, and technical requirements. Based on these requirements, we built the robot-mediation system prototype. We conducted a pilot study using the system with groups of high school children in Japan and Australia and our results show the potential of the system to drive children’s interest in communicating, sharing, and discussing cultural themes with their remote peers through the social robot. keywords: {Technical requirements;Social robots;Education;Prototypes;Focusing;Drives;Cross-cultural communication},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611253&isnumber=10609862

Z. Su and W. Sheng, "ChatAdp: ChatGPT-powered Adaptation System for Human-Robot Interaction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5512-5518, doi: 10.1109/ICRA57147.2024.10611520.Abstract: Different people have different preferences when it comes to human-robot interaction. Therefore, it is desirable for the robot to adapt its actions to fit users’ preferences. Human feedback is essential to facilitating robot adaptation. However, when the task is complex or the robot action space is large, it requires a large amount of user feedback. ChatGPT is a powerful generative AI tool based on large language models (LLMs), which possesses a significant corpus of information obtained from human society, and exhibits robust proficiency in the comprehension and acquisition of natural language. Therefore, in this paper, we proposed a ChatGPT-powered adaptation system (ChatAdp) for human-robot interaction which requires less user feedback to achieve a good adaptation result. In the proposed ChatAdp, we use ChatGPT as a user simulator to provide feedback. We evaluated ChatAdp in a case study for context-aware conversation adaptation. The results are very promising. Our proposed method can achieve a mean success rate of 92% on the user’s natural language-described preferences after receiving 33 rounds of feedback from a user on average, which is only 2% of the number of states covered by the user preferences and outperforms the two baseline methods. keywords: {Generative AI;Large language models;Natural languages;Human-robot interaction;Oral communication;Chatbots;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611520&isnumber=10609862

B. Akbas, H. T. Yuksel, A. Soylemez, M. E. Zyada, M. Sarac and F. Stroppa, "The Impact of Evolutionary Computation on Robotic Design: A Case Study with an Underactuated Hand Exoskeleton," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5519-5525, doi: 10.1109/ICRA57147.2024.10611070.Abstract: Robotic exoskeletons can enhance human strength and aid people with physical disabilities. However, designing them to ensure safety and optimal performance presents significant challenges. Developing exoskeletons should incorporate specific optimization algorithms to find the best design. This study investigates the potential of Evolutionary Computation (EC) methods in robotic design optimization, with an underactuated hand exoskeleton (U-HEx) used as a case study. We propose improving the performance and usability of the U-HEx design, which was initially optimized using a naive brute-force approach, by integrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch Algorithm. Comparative analysis revealed that EC methods consistently yield more precise and optimal solutions than brute force in a significantly shorter time. This allowed us to improve the optimization by increasing the number of variables in the design, which was impossible with naive methods. The results show significant improvements in terms of the torque magnitude the device transfers to the user, enhancing its efficiency. These findings underline the importance of performing proper optimization while designing exoskeletons, as well as providing a significant improvement to this specific robotic design. keywords: {Performance evaluation;Torque;Exoskeletons;Medical treatment;Evolutionary computation;User experience;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611070&isnumber=10609862

M. Cooper et al., "Design & Systematic Evaluation of Power Transmission Efficiency of an Ankle Exoskeleton for Walking Post-Stroke," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5526-5532, doi: 10.1109/ICRA57147.2024.10610736.Abstract: Community-based locomotor training post-stroke has shown improvements in independent ambulation by increasing dose, intensity, and specificity of walking practice. Robotic ankle exoskeletons hold the potential to facilitate continued rehabilitation at home, but understanding what aspects of the design are most relevant for successful translation to the community presents a challenge. Here, we design a portable rigid ankle exoskeleton to use as a research platform for investigating the effect of assistance on post-stroke gait during overground, community-based walking. We first test our device with stroke survivors and validate its potential for future community use. We then present a systematic method for quantifying power transmission losses at each transmission stage from the battery to the wearer, using data gathered from walking trials with healthy participants. Our evaluation method revealed inefficiencies in power transfer at the interface level, likely resulting from the compliance in the structural components of the system, which motivates future redesign considerations. Overall, our method provides a framework to identify and characterize the components that must be redesigned to lower exoskeleton weight and maximize performance. keywords: {Legged locomotion;Ankle;Training;Systematics;Exoskeletons;Power transmission;Propagation losses},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610736&isnumber=10609862

T. Shimoyama, T. Noda, T. Teramae and Y. Nakata, "Achieving Mechanical Transparency Using Fusion Hybrid Linear Actuator for Shoulder Flexion and Extension in Exoskeleton Robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5533-5539, doi: 10.1109/ICRA57147.2024.10611131.Abstract: Recently, the importance of mechanical transparency in human-assistive robots has grown. Traditionally, its primary goal was minimizing interaction forces during assistance. However, under this conventional definition, mechanical transparency was not considered when an interaction force was required during assistance. This research focuses on achieving mechanical transparency within the context of shoulder motion in upper extremity exoskeletons for rehabilitation. Our primary goal is maintaining interaction forces at target values, even with motion disturbances. To this end, we developed a shoulder actuation testbed for exoskeletons, incorporating a fusion hybrid linear actuator distinguished by high back-drivability, robust torque generation capability, and safety features. To attain mechanical transparency, we created a model for calculating the required joint torque, accounting for gravitational dynamics, and subsequently determined the necessary actuator output. The system characteristics were evaluated based on the joint torque generated by the actuator. The actuator utilized pneumatic pressure to generate force and compensated for kinetic friction using electromagnetic forces. The results showed that the compensation by the electromagnetic force reduced the root mean square error of the torque to less than 60% in relation to pneumatic pressure alone. This demonstrated the ability to generate consistent torque with high robustness to motion disturbances. keywords: {Actuators;Torque;Friction;Exoskeletons;Shoulder;Electromagnetic forces;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611131&isnumber=10609862

R. Giannattasio, S. Maludrottu, G. Zinni, E. D. Momi, M. Laffranchi and L. D. Michieli, "An adaptable ankle trajectory generation method for lower-limb exoskeletons by means of safety constraints computation and minimum jerk planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5548-5554, doi: 10.1109/ICRA57147.2024.10610191.Abstract: This paper presents a method to compute smooth ankle trajectories for lower limb exoskeletons with powered ankle joints. The proposed approach defines ankle trajectories using four polynomial functions, each representing one of the four primary phases of gait. These polynomials are computed according to different safety constraints. During the single support phase, ground contact constraints are enforced. In the swing phase, an optimization problem is solved to achieve minimum jerk planning while respecting a set of equality and inequality constraints designed to minimize the risk of stumbling. The used approach focuses on making the ankle joint able to smoothly adapt in real-time to different walking styles defined by user-selected gait parameters such as step length and clearance. The primary aim is to improve the user experience by producing a secure and comfortable walking pattern. To validate the effectiveness of the proposed method, the new ankle trajectories were tested on a group of healthy volunteers using the TWIN lower limb exoskeleton. keywords: {Ankle;Legged locomotion;Exoskeletons;Polynomials;User experience;Real-time systems;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610191&isnumber=10609862

N. Wannawas, C. Diaz-Pintado, J. Narayan and A. A. Faisal, "Controlling FES of arm movements using physics-informed reinforcement learning via co-kriging adjustment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5555-5560, doi: 10.1109/ICRA57147.2024.10610521.Abstract: Upper limb paralysis affects the quality of life. Functional Electrical Stimulation (FES) offers a solution to restore lost motor functions. Yet, there remain challenges in controlling FES to induce arbitrary arm movements. Reinforcement learning (RL) emerges as a promising method for controlling arm movement with success in simulation. However, challenges remain in translating the successes into real-world settings. One dominant challenge is the sample efficiency of RL. This study presents a practical RL setup to control FES for arm movements. We also present a flexible method, called co-kriging adjustment (CKA), which combines a biomechanical simulator and real data to build an accurate model of the real system. We demonstrate our RL-based control on a 2-DoF planar setting where the subject’s arm, placed on a frictionless supporter, is stimulated to perform point-to-point reaching. By using 90 seconds of real interaction data, our RL-based control can perform the reaching with the average error over the workspace of 5.5 cm. Beyond the application of FES, our method can be extended to other control systems, propelling RL towards general uses in the real world. keywords: {Reinforcement learning;Propulsion;Motors;Manipulators;Control systems;Iron;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610521&isnumber=10609862

A. Christou, A. J. Del-Ama, J. C. Moreno and S. Vijayakumar, "Adaptive Control for Triadic Human-Robot-FES Collaboration in Gait Rehabilitation: A Pilot Study," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5561-5568, doi: 10.1109/ICRA57147.2024.10611133.Abstract: The hybridisation of robot-assisted gait training and functional electrical stimulation (FES) can provide numerous physiological benefits to neurological patients. However, the design of an effective hybrid controller poses significant challenges. In this over-actuated system, it is extremely difficult to find the right balance between robotic assistance and FES that will provide personalised assistance, prevent muscle fatigue and encourage the patient’s active participation in order to accelerate recovery. In this paper, we present an adaptive hybrid robot-FES controller to do this and enable the triadic collaboration between the patient, the robot and FES. A patient-driven controller is designed where the voluntary movement of the patient is prioritised and assistance is provided using FES and the robot in a hierarchical order depending on the patient’s performance and their muscles’ fitness. The performance of this hybrid adaptive controller is tested in simulation and on one healthy subject. Our results indicate an increase in tracking performance with lower overall assistance, and less muscle fatigue when the hybrid adaptive controller is used, compared to its non adaptive equivalent. This suggests that our hybrid adaptive controller may be able to adapt to the behaviour of the user to provide assistance as needed and prevent the early termination of physical therapy due to muscle fatigue. keywords: {Training;Tracking;Collaboration;Muscles;Fatigue;Iron;Physiology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611133&isnumber=10609862

M. Lamsey et al., "Stretch with Stretch: Physical Therapy Exercise Games Led by a Mobile Manipulator," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5569-5576, doi: 10.1109/ICRA57147.2024.10611524.Abstract: Physical therapy (PT) is a key component of many rehabilitation regimens, such as treatments for Parkinson’s disease (PD). However, there are shortages of physical therapists and adherence to self-guided PT is low. Robots have the potential to support physical therapists and increase adherence to self-guided PT, but prior robotic systems have been large and immobile, which can be a barrier to use in homes and clinics. We present Stretch with Stretch (SWS), a novel robotic system for leading stretching exercise games for older adults with PD. SWS consists of a compact and lightweight mobile manipulator (Hello Robot Stretch RE1) that visually and verbally guides users through PT exercises. The robot’s soft end effector serves as a target that users repetitively reach towards and press with a hand, foot, or knee. For each exercise, target locations are customized for the individual via a visually estimated kinematic model, a haptically estimated range of motion, and the person’s exercise performance. The system includes sound effects and verbal feedback from the robot to keep users engaged throughout a session and augment physical exercise with cognitive exercise. We conducted a user study for which people with PD (n = 10) performed 6 exercises with the system. Participants perceived the SWS to be useful and easy to use. They also reported mild to moderate perceived exertion (RPE). keywords: {Knee;Presses;Adaptation models;Medical treatment;Kinematics;Games;End effectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611524&isnumber=10609862

P. Song, P. Li, E. Aertbeliën and R. Detry, "Robot Trajectron: Trajectory Prediction-based Shared Control for Robot Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5585-5591, doi: 10.1109/ICRA57147.2024.10611507.Abstract: We address the problem of (a) predicting the trajectory of an arm reaching motion, based on a few seconds of the motion’s onset, and (b) leveraging this predictor to facilitate shared-control manipulation tasks, by reducing the operator’s cognitive load through assistance in their anticipated direction of motion. Our novel intent estimator, dubbed the Robot Trajectron (RT), produces a probabilistic representation of the robot’s anticipated trajectory based on its recent position, velocity and acceleration history. By taking arm dynamics into account, RT can capture the operator’s intent better than other SOTA models that only use the arm’s position, making it particularly well-suited to assist in tasks where the operator’s intent is susceptible to change. We derive a novel shared-control solution that combines RT’s predictive capacity to a representation of the locations of potential reaching targets. Our experiments demonstrate RT’s effectiveness in both intent estimation and shared-control tasks. We will make the code and data supporting our experiments publicly available at https://gitlab.kuleuven.be/detry-lab/public/robot-trajectron keywords: {Codes;Dynamics;Estimation;Grasping;Probabilistic logic;Cognitive load;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611507&isnumber=10609862

H. J. Choi, S. Das, S. Peng, R. Bajcsy and N. Figueroa, "On the Feasibility of EEG-based Motor Intention Detection for Real-Time Robot Assistive Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5592-5599, doi: 10.1109/ICRA57147.2024.10610321.Abstract: This paper explores the feasibility of employing EEG-based intention detection for real-time robot assistive control. We focus on predicting and distinguishing motor intentions of left/right arm movements by presenting: i) an offline data collection and training pipeline, used to train a classifier for left/right motion intention prediction, and ii) an online real-time prediction pipeline leveraging the trained classifier and integrated with an assistive robot. Central to our approach is a rich feature representation composed of the tangent space projection of time-windowed sample covariance matrices from EEG filtered signals and derivatives; allowing for a simple SVM classifier to achieve unprecedented accuracy and real-time performance. In pre-recorded real-time settings (160 Hz), a peak accuracy of 86.88% is achieved, surpassing prior works. In robot-in-the-loop settings, our system successfully detects intended motion solely from EEG data with 70% accuracy, triggering a robot to execute an assistive task. We provide a comprehensive evaluation of the proposed classifier. keywords: {Training;Support vector machines;Accuracy;Pipelines;Data collection;Motors;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610321&isnumber=10609862

Y. Song, T. Chen, S. Li and J. Li, "Microexpression to Macroexpression: Facial Expression Magnification by Single Input," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5600-5607, doi: 10.1109/ICRA57147.2024.10610258.Abstract: Microexpressions are expressions that people inadvertently express, and therefore often represent a person’s true emotion. However, because it has a low intensity and a short duration, it is hard to be recognized correctly. In this paper, we propose a deep learning magnification method to generate macroexpressions from a single microexpression image. In the first stage, we extract the expression information from a single microexpression image. Then, We combine the idea of cyclegan and optical flow consistency to model the extracted expression features as the optical flow field between the neutral face and microexpressions. To extract a reliable optical flow field from the expression information, we design an optical flow refiner. In the second stage, we adopt an encoder-decoder network and let it learn to magnify the optical flow. Finally, the magnified optical flow guided the microexpression images to generate macroexpression images. We compare our single input based network with current two-frames-input based networks. The results show that our method performs better, even in wild images. We fed our magnified images directly into a simple ResNet18 network for recognition, achieving a competitive score under the MEGC2019 standard, compared with recent complex recognition networks. keywords: {Deep learning;Image recognition;Face recognition;Feature extraction;Reliability engineering;Data mining;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610258&isnumber=10609862

Y. -C. Kung, A. Zhang, J. Wang and J. Biswas, "Looking Inside Out: Anticipating Driver Intent From Videos," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5608-5614, doi: 10.1109/ICRA57147.2024.10610257.Abstract: Anticipating driver intention is an important task when vehicles of mixed and varying levels of human/machine autonomy share roadways. Driver intention can be leveraged to improve road safety, such as warning surrounding vehicles in the event the driver is attempting a dangerous maneuver. In this work, we propose a novel method of utilizing both in-cabin and external camera data to improve state-of-the-art performance in predicting future driver actions. Compared to existing methods, our approach explicitly extracts object and road-level features from external camera data, which we demonstrate are important features for predicting driver intention. Using our handcrafted features as inputs for both a transformer and a long-short-term-memory-based architecture, we empirically show that jointly utilizing in-cabin and external features improves performance compared to using in-cabin features alone. Furthermore, our models predict driver maneuvers more accurately and sooner than existing approaches, with an accuracy of 87.5% and an average prediction time of 4.35 seconds before the maneuver takes place. We release our model configurations and training scripts on https://github.com/ykung83/Driver-Intent-Prediction. keywords: {Training;Accuracy;Predictive models;Feature extraction;Cameras;Transformers;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610257&isnumber=10609862

D. Rivkin, N. Kakodkar, F. Hogan, B. H. Baghi and G. Dudek, "CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5615-5621, doi: 10.1109/ICRA57147.2024.10610072.Abstract: This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation. We focus on following complex instructions that are more akin to natural conversation than traditional explicit procedural directives typically seen in robotics. Unlike most prior work where navigation directives are provided as simple imperative commands (e.g., "go to the fridge"), we examine implicit directives obtained through conversational interactions.We leverage the 3D simulator AI2Thor to create household query scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot using our method CARTIER (Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots) can parse descriptive language queries up to 42% more reliably than existing LLM-enabled methods by exploiting the ability of LLMs to interpret the user interaction in the context of the objects in the scenario. keywords: {Three-dimensional displays;Navigation;Large language models;Natural languages;Oral communication;Cognition;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610072&isnumber=10609862

A. Rasouli, "A Novel Benchmarking Paradigm and a Scale- and Motion-Aware Model for Egocentric Pedestrian Trajectory Prediction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5630-5636, doi: 10.1109/ICRA57147.2024.10610614.Abstract: In this paper, we present a new paradigm for evaluating egocentric pedestrian trajectory prediction algorithms. Based on various contextual information, we extract driving scenarios for a meaningful and systematic approach to identifying challenges for prediction models. In this regard, we also propose a new metric for more effective ranking within the scenario-based evaluation. We conduct extensive empirical studies of existing models on these scenarios to expose shortcomings and strengths of different approaches. The scenario-based analysis highlights the importance of using multimodal sources of information and challenges caused by inadequate modeling of ego-motion and scale of pedestrians. To this end, we propose a novel egocentric trajectory prediction model that benefits from multimodal sources of data fused in an effective and efficient step-wise hierarchical fashion and two auxiliary tasks designed to learn more robust representation of scene dynamics. We conduct empirical evaluation on common benchmark datasets and show that our model not only achieves state-of-the-art performance, but also significantly improves performance by up to 39% in challenging scenarios, such as high ego-speed, compared to the past arts1. keywords: {Pedestrians;Systematics;Predictive models;Benchmark testing;Data models;Trajectory;Data mining},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610614&isnumber=10609862

M. Jayasuriya, G. Hu, D. D. K. Le, K. Ang, S. Sankaran and D. Liu, "A 3D Vector Field and Gaze Data Fusion Framework for Hand Motion Intention Prediction in Human-Robot Collaboration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5637-5643, doi: 10.1109/ICRA57147.2024.10609996.Abstract: In human-robot collaboration (HRC) settings, hand motion intention prediction (HMIP) plays a pivotal role in ensuring prompt decision-making, safety, and an intuitive collaboration experience. Precise and robust HMIP with low computational resources remains a challenge due to the stochastic nature of hand motion and the diversity of HRC tasks. This paper proposes a framework that combines hand trajectories and gaze data to foster robust, real-time HMIP with minimal to no training. A novel 3D vector field method is introduced for hand trajectory representation, leveraging minimum jerk trajectory predictions to discern potential hand motion endpoints. This is statistically combined with gaze fixation data using a weighted Naive Bayes Classifier (NBC). Acknowledging the potential variances in saccadic eye motion due to factors like fatigue or inattentiveness, we incorporate stationary gaze entropy to gauge visual concentration, thereby adjusting the contribution of gaze fixation to the HMIP. Empirical experiments substantiate that the proposed framework robustly predicts intended endpoints of hand motion before at least 50% of the trajectory is completed. It also successfully exploits gaze fixations when the human operator is attentive and mitigates its influence when the operator loses focus. A real-time implementation in a construction HRC scenario (collaborative tiling) showcases the intuitive nature and potential efficiency gains to be leveraged by introducing the proposed HMIP into HRC contexts. The opens-ource implementation of the framework is made available at https://github.com/maleenj/hmip_ros.git. keywords: {Training;Visualization;Three-dimensional displays;Collaboration;Data integration;Benchmark testing;Fatigue},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609996&isnumber=10609862

X. Liu, H. Chen and H. Chen, "Contrastive Learning-Based Attribute Extraction Method for Enhanced Terrain Classification," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5644-5650, doi: 10.1109/ICRA57147.2024.10611271.Abstract: The outdoor environment has many uneven surfaces that put the robot at risk of sinking or tipping over. Recognizing the type of terrain can help robot avoid risks and choose an appropriate gait. One of the critical problems is how to extract the terrain-related knowledge from sensor data collected as the robot traversed the ground. Many existing vision-based approaches are limited in directly perceiving the intrinsic properties of various terrains. The intuitive approach entails directly analyzing data recorded by the robot’s proprioceptive sensors. However, it faces challenges in being specific to certain robot leg configurations or in the lack of interpretability of the extracted features. In this paper, a terrain attribute extraction algorithm is proposed based on contrastive learning. It leverages the haptic data generated from the interaction between the robot’s legs and terrain to automatically extract terrain attributes. The results demonstrate that the attributes extracted using this method strongly correlate with the actual softness of the terrain. Furthermore, these attributes played an important role in achieving high accuracy in terrain classification tasks. keywords: {Legged locomotion;Dimensionality reduction;Accuracy;Propioception;Contrastive learning;Feature extraction;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611271&isnumber=10609862

J. -C. Peng, S. Yao and K. Hauser, "3D Force and Contact Estimation for a Soft-Bubble Visuotactile Sensor Using FEM," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5666-5672, doi: 10.1109/ICRA57147.2024.10610233.Abstract: Soft-bubble tactile sensors have the potential to capture dense contact and force information across a large contact surface. However, it is difficult to extract contact forces directly from observing the bubble surface because local contacts change the global surface shape significantly due to membrane mechanics and air pressure. This paper presents a model-based method of reconstructing dense contact forces from the bubble sensor’s internal RGBD camera and air pressure sensor. We present a finite element model of the force response of the bubble sensor that uses a linear plane stress approximation that only requires calibrating 3 variables. Our method is shown to reconstruct normal and shear forces significantly more accurately than the state-of-the-art, with comparable accuracy for detecting the contact patch, and with very little calibration data. keywords: {Deformable models;Surface reconstruction;Accuracy;Three-dimensional displays;Deformation;Atmospheric modeling;Force},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610233&isnumber=10609862

W. Lai et al., "A Detachable FBG-Based Contact Force Sensor for Capturing Gripper-Vegetable Interactions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5673-5679, doi: 10.1109/ICRA57147.2024.10611433.Abstract: Vertical farming, a sustainable key for urban agriculture, has garnered attention for its land use optimization and enhanced food production capabilities. The adoption of automation in vertical farming is a pivotal response to labor shortages, addressing the need for increased efficiency, particularly in labor-intensive tasks like harvesting. Although soft robotic grippers offer a significant promise for delicately handling fragile objects, the absence of sensors has hindered their full potential to execute precise and secure grasping. To address this challenge, we present a new solution: a detachable Fiber Bragg Grating-based flexible contact force sensor to capture gripper-vegetable interactions. The sensing module was 3D printed using soft material, and the FBG fiber was attached to the module using epoxy. From evaluation tests, this lightweight sensor demonstrated a wide measurement range of up to 9.87 N, with a high sensitivity of 141.7 pm/N, good repeatability, and a hysteresis of 7.96%. Compared to commercial load cells, our sensor achieves a small measurement RMSE of 0.41 N and a percentage error of 4.15%. The sensor was integrated into two robotic 3D-printed soft grippers to enable real-time monitoring of dynamic contact force during vegetable harvesting in vertical farming scenarios. By reflecting contact status, this sensor provides a promising glimpse into the future of agricultural automation, enhancing operational efficiency and strengthening situation awareness and decision-making capabilities in vertical farms. Beyond agriculture, the versatility of this sensor extends to application in areas such as warehousing, logistics, and the food and beverage industry. keywords: {Automation;Three-dimensional displays;Force;Warehousing;Robot sensing systems;Fiber gratings;Agriculture;Force and Tactile Sensing;Soft Sensors and Actuators;Agricultural Automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611433&isnumber=10609862

Z. Song et al., "SATac: A Thermoluminescence Enabled Tactile Sensor for Concurrent Perception of Temperature, Pressure, and Shear," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5680-5686, doi: 10.1109/ICRA57147.2024.10610373.Abstract: Most vision-based tactile sensors use elastomer deformation to infer tactile information, which can not sense some modalities, like temperature. As an important part of human tactile perception, temperature sensing can help robots better interact with the environment. In this work, we propose a novel multi-modal vision-based tactile sensor, SATac, which can simultaneously perceive information on temperature, pressure, and shear. SATac utilizes the thermoluminescence of strontium aluminate to sense a wide range of temperatures with exceptional resolution. Additionally, the pressure and shear can also be perceived by analyzing the Voronoi diagram. A series of experiments are conducted to verify the performance of our proposed sensor. We also discuss the possible application scenarios and demonstrate how SATac could benefit robot perception capabilities. keywords: {Temperature sensors;Strontium;Temperature distribution;Deformation;Tactile sensors;Elastomers;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610373&isnumber=10609862

B. Y. Su, Y. Wu, C. Wen and C. Liu, "Optimizing Multi-Touch Textile and Tactile Skin Sensing Through Circuit Parameter Estimation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5687-5693, doi: 10.1109/ICRA57147.2024.10610053.Abstract: Tactile and textile skin technologies have become increasingly important for enhancing human-robot interaction and allowing robots to adapt to different environments. Despite notable advancements, there are ongoing challenges in skin signal processing, particularly in achieving both accuracy and speed in dynamic touch sensing. This paper introduces a new framework that poses the touch sensing problem as an estimation problem of resistive sensory arrays. Utilizing a Regularized Least Squares objective function—which estimates the resistance distribution of the skin—we enhance the touch sensing accuracy and mitigate the ghosting effects, where false or misleading touches may be registered. Furthermore, our study presents a streamlined skin design that simplifies manufacturing processes without sacrificing performance. Experimental outcomes substantiate the effectiveness of our method, showing 26.9% improvement in multi-touch force-sensing accuracy for the tactile skin. keywords: {Accuracy;Parameter estimation;Surface resistance;Force;Estimation;Robot sensing systems;Skin},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610053&isnumber=10609862

B. Xu et al., "CushSense: Soft, Stretchable, and Comfortable Tactile-Sensing Skin for Physical Human-Robot Interaction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5694-5701, doi: 10.1109/ICRA57147.2024.10610014.Abstract: Whole-arm tactile feedback is crucial for robots to ensure safe physical interaction with their surroundings. This paper introduces CushSense, a fabric-based soft and stretchable tactile-sensing skin designed for physical human-robot interaction (pHRI) tasks such as robotic caregiving. Using stretchable fabric and hyper-elastic polymer, CushSense identifies contacts by monitoring capacitive changes due to skin deformation. CushSense is cost-effective (∼US$7 per taxel) and easy to fabricate. We detail the sensor design and fabrication process and perform characterization, highlighting its high sensing accuracy (relative error of 0.58%) and durability (0.054% accuracy drop after 1000 interactions). We also present a user study underscoring its perceived safety and comfort for the assistive task of limb manipulation. We open source all sensor-related resources on emprise.cs.cornell.edu/cushsense. keywords: {Accuracy;Human-robot interaction;Tactile sensors;Skin;Fabrics;Sensors;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610014&isnumber=10609862

O. Azulay, A. Mizrahi, N. Curtis and A. Sintov, "Augmenting Tactile Simulators with Real-like and Zero-Shot Capabilities," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5702-5708, doi: 10.1109/ICRA57147.2024.10610442.Abstract: Simulating tactile perception could potentially leverage the learning capabilities of robotic systems in manipulation tasks. However, the reality gap of simulators for high-resolution tactile sensors remains large. Models trained on simulated data often fail in zero-shot inference and require fine-tuning with real data. In addition, work on high-resolution sensors commonly focus on ones with flat surfaces while 3D round sensors are essential for dexterous manipulation. In this paper, we propose a bi-directional Generative Adversarial Network (GAN) termed SightGAN. SightGAN relies on the early CycleGAN while including two additional loss components aimed to accurately reconstruct background and contact patterns including small contact traces. The proposed SightGAN learns real-to-sim and sim-to-real processes over difference images. It is shown to generate real-like synthetic images while maintaining accurate contact positioning. The generated images can be used to train zero-shot models for newly fabricated sensors. Consequently, the resulted sim-to-real generator could be built on top of the tactile simulator to provide a real-world framework. Potentially, the framework can be used to train, for instance, reinforcement learning policies of manipulation tasks. The proposed model is verified in extensive experiments with test data collected from real sensors and also shown to maintain embedded force information within the tactile images. keywords: {Image sensors;Three-dimensional displays;Accuracy;Force;Tactile sensors;Generative adversarial networks;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610442&isnumber=10609862

P. Arm, M. Mittal, H. Kolvenbach and M. Hutter, "Pedipulate: Enabling Manipulation Skills using a Quadruped Robot’s Leg," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5717-5723, doi: 10.1109/ICRA57147.2024.10611307.Abstract: Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 kg at the foot. Additionally, the controller is robust to interaction forces at the foot, disturbances at the base, and slippery contact surfaces. Videos of the experiments are available at https://sites.google.com/leggedrobotics.com/pedipulate. keywords: {Legged locomotion;Training;Target tracking;Reinforcement learning;Quadrupedal robots;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611307&isnumber=10609862

H. Shi, T. Li, Q. Zhu, J. Sheng, L. Han and M. Q. . -H. Meng, "An Efficient Model-Based Approach on Learning Agile Motor Skills without Reinforcement," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5724-5730, doi: 10.1109/ICRA57147.2024.10611560.Abstract: Learning-based methods have improved locomotion skills of quadruped robots through deep reinforcement learning. However, the sim-to-real gap and low sample efficiency still limit the skill transfer. To address this issue, we propose an efficient model-based learning framework that combines a world model with a policy network. We train a differentiable world model to predict future states and use it to directly supervise a Variational Autoencoder (VAE)-based policy network to imitate real animal behaviors. This significantly reduces the need for real interaction data and allows for rapid policy updates. We also develop a high-level network to track diverse commands and trajectories. Our simulated results show a tenfold sample efficiency increase compared to reinforcement learning methods such as PPO. In real-world testing, our policy achieves proficient command-following performance with only a two-minute data collection period and generalizes well to new speeds and paths. keywords: {Training;Visualization;Adaptation models;Predictive models;Motors;Deep reinforcement learning;Turning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611560&isnumber=10609862

X. Zeng, H. Zhang, L. Yue, Z. Song, L. Zhang and Y. -H. Liu, "Adaptive Model Predictive Control with Data-driven Error Model for Quadrupedal Locomotion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5731-5737, doi: 10.1109/ICRA57147.2024.10611302.Abstract: Model Predictive Control (MPC) relies heavily on the robot model for its control law. However, a gap always exists between the reduced-order control model with uncertainties and the real robot, which degrades its performance. To address this issue, we propose the controller of integrating a data-driven error model into traditional MPC for quadruped robots. Our approach leverages real-world data from sensors to compensate for defects in the control model. Specifically, we employ the Autoregressive Moving Average Vector (ARMAV) model to construct the state error model of the quadruped robot using data. The predicted state errors are then used to adjust the predicted future robot states generated by MPC. By such an approach, our proposed controller can provide more accurate inputs to the system, enabling it to achieve desired states even in the presence of model parameter inaccuracies or disturbances. The proposed controller exhibits the capability to partially eliminate the disparity between the model and the real-world robot, thereby enhancing the locomotion performance of quadruped robots. We validate our proposed method through simulations and real-world experimental trials on a large-size quadruped robot that involves carrying a 20 kg un-modeled payload (84% of body weight). keywords: {Legged locomotion;Adaptation models;Predictive models;Robot sensing systems;Data models;Sensors;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611302&isnumber=10609862

F. Asano, T. Sedoguchi and C. Yan, "Generation of Steady Wheel Gait for Planar X-shaped Walker with Reaction Wheel," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5766-5772, doi: 10.1109/ICRA57147.2024.10610478.Abstract: This paper addresses the problem of realizing a novel robotic bipedal locomotion called wheel gait, which is achieved by rotating the stance and swing legs in the same direction. First, a model of a planar 3-DOF X-shaped walker with a reaction wheel is introduced, and the mathematical equations are described. Second, the condition for stabilizing zero dynamics is formulated as the time integral value of control input to the reaction wheel for one step becomes zero, and the control system for achieving this is designed based on the method of continuous-time output deadbeat control. Third, a typical steady wheel gait of the linearized model is numerically generated, and its extension to the nonlinear model is discussed. Although the nonlinear model has only one nonlinear term in the gravity term, numerical simulations show that there is a big gap between this and the linearized model. Through analysis of the typical nonlinear wheel gaits, the difficulty of achieving the same walking speed as the linearized model is discussed. keywords: {Legged locomotion;Analytical models;3-DOF;Wheels;Numerical simulation;Control systems;Mathematical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610478&isnumber=10609862

K. -L. Lu, I. -C. Chang, W. -S. Yu and P. -C. Lin, "Trajectory Optimization Strategy That Considers Body Tip-Over Stability, Limb Dynamics, and Motion Continuity in Legged Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5773-5779, doi: 10.1109/ICRA57147.2024.10611365.Abstract: We propose a limb trajectory planning method that considers both body and limb dynamics in robots, particularly suitable for those with non-trivial limb mass. To simplify the complexity and computation cost of using the full-body dynamics of the limbs, a reduced-order model that can simulate the dynamic characteristics of the original limb is proposed. The performance of the model is experimentally validated using an exemplary single leg-wheel of the leg-wheel transformable robot. The limb trajectory optimization is developed using a genetic algorithm that considers many aspects, including body and limb dynamics, limb workspace, limb motion continuity, body tip-over stability, and power consumption. The performance of the proposed limb trajectory planning strategy is experimentally validated using the same leg-wheel transformable robot, and the results confirm the effectiveness of the strategy. keywords: {Legged locomotion;Power demand;Trajectory planning;Heuristic algorithms;Dynamics;Stability analysis;Reduced order systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611365&isnumber=10609862

D. Sacerdoti, F. Benzi and C. Secchi, "A Reinforcement Learning-based Control Strategy for Robust Interaction of Robotic Systems with Uncertain Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5788-5794, doi: 10.1109/ICRA57147.2024.10610082.Abstract: In the context of interaction with unmodelled systems, it becomes imperative for a robot controller to possess the capability to dynamically adjust its actions in real-time, enhancing its resilience in the face of fluctuating environmental conditions. This adaptation process must be performed in a stability-preserving fashion, and resourcefully exploit the knowledge acquired during the interaction process. In this article, we propose a novel control strategy, based on the synergistic usage of state-of-the-art passivity-based control and Deep Reinforcement Learning (DRL). The concept of energy tank is used to provide stability guarantees for the interaction controller with uncertain environments, while an online learning policy allows to properly estimate the requirements of the task and adapt the controller accordingly, thus simultaneously achieving stability and performance. The proposed architecture is successfully validated through simulations and experiments with a collaborative manipulator in a surface polishing task. keywords: {Adaptation models;Collaboration;Manipulators;Deep reinforcement learning;Stability analysis;Real-time systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610082&isnumber=10609862

W. Li, W. Xu, P. Huang, B. Lin and B. Liang, "Stiffness-Based Hybrid Motion/ Force Control for Cable-Driven Serpentine Manipulator*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5795-5800, doi: 10.1109/ICRA57147.2024.10611054.Abstract: In recent years, there has been a growing demand for robotic manipulators to perform tasks in various unstructured environments and situations requiring precision and force control. However, traditional robotic arms have limitations in fully leveraging their advantages in such scenarios. To address this demand, we have designed a cable-driven serpentine manipulator (CDSM) that combines force and precision motion control. This control method allows for precise manipulation of forces and torques at the end-effector, particularly in applications like electric vehicle charging and narrow-space exploration. It also enables independent control in multiple configurations. We achieve force-position hybrid control in task space, ensuring accurate control of end-effector force while achieving precise position control in other directions. Additionally, we implement joint angle closed-loop control in joint space to reduce the impact of cable elasticity deformation and friction on joint motion accuracy. Finally, servo control is applied at the lowest motor level. This paper investigates the modeling, sensing, and control of CDSM within a unified framework of hybrid motion/force control. Through experiments and simulations, we demonstrate the high accuracy and practicality of this control method in various scenarios. keywords: {Accuracy;Force;Aerospace electronics;Arms;End effectors;Sensors;Task analysis;Cable-driven serpentine manipulator;Stiffness modeling;Hybrid motion/ force control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611054&isnumber=10609862

Y. -M. Chen, H. Bui and M. Posa, "Reinforcement Learning for Reduced-order Models of Legged Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5801-5807, doi: 10.1109/ICRA57147.2024.10610747.Abstract: Model-based approaches for planning and control for bipedal locomotion have a long history of success. It can provide stability and safety guarantees while being effective in accomplishing many locomotion tasks. Model-free reinforcement learning, on the other hand, has gained much popularity in recent years due to computational advancements. It can achieve high performance in specific tasks, but it lacks physical interpretability and flexibility in re-purposing the policy for a different set of tasks. For instance, we can initially train a neural network (NN) policy using velocity commands as inputs. However, to handle new task commands like desired hand or footstep locations at a desired walking velocity, we must retrain a new NN policy. In this work, we attempt to bridge the gap between these two bodies of work on a bipedal platform. We formulate a model-based reinforcement learning problem to learn a reduced-order model (ROM) within a model predictive control (MPC). Results show a 49% improvement in viable task region size and a 21% reduction in motor torque cost. All videos and code are available at https://sites.google.com/view/ymchen/research/rl-for-roms. keywords: {Legged locomotion;Torque;Computational modeling;Reinforcement learning;Artificial neural networks;Reduced order systems;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610747&isnumber=10609862

Z. Wang, H. Zhang and J. Wang, "K-BMPC: Derivative-based Koopman Bilinear Model Predictive Control For Tractor-trailer Trajectory Tracking With Unknown Parameters," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5808-5813, doi: 10.1109/ICRA57147.2024.10610320.Abstract: Nonlinear dynamics bring difficulties to controller design for control-affine systems such as tractor-trailer vehicles, especially when the parameters in the dynamics are unknown. To address this constraint, we propose a derivative-based lifting function construction method, show that the corresponding infinite dimensional Koopman bilinear model over the lifting function is equivalent to the original control-affine system. Further, we analyze the propagation and bounds of state prediction errors caused by the truncation in derivative order. The identified finite dimensional Koopman bilinear model would serve as predictive model in the next step. Koopman Bilinear Model Predictive control (K-BMPC) is proposed to solve the trajectory tracking problem. We linearize the bilinear model around the estimation of the lifted state and control input. Then the bilinear Model Predictive Control problem is approximated by a quadratic programming problem. Further, the estimation is updated at each iteration until the convergence is reached. Moreover, we implement our algorithm on a tractor-trailer system, taking into account the longitudinal and side slip effects. The open-loop simulation shows the proposed Koopman bilinear model captures the dynamics with unknown parameters and has good prediction performance. Closed-loop tracking results show the proposed K-BMPC exhibits elevated tracking precision with the commendable computational efficiency. The experimental results demonstrate the feasibility of K-BMPC. keywords: {Trajectory tracking;Computational modeling;Estimation;Predictive models;Control systems;Data models;Nonlinear dynamical systems;Koopman operator;tractor-trailer trajectory tracking;model predictive control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610320&isnumber=10609862

W. -C. Huang, A. Aydinoglu, W. Jin and M. Posa, "Adaptive Contact-Implicit Model Predictive Control with Online Residual Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5822-5828, doi: 10.1109/ICRA57147.2024.10610416.Abstract: The hybrid nature of multi-contact robotic systems, due to making and breaking contact with the environment, creates significant challenges for high-quality control. Existing model-based methods typically rely on either good prior knowledge of the multi-contact model or require significant offline model tuning effort, thus resulting in low adaptability and robustness. In this paper, we propose a real-time adaptive multi-contact model predictive control framework, which enables online adaption of the hybrid multi-contact model and continuous improvement of the control performance for contact-rich tasks. This framework includes an adaption module, which continuously learns a residual of the hybrid model to minimize the gap between the prior model and reality, and a real-time multi-contact MPC controller. We demonstrated the effectiveness of the framework in synthetic examples, and applied it on hardware to solve contact-rich manipulation tasks, where a robot uses its end-effector to roll different unknown objects on a table to track given paths. The hardware experiments show that with a rough prior model, the multi-contact MPC controller adapts itself on-the-fly with an adaption rate around 20 Hz and successfully manipulates previously unknown objects with non-smooth surface geometries. Accompanying media can be found at: https://sites.google.com/view/adaptive-contact-implicit-mpc/home keywords: {Geometry;Adaptation models;Adaptive systems;Uncertainty;Real-time systems;Hardware;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610416&isnumber=10609862

D. Baril et al., "DRIVE: Data-driven Robot Input Vector Exploration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5829-5836, doi: 10.1109/ICRA57147.2024.10611172.Abstract: An accurate motion model is a fundamental component of most autonomous navigation systems. While much work has been done on improving model formulation, no standard protocol exists for gathering empirical data required to train models. In this work, we address this issue by proposing Data-driven Robot Input Vector Exploration (DRIVE), a protocol that enables characterizing uncrewed ground vehicles (UGVs) input limits and gathering empirical model training data. We also propose a novel learned slip approach outperforming similar acceleration learning approaches. Our contributions are validated through an extensive experimental evaluation, cumulating over 7km and 1.8h of driving data over three distinct UGVs and four terrain types. We show that our protocol offers increased predictive performance over common human-driven data-gathering protocols. Furthermore, our protocol converges with 46 s of training data, almost four times less than the shortest human dataset gathering protocol. We show that the operational limit for our model is reached in extreme slip conditions encountered on surfaced ice. DRIVE is an efficient way of characterizing UGV motion in its operational conditions. Our code and dataset are both available online at this link: https://github.com/norlab-ulaval/DRIVE. keywords: {Training;Adaptation models;Protocols;Training data;Predictive models;Vectors;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611172&isnumber=10609862

J. Fang et al., "A Force-driven and Vision-driven Hybrid Control Method of Autonomous Laparoscope-Holding Robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5857-5863, doi: 10.1109/ICRA57147.2024.10610509.Abstract: Laparoscope-holding robots significantly enhance the stability and precision of visualization in minimally invasive surgeries. Most existing robots of this kind depend on visual servo systems and struggle with efficient, rapid adjustments in the field-of-view (FOV), especially when identifying organs and needles outside the FOV. This paper presents a laparoscope-holding robot system capable of employing both vision-driven and force-driven mechanisms for continuous and large-scale FOV adjustments, respectively. The system features an integrated tactile handle, enabling the reception of human-robot interaction forces during surgical navigation. We propose a hybrid control method that leverages both force and vision inputs for laparoscopic FOV adjustments. This approach integrates a virtual wrench, generated from visual information, and an interaction wrench, obtained from the tactile handle, into the robot's dynamic model, which complies with remote center of motion constraints. The interaction wrench's gain is adjusted with the gripping force on the integrated tactile handle, ensuring that unintended movements caused by accidental contacts are prevented, thus safeguarding operational safety. The proposed method eliminates the need to switch control modes, enabling simultaneous visual tracking and tactile interaction guidance. Experimental results demonstrate that the proposed method not only allows for FOV adjustments with surgical instrument guiding but also adapts well to large-scale FOV adjustment tasks. keywords: {Laparoscopes;Visualization;Tracking;Instruments;Force;Human-robot interaction;Switches},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610509&isnumber=10609862

T. Zhang, H. Gao and H. Ren, "Inconstant curvature kinematics of parallel continuum robot without static model," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5864-5870, doi: 10.1109/ICRA57147.2024.10610251.Abstract: In the study of minimally invasive surgical robots, a mini parallel continuum robot has shown motion advantage after passing through a long and winding working channel. However, due to the interaction force between the elastic wires of the parallel robots during motion generation processes, the constant curvature assumption has shown modeling errors. This causes the current geometric kinematic model to become unreliable. Therefore, there is a need for a more accurate kinematic model in the absence of a complicated static model. This paper aims to solve this issue. The simulation in ANSYS is carried out, and the shape of one of the driving wires, when bending, is fitted by a two-segment polynomial curve. Then, the position of the distal wrist tip can be calculated based on the curve shape. To verify the accuracy of the proposed model, bending simulation and experiment are carried out. The accuracy of the proposed model is compared with that of the kinematic model based on constant curvature assumption. The result shows that the proposed model can get more accurate results, especially when the driving wire displacement increases. For a 10 mm parallel robot, when the displacements of the two pairs of wires are both 3.0 mm, the errors of the two models are 0.42 mm and 5.79 mm (4.2% and 57.9%), respectively. keywords: {Wrist;Parallel robots;Accuracy;Shape;Computational modeling;Wires;Kinematics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610251&isnumber=10609862

Y. Yan et al., "A Novel SEA-based Haptic Interface for Robot-Assisted Vascular Interventional Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5871-5876, doi: 10.1109/ICRA57147.2024.10611034.Abstract: Robot-assisted vascular interventional surgery can isolate interventionists and X-ray radiation, and improve surgical accuracy. However, the leader side outside the operating room still has problems such as incomplete collection of operating information and unrealistic tactile feedback. The main objective of this paper is to design a haptic interface that can simultaneously capture the force-position information of the interventionists and generate force to assist the interventionists in performing surgeries on the leader side. It can capture the interventionists’ delivery displacement, twisting angle, clamping force, and provide real-time force feedback. A leader-follower bidirectional force feedback control strategy was proposed. Based on this strategy, on the one hand, the interventionist perceives the multi-modal information fed back from the follower side, makes judgments, and actively adjusts the surgical operation. On the other hand, the interventionist controls the grasping state of the instruments remotely to control the safety operating force threshold. Finally, the experimental setup was built and a series of evaluation experiments were performed. The experimental results verified the feasibility of the designed haptic interface. It can generate dynamic and accurate force feedback and realize leader-follower grasping force control. keywords: {Accuracy;Instruments;Force;Force feedback;Dynamics;Surgery;Grasping;Robot-assisted vascular interventional surgery;force feedback;Leader-follower system;haptic interface},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611034&isnumber=10609862

H. Suzuki, "A Miniature 1R1T Precision Manipulator with Remote Center of Motion for Minimally Invasive Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5877-5883, doi: 10.1109/ICRA57147.2024.10610812.Abstract: In robotic-assisted minimally invasive surgery, the remote center of motion (RCM) achieves precision and safe manipulation of surgical devices through the insertion point into the patient’s body. One of the RCM configurations, one-rotation and one-translation (1R1T) RCM based on a closed-loop design, enables two-degrees-of-freedom transmission from the proximal end of the robotic arm to the distal end. This feature offers important advantages, particularly in enhancing safety by minimizing physical contact risks with patients or other surgical tools owing to the simplified layout near the surgical field. However, conventional 1R1T RCM robots typically employ complex structures with numerous joints consisting of pin-and-hole mating mechanisms. This complexity can increase the overall size of the robot and compromise motion precision. This study presents a miniature 1R1T precision manipulator with RCM, ORIGANOID (dimensions: W60 x D120 x H30 mm, weight: 12.6 g). The robotic arm features flexure hinges, eliminates clearance issues, and can be fabricated using an origami-inspired robotic approach. Furthermore, using a novel backlash-free coupling method, the robotic arm could be easily attached and detached from the drive units. A prototype was fabricated and experimentally validated. The results demonstrated that high-resolution motion could be achieved within 10 μm. Furthermore, a demonstration using an eyeball model confirmed the successful implementation of 1R1T RCM. keywords: {Couplings;Minimally invasive surgery;Layout;Prototypes;Arms;Fasteners;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610812&isnumber=10609862

J. Im, E. Jang and C. Song, "A three-dimensional compliant bowtie-shaped mechanical amplifier to magnify coaxial displacement in a confined space," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5884-5890, doi: 10.1109/ICRA57147.2024.10611636.Abstract: This paper proposes a novel form of a three-dimensional coaxial bowtie-shaped mechanical amplifier. The proposed model incorporates a lever mechanism into the Sarrus linkage structure. It allows the target plate to move along one axis with amplified displacement in a parallel manner. The amplifier was assembled after machining the components using a computer numerical control machine. A flexible hinge was incorporated into the amplifier design for simplified fabrication and reduced friction in the actuation mechanism. Castigliano’s theorem is used to build a mathematical model of the proposed mechanical amplifier, and the performance was validated through finite element analysis and prototype fabrication. We achieved the amplification ratio of ×8.44, resulting in the axial displacement up to 86 µm. The demonstrated amplifier is expected to apply to compact microsurgical robots or biomedical imaging apparatus requiring coaxial displacement amplification in confined spaces. keywords: {Fabrication;Couplings;Technological innovation;Resonant frequency;Prototypes;Predictive models;Mathematical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611636&isnumber=10609862

J. Xue, M. Zhang, X. Liu, J. Zhu, Y. Cao and L. Zhang, "A Magnetic Continuum Robot with In-situ Magnetic Reprogramming Capability," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5891-5897, doi: 10.1109/ICRA57147.2024.10611450.Abstract: Magnetic continuum robots (MCR) have shown great potential in minimally invasive interventions because they can be actively and remotely navigated through complex in vivo environments. However, the deformation capability of current MCRs is limited by fixed magnetization congurations, preventing them from accessing hard-to-reach areas. This is due to the fact that under a global magnetic field, fixed magnetization conguration causes the magnets on the MCRs exposed to coupled magnetic forces and torques, resulting in a lack of controllable degrees of freedom. Here, we introduce a reprogrammable magnetic continuum robot (RMCR) enabled by magnetic reprogramming modules (MRM). Actuated by shape memory alloys, the magnetic moment direction of MRMs can be selectively reprogrammed in real-time and in-situ. Magnetic reprogramming capabilities enable the RMCR to achieve complex shape transformations. Results show that the range of motion in the tip direction of the RMCR increases by 193% compared with regular MCR. Besides, MRMs on the RMCR can achieve active attraction and separation under simple magnetic fields. The reprogramming process of the RMCR is theoretically investigated. A design methodology for MRMs is then proposed and the fabrication process of RMCR is described in detail. Furthermore, a kinematic model of the RMCR is established, simulated, and experimentally validated. keywords: {Shape;Deformation;Shape memory alloys;Navigation;Magnetic separation;Magnetization;Prototypes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611450&isnumber=10609862

J. Chen et al., "Design and Visual Servoing Control of a Hybrid Dual-Segment Flexible Neurosurgical Robot for Intraventricular Biopsy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5906-5912, doi: 10.1109/ICRA57147.2024.10610302.Abstract: Traditional rigid endoscopes have challenges in flexibly treating tumors located deep in the brain, and low operability and fixed viewing angles limit its development. This study introduces a novel dual-segment flexible robotic endoscope MicroNeuro, designed to perform biopsies with dexterous surgical manipulation deep in the brain. Taking into account the uncertainty of the control model, an image-based visual servoing with online robot Jacobian estimation has been implemented to enhance motion accuracy. Furthermore, the application of model predictive control with constraints significantly bolsters the flexible robot’s ability to adaptively track mobile objects and resist external interference. Experimental results underscore that the proposed control system enhances motion stability and precision. Phantom testing substantiates its considerable potential for deployment in neurosurgery. keywords: {Jacobian matrices;Visualization;Endoscopes;Estimation;Interference;Predictive models;Control systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610302&isnumber=10609862

A. Schwarz et al., "Uncertainty-Aware Shape Estimation of a Surgical Continuum Manipulator in Constrained Environments using Fiber Bragg Grating Sensors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5913-5919, doi: 10.1109/ICRA57147.2024.10610024.Abstract: Continuum Dexterous Manipulators (CDMs) are well-suited tools for minimally invasive surgery due to their inherent dexterity and reachability. Nonetheless, their flexible structure and non-linear curvature pose significant challenges for shape-based feedback control. The use of Fiber Bragg Grating (FBG) sensors for shape sensing has shown great potential in estimating the CDM’s tip position and subsequently reconstructing the shape using optimization algorithms. This optimization, however, is under-constrained and may be ill-posed for complex shapes, falling into local minima. In this work, we introduce a novel method capable of directly estimating a CDM’s shape from FBG sensor wavelengths using a deep neural network. In addition, we propose the integration of uncertainty estimation to address the critical issue of uncertainty in neural network predictions. Neural network predictions are unreliable when the input sample is outside the training distribution or corrupted by noise. Recognizing such deviations is crucial when integrating neural networks within surgical robotics, as inaccurate estimations can pose serious risks to the patient. We present a robust method that not only improves the precision upon existing techniques for FBG-based shape estimation but also incorporates a mechanism to quantify the models’ confidence through uncertainty estimation. We validate the uncertainty estimation through extensive experiments, demonstrating its effectiveness and reliability on out-of-distribution (OOD) data, adding an additional layer of safety and precision to minimally invasive surgical robotics. keywords: {Uncertainty;Minimally invasive surgery;Medical robotics;Shape;Estimation;Fiber gratings;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610024&isnumber=10609862

D. T. Kutzke, A. Wariar and J. Sattar, "Autonomous robotic re-alignment for face-to-face underwater human-robot interaction*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5920-5926, doi: 10.1109/ICRA57147.2024.10610809.Abstract: The use of autonomous underwater vehicles (AUVs) to accomplish traditionally challenging and dangerous tasks has proliferated thanks to advances in sensing, navigation, manipulation, and on-board computing technologies. Utilizing AUVs in underwater human-robot interaction (UHRI) has witnessed comparatively smaller levels of growth due to limitations in bi-directional communication and significant technical hurdles to bridge the gap between analogies with terrestrial interaction strategies and those that are possible in the underwater domain. A necessary component to support UHRI is establishing a system for safe robotic-diver approach to establish face-to-face communication that considers nonstandard human body pose. In this work, we introduce a stereo vision system for enhancing UHRI that utilizes threedimensional reconstruction from stereo image pairs and machine learning for localizing human joint estimates. We then establish a convention for a coordinate system that encodes the direction the human is facing with respect to the camera coordinate frame. This allows automatic setpoint computation that preserves human body scale and can be used as input to an image-based visual servo control scheme. We show that our setpoint computations tend to agree both quantitatively and qualitatively with experimental setpoint baselines. The methodology introduced shows promise for enhancing UHRI by improving robotic perception of human orientation underwater. keywords: {Visualization;Navigation;Robot kinematics;Human-robot interaction;Machine learning;Robot sensing systems;Stereo vision},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610809&isnumber=10609862

P. -L. Richard et al., "Mechanism Design for New Sensors Field Deployment by LineRanger Powerline Robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5927-5933, doi: 10.1109/ICRA57147.2024.10610764.Abstract: Powerline robotics is slowly becoming key tools for electric utilities. Contrary to drones that are usually limited to inspection tasks, wheeled robots like LineRanger can perform a broader range of applications. In this paper, a suite of mechanical devices is featured, as several new asset management tasks were recently added to LineRanger’s capabilities. While previous applications focused on non-contact inspection (visual, electro-magnetic, etc.), the new tasks at hand involved reaching adjacent conductors to probe line components with micro-Ohmmeter, installing and retrieving custom build sensors for multi-day line monitoring, and assessing aging conductors surface properties, to refine their thermal model and optimize the line capacity during heat waves. All three applications were recently field validated onto LineRanger, and mechanical design insights shall be presented for each module. keywords: {Visualization;Philosophical considerations;Inspection;Thermal sensors;Conductors;Thermal conductivity;Asset management;Field robotics;mechanism design;transmission line inspection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610764&isnumber=10609862

A. Romero, C. Delgado, L. Zanzi, R. Suárez and X. Costa-Pérez, "Cellular-enabled Collaborative Robots Planning and Operations for Search-and-Rescue Scenarios," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5942-5948, doi: 10.1109/ICRA57147.2024.10611179.Abstract: Mission-critical operations, particularly in the context of Search-and-Rescue (SAR) and emergency response situations, demand optimal performance and efficiency from every component involved to maximize the success probability of such operations. In these settings, cellular-enabled collaborative robotic systems have emerged as invaluable assets, assisting first responders in several tasks, ranging from victim localization to hazardous area exploration. However, a critical limitation in the deployment of cellular-enabled collaborative robots in SAR missions is their energy budget, primarily supplied by batteries, which directly impacts their task execution and mobility. This paper tackles this problem, and proposes a search-and-rescue framework for cellular-enabled collaborative robots use cases that, taking as input the area size to be explored, the robots fleet size, their energy profile, exploration rate required and target response time, finds the minimum number of robots able to meet the SAR mission goals and the path they should follow to explore the area. Our results, i) show that first responders can rely on a SAR cellular-enabled robotics framework when planning mission-critical operations to take informed decisions with limited resources, and, ii) illustrate the number of robots versus explored area and response time trade-off depending on the type of robot: wheeled vs quadruped. keywords: {Location awareness;Mission critical systems;Collaborative robots;Search problems;Planning;Mobile robots;Time factors;5G;Cellular;Collaborative Robots;Energy Saving;Search-and-rescue},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611179&isnumber=10609862

A. Patel, M. A. V. Saucedo, C. Kanellakis and G. Nikolakopoulos, "STAGE: Scalable and Traversability-Aware Graph based Exploration Planner for Dynamically Varying Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5949-5955, doi: 10.1109/ICRA57147.2024.10610939.Abstract: In this article, we propose a novel navigation framework that leverages a two layered graph representation of the environment for efficient large-scale exploration, while it integrates a novel uncertainty awareness scheme to handle dynamic scene changes in previously explored areas. The framework is structured around a novel goal oriented graph representation, that consists of, i) the local sub-graph and ii) the global graph layer respectively. The local sub-graphs encode local volumetric gain locations as frontiers, based on the direct pointcloud visibility, allowing fast graph building and path planning. Additionally, the global graph is build in an efficient way, using node-edge information exchange only on overlapping regions of sequential sub-graphs. Different from the state-of-the-art graph based exploration methods, the proposed approach efficiently re-uses sub-graphs built in previous iterations to construct the global navigation layer. Another merit of the proposed scheme is the ability to handle scene changes (e.g. blocked pathways), adaptively updating the obstructed part of the global graph from traversable to not-traversable. This operation involved oriented sample space of a path segment in the global graph layer, while removing the respective edges from connected nodes of the global graph in cases of obstructions. As such, the exploration behavior is directing the robot to follow another route in the global re-positioning phase through path-way updates in the global graph. Finally, we showcase the performance of the method both in simulation runs as well as deployed in real-world scene involving a legged robot carrying camera and lidar sensor. keywords: {Legged locomotion;Uncertainty;Laser radar;Navigation;Robot vision systems;Memory management;Cameras;Autonomous navigation;GPS-denied environments;exploration;dynamic environments;aerial and legged robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610939&isnumber=10609862

Y. -S. Li and K. -S. Tseng, "Computation-Aware Multi-object Search in 3D Space using Submodular Tree," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5956-5962, doi: 10.1109/ICRA57147.2024.10610369.Abstract: Searching for targets in 3D environments can be formulated as submodular maximization problems with routing constraints. However, it involves solving two NP-hard problems: the maximal coverage problem and the traveling salesman problem. Since the time constraint is critical for search problems, this research proposes a Computation-Aware Search for Multiple Objects (CASMO) algorithm to further consider the computational time in the cost constraints. Due to the submdularity, the greedy algorithm achieves $\frac{1}{2}\left( {1 - \frac{1}{e}} \right)\overline {OPT} $, where $\overline {OPT} $ is the approximate optimum. The experiment results show that the proposed algorithm outperforms state-of-the-art approaches in multi-object search. keywords: {Greedy algorithms;Three-dimensional displays;Costs;Traveling salesman problems;Search problems;Approximation algorithms;Routing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610369&isnumber=10609862

Y. -S. Li and K. -S. Tseng, "Multi-robot Search in a 3D Environment with Intersection System Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5963-5969, doi: 10.1109/ICRA57147.2024.10610393.Abstract: Efficient task allocation is a challenge for multirobot search. The multi-robot search problem is reformulated as submodular maximization subject to intersection system constraints. The objective function is submodular and consists of a coverage function to cover environments and a balancing function to efficiently dispatch robots. The intersection system is composed of routing and clustering constraints. The experiment results show that the proposed approach outperforms state-ofthe-art methods in multi-robot search. keywords: {Three-dimensional displays;Clustering algorithms;Search problems;Linear programming;Routing;Trajectory;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610393&isnumber=10609862

M. Zoula and J. Faigl, "Wireless Communication Infrastructure Building for Mobile Robot Search and Inspection Missions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5970-5976, doi: 10.1109/ICRA57147.2024.10611561.Abstract: In the paper, we address wireless communication infrastructure building by relay placement based on approaches utilized in wireless network sensors. The problem is motivated by search and inspection missions with mobile robots, where known sensing ranges may be exploited. We investigate the relay placement, establishing network connectivity to support robust food-based communication routing. The proposed method decomposes the given area into Open space and Corridor space where specific deployment patterns allow for guaranteed k-connectivity, making the resulting network redundant while keeping channel utilization bounded. In particular, a hexagonal tesselation coverage pattern with 3-connectivity is investigated in Open space and a linear 4-connectivity pattern in Corridor space, respectively. The proposed approach is empirically evaluated in a realistic scenario, and based on the reported results, it is found superior compared to the existing stochastic randomized dual sampling schema. keywords: {Wireless sensor networks;Wireless networks;Buildings;Stochastic processes;Inspection;Search problems;Routing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611561&isnumber=10609862

A. Seewald et al., "RB5 Low-Cost Explorer: Implementing Autonomous Long-Term Exploration on Low-Cost Robotic Hardware," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5977-5983, doi: 10.1109/ICRA57147.2024.10610399.Abstract: This systems paper presents the implementation and design of RB5, a wheeled robot for autonomous long-term exploration with fewer and cheaper sensors. Requiring just an RGB-D camera and low-power computing hardware, the system consists of an experimental platform with rocker-bogie suspension. It operates in unknown and GPS-denied environments and on indoor and outdoor terrains. The exploration consists of a methodology that extends frontier- and sampling-based exploration with a path-following vector field and a state-of-the-art SLAM algorithm. The methodology allows the robot to explore its surroundings at lower update frequencies, enabling the use of lower-performing and lower-cost hardware while still retaining good autonomous performance. The approach further consists of a methodology to interact with a remotely located human operator based on an inexpensive long-range and low-power communication technology from the internet-of-things domain (i.e., LoRa) and a customized communication protocol. The results and the feasibility analysis show the possible applications and limitations of the approach.Code—The open-source software stack is made available on the project repository webpage†. keywords: {Simultaneous localization and mapping;Protocols;Robot vision systems;LoRa;Hardware;Vectors;Sensor systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610399&isnumber=10609862

S. Singh, Z. Temel and R. S. Pierre, "Multi-modal jumping and crawling in an autonomous, springtail-inspired microrobot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 5999-6005, doi: 10.1109/ICRA57147.2024.10610130.Abstract: Springtails are tiny arthropods that crawl and jump. They jump by temporarily storing elastic energy in resilin elastic cuticular structures and releasing that energy to accelerate a tail, called a furca, propelling them in the air. This paper presents an autonomous, springtail-inspired microrobot that can crawl and jump. The microrobot has a mass of 980mg and stands 13mm tall, and has on-board sensing, computation, and power, enabling autonomy. The microrobot was designed with a super-elastic shape memory alloy (SMA) spring that is manually loaded to store elastic energy. The on-board sensing and computation triggers an actuator at the jump frequency range that unlatches the spring, launching the microrobot into the air at speeds up to 3.171ms−1. At the same time, the microrobot is capable of crawling, when actuated at frequencies lower or higher than the jump frequency range, demonstrating autonomous multi-modal locomotion. This work opens up new pathways toward autonomy in multi-modal microrobots. keywords: {Time-frequency analysis;Power system measurements;Actuators;Latches;Shape memory alloys;Density measurement;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610130&isnumber=10609862

H. Gao, S. Jung and E. F. Helbling, "High-speed interfacial flight of an insect-scale robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6006-6013, doi: 10.1109/ICRA57147.2024.10611592.Abstract: Several insect species are able to locomote across the air-water interface by leveraging surface tension to remain above the water surface. A subset of these insects, such as the stonefly and waterlily beetle, flap their wings to actively move around the two dimensional surface — a locomotion strategy referred to as interfacial flight. Here, we present an insect-scale robot, the γ-bot, inspired by these interfacial fliers. The robot is comprised of a flapping-wing vehicle that generates a thrust force parallel to the water surface, and three passive legs utilize surface tension to support the body mass and maintain contact with the air-water interface. We developed and validated a simple model to characterize the drag forces acting on the vehicle and estimate the robot’s velocity. This 112 mg robot can reach maximum velocities of 0.9 ms−1 (corresponding to 15 BLs−1) and can initiate both left and right turns, demonstrating high maneuverability along the air-water interface. In addition, the robot can carry an additional 419 mg, enabling future sensing, control, and power autonomous operation. keywords: {Legged locomotion;Insects;Drag;Atmospheric modeling;Force;Robot sensing systems;Surface tension},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611592&isnumber=10609862

E. K. Blankenship, C. K. Trygstad, F. M. F. R. Gonçalves and N. O. Pérez-Arancibia, "VLEIBot: A New 45-mg Swimming Microrobot Driven by a Bioinspired Anguilliform Propulsor," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6014-6021, doi: 10.1109/ICRA57147.2024.10610895.Abstract: This paper presents the VLEIBot* (Very Little Eel-Inspired roBot), a 45-mg/23-mm3 microrobotic swimmer that is propelled by a bioinspired anguilliform propulsor. The propulsor is excited by a single 6-mg high-work-density (HWD) microactuator and undulates periodically due to wave propagation phenomena generated by fluid-structure interaction (FSI) during swimming. The microactuator is composed of a carbon-fiber beam, which functions as a leaf spring, and shape-memory alloy (SMA) wires, which deform cyclically when excited periodically using Joule heating. The VLEIBot can swim at speeds as high as 15.1 mm • s−1 (0.33 Bl • s−1 ) when driven with a heuristically-optimized propulsor. To improve maneuverability, we evolved the VLEIBot design into the 90-mg/47-mm3 VLEIBot+, which is driven by two propulsors and fully controllable in the two-dimensional (2D) space. The VLEIBot+ can swim at speeds as high as 16.1 mm • s–1 (0.35 Bl • s–1), when driven with heuristically-optimized propulsors, and achieves turning rates as high as 0.28 rad • s–1, when tracking path references. The measured root-mean-square (RMS) values of the tracking errors are as low as 4 mm. keywords: {Shape;Two-dimensional displays;Wires;Sea measurements;Tail;Microactuators;Turning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610895&isnumber=10609862

M. V. M. Firlefyn, J. J. Hagenaars and G. C. H. E. De Croon, "Direct learning of home vector direction for insect-inspired robot navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6022-6028, doi: 10.1109/ICRA57147.2024.10611609.Abstract: Insects have long been recognized for their ability to navigate and return home using visual cues from their nest’s environment. However, the precise mechanism underlying this remarkable homing skill remains a subject of ongoing investigation. Drawing inspiration from the learning flights of honey bees and wasps, we propose a robot navigation method that directly learns the home vector direction from visual percepts during a learning flight in the vicinity of the nest. After learning, the robot will travel away from the nest, come back by means of odometry, and eliminate the resultant drift by inferring the home vector orientation from the currently experienced view. Using a compact convolutional neural network, we demonstrate successful learning in both simulated and real forest environments, as well as successful homing control of a simulated quadrotor. The average errors of the inferred home vectors in general stay well below the 90° required for successful homing, and below 24° if all images contain sufficient texture and illumination. Moreover, we show that the trajectory followed during the initial learning flight has a pronounced impact on the network’s performance. A higher density of sample points in proximity to the nest results in a more consistent return. Code and data are available at https://mavlab.tudelft.nl/learning_to_home. keywords: {Visualization;Navigation;Insects;Lighting;Forestry;Vectors;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611609&isnumber=10609862

F. Liu, S. Li, J. Xiang, D. Li and Z. Tu, "A Dragonfly-inspired Flapping Wing Robot Mimicking Force Vector Control Approach," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6029-6035, doi: 10.1109/ICRA57147.2024.10610326.Abstract: Dragonflies show impressive flying skills by achieving both high efficiency and agility. They can perform distinctive flight maneuvers, such as flying backwards, which has proven to be achieved through "force vectoring" mechanism recently. In this paper, to explore the agile flight ability of dragonflies on man-made flapping wing systems, we designed, optimized and fabricated a dragonfly-inspired flapping wing robot (DFWR) with inclinable stroke plane control degrees. The proposed platform employs a four-wing configuration, each of which integrates an extra servo motor to enable the rotation of the flapping plane and imitate the "force vectoring" mechanism. Besides, referring to the flapping kinematics of dragonflies, the installation angle and wing pitch angle of the proposed DFWR are optimized considering the total lift and energy consumption through multiobjective optimization based on NSGA-II method. The "force vector" produced by the proposed platform has been illustrated through both theoretical method and experimental method. Moreover, the feasibility of the design is further verified through a series of operation validation experiments. Such a robot has the potential to provide a highly biomimetic platform to validate the flight mechanism studying of Odonata as well as the relative on-board applications such as bio-inspired vision. keywords: {Energy consumption;Biomimetics;Force;Kinematics;Vectors;Servomotors;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610326&isnumber=10609862

Y. Gu, X. Cai, K. Thakuri, W. Yang, Y. Guo and W. Li, "Microrobotic Flight Enabled by Ultralight Ion Thrusters with High Thrust-to-Weight Ratio and Low Fabrication Cost," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6036-6042, doi: 10.1109/ICRA57147.2024.10611166.Abstract: Flying microrobots have garnered growing research interest owing to their technological intricacies and suitability for various applications leveraging miniaturized size. Electrohydrodynamic (EHD) thrust offers advantages by generating propulsion without moving parts, but real-world use is limited by insufficient thrust generation, manufacturing challenges, fragility, and cost. This work presents the design and development of an optimized ion-propelled flying microrobot that excels in low weight, high thrust-to-weight ratio, and cost efficiency. Regarding design, multiphysics simulations guided structural optimization to increase thrust while decreasing weight. For materials, metal-coated polyethylene terephthalate (PET) film was selected to leverage the combined merits of metal conductivity and polymer flexibility, light weight, and low cost, enabling further weight reduction, easy assembly, robustness, and cost-effectiveness. Various experiments, including voltage-current measurements, ionic wind speed, thrust quantification, and airflow visualization, directed design refinements and validated performance. Through structural optimization, the maximum wind speed attained 2.25 m/s. Flight demonstrations with payloads evidenced the microrobot can stably fly at an inherent 16 mg weight while carrying an additional 72 mg load, achieving a record 5.5 thrust-to-weight ratio. These results open possibilities to incorporate microelectronics, enabling autonomous flight functionality. keywords: {Visualization;Costs;Voltage measurement;Attitude control;Wind speed;Production;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611166&isnumber=10609862

M. Filliung et al., "An Augmented Catenary Model for Underwater Tethered Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6051-6057, doi: 10.1109/ICRA57147.2024.10611132.Abstract: This paper examines the relevance of using catenary-based curves to model cables in underwater tethered robotic applications in order to take into account the influence of hydrodynamic damping. To this end, an augmented catenary-based model is introduced to deal with the dynamical effects of surge motion, sway motion or a combination of both on a cable. Experimental studies are carried out with eight cables of varying stiffness, weight and buoyancy. One end of the cable is fixed, while the other end is moved by the underwater robot. The obtained results help to determine which cables and which dynamics are compatible with a fair estimation of the cable shape through the proposed models. keywords: {Semiconductor device modeling;Accuracy;Shape;Buoyancy;Hydrodynamics;Time measurement;Surges},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611132&isnumber=10609862

J. E. Hunt and L. L. Whitcomb, "A Hybrid Dynamical Model for Robotic Underwater Vehicles when Submerged or Surfaced: Approach and Preliminary Evaluation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6058-6064, doi: 10.1109/ICRA57147.2024.10610798.Abstract: This paper reports a numerical method for modeling underwater vehicle (UV) interactions with the free surface using a finite-dimensional dynamical plant model. Although finite-dimensional plant models of fully submerged UV behavior are well-established, they are unable to model the ubiquitous condition of a UV operating at or near the free surface. We report a Monte Carlo-based hybrid model approach for calculating the buoyancy and righting moment of a partially or fully submerged UV in order to model interactions with the free surface. We also report a preliminary evaluation of the hybrid model in numerical simulations, comparing the hybrid model’s performance to that of a model for fully submerged UVs and to the experimentally observed behavior of an actual vehicle while fully submerged and while interacting with the free surface. The results of this preliminary study suggest that the proposed hybrid approach may offer a simple and practical method for modeling UV behavior when submerged or interacting with the free surface. keywords: {Performance evaluation;Monte Carlo methods;Buoyancy;Numerical simulation;Numerical models;Vehicle dynamics;Underwater vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610798&isnumber=10609862

K. Macauley, L. Cai, P. Adamczyk and Y. Girdhar, "ReefGlider: A Highly Maneuverable Vectored Buoyancy Engine Based Underwater Robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6082-6088, doi: 10.1109/ICRA57147.2024.10610140.Abstract: There exists a capability gap in the design of currently available autonomous underwater vehicles (AUV). Most AUVs use a set of thrusters, and optionally control surfaces, to control their depth and pose. AUVs utilizing thrusters can be highly maneuverable, making them well-suited to operate in complex environments such as in close-proximity to coral reefs. However, they are inherently power-inefficient and produce significant noise and disturbance. Underwater gliders, on the other hand, use changes in buoyancy and center of mass, in combination with a control surface to move around. They are extremely power efficient but not very maneuverable. Gliders are designed for long-range missions that do not require precision maneuvering. Furthermore, since gliders only activate the buoyancy engine for small time intervals, they do not disturb the environment and can also be used for passive acoustic observations. In this paper we present ReefGlider, a novel AUV that uses only buoyancy for control but is still highly maneuverable from additional buoyancy control devices. ReefGlider bridges the gap between the capabilities of thruster-driven AUVs and gliders. These combined characteristics make ReefGlider ideal for tasks such as long-term visual and acoustic monitoring of coral reefs. We present the overall design and implementation of the system, as well as provide analysis of some of its capabilities. keywords: {Autonomous underwater vehicles;Attitude control;Noise;Marine vegetation;Buoyancy;Acoustics;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610140&isnumber=10609862

W. Wang, W. Xiao, A. Gonzalez-Garcia, J. Swevers, C. Ratti and D. Rus, "Robust Model Predictive Control with Control Barrier Functions for Autonomous Surface Vessels," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6089-6095, doi: 10.1109/ICRA57147.2024.10610620.Abstract: In autonomous robot navigation, the trajectories from path planners are considered to be safe regions, and deviations could endanger vessels. Model Predictive Control (MPC) stands as a popular choice for trajectory tracking problems as it naturally addresses operational constraints, such as dynamics and control constraints. Nevertheless, achieving robustness in changing environments like oceans and rivers, which are constantly subject to significant external disturbances, remains an ongoing challenge for MPC. It must consistently keep the system within a predefined safe region (such as a reference trajectory) even in the presence of model inaccuracies and perturbations. To address this challenge, we present a robust model predictive control strategy utilizing Control Barrier Functions (CBFs), which increases the disturbance-rejection abilities. We verify our method on an autonomous surface vessel in simulation and natural waters, both with external disturbances. Specifically, compared with the traditional MPC method, our proposed MPC-CBF strategy reduces tracking errors by 17.82% and 40.26% in simulations and field experiments, respectively. Although the control effort slightly increases by 7.78% and 4.20%, respectively, these results clearly demonstrate the enhanced resilience of MPC-CBF to disturbances. keywords: {Sea surface;Trajectory tracking;Navigation;Perturbation methods;Robustness;Trajectory;Rivers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610620&isnumber=10609862

M. Roznere, A. K. Pediredla, S. E. Lensgraf, Y. Girdhar and A. Q. Li, "Underwater Dome-Port Camera Calibration: Modeling of Refraction and Offset through N-Sphere Camera Model," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6110-6117, doi: 10.1109/ICRA57147.2024.10611338.Abstract: The optical effects that are observed in underwater imagery are more complex than those in-air. This is partially because we enclose most underwater cameras in a watertight enclosure, such as a hemispheric dome window. We then observe optical issues including the distortion effects of the lens, e.g., wide-angle field-of-view (FOV), the refractive effects at the enclosure (water-acrylic and acrylic-air) interfaces, and offset effects of a non-centered camera with respect to the dome. In this paper, we present an N-Sphere (NS) and Shifted N-Sphere (S-NS) camera models, tailored to these cameras and lenses mounted in water-tight dome enclosures. The proposed camera models treat each layer of effects as a ‘sphere’ that a 3D point will project on. Furthermore, the S-NS model includes additional parameters to address the camera offset variability. The versatility of the NS model makes it applicable to various lenses, as validated with fisheye (FOV >120°) and wide-FOV (FOV ≈ 120°). We validated our models with different in-water calibration sequences, lenses, and housing setups, as well as with comparisons with other state-of-the-art camera models. Additionally, we demonstrated the performance of our proposed models in an example stereo-based visual odometry application. The low computational load of the proposed models makes it ideal for integrating in real-time visual navigation and reconstruction frameworks. We provide full math derivations of the proposed models as well as example C++ header files1 for easy incorporation in independent projects. keywords: {Visualization;Computational modeling;Robot vision systems;Optical distortion;Cameras;Optical imaging;Calibration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611338&isnumber=10609862

L. Ling, J. Zhang, N. Bore, J. Folkesson and A. Wåhlin, "Benchmarking Classical and Learning-Based Multibeam Point Cloud Registration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6118-6125, doi: 10.1109/ICRA57147.2024.10610118.Abstract: Deep learning has shown promising results for multiple 3D point cloud registration datasets. However, in the underwater domain, most registration of multibeam echo-sounder (MBES) point cloud data are still performed using classical methods in the iterative closest point (ICP) family. In this work, we curate and release DotsonEast Dataset, a semi-synthetic MBES registration dataset constructed from an autonomous underwater vehicle in West Antarctica. Using this dataset, we systematically benchmark the performance of 2 classical and 4 learning-based methods. The experimental results show that the learning-based methods work well for coarse alignment, and are better at recovering rough transforms consistently at high overlap (20-50%). In comparison, GICP (a variant of ICP) performs well for fine alignment and is better across all metrics at extremely low overlap (10%). To the best of our knowledge, this is the first work to benchmark both learning-based and classical registration methods on an AUV-based MBES dataset. To facilitate future research, both the code and data are made available online.1 keywords: {Point cloud compression;Learning systems;Measurement;Three-dimensional displays;Simultaneous localization and mapping;Iterative closest point algorithm;Transforms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610118&isnumber=10609862

E. Palmer, C. Holm and G. Hollinger, "Angler: An Autonomy Framework for Intervention Tasks with Lightweight Underwater Vehicle Manipulator Systems," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6126-6132, doi: 10.1109/ICRA57147.2024.10610184.Abstract: Developing autonomous intervention capabilities for lightweight underwater vehicle manipulator systems (UVMS) has garnered significant attention within recent years because of the opportunity for these systems to reduce intervention operating costs. Developing autonomous UVMS capabilities is challenging, however, because of the lack of available standardized software frameworks and pipelines. Previous works offer simulation environments and deployment pipelines for underwater vehicles, but fall short of providing a complete UVMS software framework. We address this gap by creating Angler: a software framework for developing localization, control, and decision-making algorithms with support for sim-to-real transfer. We validate this framework by implementing a state-of-the-art control architecture and demonstrate the ability to perform station keeping with a mean error below 0.25 m and waypoint tracking with an average final error of 0.398 m. keywords: {Location awareness;Costs;Pipelines;Software algorithms;Decision making;Manipulators;Software},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610184&isnumber=10609862

E. Tang, W. J. Ang, K. W. Tan and S. Foong, "Harnessing the Differential Flatness of Monocopter Dynamics for the Purpose of Trajectory Tracking in a Stable Invertible Coaxial Actuated ROtorcraft (SICARO)," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6145-6151, doi: 10.1109/ICRA57147.2024.10611251.Abstract: In this paper, the dynamics of an emerging class of rotating nature-inspired micro aerial vehicles known as the Monocopter is proven and shown to be differentially flat. By exploiting this phenomenon, trajectory tracking can now be implemented on Monocopters via feed-forward terms that are computed per the trajectory. To demonstrate this, a Monocopter in the form of a Stable Invertible Coaxial Actuated ROtorcraft (SICARO) is chosen to harness this approach fully. The SICARO is capable of flying with either side of the wing facing up and this feature determines the craft’s direction of rotation about its body Z axis as well. In addition, it has the unique feature of a coaxial motor configuration that allows for a pitching-up moment regardless of the wing side facing up. The feed-forward terms computed are fused into a cascaded nonlinear controller on the craft to ensure its effectiveness in tracking trajectories. Lastly, the flight experiments extend to both sides of the wing to validate this method as being applicable for trajectory tracking for Monocopters such as the SICARO which has an extended range of flying capabilities. keywords: {Trajectory tracking;Reviews;Navigation;Dynamics;Optimal control;Motors;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611251&isnumber=10609862

T. Hui, E. Cuniato, M. Pantic, M. Tognon, M. Fumagalli and R. Siegwart, "Passive Aligning Physical Interaction of Fully-Actuated Aerial Vehicles for Pushing Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6152-6158, doi: 10.1109/ICRA57147.2024.10610711.Abstract: Recently, the utilization of aerial manipulators for performing pushing tasks in non-destructive testing (NDT) applications has seen significant growth. Such operations entail physical interactions between the aerial robotic system and the environment. End-effectors with multiple contact points are often used for placing NDT sensors in contact with a surface to be inspected. Aligning the NDT sensor and the work surface while preserving contact, requires that all available contact points at the end-effector tip are in contact with the work surface. With a standard full-pose controller, attitude errors often occur due to perturbations caused by modeling uncertainties, sensor noise, and environmental uncertainties. Even small attitude errors can cause a loss of contact points between the end-effector tip and the work surface. To preserve full alignment amidst these uncertainties, we propose a control strategy which selectively deactivates angular motion control and enables direct force control in specific directions. In particular, we derive two essential conditions to be met, such that the robot can passively align with flat work surfaces achieving full alignment through the rotation along non-actively controlled axes. Additionally, these conditions serve as hardware design and control guidelines for effectively integrating the proposed control method for practical usage. Real world experiments are conducted to validate both the control design and the guidelines. keywords: {Uncertainty;Perturbation methods;Noise;Robot sensing systems;End effectors;Sensors;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610711&isnumber=10609862

J. Sun et al., "Modeling and Control of PADUAV: a Passively Articulated Dual UAVs Platform for Aerial Manipulation*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 6159-6165, doi: 10.1109/ICRA57147.2024.10610094.Abstract: In this paper, we introduce PADUAV, a novel 5-DOF aerial platform designed to overcome the limitations of traditional tiltrotor vehicles. PADUAV features a unique mechanical design that incorporates two off-the-shelf quadrotors passively articulated to a rigid frame. This innovation enables free pitch rotation without mechanical constraints like cable winding, significantly enhancing its capabilities for various tasks. To control PADUAV’s 5 degrees of freedom, we propose a versatile and straightforward 5-DOF geometric tracking control strategy that generates 2D force and 3D torque. A decomposition approach is designed to distribute the output to the torque and thrust commands for each subplane, with no need for complex optimization. We validate our approach through three simulation experiments conducted in the Gazebo environment, leveraging the utilities provided by the RotorS simulator. These experiments not only demonstrate the feasibility of our platform but also provide new perspectives for future aerial platform development, particularly in terms of simulation-based approaches. keywords: {Technological innovation;Torque;Three-dimensional displays;5-DOF;Windings;Rotors;Valves},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610094&isnumber=10609862

