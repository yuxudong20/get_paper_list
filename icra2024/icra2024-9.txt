M. A. Pérez-Cutiño, J. Capitán, J. M. Díaz-Báñez and J. Valverde, "Measuring Ball Joint Faults in Parabolic-Trough Solar Plants with Data Augmentation and Deep Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8435-8441, doi: 10.1109/ICRA57147.2024.10610205.Abstract: Automatic inspection of parabolic-trough solar plants is key to preventing failures that can harm the environment and the production of green energy. In this work, we propose a novel methodology to inspect ball joints in parabolic trough collectors, which is a relevant problem that is not adequately covered in the literature. Images collected by an Unmanned Aerial Vehicle are segmented using deep learning to extract ball joint components. In order to generate rich training datasets, we develop a novel data augmentation technique by rotating joints and adding synthetic image background, and demonstrate its impact on the object detection accuracy. Then two types of faults are analyzed: fluid leaks, by means of image color filtering; and geometric shape anomalies, by measuring joint angles of the robotic arms. We propose metrics to quantify these faults and evaluate the damage of the inspected components. Our experimental results with images from operating commercial plants show that we can automatically detect leaks and anomalous angular geometry with a low failure rate compared to human labeling. keywords: {Geometry;Deep learning;Image segmentation;Fluids;Shape;Object detection;Inspection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610205&isnumber=10609862

A. Moortgat-Pick, M. Schwahn, A. Adamczyk, D. A. Duecker and S. Haddadin, "Autonomous UAV Mission Cycling: A Mobile Hub Approach for Precise Landings and Continuous Operations in Challenging Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8450-8456, doi: 10.1109/ICRA57147.2024.10611292.Abstract: Environmental monitoring via UAVs offers unprecedented aerial observation capabilities. However, the limited flight durations of typical multirotors and the demands on human attention in outdoor missions call for more autonomous solutions. Addressing the specific challenges of precise UAV landings – especially amidst wind disturbances, obstacles, and unreliable global localization – we introduce a mobile hub concept. This hub facilitates continuous mission cycling for unmodified off-the-shelf UAVs. Our approach centers on a small landing platform affixed to a robotic arm, adeptly correcting UAV pose errors in windy conditions. Compact enough for installation in an economy car, the system emphasizes two novel strategies. Firstly, external visual tracking of the UAV informs the landing controls for both the drone and the robotic arm. The arm compensates for UAV positioning errors and aligns the platform’s attitude with the UAV for stable landings, even on small platforms under windy conditions. Secondly, the robotic arm can transport the UAV inside the hub, perform maintenance tasks like battery replacements, and then facilitate direct relaunches. Importantly, our design places all operational responsibility on the hub, ensuring the UAV remains unaltered. This ensures broad compatibility with standard UAVs, only necessitating an API for attitude setpoints. Experimental results underscore the efficiency of our model, achieving safe landings with minimal errors (≤ 7 cm) in winds up to 5 Beaufort (8.1 m/s). In essence, our mobile hub concept significantly boosts UAV mission availability, allowing for autonomous operations even under challenging conditions. keywords: {Location awareness;Visualization;Wind speed;Stability criteria;Autonomous aerial vehicles;Manipulators;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611292&isnumber=10609862

R. V. Nanavati, C. Rhodes, M. J. Coombes and C. Liu, "Low-to-High Resolution Path Planner for Robotic Gas Distribution Mapping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8457-8463, doi: 10.1109/ICRA57147.2024.10610044.Abstract: Robotic gas distribution mapping improves the understanding of a hazardous gas dispersion while putting the human operator out of danger. Generating an accurate gas distribution map quickly is of utmost importance in situations such as gas leaks and industrial incidents, so that the efficient use of resources in response to incidents can be facilitated. In this paper, to incorporate the operational requirement on map granularity, we propose a low-to-high resolution path planner that first guides a single robots to quickly and sparsely sample the region of interest to generate a low resolution gas distribution map, followed by high resolution sampling informed by the low resolution map as a prior. The low resolution prior acts as a coverage survey allowing the algorithm to perform a relatively exploitative search of high concentration regions, resulting in overall shorter mission times. The proposed framework is designed to iteratively identify the next best T locations to sample, which prioritises the potentially high reward locations, while ensuring that the robot can travel to and sample the chosen locations within a user specified map update cycle. We present a simulation study to demonstrate the alternating exploration-exploitation like behaviour along with bench-marking its performance in contrast to the traditional sampling path planners and various reward functions. keywords: {Surveys;Accuracy;Service robots;Dispersion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610044&isnumber=10609862

K. Masaba, M. Roznere, M. Jeong and A. Q. Li, "Persistent Monitoring of Large Environments with Robot Deployment Scheduling in between Remote Sensing Cycles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8464-8470, doi: 10.1109/ICRA57147.2024.10611421.Abstract: This paper proposes a novel decision-making framework for planning "when" and "where" to deploy robots based on prior data with the goal of persistently monitoring a spatio-temporal phenomenon in an environment. We specifically focus on large lake monitoring, where remote sensors, such as satellites, can provide a snapshot of the target phenomenon at regular cycles. Between these cycles, Autonomous Surface Vehicles (ASVs) can be deployed to maintain an up-to-date model of the phenomenon. However, deploying ASVs has a significant logistical overhead in terms of time and cost. It requires a team of people to go on site and spend typically a day to monitor the deployment. It is vital to not only be intentional about where to sample in the environment on a given day, but also determine the worth of deploying the ASVs that day at all. Therefore, we propose a persistent monitoring strategy that provides the days and locations of when and where to sample with the robots by leveraging Gaussian Process model estimates of future trends based on collected remote sensing and point measurement data. Our approach minimizes the number of days and locations for sampling, while preserving the quality of estimates. Through simulation experiments using realistic spatio-temporal datasets, we demonstrate the benefits of our approach over traditional deployment strategies, including significant savings on the effort and operational cost of deploying the ASVs. keywords: {Costs;Satellites;Decision making;Sea measurements;Lakes;Predictive models;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611421&isnumber=10609862

F. Esser, G. Tombrink, A. Cornelißen, L. Klingbeil and H. Kuhlmann, "System Calibration of a Field Phenotyping Robot with Multiple High-Precision Profile Laser Scanners," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8471-8477, doi: 10.1109/ICRA57147.2024.10610208.Abstract: The creation of precise and high-resolution crop point clouds in agricultural fields has become a key challenge for high-throughput phenotyping applications. This work implements a novel calibration method to calibrate the laser scanning system of an agricultural field robot consisting of two industrial-grade laser scanners used for high-precise 3D crop point cloud creation. The calibration method optimizes the transformation between the scanner origins and the robot pose by minimizing 3D point omnivariances within the point cloud. Moreover, we present a novel factor graph-based pose estimation method that fuses total station prism measurements with IMU and GNSS heading information for high-precise pose determination during calibration. The root-mean-square error of the distances to a georeferenced ground truth point cloud results in 0.8 cm after parameter optimization. Furthermore, our results show the importance of a reference point cloud in the calibration method needed to estimate the vertical translation of the calibration. Challenges arise due to non-static parameters while the robot moves, indicated by systematic deviations to a ground truth terrestrial laser scan. keywords: {Point cloud compression;Accuracy;Three-dimensional displays;Systematics;Service robots;Lasers;Measurement by laser beam},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610208&isnumber=10609862

M. V. Gasparino, A. N. Sivakumar and G. Chowdhary, "WayFASTER: a Self-Supervised Traversability Prediction for Increased Navigation Awareness," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8486-8492, doi: 10.1109/ICRA57147.2024.10610436.Abstract: Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors. Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible but were visible at a different time. To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations. Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics. Our experiments demonstrate that our method excels at avoiding obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable. By using a sequence of images, WayFASTER significantly enhances the robot’s awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible. This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential. keywords: {Solid modeling;Three-dimensional displays;Navigation;Neural networks;Predictive models;Robot sensing systems;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610436&isnumber=10609862

C. Fu et al., "A Coarse-to-Fine Place Recognition Approach using Attention-guided Descriptors and Overlap Estimation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8493-8499, doi: 10.1109/ICRA57147.2024.10611569.Abstract: Place recognition is a challenging but crucial task in robotics. Current description-based methods may be limited by representation capabilities, while pairwise similarity-based methods require exhaustive searches, which is time-consuming. In this paper, we present a novel coarse-to-fine approach to address these problems, which combines BEV (Bird’s Eye View) feature extraction, coarse-grained matching and fine-grained verification. In the coarse stage, our approach utilizes an attention-guided network to generate attention-guided descriptors. We then employ a fast affinity-based candidate selection process to identify the Top-K most similar candidates. In the fine stage, we estimate pairwise overlap among the narrowed-down place candidates to determine the final match. Experimental results on the KITTI and KITTI-360 datasets demonstrate that our approach outperforms state-of-the-art methods. The code will be released publicly soon. keywords: {Codes;Accuracy;Estimation;Feature extraction;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611569&isnumber=10609862

X. Wu, J. Xu, P. Hu, G. Wang and H. Wang, "LHMap-loc: Cross-Modal Monocular Localization Using LiDAR Point Cloud Heat Map," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8500-8506, doi: 10.1109/ICRA57147.2024.10610718.Abstract: Localization using a monocular camera in the pre-built LiDAR point cloud map has drawn increasing attention in the field of autonomous driving and mobile robotics. However, there are still many challenges (e.g. difficulties of map storage, poor localization robustness in large scenes) in accurately and efficiently implementing cross-modal localization. To solve these problems, a novel pipeline termed LHMap-loc is proposed, which achieves accurate and efficient monocular localization in LiDAR maps. Firstly, feature encoding is carried out on the original LiDAR point cloud map by generating offline heat point clouds, by which the size of the original LiDAR map is compressed. Then, an end-to-end online pose regression network is designed based on optical flow estimation and spatial attention to achieve real-time monocular visual localization in a pre-built map. In addition, a series of experiments have been conducted to prove the effectiveness of the proposed method. Our code is available at: https://github.com/IRMVLab/LHMap-loc. keywords: {Location awareness;Point cloud compression;Visualization;Laser radar;Accuracy;Robot vision systems;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610718&isnumber=10609862

P. Zhang et al., "Looking Beneath More: A Sequence-based Localizing Ground Penetrating Radar Framework," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8515-8521, doi: 10.1109/ICRA57147.2024.10610174.Abstract: Localizing ground penetrating radar (LGPR) has been proven to be a promising technology for robot localization in various dynamic environments. However, the extreme scarcity of underground features introduces false candidate matches and brings unique challenges to this task. In this paper, we propose a sequence-based framework for LGPR to address the aforementioned issues. Specifically, we first introduce a trainable strategy to extract robust underground features in multi-weather conditions. By further using sequential information, our LGPR system can observe richer underground scene contexts, and the associated multi-frame scans could also improve the performance of underground place recognition. We demonstrate the superiority of our proposed method by comparing it against several recent state-of-the-art baseline methods applied to GPR image tasks. Experimental results on large public and self-collected datasets show that our proposed framework significantly improves the performance of various baselines in different scenarios. keywords: {Location awareness;Visualization;Ground penetrating radar;Redundancy;Pipelines;Network architecture;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610174&isnumber=10609862

Y. Zhang et al., "Increasing SLAM Pose Accuracy by Ground-to-Satellite Image Registration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8522-8528, doi: 10.1109/ICRA57147.2024.10611079.Abstract: Vision-based localization for autonomous driving has been of great interest among researchers. When a pre-built 3D map is not available, the techniques of visual simultaneous localization and mapping (SLAM) are typically adopted. Due to error accumulation, visual SLAM (vSLAM) usually suffers from long-term drift. This paper proposes a framework to increase the localization accuracy by fusing the vSLAM with a deep-learning based ground-to-satellite (G2S) image registration method. In this framework, a coarse (spatial correlation bound check) to fine (visual odometry consistency check) method is designed to select the valid G2S prediction. The selected prediction is then fused with the SLAM measurement by solving a scaled pose graph problem. To further increase the localization accuracy, we provide an iterative trajectory fusion pipeline. The proposed framework is evaluated on two well-known autonomous driving datasets, and the results demonstrate the accuracy and robustness in terms of vehicle localization. The code will be available at https://github.com/YanhaoZhang/SLAM-G2S-Fusion. keywords: {Location awareness;Visualization;Image registration;Simultaneous localization and mapping;Accuracy;Three-dimensional displays;Robustness;visual SLAM;cross-view localization;autonomous driving},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611079&isnumber=10609862

Z. Xiao, C. Chen, S. Yang and W. Wei, "EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8529-8536, doi: 10.1109/ICRA57147.2024.10611622.Abstract: Camera relocalization is pivotal in computer vision, with applications in AR, drones, robotics, and autonomous driving. It estimates 3D camera position and orientation (6-DoF) from images. Unlike traditional methods like SLAM, recent strides use deep learning for direct end-to-end pose estimation. We propose EffLoc, a novel efficient Vision Transformer for single-image camera relocalization. EffLoc’s hierarchical layout, memory-bound self-attention, and feed-forward layers boost memory efficiency and inter-channel communication. Our introduced sequential group attention (SGA) module enhances computational efficiency by diversifying input features, reducing redundancy, and expanding model capacity. EffLoc excels in efficiency and accuracy, outperforming prior methods, such as AtLoc and MapNet. It thrives on large-scale outdoor car-driving scenario, ensuring simplicity, end-to-end trainability, and eliminating handcrafted loss functions. keywords: {Computer vision;Accuracy;Three-dimensional displays;Simultaneous localization and mapping;Computational modeling;Robot vision systems;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611622&isnumber=10609862

J. Cui, J. Chen and L. Li, "SAGE-ICP: Semantic Information-Assisted ICP," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8537-8543, doi: 10.1109/ICRA57147.2024.10610280.Abstract: Robust and accurate pose estimation in unknown environments is an essential part of robotic applications. We focus on LiDAR-based point-to-point ICP combined with effective semantic information. This paper proposes a novel semantic information-assisted ICP method named SAGE-ICP, which leverages semantics in odometry. The semantic information for the whole scan is timely and efficiently extracted by a 3D convolution network, and these point-wise labels are deeply involved in every part of the registration, including semantic voxel downsampling, data association, adaptive local map, and dynamic vehicle removal. Unlike previous semantic-aided approaches, the proposed method can improve localization accuracy in large-scale scenes even if the semantic information has certain errors. Experimental evaluations on KITTI and KITTI-360 show that our method outperforms the baseline methods, and improves accuracy while maintaining real-time performance, i.e., runs faster than the sensor frame rate. keywords: {Location awareness;Training;Laser radar;Accuracy;Semantic segmentation;Semantics;Pose estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610280&isnumber=10609862

C. Liu, S. Chen, Y. Zhao, H. Huang, V. Prisacariu and T. Braud, "HR-APR: APR-agnostic Framework with Uncertainty Estimation and Hierarchical Refinement for Camera Relocalisation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8544-8550, doi: 10.1109/ICRA57147.2024.10610903.Abstract: Absolute Pose Regressors (APRs) directly estimate camera poses from monocular images, but their accuracy is unstable for different queries. Uncertainty-aware APRs provide uncertainty information on the estimated pose, alleviating the impact of these unreliable predictions. However, existing uncertainty modelling techniques are often coupled with a specific APR architecture, resulting in suboptimal performance compared to state-of-the-art (SOTA) APR methods. This work introduces a novel APR-agnostic framework, HR-APR, that formulates uncertainty estimation as cosine similarity estimation between the query and database features. It does not rely on or affect APR network architecture, which is flexible and computationally efficient. In addition, we take advantage of the uncertainty for pose refinement to enhance the performance of APR. The extensive experiments demonstrate the effectiveness of our framework, reducing 27.4% and 15.2% of computational overhead on the 7Scenes and Cambridge Landmarks datasets while maintaining the SOTA accuracy in single-image APRs. keywords: {Training;Uncertainty;Accuracy;Databases;Pipelines;Estimation;Network architecture},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610903&isnumber=10609862

R. Huang, M. Zhao, J. Chen and L. Li, "KDD-LOAM: Jointly Learned Keypoint Detector and Descriptors Assisted LiDAR Odometry and Mapping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8559-8565, doi: 10.1109/ICRA57147.2024.10610557.Abstract: Sparse keypoint matching based on distinct 3D feature representations can improve the efficiency and robustness of point cloud registration. Existing learning-based 3D descriptors and keypoint detectors are either independent or loosely coupled, so they cannot fully adapt to each other. In this work, we propose a tightly coupled keypoint detector and descriptor (TCKDD) based on a multi-task fully convolutional network with a probabilistic detection loss. In particular, this self-supervised detection loss fully adapts the keypoint detector to any jointly learned descriptors and benefits the self-supervised learning of descriptors. Extensive experiments on both indoor and outdoor datasets show that our TCKDD achieves state-of- the-art performance in point cloud registration. Furthermore, we design a keypoint detector and descriptors-assisted LiDAR odometry and mapping framework (KDD-LOAM), whose real-time odometry relies on keypoint descriptor matching-based RANSAC. The sparse keypoints are further used for efficient scan-to-map registration and mapping. Experiments on KITTI dataset demonstrate that KDD-LOAM significantly surpasses LOAM and shows competitive performance in odometry. keywords: {Point cloud compression;Three-dimensional displays;Laser radar;Simultaneous localization and mapping;Detectors;Self-supervised learning;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610557&isnumber=10609862

J. Ross, N. Kaygusuz, O. Mendez and R. Bowden, "Campus Map: A Large-Scale Dataset to Support Multi-View VO, SLAM and BEV Estimation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8566-8572, doi: 10.1109/ICRA57147.2024.10610656.Abstract: Significant advances in robotics and machine learning have resulted in many datasets designed to support research into autonomous vehicle technology. However, these datasets are rarely suitable for a wide variety of navigation tasks. For example, datasets that include multiple cameras often have short trajectories without loops that are unsuitable for the evaluation of longer-range SLAM or odometry systems, and datasets with a single camera often lack other sensors, making them unsuitable for sensor fusion approaches. Furthermore, alternative environmental representations such as semantic Bird’s Eye View (BEV) maps are growing in popularity, but datasets often lack accurate ground truth and are not flexible enough to adapt to new research trends.To address this gap, we introduce Campus Map, a novel large-scale multi-camera dataset with 2M images from 6 mounted cameras that includes GPS data and 64-beam, 125k point LiDAR scans totalling 8M points (raw packets also provided). The dataset consists of 16 sequences in a large car park and 6 long-term trajectories around a university campus that provide data to support research into a variety of autonomous driving and parking tasks. Long trajectories (average 10 min) and many loops make the dataset ideal for the evaluation of SLAM, odometry and loop closure algorithms, and we provide several state-of-the-art baselines.We also include 40k semantic BEV maps rendered from a digital twin. This novel approach to ground truth generation allows us to produce more accurate and crisp semantic maps than are currently available. We make the simulation environment available to allow researchers to adapt the dataset to their specific needs. Dataset available at: cvssp.org/data/twizy_data keywords: {Simultaneous localization and mapping;Accuracy;Navigation;Semantics;Sensor fusion;Cameras;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610656&isnumber=10609862

S. Xu, K. Zhang, Z. Hong, Y. Liu and S. Wang, "DISO: Direct Imaging Sonar Odometry," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8573-8579, doi: 10.1109/ICRA57147.2024.10611064.Abstract: This paper introduces a novel sonar odometry system that estimates the relative spatial transformation between two sonar image frames. Considering the unique challenges, such as low resolution and high noise, of sonar imagery for odometry and Simultaneous Localization and Mapping (SLAM), the proposed Direct Imaging Sonar Odometry (DISO) system is designed to estimate the relative transformation between two sonar frames by minimizing the aggregated sonar intensity errors of points with high intensity gradients. Moreover, DISO is implemented to incorporate a multi-sensor window optimization technique, a data association strategy and an acoustic intensity outlier rejection algorithm for reliability and accuracy. The effectiveness of DISO is evaluated using both simulated and real-world sonar datasets, showing that it outperforms the existing geometric-only method on localization accuracy and achieves state-of-the-art sonar odometry performance. We release the source codes of the DISO implementation to the community. The source code is available at https://github.com/SenseRoboticsLab/DISO. keywords: {Simultaneous localization and mapping;Accuracy;Three-dimensional displays;Source coding;Sonar;Imaging;Acoustics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611064&isnumber=10609862

K. Zhang, Y. Ding, S. Xu, Z. Hong, X. Kong and S. Wang, "CURL-MAP: Continuous Mapping and Positioning with CURL Representation†," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8580-8586, doi: 10.1109/ICRA57147.2024.10610760.Abstract: Maps of LiDAR Simultaneous Localisation and Mapping (SLAM) are often represented as point clouds. They usually take up a huge amount of storage space for large-scale environments, otherwise much structural detail may not be kept. In this paper, a novel paradigm of LiDAR mapping and odometry is designed by leveraging the Continuous and Ultra-compact Representation of LiDAR (CURL) proposed in [1]. Termed CURL-MAP (Mapping and Positioning), the proposed approach can not only reconstruct 3D maps with a continuously varying density but also efficiently reduce map storage space by using CURL’s spherical harmonics implicit encoding. Different from the popular Iterative Closest Point (ICP) based LiDAR odometry techniques, CURL-MAP formulates LiDAR pose estimation as a unique optimisation problem tailored for CURL. Experiment evaluation shows that CURL-MAP achieves state-of-the-art 3D mapping results and competitive LiDAR odometry accuracy. We will release the CURL-MAP codes for the community. keywords: {Point cloud compression;Laser radar;Three-dimensional displays;Accuracy;Simultaneous localization and mapping;Pose estimation;Harmonic analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610760&isnumber=10609862

M. Nissov, N. Khedekar and K. Alexis, "Degradation Resilient LiDAR-Radar-Inertial Odometry," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8587-8594, doi: 10.1109/ICRA57147.2024.10611444.Abstract: Enabling autonomous robots to operate robustly in challenging environments is necessary in a future with increased autonomy. For many autonomous systems, estimation and odometry remains a single point of failure, from which it can often be difficult, if not impossible, to recover. As such robust odometry solutions are of key importance. In this work a method for tightly-coupled LiDAR-Radar-Inertial fusion for odometry is proposed, enabling the mitigation of the effects of LiDAR degeneracy by leveraging a complementary perception modality while preserving the accuracy of LiDAR in well-conditioned environments. The proposed approach combines modalities in a factor graph-based windowed smoother with sensor information-specific factor formulations which enable, in the case of degeneracy, partial information to be conveyed to the graph along the non-degenerate axes. The proposed method is evaluated in real-world tests on a flying robot experiencing degraded conditions including geometric self-similarity as well as obscurant occlusion. For the benefit of the community we release the datasets presented: https://github.com/ntnu-arl/lidar_degeneracy_datasets. keywords: {Degradation;Laser radar;Prevention and mitigation;Estimation;Robot sensing systems;Sensors;Odometry},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611444&isnumber=10609862

J. Yin, A. Li, W. Xi, W. Yu and D. Zou, "Ground-Fusion: A Low-cost Ground SLAM System Robust to Corner Cases," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8603-8609, doi: 10.1109/ICRA57147.2024.10610070.Abstract: We introduce Ground-Fusion, a low-cost sensor fusion simultaneous localization and mapping (SLAM) system for ground vehicles. Our system features efficient initialization, effective sensor anomaly detection and handling, real-time dense color mapping, and robust localization in diverse environments. We tightly integrate RGB-D images, inertial measurements, wheel odometer and GNSS signals within a factor graph to achieve accurate and reliable localization both indoors and outdoors. To ensure successful initialization, we propose an efficient strategy that comprises three different methods: stationary, visual, and dynamic, tailored to handle diverse cases. Furthermore, we develop mechanisms to detect sensor anomalies and degradation, handling them adeptly to maintain system accuracy. Our experimental results on both public and self-collected datasets demonstrate that Ground-Fusion outperforms existing low-cost SLAM systems in corner cases. We release the code and datasets at https://github.com/SJTU-ViSYS/Ground-Fusion. keywords: {Location awareness;Visualization;Simultaneous localization and mapping;Accuracy;Wheels;Sensor fusion;Land vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610070&isnumber=10609862

Z. Xin, Y. Yue, L. Zhang and C. Wu, "HERO-SLAM: Hybrid Enhanced Robust Optimization of Neural SLAM," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8610-8616, doi: 10.1109/ICRA57147.2024.10610000.Abstract: Simultaneous Localization and Mapping (SLAM) is a fundamental task in robotics, driving numerous applications such as autonomous driving and virtual reality. Recent progress on neural implicit SLAM has shown encouraging and impressive results. However, the robustness of neural SLAM, particularly in challenging or data-limited situations, remains an unresolved issue. This paper presents HERO-SLAM, a Hybrid Enhanced Robust Optimization method for neural SLAM, which combines the benefits of neural implicit field and feature-metric optimization. This hybrid method optimizes a multi-resolution implicit field and enhances robustness in challenging environments with sudden viewpoint changes or sparse data collection. Our comprehensive experimental results on benchmarking datasets validate the effectiveness of our hybrid approach, demonstrating its superior performance over existing implicit field-based methods in challenging scenarios. HERO-SLAM provides a new pathway to enhance the stability, performance, and applicability of neural SLAM in real-world scenarios. Project page: https://hero-slam.github.io. keywords: {Simultaneous localization and mapping;Optimization methods;Virtual reality;Data collection;Benchmark testing;Robustness;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610000&isnumber=10609862

J. Nash et al., "Censible: A Robust and Practical Global Localization Framework for Planetary Surface Missions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8642-8648, doi: 10.1109/ICRA57147.2024.10611697.Abstract: To achieve longer driving distances, planetary robotics missions require accurate localization to counteract position uncertainty. Freedom and precision in driving allows scientists to reach and study sites of interest. Typically, rover global localization has been performed manually by humans, which is accurate but time-consuming as data is relayed between planets. This paper describes a global localization algorithm that is run onboard the Perseverance Mars rover. Our approach matches rover images to orbital maps using a modified census transform to achieve sub-meter accurate, near-human localization performance on a real dataset of 264 Mars rover panoramas. The proposed solution has also been successfully executed on the Perseverance Mars Rover, demonstrating the practicality of our approach. keywords: {Location awareness;Space vehicles;Mars;Accuracy;Uncertainty;Transforms;Planetary orbits},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611697&isnumber=10609862

T. Miki, J. Lee, L. Wellhausen and M. Hutter, "Learning to walk in confined spaces using 3D representation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8649-8656, doi: 10.1109/ICRA57147.2024.10610271.Abstract: Legged robots have the potential to traverse complex terrain and access confined spaces beyond the reach of traditional platforms thanks to their ability to carefully select footholds and flexibly adapt their body posture while walking. However, robust deployment in real-world applications is still an open challenge. In this paper, we present a method for legged locomotion control using reinforcement learning and 3D volumetric representations to enable robust and versatile locomotion in confined and unstructured environments. By employing a two-layer hierarchical policy structure, we exploit the capabilities of a highly robust low-level policy to follow 6D commands and a high-level policy to enable three-dimensional spatial awareness for navigating under overhanging obstacles. Our study includes the development of a procedural terrain generator to create diverse training environments. We present a series of experimental evaluations in both simulation and real-world settings, demonstrating the effectiveness of our approach in controlling a quadruped robot in confined, rough terrain. By achieving this, our work extends the applicability of legged robots to a broader range of scenarios. keywords: {Legged locomotion;Training;Three-dimensional displays;Navigation;Buildings;Reinforcement learning;Aerospace electronics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610271&isnumber=10609862

F. Magistri, R. Marcuzzi, E. Marks, M. Sodano, J. Behley and C. Stachniss, "Efficient and Accurate Transformer-Based 3D Shape Completion and Reconstruction of Fruits for Agricultural Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8657-8663, doi: 10.1109/ICRA57147.2024.10611717.Abstract: Robots that operate in agricultural environments need a robust perception system that can deal with occlusions, which are naturally present in agricultural scenarios. In this paper, we address the problem of estimating 3D shapes of fruits when only partial observations are available. Generally speaking, such a shape completion can be realized by exploiting prior knowledge about the geometry of the fruit. This is typically done by template matching using traditional optimization algorithms, which are slow but accurate, or by encoding such knowledge into the weights of a neural network, leading to faster but often less accurate estimates. Our approach combines the best of both worlds. It exploits the benefit of having a template representing our object of interest with the advantages of using a neural network to learn how to deform a template. Our experimental evaluation demonstrates that our approach yields accurate estimation at a competitively low inference time in challenging greenhouse environments. keywords: {Knowledge engineering;Geometry;Accuracy;Three-dimensional displays;Shape;Neural networks;Green products},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611717&isnumber=10609862

F. Joublin et al., "CoPAL: Corrective Planning of Robot Actions with Large Language Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8664-8670, doi: 10.1109/ICRA57147.2024.10610434.Abstract: In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation. keywords: {Large language models;Semantics;Systems architecture;Cognition;Planning;Complexity theory;Task analysis;large language models;robotics;planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610434&isnumber=10609862

Y. Luo, Z. Wu and Z. Lian, "CalliRewrite: Recovering Handwriting Behaviors from Calligraphy Images without Supervision," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8671-8678, doi: 10.1109/ICRA57147.2024.10610332.Abstract: Human-like planning skills and dexterous manipulation have long posed challenges in the fields of robotics and artificial intelligence (AI). The task of reinterpreting calligraphy presents a formidable challenge, as it involves the decomposition of strokes and dexterous utensil control. Previous efforts have primarily focused on supervised learning of a single instrument, limiting the performance of robots in the realm of cross-domain text replication. To address these challenges, we propose CalliRewrite: a coarse-to-fine approach for robot arms to discover and recover plausible writing orders from diverse calligraphy images without requiring labeled demonstrations. Our model achieves fine-grained control of various writing utensils. Specifically, an unsupervised image-to-sequence model decomposes a given calligraphy glyph to obtain a coarse stroke sequence. Using an RL algorithm, a simulated brush is fine-tuned to generate stylized trajectories for robotic arm control. Evaluation in simulation and physical robot scenarios reveals that our method successfully replicates unseen fonts and styles while achieving integrity in unknown characters. To access our code and supplementary materials, please visit our project page: https://luoprojectpage.github.io/callirewrite/. keywords: {Training;Limiting;Instruments;Supervised learning;Writing;Manipulators;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610332&isnumber=10609862

F. Bergonti et al., "Co-Design Optimisation of Morphing Topology and Control of Winged Drones," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8679-8685, doi: 10.1109/ICRA57147.2024.10611506.Abstract: The design and control of winged aircraft and drones is an iterative process aimed at identifying a compromise of mission-specific costs and constraints. When agility is required, shape-shifting (morphing) drones represent an efficient solution. However, morphing drones require the addition of actuated joints that increase the topology and control coupling, making the design process more complex. We propose a co-design optimisation method that assists the engineers by proposing a morphing drone’s conceptual design that includes topology, actuation, morphing strategy, and controller parameters. The method consists of applying multi-objective constraint-based optimisation to a multi-body winged drone with trajectory optimisation to solve the motion intelligence problem under diverse flight mission requirements, such as energy consumption and mission completion time. We show that co-designed morphing drones outperform fixed-winged drones in terms of energy efficiency and mission time, suggesting that the proposed co-design method could be a useful addition to the aircraft engineering toolbox. keywords: {Energy consumption;Costs;Computational modeling;Process control;Energy efficiency;Topology;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611506&isnumber=10609862

C. Feng, H. Li, M. Zhang, X. Chen, B. Zhou and S. Shen, "FC-Planner: A Skeleton-guided Planning Framework for Fast Aerial Coverage of Complex 3D Scenes," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8686-8692, doi: 10.1109/ICRA57147.2024.10610621.Abstract: 3D coverage path planning for UAVs is a crucial problem in diverse practical applications. However, existing methods have shown unsatisfactory system simplicity, computation efficiency, and path quality in large and complex scenes. To address these challenges, we propose FC-Planner, a skeleton-guided planning framework that can achieve fast aerial coverage of complex 3D scenes without pre-processing. We decompose the scene into several simple subspaces by a skeleton-based space decomposition (SSD). Additionally, the skeleton guides us to effortlessly determine free space. We utilize the skeleton to efficiently generate a minimal set of specialized and informative viewpoints for complete coverage. Based on SSD, a hierarchical planner effectively divides the large planning problem into independent sub-problems, enabling parallel planning for each subspace. The carefully designed global and local planning strategies are then incorporated to guarantee both high quality and efficiency in path generation. We conduct extensive benchmark and real-world tests, where FC-Planner computes over 10 times faster compared to state-of-the-art methods with shorter path and more complete coverage. The source code will be made publicly available to benefit the community3. Project page: https://hkust-aerial-robotics.github.io/FC-Planner. keywords: {Three-dimensional displays;Source coding;Robot vision systems;Skeleton;Planning;Trajectory;Computational efficiency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610621&isnumber=10609862

C. Qin, M. S. J. Michet, J. Chen and H. H. . -T. Liu, "Time-Optimal Gate-Traversing Planner for Autonomous Drone Racing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8693-8699, doi: 10.1109/ICRA57147.2024.10610148.Abstract: In drone racing, the time-minimum trajectory is affected by the drone’s capabilities, the layout of the race track, and the configurations of the gates (e.g., their shapes and sizes). However, previous studies neglect the configuration of the gates, simply rendering drone racing a waypoint-passing task. This formulation often leads to a conservative choice of paths through the gates, as the spatial potential of the gates is not fully utilized. To address this issue, we present a time-optimal planner that can faithfully model gate constraints with various configurations and thereby generate a more time-efficient trajectory while considering the single-rotor-thrust limits. Our approach excels in computational efficiency which only takes a few seconds to compute the full state and control trajectories of the drone through tracks with dozens of different gates. Extensive simulations and experiments confirm the effectiveness of the proposed methodology, showing that the lap time can be further reduced by taking into account the gate’s configuration. We validate our planner in real-world flights and demonstrate super-extreme flight trajectory through race tracks. keywords: {Shape;Computational modeling;Layout;Logic gates;Rendering (computer graphics);Trajectory;Computational efficiency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610148&isnumber=10609862

J. Choe, J. Lee, H. Yang, H. -N. Nguyen and D. Lee, "Sequential Trajectory Optimization for Externally-Actuated Modular Manipulators with Joint Locking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8700-8706, doi: 10.1109/ICRA57147.2024.10611268.Abstract: In this paper, we present a novel trajectory planning method for externally-actuated modular manipulators (EAMMs), consisting of multiple rotor-actuated links with joints that can be either locked or unlocked. This joint-locking feature allows effective balancing of the payload capacity and dexterity of the robot but significantly complicates the planning problem by introducing binary decision variables. To address this challenge, we leverage the problem’s intrinsic structure, i.e., the payload at the end-effector being enhanced by merely locking its immediate connected links; this allows us to break down the complex planning problem into a series of manageable subproblems and solve them sequentially. Our approach significantly reduces the problem’s complexity: in a serial n-link EAMM with m joint-lock mechanisms, where there could potentially be 2m distinct configurational dynamics, we require solving only n + 1 trajectory optimization problems for single rigid body dynamics sequentially, thereby rendering the problem tractable. We substantiate the efficacy of our method through various simulation and experimental studies, covering ground-free and ground-bound configurations as well as both motion-only and manipulation tasks. keywords: {Trajectory planning;Dynamics;Refining;Rotors;Rendering (computer graphics);Planning;Complexity theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611268&isnumber=10609862

L. Morando and G. Loianno, "Spatial Assisted Human-Drone Collaborative Navigation and Interaction through Immersive Mixed Reality," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8707-8713, doi: 10.1109/ICRA57147.2024.10611351.Abstract: Aerial robots have the potential to play a crucial role in assisting humans with complex and dangerous tasks. Nevertheless, the future industry demands innovative solutions to streamline the interaction process between humans and drones to enable seamless collaboration and efficient coworking. In this paper, we present a novel tele-immersive framework that promotes cognitive and physical collaboration between humans and robots through Mixed Reality (MR). This framework incorporates a novel bi-directional spatial awareness and a multi-modal virtual-physical interaction approaches. The former seamlessly integrates the physical and virtual worlds, offering bidirectional egocentric and exocentric environmental representations. The latter, leveraging the proposed spatial representation, further enhances the collaboration combining a robot planning algorithm for obstacle avoidance with a variable admittance control. This allows users to issue commands based on virtual forces while maintaining compatibility with the environment map. We validate the proposed approach by performing several collaborative planning and exploration tasks involving a drone and an user equipped with a MR headset. keywords: {Performance evaluation;Service robots;Collaboration;Mixed reality;Virtual reality;Bidirectional control;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611351&isnumber=10609862

Y. Zhong, G. Zhao, Q. Wang, G. Xu, C. Xu and F. Gao, "A Trajectory-based Flight Assistive System for Novice Pilots in Drone Racing Scenario," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8714-8720, doi: 10.1109/ICRA57147.2024.10610179.Abstract: Drone racing has become a popular international competition and has attained wide attention in recent years. However, the requirements of high-level operation keep the novice pilots away from participating in it. This paper presents a trajectory-based flight assistive system that enables various operators to fly the drone in a racing scene at a high speed. The whole system is structured hierarchically, consisting of both offline and online components. In the offline part, a global time-optimal trajectory is generated as the expert reference, and a dense flight corridor is constructed to provide sufficiently large safe region. In the online part, a remote control-mapped primitive is designed to fast encapsulate pilots’ inputs, and the time mapping based trajectory progress is customized to further capture intention. Then, a trajectory planner is proposed to generate intention-aligned, smooth, feasible, and safe trajectories periodically. Additionally, a yaw planning that provides the pilot with the best suitable view angle is employed to further alleviate the operation difficulty. Simulations and real world experiments are implemented to verify the performance of our system. The maximum flight speed can reach 6.0 m/s for a novice drone pilot in a real racing scene. Our code is released as an open-source package 1. keywords: {Codes;Trajectory;Planning;Robotics and automation;Drones},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610179&isnumber=10609862

F. Chen, Y. Zheng, Z. Wang, W. Chi and S. Liu, "RBI-RRT*: Efficient Sampling-based Path Planning for High-dimensional State Space," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8721-8727, doi: 10.1109/ICRA57147.2024.10610975.Abstract: Sampling-based planning algorithms such as RRT have been proved to be efficient in solving path planning problems for robotic systems. Various improvements to the RRT algorithm have been presented to improve the performance of the extension and convergence of the random trees, such as Informed RRT*. However, with the growth of spatial dimensions, the time consumption of randomly sampling the entire state space and incrementally rewiring the random trees raises drastically before a feasible solution is found. In this paper, to enhance the convergence performance of optimal solutions, we present Reconstructed Bi-directional Informed RRT* (RBI-RRT*) path planning algorithm. The algorithm acts as RRT-Connect to rapidly find a feasible solution, which helps compress the sampling space as Informed RRT* does. After the random trees are transformed into RRT* structure by the reconstruction process in RBI-RRT*, the algorithm continues to find the near-optimal path. A series of simulations and real-world robot experiments were conducted to evaluate the algorithm against existing planning algorithms. Compared to Informed RRT* Connect, RBI-RRT* reduced the computation time of achieving a specific cost by 22.1% on average in simulations and 11.2% in the real-world robotic arm experiments. The results show that RBI-RRT* is more efficient in high-dimensional planning problems. keywords: {Costs;Computational modeling;Bidirectional control;Reconstruction algorithms;Manipulators;Path planning;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610975&isnumber=10609862

Y. Wang and Y. Chen, "Quasi-static Path Planning for Continuum Robots By Sampling on Implicit Manifold," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8728-8734, doi: 10.1109/ICRA57147.2024.10611372.Abstract: Continuum robots (CR) offer excellent dexterity and compliance in contrast to rigid-link robots, making them suitable for navigating through, and interacting with, confined environments. However, the study of path planning for CRs while considering external elastic contact is limited. The challenge lies in the fact that CRs can have multiple possible configurations when in contact, rendering the forward kinematics not well-defined, and characterizing the set of feasible robot configurations is non-trivial. In this paper, we propose to perform quasi-static path planning on an implicit manifold. We model elastic obstacles as external potential fields and formulate the robot statics in the potential field as the extremal trajectory of an optimal control problem. We show that the set of stable robot configurations is a smooth manifold diffeomorphic to a submanifold embedded in the product space of the CR actuation and base internal wrench. We then propose to perform path planning on this manifold using AtlasRRT*, a sampling-based planner dedicated to planning on implicit manifolds. Simulations in different operation scenarios were conducted and the results show that the proposed planner outperforms Euclidean space planners in terms of success rate and computational efficiency. keywords: {Manifolds;Navigation;Computational modeling;Optimal control;Rendering (computer graphics);Path planning;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611372&isnumber=10609862

J. Garcia, M. Yannuzzi, P. Kramer, C. Rieck, S. P. Fekete and A. T. Becker, "Reconfiguration of a 2D Structure Using Spatio-Temporal Planning and Load Transferring," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8735-8741, doi: 10.1109/ICRA57147.2024.10611057.Abstract: We present progress on the problem of reconfiguring a 2D arrangement of building material by a cooperative group of robots. These robots must avoid collisions, deadlocks, and are subjected to the constraint of maintaining connectivity of the structure. We develop two reconfiguration methods, one based on spatio-temporal planning, and one based on target swapping, to increase building efficiency. The first method can significantly reduce planning times compared to other multi-robot planners. The second method helps to reduce the amount of time robots spend waiting for paths to be cleared, and the overall distance traveled by the robots. keywords: {Building materials;System recovery;Planning;Collision avoidance;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611057&isnumber=10609862

Z. Huang, H. Chen, J. Pohovey and K. Driggs-Campbell, "Neural Informed RRT*: Learning-based Path Planning with Point Cloud State Representations under Admissible Ellipsoidal Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8742-8748, doi: 10.1109/ICRA57147.2024.10611099.Abstract: Sampling-based planning algorithms like Rapidly-exploring Random Tree (RRT) are versatile in solving path planning problems. RRT* offers asymptotic optimality but requires growing the tree uniformly over the free space, which leaves room for efficiency improvement. To accelerate convergence, rule-based informed approaches sample states in an admissible ellipsoidal subset of the space determined by the current path cost. Learning-based alternatives model the topology of the free space and infer the states close to the optimal path to guide planning. We propose Neural Informed RRT* to combine the strengths from both sides. We define point cloud representations of free states. We perform Neural Focus, which constrains the point cloud within the admissible ellipsoidal subset from Informed RRT*, and feeds into PointNet++ for refined guidance state inference. In addition, we introduce Neural Connect to build connectivity of the guidance state set and further boost performance in challenging planning problems. Our method surpasses previous works in path planning benchmarks while preserving probabilistic completeness and asymptotic optimality. We deploy our method on a mobile robot and demonstrate real world navigation around static obstacles and dynamic humans. Code is available at https://github.com/tedhuang96/nirrt_star. keywords: {Point cloud compression;Training;Navigation;Probabilistic logic;Path planning;Planning;Topology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611099&isnumber=10609862

W. Thomason, Z. Kingston and L. E. Kavraki, "Motions in Microseconds via Vectorized Sampling-Based Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8749-8756, doi: 10.1109/ICRA57147.2024.10611190.Abstract: Modern sampling-based motion planning algorithms typically take between hundreds of milliseconds to dozens of seconds to find collision-free motions for high degree-of-freedom problems. This paper presents performance improvements of more than 500x over the state-of-the-art, bringing planning times into the range of microseconds and solution rates into the range of kilohertz, without specialized hardware. Our key insight is how to exploit fine-grained parallelism within planning, providing generality-preserving algorithmic improvements to any such planner and significantly accelerating critical subroutines, such as forward kinematics and collision checking. We demonstrate our approach over a diverse set of challenging, realistic problems for complex robots ranging from 7 to 14 degrees-of-freedom. Moreover, we show our approach does not require high-power hardware by evaluating on a low-power single-board computer. The planning speeds demonstrated are fast enough to reside in the range of control frequencies and open up new avenues of motion planning research. keywords: {Algorithms;Kinematics;Parallel processing;Hardware;Distance measurement;Planning;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611190&isnumber=10609862

B. Brodt and A. Pierson, "Gathering Data from Risky Situations with Pareto-Optimal Trajectories," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8757-8763, doi: 10.1109/ICRA57147.2024.10611689.Abstract: This paper proposes a formulation for the risk-aware path planning problem which utilizes multi-objective optimization to dynamically plan trajectories that satisfy multiple complex mission specifications. In the setting of persistent monitoring, we develop a method for representing environmental information and risk in a way that allows for local sampling to generate Pareto-dominant solutions over a receding horizon. We propose two algorithms capable of solving these problems: a dense sampling approach and an improved method utilizing noisy gradient descent. Simulation results demonstrate the efficacy of our methods at persistently gathering information while avoiding risk, robust to randomly-generated environments. keywords: {Uncertainty;Heuristic algorithms;Simulation;Decision making;Trajectory;Noise measurement;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611689&isnumber=10609862

A. Dastider, H. Fang and M. Lin, "RETRO: Reactive Trajectory Optimization for Real-Time Robot Motion Planning in Dynamic Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8764-8770, doi: 10.1109/ICRA57147.2024.10610542.Abstract: Reactive trajectory optimization for robotics presents formidable challenges, demanding the rapid generation of purposeful robot motion in complex and swiftly changing dynamic environments. While much existing research predominantly addresses robotic motion planning with predefined objectives, emerging problems in robotic trajectory optimization frequently involve dynamically evolving objectives and stochastic motion dynamics. However, effectively addressing such reactive trajectory optimization challenges for robot manipulators proves difficult due to inefficient, high-dimensional trajectory representations and a lack of consideration for time optimization.In response, we introduce a novel trajectory optimization framework called RETRO. RETRO employs adaptive optimization techniques that span both spatial and temporal dimensions. As a result, it achieves a remarkable computing complexity of O(T2.4)+O(Tn2), a significant improvement over the traditional application of DDP, which leads to a complexity of O(n4) when reasonable time step sizes are used. To evaluate RETRO’s performance in terms of error, we conducted a comprehensive analysis of its regret bounds, comparing it to an Oracle value function obtained through an Oracle trajectory optimization algorithm. Our analytical findings demonstrate that RETRO’s total regret can be upper-bounded by a function of the chosen time step size. Moreover, our approach delivers smoothly optimized robot trajectories within the joint space, offering flexibility and adaptability for various tasks. It can seamlessly integrate task-specific requirements such as collision avoidance while maintaining real-time control rates. We validate the effectiveness of our framework through extensive simulations and real-world robot experiments in closed-loop manipulation scenarios.For further details and supplementary materials, please visit: https://sites.google.com/view/retro-optimal-control/home keywords: {Robot motion;Dynamics;Real-time systems;Planning;Complexity theory;Time factors;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610542&isnumber=10609862

V. S. Chipade, R. Kumar and S. Z. Yong, "WiTHy A*: Winding-Constrained Motion Planning for Tethered Robot using Hybrid A*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8771-8777, doi: 10.1109/ICRA57147.2024.10611175.Abstract: In this paper, a variant of hybrid A* is developed to find the shortest path for a curvature-constrained robot, that is tethered at its start position, such that the tether satisfies user-defined winding angle constraints. A variant of tangent graphs is used as an underlying graph for searching a path using A* in order to reduce the overall computation and define appropriate cost metrics to ensure winding angle constraints are satisfied. Conditions are provided under which the proposed algorithm is guaranteed to find a winding angle constrained path. The effectiveness and performance of the proposed algorithm are studied in simulation. keywords: {Measurement;Costs;Windings;Planning;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611175&isnumber=10609862

T. Manzini and R. Murphy, "Differentiable Boustrophedon Paths That Enable Optimization Via Gradient Descent," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8778-8783, doi: 10.1109/ICRA57147.2024.10610136.Abstract: This paper introduces a differentiable representation for the optimization of boustrophedon path plans in convex polygons, explores an additional parameter of these path plans that can be optimized, discusses the properties of this representation that can be leveraged during the optimization process and shows that the previously published attempt at optimization of these path plans was too coarse to be practically useful. Experiments were conducted to show that this differentiable representation can reproduce scores from traditional discrete representations of boustrophedon path plans with high fidelity. Finally, optimization via gradient descent was attempted but found to fail because the search space is far more non-convex than was previously considered in the literature. The wide range of applications for boustrophedon path plans means that this work has the potential to improve path planning efficiency in numerous areas of robotics, including mapping and search tasks using uncrewed aerial systems, environmental sampling tasks using uncrewed marine vehicles, and agricultural tasks using ground vehicles, among numerous others applications. keywords: {Closed-form solutions;Search methods;Integral equations;Refining;Mathematical models;Path planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610136&isnumber=10609862

C. E. Winston and L. F. Casey, "OriTrack: A Small, 3 Degree-of-Freedom, Origami Solar Tracker," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8792-8798, doi: 10.1109/ICRA57147.2024.10611374.Abstract: In response to the need for sustainable energy solutions, solar panels have gained significant traction. One way to increase the energy capture of solar systems is through solar tracking, a means of reorienting solar panels throughout the day in order to face the sun. The energy consumption increase that comes with solar tracking often far outweighs the amount of energy required to move the panel, which makes it a compelling strategy for improving solar systems. Unfortunately, while solar trackers are commonly used in large solar farms, they are rarely used on rooftops, an area where solar panels are commonly installed. This is for two primary reasons: (1) most commercially available solar trackers are too large to be installed on roofs and (2) even if traditional solar trackers were made in a more compact form-factor it would be difficult to densely lay them out on a roof without the trackers substantially shading each other. In order to address these issues, we introduce OriTrack, a small three-degree-of-freedom (3 DOF) solar tracker which reduces the area of its shadow by reducing its height as it tracks the sun. In this paper we discuss the design, manufacturing, and control of OriTrack. We then compare OriTrack to a flat reference panel, the solar energy solution commonly used on roofs today, and find that OriTrack demonstrates 23% increased energy production. This result suggests OriTrack could be used as a future solution for solar tracking on rooftops. keywords: {Energy consumption;Energy capture;Tracking;Solar energy;Production;Manufacturing;Solar panels;Energy and Environment-Aware Automation;Soft Robot Applications;Soft Robot Materials and Design},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611374&isnumber=10609862

M. Li, D. Matthews and S. Kriegman, "Reinforcement learning for freeform robot design," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8799-8806, doi: 10.1109/ICRA57147.2024.10610048.Abstract: Inspired by the necessity of morphological adaptation in animals, a growing body of work has attempted to expand robot training to encompass physical aspects of a robot’s design. However, reinforcement learning methods capable of optimizing the 3D morphology of a robot have been restricted to reorienting or resizing the limbs of a predetermined and static topological genus. Here we show policy gradients for designing freeform robots with arbitrary external and internal structure. This is achieved through actions that deposit or remove bundles of atomic building blocks to form higher-level nonparametric macrostructures such as appendages, organs and cavities. Although results are provided for open loop control only, we discuss how this method could be adapted for closed loop control and sim2real transfer to physical machines in future. keywords: {Training;Three-dimensional displays;Animals;Atomic layer deposition;Morphology;Reinforcement learning;Open loop systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610048&isnumber=10609862

X. Yin, J. Xie, P. Zhou, S. Wen and J. Zhang, "A Helical Bistable Soft Gripper Enable by Pneumatic Actuation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8807-8813, doi: 10.1109/ICRA57147.2024.10610729.Abstract: There are many instances of helical mechanisms that are used to efficiently grasp different objects with various shapes and sizes in nature. Inspired by the helical grasping in the nature, we proposed a helical bistable soft gripper with high load capacity and energy saving. An off-the-shelf bistable steel shell (BSS) as the stiff element was inserted into a 3D printing soft helical exoskeleton to achieve coiling around and holding the objects without energy consumption. Two air pouches were designed as the actuator to control the transition between the two stable states. To facilitate gripper design, a simplified model of the gripper was conducted, and the geometric parameters of the gripper are listed in a table for reference. The transition pressures between the two stable states were experimentally characterized. Moreover, we conduct experiments to demonstrate the capability of the gripper in two working modes. The gripper exhibits coiling diameters ranging between 40 mm and 60 mm and is successfully attached to various slender objects of different geometries with a maximum holding force of 92.67 N (up to 135.1 times of its mass) in hanging mode. Finally, the gripper was integrated into a robot arm and successfully grasped different objects, and the maximum grasping weight is 221.6 g in the grasping mode. keywords: {Geometry;Shape;Force;Exoskeletons;Grasping;Pneumatic systems;Three-dimensional printing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610729&isnumber=10609862

M. Asgari, I. A. Bonev and C. Gosselin, "Singularity Analysis of Kinova’s Link 6 Robot Arm via Grassmann Line Geometry," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8814-8820, doi: 10.1109/ICRA57147.2024.10610252.Abstract: Unlike parallel robots, for which hundreds of different architectures have been proposed, the vast majority of six-degree-of-freedom (DOF) serial robots have one of two simple architectures. In both architectures, the inverse kinematics can be solved in closed form and the singularities described by trivial geometric and algebraic conditions. These conditions can be readily obtained by analyzing the determinant of the robot’s Jacobian matrix, and provide an in-depth understanding of the robot’s singularities, which is essential for its optimal use. However, for various reasons, robot arms with unorthodox architectures are occasionally designed. Such arms do not have closed-form inverse kinematics and little insight into their singularities can be gained by analyzing the determinant of their Jacobian. One such robot arm for which the conventional singularity analysis approach fails is the new Link 6 collaborative robot by Kinova. In this paper, we study the complex singularities of Link 6 by investigating all possibilities for screw dependencies, deriving a simple equation for each case, and then describing each singularity type using Grassmann line geometry. Twelve different singularity configurations are identified and described with seven relatively simple geometric conditions. Our approach is general and can be applied to other robot arms. keywords: {Jacobian matrices;Geometry;Parallel robots;Kinematics;Collaborative robots;Fasteners;Manipulators;Singularities;industrial robot arms;Screw dependency;Grassmann line geometry;Kinova Link 6},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610252&isnumber=10609862

R. Hall, G. Espinosa, S. -S. Chiang and C. D. Onal, "Design and Testing of a Multi-Module, Tetherless, Soft Robotic Eel," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8821-8827, doi: 10.1109/ICRA57147.2024.10611531.Abstract: This paper presents a free-swimming, tetherless, cable-driven modular soft robotic fish. The body comprises a series of 3D-printed wave spring structures that create a flexible biologically inspired shape that is capable of an anguilliform swimming gait. A three-module soft robotic fish was designed, fabricated, and evaluated. The motion of the robot was characterized and different combinations of actuation amplitude, frequency, and phase shift were determined experimentally to determine the optimal parameters that maximized speed and minimized the cost of transport (COT). The maximum speed recorded was 0.20 BL/s (body lengths per second) with a COT of 15.82. These results were compared against other robotic and biological fish. We operated the robot, untethered, in a variety of environments to test how it was able to function outside of laboratory settings. keywords: {Shape;Tail;Soft robotics;Fish;Motors;Biology;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611531&isnumber=10609862

R. Hall and C. D. Onal, "Untethered Underwater Soft Robot with Thrust Vectoring," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8828-8834, doi: 10.1109/ICRA57147.2024.10610430.Abstract: This paper introduces DRAGON: Deformable Robot for Agile Guided Observation and Navigation, a free-swimming deformable impeller-powered vectored underwater vehicle (VUV). A 3D-printed wave spring structure directs the water drawn through the center of the robot by an impeller, enabling it to move smoothly in different directions. The robot is designed to have a narrow cylindrical profile to lower drag and improve agility. It has a maximum recorded speed of 2.1 BL/s (body lengths per second) and a minimum cost of transport (COT) of 2.9. The robot has two degrees of freedom (DoFs) and is capable of performing a variety of maneuvers including a full circle with a radius of 0.23 m (1.4 BL) and a figure eight, which it completed in 4.98 s (72.3 °/s) and 10.74 s respectively. We operated the robot, untethered, in various environments to test the robustness of the design and analyze its motion and performance. keywords: {Space vehicles;Autonomous underwater vehicles;Impellers;Tail;Soft robotics;Springs;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610430&isnumber=10609862

J. Kim and C. Gosselin, "A Backdrivable Axisymmetric Kinematically Redundant (6+3)-Degree-of-Freedom Hybrid Parallel Manipulator," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8835-8841, doi: 10.1109/ICRA57147.2024.10610821.Abstract: A kinematically redundant (6+3)-degree-of-freedom (DOF) hybrid parallel robot with an axisymmetric workspace is proposed. By arranging the first revolute joint of each leg such that they have the same rotation axis, this robot can achieve an axisymmetric workspace, resulting in a large reachable workspace. In addition, type II singularities, which critically limit the orientational workspace, can be fully avoided by utilizing kinematic redundancy. A gripper mechanism is developed to increase the orientational workspace by exploiting the redundant DOFs. Moreover, the orientational workspace can be further increased by controlling one of the redundant DOFs to keep a certain constant angle. As a result, the proposed hybrid parallel robot achieves a high workspace-to-footprint ratio comparable to that of serial robots. A CAD model of the robot and computer animations are provided to demonstrate the large workspaces and the gripper mechanism. A significant advantage of the proposed robot over serial architectures is that the robot is backdrivable since it uses direct-drive or quasi-direct-drive actuators. keywords: {Legged locomotion;Parallel robots;Actuators;Solid modeling;Redundancy;Prototypes;Kinematics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610821&isnumber=10609862

R. Black et al., "RASCAL: A Scalable, High-redundancy Robot for Automated Storage and Retrieval Systems," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8868-8874, doi: 10.1109/ICRA57147.2024.10610551.Abstract: Automated storage and retrieval systems (ASRS) are a key component of the modern storage industry, and are used in a wide range of applications, carrying anything from lightweight tape cartridges to entire pallets of goods. Many of these systems are under pressure to maximise the use of space by growing in height and density, but this can create challenges for the the robots that service them. In this context, we present RASCAL, a novel ASRS robot for small payload items in structured environments, with a focus on system-level scalability and redundancy. We describe the design objectives of RASCAL and how they address some of the limitations of existing robotic systems in this area, such as scalability and redundancy. We then demonstrate the viability of our design with a proof-of-concept implementation of a data centre storage media robot, and show through a series of experiments that its design, speed, accuracy, and energy efficiency are appropriate for this application. keywords: {Industries;Data centers;Service robots;Scalability;Redundancy;Storage automation;Media},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610551&isnumber=10609862

A. S. Roos, Z. Zake, T. Rasheed, N. Pedemonte and S. Caro, "Towards Solving Cable-Driven Parallel Robot Inaccuracy due to Cable Elasticity," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8898-8904, doi: 10.1109/ICRA57147.2024.10610384.Abstract: Cable elasticity can significantly impact the accuracy of Cable-Driven Parallel Robots (CDPRs). However, it’s frequently disregarded as negligible in CDPR simulations and designs. In this paper, we propose a numerical approach, referred to as SEECR, which is designed to estimate the behavior of a CDPR featuring elastic cables while ensuring the Static Equilibrium (SE) of the Moving-Platform (MP). By modeling the cables as elastic springs, the proposed approach correctly predicts which cables become slack, estimates the tension distribution among cables and computes unwanted MP motions, allowing to predict the impact of design choices. The results have been validated experimentally on two cable types and configurations. keywords: {Parallel robots;Accuracy;Power cables;Computational modeling;Elasticity;Predictive models;Numerical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610384&isnumber=10609862

S. S. Grama, M. Javadi, S. Kumar, H. Z. Boroujeni and F. Kirchner, "Ricmonk: A Three-Link Brachiation Robot with Passive Grippers for Energy-Efficient Brachiation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8920-8926, doi: 10.1109/ICRA57147.2024.10611003.Abstract: This paper presents the design, analysis, and performance evaluation of RicMonk, a novel three-link brachiation robot equipped with passive hook-shaped grippers. Brachiation, an agile and energy-efficient mode of locomotion observed in primates, has inspired the development of RicMonk to explore versatile locomotion and maneuvers on ladder-like structures. The robot’s anatomical resemblance to gibbons and the integration of a tail mechanism for energy injection contribute to its unique capabilities. The paper discusses the use of the Direct Collocation methodology for optimizing trajectories for the robot’s dynamic behaviors and stabilization of these trajectories using a Time-varying Linear Quadratic Regulator. With RicMonk we demonstrate bidirectional brachiation, and provide comparative analysis with its predecessor, AcroMonk - a two-link brachiation robot, to demonstrate that the presence of a passive tail helps improve energy efficiency. The system design, controllers, and software implementation are publicly available on GitHub at https://github.com/dfki-ric-underactuated-lab/ricmonk and the video demonstration of the experiments can be viewed at https://youtu.be/hOuDQI7CD8w. keywords: {Performance evaluation;Regulators;Tail;Energy efficiency;Software;Trajectory;Grippers;Underactuated robots;biologically-inspired robots;education robotics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611003&isnumber=10609862

F. Han and J. Yi, "Gaussian Process-Enhanced, External and Internal Convertible Form-Based Control of Underactuated Balance Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8927-8933, doi: 10.1109/ICRA57147.2024.10610172.Abstract: External and internal convertible (EIC) form-based motion control (i.e., EIC-based control) is one of the effective approaches for underactuated balance robots. By sequentially controller design, trajectory tracking of the actuated subsystem and balance of the unactuated subsystem can be achieved simultaneously. However, with certain conditions, there exists uncontrolled robot motion under the EIC-based control. We first identify these conditions and then propose an enhanced EIC-based control with a Gaussian process data-driven robot dynamic model. Under the new enhanced EIC-based control, the stability and performance of the closed-loop system are guaranteed. We demonstrate the GP-enhanced control experimentally using two examples of underactuated balance robots. keywords: {Robot motion;Trajectory tracking;Control design;Dynamics;Process control;Gaussian processes;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610172&isnumber=10609862

L. Zhang, M. Li, W. Yang and S. Yang, "Task Allocation in Heterogeneous Multi-Robot Systems Based on Preference-Driven Hedonic Game," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8967-8972, doi: 10.1109/ICRA57147.2024.10611476.Abstract: Multiple preferences between robots and tasks have been largely overlooked in previous research on Multi-Robot Task Allocation (MRTA) problems. In this paper, we propose a preference-driven approach based on hedonic game to address the task allocation problem of muti-robot systems in emergency rescue scenarios. We present a distributed framework considering various preferences between robots and tasks to determine the division of coalitions in such problems and evaluate the scalability and adaptability of our algorithm through relevant experiments. Furthermore, considering the strict communication limitations in emergency rescue scenarios, we have verified that our algorithm can efficiently converge to a Nash-stable coalition partition even in conditions of insufficient communication distance. keywords: {Scalability;Games;Robustness;Partitioning algorithms;Resource management;Multi-robot systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611476&isnumber=10609862

J. Bautista and H. G. de Marina, "Behavioral-based circular formation control for robot swarms," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8989-8995, doi: 10.1109/ICRA57147.2024.10610826.Abstract: This paper focuses on coordinating a robot swarm orbiting a convex path without collisions among the individuals. The individual robots lack braking capabilities and can only adjust their courses while maintaining their constant but different speeds. Instead of controlling the spatial relations between the robots, our formation control algorithm aims to deploy a dense robot swarm that mimics the behavior of tornado schooling fish. To achieve this objective safely, we employ a combination of a scalable overtaking rule, a guiding vector field, and a control barrier function with an adaptive radius to facilitate smooth overtakes. The decision-making process of the robots is distributed, relying only on local information. Practical applications include defensive structures or escorting missions with the added resiliency of a swarm without a centralized command. We provide a rigorous analysis of the proposed strategy and validate its effectiveness through numerical simulations involving a high density of unicycles. keywords: {Robot kinematics;MIMICs;Numerical simulation;Fish;Formation control;Vectors;Orbits},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610826&isnumber=10609862

D. Chiu, R. Nagpal and B. Haghighat, "Optimization and Evaluation of a Multi Robot Surface Inspection Task Through Particle Swarm Optimization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 8996-9002, doi: 10.1109/ICRA57147.2024.10611661.Abstract: Robot swarms can be tasked with a variety of automated sensing and inspection applications in aerial, aquatic, and surface environments. In this paper, we study a simplified two-outcome surface inspection task. We task a group of robots to inspect and collectively classify a 2D surface section based on a binary pattern projected on the surface. We use a decentralized Bayesian decision-making algorithm and deploy a swarm of 3-cm sized wheeled robots to inspect a randomized black and white tiled surface section of size 1m×1m in simulation. We first describe the model parameters that characterize our simulated environment, the robot swarm, and the inspection algorithm. We then employ a noise-resistant heuristic optimization scheme based on the Particle Swarm Optimization (PSO) using a fitness evaluation that combines the swarm’s classification decision accuracy and decision time. We use our fitness measure definition to asses the optimized parameters through 100 randomized simulations that vary surface pattern and initial robot poses. The optimized algorithm parameters show up to 55% improvement in median of fitness evaluations against an empirically chosen parameter set. keywords: {Decision making;Inspection;Robot sensing systems;Particle measurements;Classification algorithms;Sensors;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611661&isnumber=10609862

S. Singh, Z. Huang, A. K. Srinivasan, G. Gutow, B. Vundurthy and H. Choset, "Hierarchical Planning for Long-Horizon Multi-Agent Collective Construction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9003-9009, doi: 10.1109/ICRA57147.2024.10611496.Abstract: We develop a planner that directs robots to construct a 3D target structure composed of blocks. The robots themselves are cubes of the same size as the blocks, and they may place, carry, or remove one block at a time. When moving, robots are also allowed to climb or descend a block. A construction plan may thus build a staircase-like scaffolding of blocks to reach other blocks at higher levels. The order of block placement is important; for example, a block that sits atop other blocks must be placed after the blocks below it, and a block that needs scaffolding cannot be placed until after the scaffolding is. Prior works focus on end-to-end approaches that simultaneously plan for block placement order and inter-robot collisions. Larger structures are either intractable or yield high-cost solutions. A prior approach mitigates this by decomposing the structure into smaller components that can be planned for independently, but the computational challenge remains. We present a hierarchical approach that first 1) uses A* to determine a sequence of block placements and removals while ignoring inter-robot collision, then 2) identifies ordering constraints between block placement and removal actions, and finally (3) computes collision-free paths for multiple robots to perform said actions. Compared to an optimization approach that minimizes the number of timesteps to complete the structure, we observe a 100x reduction in computation time for comparable solutions. keywords: {Three-dimensional displays;Planning;Collision avoidance;Robots;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611496&isnumber=10609862

C. Fan, S. Zhang, K. Liu, S. Wang, Z. Yang and W. Wang, "Enhancing mmWave Radar Point Cloud via Visual-inertial Supervision," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9010-9017, doi: 10.1109/ICRA57147.2024.10610091.Abstract: Complementary to prevalent LiDAR and camera systems, millimeter-wave (mmWave) radar is robust to adverse weather conditions like fog, rainstorms, and blizzards but offers sparse point clouds. Current techniques enhance the point cloud by the supervision of LiDAR’s data. However, high-performance LiDAR is notably expensive and is not commonly available on vehicles. This paper presents mmEMP, a supervised learning approach that enhances radar point clouds using a low-cost camera and an inertial measurement unit (IMU), enabling crowd-sourcing training data from commercial vehicles. Bringing the visual-inertial (VI) supervision is challenging due to the spatial agnostic of dynamic objects. Moreover, spurious radar points from the curse of RF multipath make robots misunderstand the scene. mmEMP first devises a dynamic 3D reconstruction algorithm that restores the 3D positions of dynamic features. Then, we design a neural network that densifies radar data and eliminates spurious radar points. We build a new dataset in the real world. Extensive experiments show that mmEMP achieves competitive performance compared with the SOTA approach training by LiDAR’s data. In addition, we use the enhanced point cloud to perform object detection, localization, and mapping to demonstrate mmEMP’s effectiveness. keywords: {Point cloud compression;Three-dimensional displays;Heuristic algorithms;Supervised learning;Robot vision systems;Training data;Radar},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610091&isnumber=10609862

Y. Li, H. Hu, Z. Liu, X. Xu, X. Huang and D. Zhao, "Influence of Camera-LiDAR Configuration on 3D Object Detection for Autonomous Driving," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9018-9025, doi: 10.1109/ICRA57147.2024.10610896.Abstract: Cameras and LiDARs are both important sensors for autonomous driving, playing critical roles in 3D object detection. Camera-LiDAR Fusion has been a prevalent solution for robust and accurate driving perception. In contrast to the vast majority of existing arts that focus on how to improve the performance of 3D target detection through cross-modal schemes, deep learning algorithms, and training tricks, we devote attention to the impact of sensor configurations on the performance of learning-based methods. To achieve this, we propose a unified information-theoretic surrogate metric for camera and LiDAR evaluation based on the proposed sensor perception model. We also design an accelerated high-quality framework for data acquisition, model training, and performance evaluation that functions with the CARLA simulator. To show the correlation between detection performance and our surrogate metrics, We conduct experiments using several camera-LiDAR placements and parameters inspired by selfdriving companies and research institutions. Extensive experimental results of representative algorithms on nuScenes dataset validate the effectiveness of our surrogate metric, demonstrating that sensor configurations significantly impact point-cloudimage fusion based detection models, which contribute up to 30% discrepancy in terms of the average precision. keywords: {Training;Performance evaluation;Three-dimensional displays;Laser radar;Data acquisition;Object detection;Sensor fusion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610896&isnumber=10609862

J. Cao, X. Zheng, Y. Lyu, J. Wang, R. Xu and L. Wang, "Chasing Day and Night: Towards Robust and Efficient All-Day Object Detection Guided by an Event Camera," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9026-9032, doi: 10.1109/ICRA57147.2024.10611705.Abstract: The ability to detect objects in all lighting (i.e., normal-, over-, and under-exposed) conditions is crucial for real-world applications, such as self-driving. Traditional RGB-based detectors often fail under such varying lighting conditions. Therefore, recent works utilize novel event cameras to supplement or guide the RGB modality; however, these methods typically adopt asymmetric network structures that rely predominantly on the RGB modality, resulting in limited robustness for all-day detection. In this paper, we propose EOLO, a novel object detection framework that achieves robust and efficient all-day detection by fusing both RGB and event modalities. Our EOLO framework is built based on a lightweight spiking neural network (SNN) to efficiently leverage the asynchronous property of events. Buttressed by it, we first introduce an Event Temporal Attention (ETA) module to learn the high temporal information from events while preserving crucial edge information. Secondly, as different modalities exhibit varying levels of importance under diverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion (SREF) module to effectively fuse RGB-Event features without relying on a specific modality, thus ensuring a balanced and adaptive fusion for all-day detection. In addition, to compensate for the lack of paired RGB-Event datasets for all-day training and evaluation, we propose an event synthesis approach based on the randomized optical flow that allows for directly generating the event frame from a single exposure image. We further build two new datasets, E-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC. Extensive experiments demonstrate that our EOLO outperforms the state-of-the-art detectors, e.g., RENet [1], by a substantial margin (+3.74% mAP50) in all lighting conditions. Our code and datasets will be available at https://vlislab22.github.io/EOLO/. keywords: {Training;Lighting;Object detection;Spiking neural networks;Detectors;Feature extraction;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611705&isnumber=10609862

A. Sikdar, J. Teotia and S. Sundaram, "SKD-Net: Spectral-based Knowledge Distillation in Low-Light Thermal Imagery for robotic perception," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9041-9047, doi: 10.1109/ICRA57147.2024.10611323.Abstract: Enhancing the generalization capacity for semantic segmentation of aerial perception systems for safety-critical applications is vital, especially for environments with low-light and adverse conditions. Multi-spectral fusion techniques aim to maintain the merits of electro-optical (EO) and infrared (IR) images, e.g., retaining low-level features and capturing detailed textures from both modalities. However, these techniques encounter limitations when faced with scenarios involving missing modalities, especially during inference when only IR images are available. In this paper, we propose a novel spectral-based knowledge distillation architecture known as SKD-Net to improve the performance of deep learning models for missing modality scenarios for semantic segmentation tasks. In this architecture, we make use of Gated Spectral Unit to combine information from both modalities. SKD-Net aims to extract valuable semantic information from EO images while preserving spectral knowledge from the IR images within the feature space. The model retains the style information in the shallow layers while simultaneously fusing the high-level semantic context obtained from EO and IR images to improve the feature generation capacity when dealing with only IR images during inference. SKD-Net outperforms state-of-the-art multi-modal fusion and distillation models by 2.8% on average in scenarios with missing modalities when using only IR data during inference in two public benchmarking datasets. This performance increase is achieved without additional computational costs compared to the baseline segmentation models. keywords: {Training;Semantic segmentation;Computational modeling;Semantics;Computer architecture;Logic gates;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611323&isnumber=10609862

H. Dong et al., "SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9056-9062, doi: 10.1109/ICRA57147.2024.10611320.Abstract: High-definition (HD) semantic map generation of the environment is an essential component of autonomous driving. Existing methods have achieved good performance in this task by fusing different sensor modalities, such as LiDAR and camera. However, current works are based on raw data or network feature-level fusion and only consider short-range HD map generation, limiting their deployment to realistic autonomous driving applications. In this paper, we focus on the task of building the HD maps in both short ranges, i.e., within 30m, and also predicting long-range HD maps up to 90m, which is required by downstream path planning and control tasks to improve the smoothness and safety of autonomous driving. To this end, we propose a novel network named SuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels. We use LiDAR depth to improve image depth estimation and use image features to guide long-range LiDAR feature prediction. We benchmark our SuperFusion on the nuScenes dataset and a self-recorded dataset and show that it outperforms the state-of-the-art baseline methods with large margins on all intervals. Additionally, we apply the generated HD map to a downstream path planning task, demonstrating that the long-range HD maps predicted by our method can lead to better path planning for autonomous vehicles. Our code and self-recorded dataset have been released at https://github.com/haomo-ai/SuperFusion. keywords: {Laser radar;Limiting;Semantics;Estimation;Cameras;Path planning;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611320&isnumber=10609862

J. Zhu, L. Liu, Y. Tang, F. Wen, W. Li and Y. Liu, "Semi-Supervised Learning for Visual Bird’s Eye View Semantic Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9079-9085, doi: 10.1109/ICRA57147.2024.10611420.Abstract: Visual bird’s eye view (BEV) semantic segmentation helps autonomous vehicles understand the surrounding environment only from front-view (FV) images, including static elements (e.g., roads) and dynamic elements (e.g., vehicles, pedestrians). However, the high cost of annotation procedures of full-supervised methods limits the capability of the visual BEV semantic segmentation, which usually needs HD maps, 3D object bounding boxes, and camera extrinsic matrixes. In this paper, we present a novel semi-supervised framework for visual BEV semantic segmentation to boost performance by exploiting unlabeled images during the training. A consistency loss that makes full use of unlabeled data is then proposed to constrain the model on not only semantic prediction but also the BEV feature. Furthermore, we propose a novel and effective data augmentation method named conjoint rotation which reasonably augments the dataset while maintaining the geometric relationship between the FV images and the BEV semantic segmentation. Extensive experiments on the nuScenes dataset show that our semi-supervised framework can effectively improve prediction accuracy. To the best of our knowledge, this is the first work that explores improving visual BEV semantic segmentation performance using unlabeled data. The code is available at https://github.com/Junyu-Z/Semi-BEVseg. keywords: {Training;Visualization;Three-dimensional displays;Semantic segmentation;Semantics;Data augmentation;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611420&isnumber=10609862

Y. Zhou, L. Cai, X. Cheng, Z. Gan, X. Xue and W. Ding, "OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9086-9092, doi: 10.1109/ICRA57147.2024.10610779.Abstract: In the era of big data and large models, automatic annotating functions for multi-modal data are of great significance for real-world AI-driven applications, such as autonomous driving and embodied AI. Unlike traditional closed-set annotation, open-vocabulary annotation is essential to achieve human-level cognition capability. However, there are few open-vocabulary auto-labeling systems for multi-modal 3D data. In this paper, we introduce OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can automatically generate 2D masks, 3D masks, and 3D bounding box annotations for vision and point cloud data. Our system integrates the chain-of-thought capabilities of Large Language Models (LLMs) and the cross-modality capabilities of vision-language models (VLMs). To the best of our knowledge, OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal 3D auto-labeling. We conduct comprehensive evaluations on both public and in-house real-world datasets, which demonstrate that the system significantly improves annotation efficiency compared to manual annotation while providing accurate open-vocabulary auto-annotating results. keywords: {Point cloud compression;Three-dimensional displays;Annotations;Large language models;Manuals;Big Data;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610779&isnumber=10609862

B. Yao, Y. Deng, Y. Liu, H. Chen, Y. Li and Z. Yang, "SAM-Event-Adapter: Adapting Segment Anything Model for Event-RGB Semantic Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9093-9100, doi: 10.1109/ICRA57147.2024.10611127.Abstract: Semantic segmentation, a fundamental visual task ubiquitously employed in sectors ranging from transportation and robotics to healthcare, has always captivated the research community. In the wake of rapid advancements in large model research, the foundation model for semantic segmentation tasks, termed the Segment Anything Model (SAM), has been introduced. This model substantially addresses the dilemma of poor generalizability of previous segmentation models and the disadvantage in requiring to retrain the whole model on variant datasets. Nonetheless, segmentation models developed on SAM remain constrained by the inherent limitations of RGB sensors, particularly in scenarios characterized by complex lighting conditions and high-speed motion. Motivated by these observations, a natural recourse is to adapt SAM to additional visual modalities without compromising its robust generalizability. To achieve this, we introduce a lightweight SAM-Event-Adapter (SE-Adapter) module, which incorporates event camera data into a cross-modal learning architecture based on SAM, with only limited tunable parameters incremental. Capitalizing on the high dynamic range and temporal resolution afforded by event cameras, our proposed multi-modal Event-RGB learning architecture effectively augments the performance of semantic segmentation tasks. In addition, we propose a novel paradigm for representing event data in a patch format compatible with transformer-based models, employing multi-spatiotemporal scale encoding to efficiently extract motion and semantic correlations from event representations. Exhaustive empirical evaluations conducted on the DSEC-Semantic and DDD17 datasets provide validation of the effectiveness and rationality of our proposed approach. keywords: {Adaptation models;Visualization;Semantic segmentation;Motion segmentation;Transportation;Cameras;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611127&isnumber=10609862

J. Liu et al., "Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9109-9116, doi: 10.1109/ICRA57147.2024.10610511.Abstract: Neuromorphic spike data, an upcoming modality with high temporal resolution, has shown promising potential in autonomous driving by mitigating the challenges posed by high-velocity motion blur. However, training the spike depth estimation network holds significant challenges in two aspects: sparse spatial information for pixel-wise tasks and difficulties in achieving paired depth labels for temporally intensive spike streams. Therefore, we introduce open-source RGB data to support spike depth estimation, leveraging its annotations and spatial information. The inherent differences in modalities and data distribution make it challenging to directly apply transfer learning from open-source RGB to target spike data. To this end, we propose a cross-modality cross-domain (BiCross) framework to realize unsupervised spike depth estimation by introducing simulated mediate source spike data. Specifically, we design a Coarse-to-Fine Knowledge Distillation (CFKD) approach to facilitate comprehensive cross-modality knowledge transfer while preserving the unique strengths of both modalities, utilizing a spike-oriented uncertainty scheme. Then, we propose a Self-Correcting Teacher-Student (SCTS) mechanism to screen out reliable pixel-wise pseudo labels and ease the domain shift of the student model, which avoids error accumulation in target spike data. To verify the effectiveness of BiCross, we conduct extensive experiments on four scenarios, including Synthetic to Real, Extreme Weather, Scene Changing, and Real Spike. Our method achieves state-of-the-art (SOTA) performances, compared with RGB-oriented unsupervised depth estimation methods. Code and dataset: https://github.com/Theia-4869/BiCross. keywords: {Training;Uncertainty;Annotations;Transfer learning;Estimation;Task analysis;Spatial resolution},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610511&isnumber=10609862

J. Kim et al., "PillarGen: Enhancing Radar Point Cloud Density and Quality via Pillar-based Point Generation Network," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9117-9124, doi: 10.1109/ICRA57147.2024.10611144.Abstract: In this paper, we present a novel point generation model, referred to as Pillar-based Point Generation Network (PillarGen), which facilitates the transformation of point clouds from one domain into another. PillarGen can produce synthetic point clouds with enhanced density and quality based on the provided input point clouds. The PillarGen model performs the following three steps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar to Point Generation (PPG). The input point clouds are encoded using a pillar grid structure to generate pillar features. Then, OPP determines the active pillars used for point generation and predicts the center of points and the number of points to be generated for each active pillar. PPG generates the synthetic points for each active pillar based on the information provided by OPP. We evaluate the performance of PillarGen using our proprietary radar dataset, focusing on enhancing the density and quality of short-range radar data using the long-range radar data as supervision. Our experiments demonstrate that PillarGen outperforms traditional point upsampling methods in quantitative and qualitative measures. We also confirm that when PillarGen is incorporated into bird’s eye view object detection, a significant improvement in detection accuracy is achieved. keywords: {Point cloud compression;Accuracy;Focusing;Radar;Object detection;Predictive models;Encoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611144&isnumber=10609862

Z. Qiu, J. Yang and J. Wang, "MMA-Net: Multiple Morphology-Aware Network for Automated Cobb Angle Measurement," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9139-9145, doi: 10.1109/ICRA57147.2024.10610928.Abstract: Scoliosis diagnosis and assessment depend largely on the measurement of the Cobb angle in spine X-ray images. With the emergence of deep learning techniques that employ landmark detection, tilt prediction, and spine segmentation, automated Cobb angle measurement has become increasingly popular. However, these methods encounter difficulties such as high noise sensitivity, intricate computational procedures, and exclusive reliance on a single type of morphological information. In this paper, we introduce the Multiple Morphology-Aware Network (MMA-Net), a novel framework that improves Cobb angle measurement accuracy by integrating multiple spine morphology as attention information. In the MMA-Net, we first feed spine X-ray images into the segmentation network to produce multiple morphological information (spine region, centerline, and boundary) and then concatenate the original X-ray image with the resulting segmentation maps as input for the regression module to perform precise Cobb angle measurement. Furthermore, we devise joint loss functions for our segmentation and regression network training, respectively. We evaluate our method on the AASCE challenge dataset and achieve superior performance with the SMAPE of 7.28% and the MAE of 3.18°, indicating a strong competitiveness compared to other outstanding methods. Consequently, we can offer clinicians automated, efficient, and reliable Cobb angle measurement. keywords: {Training;Image segmentation;Sensitivity;Scoliosis;Noise;Morphology;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610928&isnumber=10609862

A. Sielemann, S. Wolf, M. Roschani, J. Ziehn and J. Beyerer, "Synset Boulevard: A Synthetic Image Dataset for VMMR*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9146-9153, doi: 10.1109/ICRA57147.2024.10610650.Abstract: We present and discuss the Synset Boulevard dataset, designed for the task of surveillance-nature vehicle make and model recognition (VMMR)—to the best of our knowledge the first entirely synthetically generated large-scale VMMR image dataset. Through the simulation of image data rather than the manual annotation of real data, we intend to mitigate common challenges in state-of-the-art VMMR datasets, namely bias, human error, privacy, and the challenge of providing systematic updates. On the other hand, the provision and use of synthetic data introduce individual challenges, such as potential domain gaps and a less pronounced intra-class variance. Our approach to address these challenges, using path tracing and physically-based, data-driven models, is evaluated on an existing large real-world dataset. Overall, our synthetic dataset contains 32 400 independent images (each with different imaging simulations and with/without masked license plates, leading to a total of 259 200 images) from 162 different vehicle models of 43 makes depicted in front view. It is split into 8 sub-datasets to investigate the influence of optical/imaging effects on the classification ability. keywords: {Integrated optics;Data privacy;Systematics;Image recognition;Manuals;Optical imaging;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610650&isnumber=10609862

R. Chitnis et al., "IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9154-9160, doi: 10.1109/ICRA57147.2024.10611711.Abstract: Model-based reinforcement learning (RL) has shown great promise due to its sample efficiency, but still struggles with long-horizon sparse-reward tasks, especially in offline settings where the agent learns from a fixed dataset. We hypothesize that model-based RL agents struggle in these environments due to a lack of long-term planning capabilities, and that planning in a temporally abstract model of the environment can alleviate this issue. In this paper, we make two key contributions: 1) we introduce an offline model-based RL algorithm, IQL-TD-MPC, that extends the state- of-the-art Temporal Difference Learning for Model Predictive Control (TD-MPC) with Implicit Q-Learning (IQL); and 2) we propose to use IQL-TD-MPC as a Manager in a hierarchical setting with any off-the-shelf offline RL algorithm as a Worker. More specifically, we pre-train a temporally abstract IQL-TD-MPC Manager to predict "intent embeddings", which roughly correspond to subgoals, via planning. We show that augmenting state representations with intent embeddings generated by an IQL-TD-MPC manager significantly improves off-the-shelf offline RL agents' performance on some of the most challenging D4RL benchmark tasks. For instance, the offline RL algorithms AWAC, TD3-BC, DT, and CQL all get zero or near-zero normalized evaluation scores on the medium and large antmaze tasks, while our modification gives an average score over 40. keywords: {Q-learning;Temporal difference learning;Benchmark testing;Prediction algorithms;Planning;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611711&isnumber=10609862

D. Emukpere, B. Wu, J. Perez and J. -M. Renders, "SLIM: Skill Learning with Multiple Critics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9161-9167, doi: 10.1109/ICRA57147.2024.10610691.Abstract: Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful and safe manipulation behaviors. Furthermore, tackling this by augmenting skill discovery rewards with additional rewards through a naive combination might fail to produce desired behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, significantly surpassing baseline approaches for skill discovery. keywords: {Navigation;Reinforcement learning;Interference;Motors;Planning;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610691&isnumber=10609862

J. Zhang, K. Pertsch, J. Zhang and J. J. Lim, "SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9168-9175, doi: 10.1109/ICRA57147.2024.10610606.Abstract: Pre-training robots with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining with offline reinforcement learning. As a result, SPRINT pre-training equips robots with a richer repertoire of skills that can help an agent generalize to new tasks. Experiments in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/sprint. keywords: {Annotations;Large language models;Natural languages;Reinforcement learning;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610606&isnumber=10609862

J. Zheng and Y. Song, "Effective Representation Learning is More Effective in Reinforcement Learning than You Think," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9176-9182, doi: 10.1109/ICRA57147.2024.10611330.Abstract: In reinforcement learning (RL), learning directly from pixels, is commonly known as vision-based RL. Effective state representations are crucial for high performance in vision-based RL. However, in order to learn effective state representations, most current vision-based RL methods based on contrastive unsupervised learning use auxiliary tasks similar to those in computer vision, which does not guarantee the effective information interaction between representation learning and RL. To learn more efficient states, we propose a simple and effective vision-based RL method. It leverages the representations acquired through contrastive learning by the Teacher Encoder and the Student Encoder to collaboratively estimate the Q-function. This cooperative process utilizes the TD error to steer updates to the Teacher Encoder, thereby ensuring effective information exchange between representation learning and RL. We refer to this approach as Reinforcement Learning with Teacher-Student Collaboration (RLTSC). RLTSC incorporates recent advancements in contrastive unsupervised learning, endowing it with potent representation learning capabilities. It provides a robust estimate of the Q-function with minimal variance and effectively guides the Teacher Ecoder to update and acquire a more efficient representation. RLTSC substantially enhances data efficiency in vision-based RL, surpassing state-of-the-art methods on various continuous and discrete control benchmarks. Remarkably, RLTSC even outperforms RL methods based on physical state features in terms of data efficiency for continuous control benchmarks. This may enlighten us: effective representation learning is more effective in reinforcement learning than you think! keywords: {Representation learning;Visualization;Estimation;Reinforcement learning;Contrastive learning;Benchmark testing;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611330&isnumber=10609862

C. Zhang et al., "Learning Highly Dynamic Behaviors for Quadrupedal Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9183-9189, doi: 10.1109/ICRA57147.2024.10610440.Abstract: Learning highly dynamic behaviors for robots has been a longstanding challenge. Traditional approaches have demonstrated robust locomotion, but the exhibited behaviors lack diversity and agility. They employ approximate models, which lead to compromises in performance. Data-driven approaches have been shown to reproduce agile behaviors of animals, but typically have not been able to learn highly dynamic behaviors. In this paper, we propose a learning-based approach to enable robots to learn highly dynamic behaviors from animal motion data. The learned controller is deployed on a quadrupedal robot and the results show that the controller is able to reproduce highly dynamic behaviors including sprinting, jumping and sharp turning. Various behaviors can be activated through human interaction using a stick with markers attached to it. Based on the motion pattern of the stick, the robot exhibits walking, running, sitting and jumping, much like the way humans interact with a pet. keywords: {Legged locomotion;Target tracking;Animals;Dynamics;Force;Turning;Angular velocity},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610440&isnumber=10609862

J. Yamada, M. Rigter, J. Collins and I. Posner, "TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9190-9196, doi: 10.1109/ICRA57147.2024.10610450.Abstract: Model-based RL is a promising approach for real-world robotics due to its improved sample efficiency and generalization capabilities compared to model-free RL. However, effective model-based RL solutions for vision-based real-world applications require bridging the sim-to-real gap for any world model learnt. Due to its significant computational cost, standard domain randomisation does not provide an effective solution to this problem. This paper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real Transfer) to achieve efficient sim-to-real transfer of vision-based model-based RL using distillation. Specifically, TWIST leverages state observations as readily accessible, privileged information commonly garnered from a simulator to significantly accelerate sim-to-real transfer. Specifically, a teacher world model is trained efficiently on state information. At the same time, a matching dataset is collected of domain-randomised image observations. The teacher world model then supervises a student world model that takes the domain-randomised image observations as input. By distilling the learned latent dynamics model from the teacher to the student model, TWIST achieves efficient and effective sim-to-real transfer for vision-based model-based RL tasks. Experiments in simulated and real robotics tasks demonstrate that our approach outperforms naive domain randomisation and model-free methods in terms of sample efficiency and task performance of sim-to-real transfer. keywords: {Computational modeling;Computational efficiency;Task analysis;Robots;Standards},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610450&isnumber=10609862

A. Bajcsy, A. Loquercio, A. Kumar and J. Malik, "Learning Vision-based Pursuit-Evasion Robot Policies," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9197-9204, doi: 10.1109/ICRA57147.2024.10610498.Abstract: Learning strategic robot behavior—like that required in pursuit-evasion interactions—under real-world constraints is extremely challenging. It requires exploiting the dynamics of the interaction, and planning through both physical state and latent intent uncertainty. In this paper, we transform this intractable problem into a supervised learning problem, where a fully-observable robot policy generates supervision for a partially-observable one. We find that the quality of the supervision signal for the partially-observable pursuer policy depends on two key factors: the balance of diversity and optimality of the evader’s behavior, and the strength of the modeling assumptions in the fully-observable policy. We deploy our policy on a physical quadruped robot with an RGB-D camera on pursuit-evasion interactions in the wild. Despite all the challenges, the sensing constraints bring about creativity: the robot is pushed to gather information when uncertain, predict intent from noisy measurements, and anticipate in order to intercept. keywords: {Uncertainty;Robot vision systems;Supervised learning;Measurement uncertainty;Transforms;Cameras;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610498&isnumber=10609862

K. Lu, J. -X. Zhong, B. Yang, B. Wang and A. Markham, "Learning to Catch Reactive Objects with a Behavior Predictor," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9205-9211, doi: 10.1109/ICRA57147.2024.10611106.Abstract: Tracking and catching moving objects is an important ability for robots in a dynamic world. Whilst some objects have highly predictable state evolution e.g., the ballistic trajectory of a tennis ball, reactive targets alter their behavior in response to motion of the manipulator. Reactive applications range from gently capturing living animals such as snakes or fish for biological investigations, to smoothly interacting with and assisting a person. Existing works for dynamic catching usually perform target prediction followed by planning, but seldom account for highly non-linear reactive behaviors. Alternatively, Reinforcement Learning (RL) based methods simply treat the target and its motion as part of the observation of the world-state, but perform poorly due to the weak reward signal. In this work, we blend the approach of an explicit, yet learned, target state predictor with RL. We further show how a tightly coupled predictor which ‘observes’ the state of the robot leads to significantly improved anticipatory action, especially with targets that seek to evade the robot following a simple policy. Experiments show that our method achieves an 86.4% (open plane area) and a 73.8% (room) success rate on evasive objects, outperforming monolithic reinforcement learning and other techniques. We also demonstrate the efficacy of our approach across varied targets and trajectories. All code, data, and additional videos are at this GitHub link: https://kl-research.github.io/dyncatch. keywords: {Target tracking;Sports equipment;Dynamics;Reinforcement learning;Trajectory;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611106&isnumber=10609862

H. Bui and M. Posa, "Enhancing Task Performance of Learned Simplified Models via Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9212-9219, doi: 10.1109/ICRA57147.2024.10611461.Abstract: In contact-rich tasks, the hybrid, multi-modal nature of contact dynamics poses great challenges in model representation, planning, and control. Recent efforts have attempted to address these challenges via data-driven methods, learning dynamical models in combination with model predictive control. Those methods, while effective, rely solely on minimizing forward prediction errors to hope for better task performance with MPC controllers. This weak correlation can result in data inefficiency as well as limitations to overall performance. In response, we propose a novel strategy: using a policy gradient algorithm to find a simplified dynamics model that explicitly maximizes task performance. Specifically, we parameterize the stochastic policy as the perturbed output of the MPC controller, thus, the learned model representation can directly associate with the policy or task performance. We apply the proposed method to contact-rich tasks where a three-fingered robotic hand manipulates previously unknown objects. Our method significantly enhances task success rate by up to 15% in manipulating diverse objects compared to the existing method while sustaining data efficiency. Our method can solve some tasks with success rates of 70% or higher using under 30 minutes of data. All videos and codes are available at https://sites.google.com/view/lcs-rl. keywords: {System dynamics;Heuristic algorithms;Transfer learning;Stochastic processes;Reinforcement learning;Predictive models;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611461&isnumber=10609862

H. Qian et al., "Leveraging the efficiency of multi-task robot manipulation via task-evoked planner and reinforcement learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9220-9226, doi: 10.1109/ICRA57147.2024.10611076.Abstract: Multi-task learning has expanded the boundaries of robotic manipulation, enabling the execution of increasingly complex tasks. However, policies learned through reinforcement learning exhibit limited generalization and narrow distributions, which restrict their effectiveness in multi-task training. Addressing the challenge of obtaining policies with generalization and stability represents a non-trivial problem. To tackle this issue, we propose a planning-guided reinforcement learning method. It leverages a task-evoked planner(TEP) and a reinforcement learning approach with planner’s guidance. TEP utilizes reusable samples as the source, with the aim of learning reachability information across different task scenarios. Then in reinforcement learning, TEP assesses and guides the Actor towards better outputs and smoothly enhances the performance in multi-task benchmarks. We evaluate this approach within the Meta-World framework and compare it with prior works in terms of learning efficiency and effectiveness. Depending on experimental results, our method has more efficiency, higher success rates, and demonstrates more realistic behavior. keywords: {Training;Reinforcement learning;Benchmark testing;Multitasking;Planning;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611076&isnumber=10609862

H. Lin, R. Corcodel and D. Zhao, "Generalize by Touching: Tactile Ensemble Skill Transfer for Robotic Furniture Assembly," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9227-9233, doi: 10.1109/ICRA57147.2024.10610567.Abstract: Furniture assembly remains an unsolved problem in robotic manipulation due to its long task horizon and nongeneralizable operations plan. This paper presents the Tactile Ensemble Skill Transfer (TEST) framework, a pioneering offline reinforcement learning (RL) approach that incorporates tactile feedback in the control loop. TEST’s core design is to learn a skill transition model for high-level planning, along with a set of adaptive intra-skill goal-reaching policies. Such design aims to solve the robotic furniture assembly problem in a more generalizable way, facilitating seamless chaining of skills for this long-horizon task. We first sample demonstration from a set of heuristic policies and trajectories consisting of a set of randomized sub-skill segments, enabling the acquisition of rich robot trajectories that capture skill stages, robot states, visual indicators, and crucially, tactile signals. Leveraging these trajectories, our offline RL method discerns skill termination conditions and coordinates skill transitions. Our evaluations highlight the proficiency of TEST on the in-distribution furniture assemblies, its adaptability to unseen furniture configurations, and its robustness against visual disturbances. Ablation studies further accentuate the pivotal role of two algorithmic components: the skill transition model and tactile ensemble policies. Results indicate that TEST can achieve a success rate of 90% and is over 4 times more efficient than the heuristic policy in both in-distribution and generalization settings, suggesting a scalable skill transfer approach for contact-rich manipulation. keywords: {Robotic assembly;Visualization;Adaptation models;Robot kinematics;Decision making;Tactile sensors;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610567&isnumber=10609862

E. Su et al., "Sim2Real Manipulation on Unknown Objects with Tactile-based Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9234-9241, doi: 10.1109/ICRA57147.2024.10611113.Abstract: Using tactile sensors for manipulation remains one of the most challenging problems in robotics. At the heart of these challenges is generalization: How can we train a tactile-based policy that can manipulate unseen and diverse objects? In this paper, we propose to perform Reinforcement Learning with only visual tactile sensing inputs on diverse objects in a physical simulator. By training with diverse objects in simulation, it enables the policy to generalize to unseen objects. However, leveraging simulation introduces the Sim2Real transfer problem. To mitigate this problem, we study different tactile representations and evaluate how each affects real-robot manipulation results after transfer. We conduct our experiments on diverse real-world objects and show significant improvements over baselines. Our project page is available at https://tactilerl.github.io/. keywords: {Training;Heart;Visualization;Tactile sensors;Reinforcement learning;Sensors;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611113&isnumber=10609862

W. Li et al., "Synchronized Dual-arm Rearrangement via Cooperative mTSP," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9242-9248, doi: 10.1109/ICRA57147.2024.10610424.Abstract: Synchronized dual-arm rearrangement is widely studied as a common scenario in industrial applications. It often faces scalability challenges due to the computational complexity of robotic arm rearrangement and the high-dimensional nature of dual-arm planning. To address these challenges, we formulated the problem as cooperative mTSP, a variant of mTSP where agents share cooperative costs, and utilized reinforcement learning for its solution. Our approach involved representing rearrangement tasks using a task state graph that captured spatial relationships and a cooperative cost matrix that provided details about action costs. Taking these representations as observations, we designed an attention-based network to effectively combine them and provide rational task scheduling. Furthermore, a cost predictor is also introduced to directly evaluate actions during both training and planning, significantly expediting the planning process. Our experimental results demonstrate that our approach outperforms existing methods in terms of both performance and planning efficiency. keywords: {Training;Costs;Service robots;Scalability;Reinforcement learning;Manipulators;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610424&isnumber=10609862

J. Yang, C. Deng, J. Wu, R. Antonova, L. Guibas and J. Bohg, "EquivAct: SIM(3)-Equivariant Visuomotor Policies beyond Rigid Object Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9249-9255, doi: 10.1109/ICRA57147.2024.10611491.Abstract: If a robot masters folding a kitchen towel, we would expect it to master folding a large beach towel. However, existing policy learning methods that rely on data augmentation still don’t guarantee such generalization. Our insight is to add equivariance to both the visual object representation and policy architecture. We propose EquivAct which utilizes SIM(3)-equivariant network structures that guarantee generalization across all possible object translations, 3D rotations, and scales by construction. EquivAct is trained in two phases. We first pre-train a SIM(3)-equivariant visual representation on simulated scene point clouds. Then, we learn a SIM(3)-equivariant visuomotor policy using a small amount of source task demonstrations. We show that the learned policy directly transfers to objects that substantially differ from demonstrations in scale, position, and orientation. We evaluate our method in three manipulation tasks involving deformable and articulated objects, going beyond typical rigid object manipulation tasks considered in prior work. We conduct experiments both in simulation and in reality. For real robot experiments, our method uses 20 human demonstrations of a tabletop task and transfers zero-shot to a mobile manipulation task in a much larger setup. Experiments confirm that our contrastive pre-training procedure and equivariant architecture offer significant improvements over prior work. Project website: equivact.github.io keywords: {Learning systems;Point cloud compression;Visualization;Three-dimensional displays;Data augmentation;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611491&isnumber=10609862

J. Liu, M. Stamatopoulou and D. Kanoulas, "DiPPeR: Diffusion-based 2D Path Planner applied on Legged Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9264-9270, doi: 10.1109/ICRA57147.2024.10610013.Abstract: In this work, we present DiPPeR, a novel and fast 2D path planning framework for quadrupedal locomotion, leveraging diffusion-driven techniques. Our contributions include a scalable dataset generator for map images and corresponding trajectories, an image-conditioned diffusion planner for mobile robots, and a training/inference pipeline employing CNNs. We validate our approach in several mazes, as well as in real-world deployment scenarios on Boston Dynamic’s Spot and Unitree’s Go1 robots. DiPPeR performs on average 23 times faster for trajectory generation against both search based and data driven path planning algorithms with an average of 87% consistency in producing feasible paths of various length in maps of variable size, and obstacle structure. Website: https://rpl-cs-ucl.github.io/DiPPeR/ keywords: {Training;Legged locomotion;Heuristic algorithms;Pipelines;Network architecture;Transformers;Generators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610013&isnumber=10609862

D. Keren, A. Shahar and R. Poranne, "Efficient Polynomial Sum-Of-Squares Programming for Planar Robotic Arms," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9271-9271, doi: 10.1109/ICRA57147.2024.10611508.Abstract: Collision-avoiding motion planning for articulated robotic arms is one of the major challenges in robotics. The difficulty of the problem arises from its high dimensionality and the intricate geometry of the feasible space. Our goal is to seek large convex domains in configuration space, which contain no obstacles. In these domains, simple linear trajectories are guaranteed to be collision free, and can be leveraged for further optimization. To find such domains, practitioners have harnessed a methodology known as Sum-Of-Squares (SOS) Programming. SOS programs, however, are notorious for their poor scaling properties, which makes it challenging to employ them for complex problems. In this paper, we explore a simple formulation for a two-dimensional arm, which results in smaller SOS programs than previous suggested ones. We show that this formulation can express a variety of scenarios in a unified manner. keywords: {Geometry;Shape;Programming;Manipulators;Polynomials;Trajectory;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611508&isnumber=10609862

W. Yu, J. Peng, Q. Qiu, H. Wang, L. Zhang and J. Ji, "PathRL: An End-to-End Path Generation Method for Collision Avoidance via Deep Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9278-9284, doi: 10.1109/ICRA57147.2024.10610107.Abstract: Robot navigation using deep reinforcement learning (DRL) has shown great potential in improving the performance of mobile robots. Nevertheless, most existing DRL-based navigation methods primarily focus on training a policy that directly commands the robot with low-level controls, like linear and angular velocities, which leads to unstable speeds and unsmooth trajectories of the robot during the long-term execution. An alternative method is to train a DRL policy that outputs the navigation path directly. Then the robot can follow the generated path smoothly using sophisticated velocity-planning and path-following controllers, whose parameters are specified according to the hardware platform. However, two roadblocks arise for training a DRL policy that outputs paths: (1) The action space for potential paths often involves higher dimensions comparing to low-level commands, which increases the difficulties of training; (2) It takes multiple time steps to track a path instead of a single time step, which requires the path to predicate the interactions of the robot w.r.t. the dynamic environment in multiple time steps. This, in turn, amplifies the challenges associated with training. In response to these challenges, we propose PathRL, a novel DRL method that trains the policy to generate the navigation path for the robot. Specifically, we employ specific action space discretization techniques and tailored state space representation methods to address the associated challenges. Curriculum learning is employed to expedite the training process, while the reward function also takes into account the smooth transition between adjacent paths. In our experiments, PathRL achieves better success rates and reduces angular rotation variability compared to other DRL navigation methods, facilitating stable and smooth robot movement. We demonstrate the competitive edge of PathRL in both real-world scenarios and multiple challenging simulation environments. keywords: {Training;Navigation;Robot kinematics;Aerospace electronics;Deep reinforcement learning;Trajectory;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610107&isnumber=10609862

L. Paparusso, S. Kousik, E. Schmerling, F. Braghin and M. Pavone, "ZAPP! Zonotope Agreement of Prediction and Planning for Continuous-Time Collision Avoidance with Discrete-Time Dynamics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9285-9292, doi: 10.1109/ICRA57147.2024.10610953.Abstract: The past few years have seen immense progress on two fronts that are critical to safe, widespread mobile robot deployment: predicting uncertain motion of multiple agents, and planning robot motion under uncertainty. However, the numerical methods required on each front have resulted in a mismatch of representation for prediction and planning. In prediction, numerical tractability is usually achieved by coarsely discretizing time, and by representing multimodal multi-agent interactions as distributions with infinite support. On the other hand, safe planning typically requires very fine time discretization, paired with distributions with compact support, to reduce conservativeness and ensure numerical tractability. The result is, when existing predictors are coupled with planning and control, one may often find unsafe motion plans. This paper proposes ZAPP (Zonotope Agreement of Prediction and Planning) to resolve the representation mismatch. ZAPP unites a prediction-friendly coarse time discretization and a planning-friendly zonotope uncertainty representation; the method also enables differentiating through a zonotope collision check, allowing one to integrate prediction and planning within a gradient-based optimization framework. Numerical examples show how ZAPP can produce safer trajectories compared to baselines in interactive scenes. keywords: {Robot motion;Uncertainty;Dynamics;Hardware;Planning;Safety;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610953&isnumber=10609862

A. Amice, P. Werner and R. Tedrake, "Certifying Bimanual RRT Motion Plans in a Second," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9293-9299, doi: 10.1109/ICRA57147.2024.10611296.Abstract: We present an efficient method for certifying non-collision for piecewise-polynomial motion plans in algebraic reparametrizations of configuration space. Such motion plans include those generated by popular randomized methods including RRTs and PRMs, as well as those generated by many methods in trajectory optimization. Based on Sums-of-Squares optimization, our method provides exact, rigorous certificates of non-collision; it can never falsely claim that a motion plan containing collisions is collision-free. We demonstrate that our formulation is practical for real world deployment, certifying the safety of a twelve degree of freedom motion plan in just over a second. Moreover, the method is capable of discriminating the safety or lack thereof of two motion plans which differ by only millimeters. keywords: {Safety;Robotics and automation;Trajectory optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611296&isnumber=10609862

Y. Yin, Z. Sun, P. Ruan, F. Duan, R. Li and C. Zhu, "Cross View Capture for Distributed Image Compression with Decoder Side Information," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9300-9305, doi: 10.1109/ICRA57147.2024.10611242.Abstract: Image compression is increasingly important in applications like intelligent driving and smart surveillance systems. This study presents a novel cross view capture distributed image compression network (CVCDIC) to improve the compression quality by using decoder side information. The CVCDIC’s decoder utilizes feature extraction networks to extract features from both the primary image and the side information. Furthermore, a multi-level cross view attention module is designed to capture interrelated details between images at multiple hierarchical levels. Finally, a spatial refinement module, constructed on the foundation of information distillation networks, is designed to further refine the quality of reconstructed images. The results show that CVCDIC can achieve an MS-SSIM of 0.978 at 0.15 bpp, surpassing DSIN (0.925), NDIC (0.956), and ATN (0.955) on the KITTI Stereo dataset. keywords: {Measurement;Image coding;Fuses;Surveillance;Rate-distortion;Feature extraction;Decoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611242&isnumber=10609862

X. Huang, G. Sóti, C. Ledermann, B. Hein and T. Kröger, "Planning with Learned Subgoals Selected by Temporal Information," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9306-9312, doi: 10.1109/ICRA57147.2024.10610538.Abstract: Path planning in a changing environment is a challenging task in robotics, as moving objects impose time-dependent constraints. Recent planning methods primarily focus on the spatial aspects, lacking the capability to directly incorporate time constraints. In this paper, we propose a method that leverages a generative model to decompose a complex planning problem into small manageable ones by incrementally generating subgoals given the current planning context. Then, we take into account the temporal information and use learned time estimators based on different statistic distributions to examine and select the generated subgoal candidates. Experiments show that planning from the current robot state to the selected subgoal can satisfy the given time-dependent constraints while being goal-oriented. keywords: {Estimation;Path planning;Planning;Time factors;Task analysis;Robots;Context modeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610538&isnumber=10609862

M. Alhaddad, K. Mironov, A. Staroverov and A. Panov, "Neural Potential Field for Obstacle-Aware Local Motion Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9313-9320, doi: 10.1109/ICRA57147.2024.10611635.Abstract: Model predictive control (MPC) may provide local motion planning for mobile robotic platforms. The challenging aspect is the analytic representation of collision cost for the case when both the obstacle map and robot footprint are arbitrary. We propose a Neural Potential Field: a neural network model that returns a differentiable collision cost based on robot pose, obstacle map, and robot footprint. The differentiability of our model allows its usage within the MPC solver. It is computationally hard to solve problems with a very high number of parameters. Therefore, our architecture includes neural image encoders, which transform obstacle maps and robot footprints into embeddings, which reduce problem dimensionality by two orders of magnitude. The reference data for network training are generated based on algorithmic calculation of a signed distance function. Comparative experiments showed that the proposed approach is comparable with existing local planners: it provides trajectories with outperforming smoothness, comparable path length, and safe distance from obstacles. keywords: {Costs;Trajectory planning;Computational modeling;Computer architecture;Predictive models;Vectors;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611635&isnumber=10609862

S. H. Arul, J. Jin Park, V. Prem, Y. Zhang and D. Manocha, "Unconstrained Model Predictive Control for Robot Navigation under Uncertainty," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9321-9327, doi: 10.1109/ICRA57147.2024.10610531.Abstract: In this paper, we present a probabilistic and unconstrained model predictive control formulation for robot navigation under uncertainty. We present (1) a closed-form approximation of the probability of collision that naturally models the propagation of uncertainty over the planning horizon and is computationally cheap to evaluate, and (2) a collision-cost formulation which provably preserves forward invariance (i.e., keeps the robot away from obstacles) when combined with the probability formulation. Notably, our formulation avoids hard constraints by construction, which in turn avoids abrupt transitions in robot behavior around the constraint boundaries ensuring graceful navigation. Further, we present proof for the forward invariance and the stability of the approach. We compare the efficacy of our method with the baseline [1], which the proposed approach builds on. We demonstrate that the approach results in confident and safe robot navigation in tight spaces by smoothly slowing down the robot in low survivability environments (e.g., tight corridors), but also allows it to move away from obstacles safely when needed. keywords: {Uncertainty;Navigation;Computational modeling;Aerospace electronics;Probabilistic logic;Stability analysis;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610531&isnumber=10609862

P. Luo, S. Yao, Y. Yue, J. Wang, H. Yan and M. Q. . -H. Meng, "Efficient RRT*-based Safety-Constrained Motion Planning for Continuum Robots in Dynamic Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9328-9334, doi: 10.1109/ICRA57147.2024.10610309.Abstract: Continuum robots, characterized by their high flexibility and infinite degrees of freedom (DoFs), have gained prominence in applications such as minimally invasive surgery and hazardous environment exploration. However, the intrinsic complexity of continuum robots requires a significant amount of time for their motion planning, posing a hurdle to their practical implementation. To tackle these challenges, efficient motion planning methods such as Rapidly Exploring Random Trees (RRT) and its variant, RRT*, have been employed. This paper introduces a unique RRT*-based motion control method tailored for continuum robots. Our approach embeds safety constraints derived from the robots’ posture states, facilitating autonomous navigation and obstacle avoidance in rapidly changing environments. Simulation results show efficient trajectory planning amidst multiple dynamic obstacles and provide a robust performance evaluation based on the generated postures. Finally, preliminary tests were conducted on a two-segment cable-driven continuum robot prototype, confirming the effectiveness of the proposed planning approach. This method is versatile and can be adapted and deployed for various types of continuum robots through parameter adjustments. keywords: {Performance evaluation;Minimally invasive surgery;Trajectory planning;Simulation;Dynamics;Prototypes;Planning;Soft robots;Safety-Constrained planning and control;Dynamic Environment;RRT*-based motion planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610309&isnumber=10609862

Y. Jiang et al., "Ultrafast capturing in-flight objects with reprogrammable working speed ranges," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9335-9341, doi: 10.1109/ICRA57147.2024.10610901.Abstract: In-flight high-speed object capturing is crucial in nature to improve survival and adaptation to the environment, such as the predation of frogs, leopards, and eagles. Despite its ubiquitousness in nature, capturing fast-moving objects is extremely challenging in engineering implementations. In this paper, we report an ultrafast gripper based on tunable bistable structures. Different from current designs which are only suitable for objects with certain speed ranges once the grippers are fabricated, the working range of object speed of the proposed gripper could be reprogrammed by controlling the sensitivity of the structures. We present the design and fabrication of the proposed gripper in detail. A theoretical model is introduced to construct the energy landscape of the structures and the force response of the gripper when programmed to different states. The results show that in the original state, the gripper is capable of capturing a flying table tennis ball with a high speed of 15 m/s in only 6 ms. When the proposed gripper is controlled to the ultra-sensitive state, a flying ball with only 1 m/s could also be captured. This work broadens the frontiers of in-flight capturing design, and we envision broader promising applications. keywords: {Fabrication;Sensitivity;Sports equipment;Force;Aerospace electronics;Grippers;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610901&isnumber=10609862

C. M. Sourkounis, D. S. G. Morales, T. Kwasnitschka and A. Raatz, "Hard Shell, Soft Core: Binary Actuators for Deep-Sea Applications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9355-9361, doi: 10.1109/ICRA57147.2024.10610349.Abstract: Deep-sea research represents invaluable opportunities to unravel hidden ecosystems, uncover unknown biodiversity, and provide critical insights into the Earth’s history and the impacts of climate change. Due to the extreme conditions, exploring the deep-sea traditionally requires costly equipment, such as specific diving robots, engineered to withstand the high pressure. Our research aims to reduce the costs of deep-sea sediment sampling by introducing a novel actuation system for suction samplers, that capitalises the advantages of soft material actuators. At first glance, soft material actuators may not appear suitable for the harsh conditions that prevail in the deep-sea, but when combined with a rigid, bistable mechanism there is great potential for improving the accessibility of sampling and research in this challenging environment. The binary actuation system that results from this combination, is modular, scalable, lightweight, and low cost in comparison to existing solutions. keywords: {Actuators;Climate change;Costs;Ecosystems;Sediments;History;Biodiversity},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610349&isnumber=10609862

O. G. Osele, K. Barhydt, N. Cerone, A. M. Okamura and H. Harry Asada, "Tip-Clutching Winch for High Tensile Force Application with Soft Growing Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9362-9368, doi: 10.1109/ICRA57147.2024.10610362.Abstract: The navigational abilities of tip-everting soft growing robots, known as vine robots, are compromised when tip-mount devices are added to enable carrying of payloads. We present a new method for securing a vine robot to objects or its environment that exploits the unique eversion-based growth mechanism and flexibility of vine robots, while keeping the tip of the vine robot free of encumbrance. Our implementation is a tip-clutching winch, into which vine robots can insert themselves and anchor to via powerful overlapping belt friction. The device enables passive, high-strength, and reversible fastening, and can easily release the vine robot. This approach enables carrying of loads of at least 28 kg (limited by the tensile strength of the vine robot body material and winch actuator torque capacity), as well as novel material transport and locomotion capabilities. keywords: {Actuators;Torque;Navigation;Friction;Force;Winches;Belts},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610362&isnumber=10609862

H. Nguyen, T. Kozuno, C. C. Beltran-Hernandez and M. Hamaya, "Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9369-9375, doi: 10.1109/ICRA57147.2024.10610103.Abstract: This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one. Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose. In contrast, we use a partially observable formulation and deep reinforcement learning from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals. Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space. Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry. Results in simulation with five different symmetric peg shapes show that our proposed agent can be comparable to or even outperform a state-based agent. In particular, the sample efficiency also allows us to learn directly on the real robot within 3 hours. keywords: {Wrist;Robotic assembly;Shape;Training data;Tactile sensors;Propioception;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610103&isnumber=10609862

S. Pilch, D. Klug and O. Sawodny, "Force Estimation at the Bionic Soft Arm’s Tool-center-point during the Interaction with the Environment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9376-9381, doi: 10.1109/ICRA57147.2024.10611006.Abstract: Soft continuum robots enable new application areas in contrast to standard rigid robots, such as interaction with a varying environment. Due to their compliant continuous structure, they are inherently safe and adaptive to environmental conditions. In this paper, the interaction with the environment is performed at the tool-center-point of a soft continuum manipulator and is realized by a hybrid force-position control. For this, a force estimation model is derived to substitute the force sensor at the tool-center-point. The force estimation is probabilistic and relies on normal distributions considering model parameters and deviations from model identification of the soft continuum robot. It also provides a qualitative measure for the contact estimation. This paper first presents the probabilistic force estimation model and then shows the hybrid force-position control using the presented model. From the results, it is concluded that force sensing is replaceable for the environment interaction. keywords: {Biological system modeling;Force;Estimation;Arms;Gaussian distribution;Robot sensing systems;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611006&isnumber=10609862

P. H. Johnson, K. Junge, C. Whitfield, J. Hughes and M. Calisti, "Field-evaluated Closed Structure Soft Gripper Enhances the Shelf Life of Harvested Blackberries," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9382-9388, doi: 10.1109/ICRA57147.2024.10610387.Abstract: Soft robotic grippers are intrinsically delicate while grasping objects, and can rely on mechanical deformation to adapt to different shapes without explicit control. These characteristics are particularly appealing for agriculture, where items of produce from the same crop can vary significantly in shape and size, and delicate harvesting is among the first concerns for fruit quality. Various soft robotic grippers have been proposed for harvesting different produce types, however their employment in field testing has been extremely limited. In this paper we developed the first closed structure soft gripper for the harvest of blackberries. We adapted an existing gripper concept, initially testing it on a sensorised raspberry physical twin. Then, followed grower-guided protocols to pick blackberries in farm polytunnels, and to evaluate the shelf life in comparison with berries picked by professional human pickers. Our results with ten experimental varieties showed a picking success rate of 95.4% demonstrating the capability of a closed structure gripper to adapt mechanically to fruit-shape variability. Moreover, a shelf life assessment on seven measured traits reported greatly improved shelf life of between 30 and 150%, across all traits for gripper harvested blackberries. Our study demonstrates the potential of soft grippers for delicate fruit harvesting, and indicates how to increase the impact of robotics in agriculture. keywords: {Pressure sensors;Protocols;Shape;Personal digital devices;Soft robotics;Robot sensing systems;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610387&isnumber=10609862

G. H. Nguyen, D. Beßler, S. Stelter, M. Pomarlan and M. Beetz, "Translating Universal Scene Descriptions into Knowledge Graphs for Robotic Environment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9389-9395, doi: 10.1109/ICRA57147.2024.10611691.Abstract: Robots performing human-scale manipulation tasks require an extensive amount of knowledge about their surroundings in order to perform their actions competently and human-like. In this work, we investigate the use of virtual reality technology as an implementation for robot environment modeling, and present a technique for translating scene graphs into knowledge bases. To this end, we take advantage of the Universal Scene Description (USD) format which is an emerging standard for the authoring, visualization and simulation of complex environments. We investigate the conversion of USD-based environment models into Knowledge Graph (KG) representations that facilitate semantic querying and integration with additional knowledge sources. The contributions of the paper are validated through an application scenario in the service robotics domain. keywords: {Solid modeling;Service robots;Semantics;Knowledge based systems;Knowledge graphs;Virtual reality;Tagging},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611691&isnumber=10609862

S. Fredriksson, A. Saradagi and G. Nikolakopoulos, "Robotic Exploration through Semantic Topometric Mapping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9404-9410, doi: 10.1109/ICRA57147.2024.10610585.Abstract: In this article, we introduce a novel strategy for robotic exploration in unknown environments using a semantic topometric map. As it will be presented, the semantic topometric map is generated by segmenting the grid map of the currently explored parts of the environment into regions, such as intersections, pathways, dead-ends, and unexplored frontiers, which constitute the structural semantics of an environment. The proposed exploration strategy leverages metric information of the frontier, such as distance and angle to the frontier, similar to existing frameworks, with the key difference being the additional utilization of structural semantic information, such as properties of the intersections leading to frontiers. The algorithm for generating semantic topometric mapping utilized by the proposed method is lightweight, resulting in the method’s online execution being both rapid and computationally efficient. Moreover, the proposed framework can be applied to both structured and unstructured indoor and outdoor environments, which enhances the versatility of the proposed exploration algorithm. We validate our exploration strategy and demonstrate the utility of structural semantics in exploration in two complex indoor environments by utilizing a Turtlebot3 as the robotic agent. Compared to traditional frontier-based methods, our findings indicate that the proposed approach leads to faster exploration and requires less computation time. keywords: {Measurement;Laser radar;Navigation;Semantics;Skeleton;Iron;Indoor environment},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610585&isnumber=10609862

K. Yamazaki et al., "Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9411-9417, doi: 10.1109/ICRA57147.2024.10610193.Abstract: Precise 3D environmental mapping with semantics is essential in robotics. Existing methods often rely on pre-defined concepts during training or are time-intensive when generating semantic maps. This paper presents Open-Fusion, an approach for real-time open-vocabulary 3D mapping and queryable scene representation using RGB-D data. Open-Fusion harnesses the power of a pretrained vision-language foundation model (VLFM) for open-set semantic comprehension and employs the Truncated Signed Distance Function (TSDF) for swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based embeddings and their associated confidence maps. These are then integrated with the 3D knowledge from TSDF using an enhanced Hungarian-based feature-matching mechanism. In particular, Open-Fusion delivers outstanding annotation-free 3D segmentation for open vocabulary query without the need for additional 3D training. Benchmark tests on the ScanNet dataset against leading zero-shot methods highlight Open-Fusion’s superiority. Furthermore, it seamlessly combines the strengths of region-based VLFM and TSDF, facilitating real-time 3D scene comprehension that includes object concepts and open-world semantics. We encourage the readers to view the demos on our project page: https://uark-aicv.github.io/OpenFusion keywords: {Training;Vocabulary;Three-dimensional displays;Semantics;Benchmark testing;Feature extraction;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610193&isnumber=10609862

