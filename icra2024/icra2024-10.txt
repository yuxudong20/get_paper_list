K. Yilmaz, J. Schult, A. Nekrasov and B. Leibe, "Mask4Former: Mask Transformer for 4D Panoptic Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9418-9425, doi: 10.1109/ICRA57147.2024.10610262.Abstract: Accurately perceiving and tracking instances over time is essential for the decision-making processes of autonomous agents interacting safely in dynamic environments. With this intention, we propose Mask4Former for the challenging task of 4D panoptic segmentation of LiDAR point clouds. Mask4Former is the first transformer-based approach unifying semantic instance segmentation and tracking of sparse and irregular sequences of 3D point clouds into a single joint model. Our model directly predicts semantic instances and their temporal associations without relying on hand-crafted non-learned association strategies such as probabilistic clustering or voting-based center prediction. Instead, Mask4Former introduces spatio-temporal instance queries that encode the semantic and geometric properties of each semantic tracklet in the sequence. In an in-depth study, we find that promoting spatially compact instance predictions is critical as spatiotemporal instance queries tend to merge multiple semantically similar instances, even if they are spatially distant. To this end, we regress 6-DOF bounding box parameters from spatiotemporal instance queries, which are used as an auxiliary task to foster spatially compact predictions. Mask4Former achieves a new state-of-the-art on the SemanticKITTI test set with a score of 68.4 LSTQ. keywords: {Point cloud compression;Solid modeling;Three-dimensional displays;Semantics;Transformers;Probabilistic logic;6-DOF},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610262&isnumber=10609862

J. Kang, B. Chen, P. Zhong, H. Yang, Y. Sheng and J. Wang, "HSPNav: Hierarchical Scene Prior Learning for Visual Semantic Navigation Towards Real Settings," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9434-9440, doi: 10.1109/ICRA57147.2024.10610061.Abstract: Visual Semantic Navigation (VSN) aims at navigating a robot to a given target object in a previously unseen scene. To tackle this task, the robot must learn a nimble navigation policy by utilizing spatial patterns and semantic co-occurrence relations among objects in the scene. Prevailing approaches extract scene priors from the instant visual observations and solidify them in neural episodic memory to achieve flexible navigation. However, due to the oblivion and underuse of the scene priors, these methods are plagued by repeated exploration, effective-knowledge sparsity, and wrong decisions. To alleviate these issues, we propose a novel VSN policy, HSPNav, based on Hierarchical Scene Priors (HSP) and Deep Reinforcement Learning (DRL). The HSP contains two components, i.e., the egocentric semantic map-based Local Scene Priors (LSP) and the commonsense relational graph-based Global Scene Priors (GSP). Then, efficient semantic navigation is achieved by employing an immediate LSP to retrieve conducive contextual memories from the GSP. By utilizing the MP3D dataset, the experimental results in the Habitat simulator demonstrate that our HSP brings a significant boost over the baselines. Furthermore, we take an essential step from simulation to reality by bridging the gap from Habitat to ROS. The migration evaluations show that HSPNav can generalize to realistic settings well and achieve promising performance. keywords: {Visualization;Navigation;Semantics;Habitats;Memory management;Deep reinforcement learning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610061&isnumber=10609862

M. A. V. Saucedo, A. Patel, A. Saradagi, C. Kanellakis and G. Nikolakopoulos, "Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9441-9447, doi: 10.1109/ICRA57147.2024.10611352.Abstract: In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information. We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant to a robotic mission. We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data. A novel Graph Convolutional Neural Network (GCN) model is developed, to learn CECI from a repository of 3D scene graphs. As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces. The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method. We establish the novel concept of Belief Scene Graphs (BSG), as a core component to integrate expectations into abstract representations. This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level reasoning for task planning and optimization of a variety of robotics missions. The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested in a real-life experiment to emulate human common sense of unseen-objects.For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/hsGlSCa12iY keywords: {Training;Solid modeling;Histograms;Three-dimensional displays;Computational modeling;Robot sensing systems;Search problems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611352&isnumber=10609862

C. R. Samuelson and J. G. Mangelson, "A Guided Gaussian-Dirichlet Random Field for Scientist-in-the-Loop Inference in Underwater Robotics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9448-9454, doi: 10.1109/ICRA57147.2024.10611290.Abstract: Visual topic modeling (VTM) provides key insight into data sets based on learned semantic topic models. The Gaussian-Dirichlet Random Field (GDRF), a state-of-the-art VTM technique, models these semantic topics in continuous space as densities. However, ambiguity in learned topics is a disadvantage of such Dirichlet-based VTM algorithms. We propose the Guided Gaussian-Dirichlet Random Field (GGDRF). Our method applies Dirichlet Forest priors from natural language processing (NLP) to the vision domain as a way to embed visual scientific knowledge into the estimation process. This modification and addition to the GDRF provides a key shift from unsupervised machine learning to semi-supervised machine learning in the robotic VTM domain. We show through simulation and real-world underwater data that the proposed GGDRF outperforms the previous GDRF method both quantitatively and qualitatively by improving alignment between estimated topics and scientific interests. keywords: {Visualization;Machine learning algorithms;Semantics;Estimation;Machine learning;Forestry;Natural language processing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611290&isnumber=10609862

J. Fei and Z. Deng, "Fine-Tuning Point Cloud Transformers with Dynamic Aggregation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9455-9462, doi: 10.1109/ICRA57147.2024.10610767.Abstract: Point clouds play an important role in 3D analysis, which has broad applications in robotics and autonomous driving. The pre-training fine-tuning paradigm has shown great potential in the point cloud domain. Full fine-tuning is generally effective but leads to a heavy storage and computational burden, which becomes inefficient and unacceptable as the size of pre-trained models scales. Although efficient fine-tuning approaches have significant progress in other domains, they generally perform worse for point clouds. To overcome this dilemma, we revisit the official Point-MAE implementation and find the critical role of aggregation in fine-tuning performances. Inspired by such discoveries, we propose a novel dynamic aggregation (DA) method to replace previous static aggregation like mean or max pooling for pre-trained point cloud Transformers. Besides standard metrics such as accuracy or mIoU, we evaluate the number of tunable parameters and additional FLOPs for a fair comparison of our method to different fine-tuning approaches. We construct several DA variants and validate them through extensive experiments. Experimental results demonstrate that DA has competitive performances against full fine-tuning and other efficient fine-tuning approaches. The code is publicly available at https://github.com/JaronTHU/DynamicAggregation. keywords: {Point cloud compression;Measurement;Three-dimensional displays;Codes;Computational modeling;Aggregates;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610767&isnumber=10609862

H. Cao, Y. Xu, J. Yang, P. Yin, S. Yuan and L. Xie, "MoPA: Multi-Modal Prior Aided Domain Adaptation for 3D Semantic Segmentation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9463-9470, doi: 10.1109/ICRA57147.2024.10610316.Abstract: Multi-modal unsupervised domain adaptation (MM-UDA) for 3D semantic segmentation is a practical solution to embed semantic understanding in autonomous systems without expensive point-wise annotations. While previous MM-UDA methods can achieve overall improvement, they suffer from significant class-imbalanced performance, restricting their adoption in real applications. This imbalanced performance is mainly caused by: 1) self-training with imbalanced data and 2) the lack of pixel-wise 2D supervision signals. In this work, we propose Multi-modal Prior Aided (MoPA) domain adaptation to improve the performance of rare objects. Specifically, we develop Valid Ground-based Insertion (VGI) to rectify the imbalance supervision signals by inserting prior rare objects collected from the wild while avoiding introducing artificial artifacts that lead to trivial solutions. Meanwhile, our SAM consistency loss leverages the 2D prior semantic masks from SAM as pixel-wise supervision signals to encourage consistent predictions for each object in the semantic mask. The knowledge learned from modal-specific prior is then shared across modalities to achieve better rare object segmentation. Extensive experiments show that our method achieves state-of-the-art performance on the challenging MM-UDA benchmark. Code will be available at https://github.com/AronCao49/MoPA. keywords: {Three-dimensional displays;Codes;Accuracy;Autonomous systems;Annotations;Semantic segmentation;Semantics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610316&isnumber=10609862

R. Zhu, T. Dai and O. Celiktutan, "Cross Domain Policy Transfer with Effect Cycle-Consistency," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9471-9477, doi: 10.1109/ICRA57147.2024.10611110.Abstract: Training a robotic policy from scratch using deep reinforcement learning methods can be prohibitively expensive due to sample inefficiency. To address this challenge, transferring policies trained in the source domain to the target domain becomes an attractive paradigm. Previous research has typically focused on domains with similar state and action spaces but differing in other aspects. In this paper, our primary focus lies in domains with different state and action spaces, which has broader practical implications, i.e. transfer the policy from robot A to robot B. Unlike prior methods that rely on paired data, we propose a novel approach for learning the mapping functions between state and action spaces across domains using unpaired data. We propose effect cycle-consistency, which aligns the effects of transitions across two domains through a symmetrical optimization structure for learning these mapping functions. Once the mapping functions are learned, we can seamlessly transfer the policy from the source domain to the target domain. Our approach has been tested on three locomotion tasks and two robotic manipulation tasks. The empirical results demonstrate that our method can reduce alignment errors significantly and achieve better performance compared to the state-of-the-art method. Project page: https://ricky-zhu.github.io/effect_cycle_consistency. keywords: {Training;Deep reinforcement learning;Task analysis;Robots;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611110&isnumber=10609862

H. Sun, Y. Wang, W. Chen, H. Deng and D. Li, "Parameter-efficient Prompt Learning for 3D Point Cloud Understanding," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9478-9486, doi: 10.1109/ICRA57147.2024.10610093.Abstract: This paper presents a parameter-efficient prompt tuning method, named PPT, to adapt a large multi-modal model for 3D point cloud understanding. Existing strategies are quite expensive in computation and storage, and depend on timeconsuming prompt engineering. We address the problems from three aspects. Firstly, a PromptLearner module is devised to replace hand-crafted prompts with learnable contexts to automate the prompt tuning process. Then, we lock the pre-trained backbone instead of adopting the full fine-tuning paradigm to substantially improve the parameter efficiency. Finally, a lightweight PointAdapter module is arranged near target tasks to enhance prompt tuning for 3D point cloud understanding. Comprehensive experiments are conducted to demonstrate the superior parameter and data efficiency of the proposed method. Meanwhile, we obtain new records on 4 public datasets and multiple 3D tasks, i.e., point cloud recognition, few-shot learning, and part segmentation. The implementation is available at https://github.com/auniquesun/PPT. keywords: {Point cloud compression;Solid modeling;Adaptation models;Three-dimensional displays;Prompt engineering;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610093&isnumber=10609862

J. Liu et al., "BEVUDA: Multi-geometric Space Alignments for Domain Adaptive BEV 3D Object Detection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9487-9494, doi: 10.1109/ICRA57147.2024.10610096.Abstract: Vision-centric bird-eye-view (BEV) perception has shown promising potential in autonomous driving. Recent works mainly focus on improving efficiency or accuracy but neglect the challenges when facing environment changing, resulting in severe degradation of transfer performance. For BEV perception, we figure out the significant domain gaps existing in typical real-world cross-domain scenarios and comprehensively solve the Domain Adaption (DA) problem for multi-view 3D object detection. Since BEV perception approaches are complicated and contain several components, the domain shift accumulation on multiple geometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In this paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework to ease the domain shift accumulation, which consists of a Depth-Aware Teacher (DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines target lidar and reliable depth prediction to construct depth-aware information, extracting target domain-specific knowledge in Voxel and BEV feature spaces. It then transfers the sufficient domain knowledge of multiple spaces to the student model. In order to jointly alleviate the domain shift, GAS projects multi-geometric space features to a shared geometric embedding space and decreases data distribution distance between two domains. To verify the effectiveness of our method, we conduct BEV 3D object detection experiments on three cross-domain scenarios and achieve state-of-the-art performance. Code: https://github.com/liujiaming1996/BEVUDA. keywords: {Degradation;Three-dimensional displays;Laser radar;Codes;Object detection;Feature extraction;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610096&isnumber=10609862

G. Sóti, X. Huang, C. Wurll and B. Hein, "6-DoF Grasp Pose Evaluation and Optimization via Transfer Learning from NeRFs," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9495-9501, doi: 10.1109/ICRA57147.2024.10610402.Abstract: We address the problem of robotic grasping of known and unknown objects using implicit behavior cloning. We train a grasp evaluation model from a small number of demonstrations that outputs higher values for grasp candidates that are more likely to succeed in grasping. This evaluation model serves as an objective function, that we maximize to identify successful grasps. Key to our approach is the utilization of learned implicit representations of visual and geometric features derived from a pre-trained NeRF. Though trained exclusively in a simulated environment with simplified objects and 4-DoF topdown grasps, our evaluation model and optimization procedure demonstrate generalization to 6-DoF grasps and novel objects both in simulation and in real-world settings, without the need for additional data. Supplementary material is available at: https://gergely-soti.github.io/grasp keywords: {Training;Visualization;Evaluation models;Transfer learning;Grasping;Neural radiance field;6-DOF},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610402&isnumber=10609862

K. Wu, X. He, Y. Wang and X. Liu, "Multi-Level Progressive Reinforcement Learning for Control Policy in Physical Simulations," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9502-9508, doi: 10.1109/ICRA57147.2024.10610992.Abstract: Training model-free intelligent agents in complex real-world scenarios using reinforcement learning (RL) often necessitates simulation-based environments due to high physical expenses. However, when simulation takes a long time, e.g., in an unsteady 3D fluid simulation with interactions to the controllable solids, existing RL algorithms meet difficulty to accomplish training within a reasonable timeframes. In this paper, we propose a novel multi-level framework for RL to accelerate convergence as the first attempt to address this difficulty. Motivated by the idea of multi-grid solver, the control policy on a virtual agent over time can be decomposed into different frequency levels, which can be progressively learned via a set of simulations in a coarse-to-fine manner. It is expected that most RL trials are performed in coarser simulations to learn lower control frequency levels with more efficient convergence, while higher frequency levels require much less RL trials, thus significantly accelerating the learning process. To implement our idea, we designed a novel multi-level residual network with a filter module attached, where each level of the network is learned by performing RL for a given simulation resolution. The proposed framework is evaluated by conducting policy learning experiments on a virtual aerial (2D) and an underwater (3D) robot, both requiring time-consuming physical simulations. Our results demonstrate a decrease in almost half in learning time compared to a direct RL approach, while achieving similar control performance. keywords: {Training;Time-frequency analysis;Three-dimensional displays;Process control;Reinforcement learning;Switches;Solids},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610992&isnumber=10609862

C. Zheng, L. Zhang, H. Wang, R. Gomez, E. Nichols and G. Li, "Shaping Social Robot to Play Games with Human Demonstrations and Evaluative Feedback," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9517-9523, doi: 10.1109/ICRA57147.2024.10611605.Abstract: In this paper, building on recent advances in the fields of gaming AI and social robotics, we present a new approach to facilitate the social robot Haru to imitate game strategies from human players’ demonstrated trajectories and evaluative feedback in a real-time two-player game. Our research shows that Haru is able to learn and imitate human different game strategies from human players in a human time scale. In addition, our results show that human evaluative feedback plays an important role in allowing Haru to obtain a better performance via our method than human player’s demonstrations. Finally, results of our user study indicate that Haru imitating human player’s game strategies via our method is perceived to be more human-like and have better game performance and experience than self-learning from pre-defined reward functions via traditional deep reinforcement learning. keywords: {Social robots;Buildings;Games;Deep reinforcement learning;Real-time systems;Trajectory;Artificial intelligence},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611605&isnumber=10609862

R. Ye, Y. Hu, Y. A. Bian, L. Kulm and T. Bhattacharjee, "MORPHeus: a Multimodal One-armed Robot-assisted Peeling System with Human Users In-the-loop," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9540-9547, doi: 10.1109/ICRA57147.2024.10610050.Abstract: Meal preparation is an important instrumental activity of daily living (IADL). While existing research has explored robotic assistance in meal preparation tasks such as cutting and cooking, the crucial task of peeling has received less attention. Robot-assisted peeling, conventionally a bimanual task, is challenging to deploy in the homes of care recipients using two wheelchair-mounted robot arms due to ergonomic and transferring challenges. This paper introduces a robot-assisted peeling system utilizing a single robotic arm and an assistive cutting board, inspired by the way individuals with one functional hand prepare meals. Our system incorporates a multimodal active perception module to determine whether an area on the food is peeled, a human-in-the-loop long-horizon planner to perform task planning while catering to a user’s preference for peeling coverage, and a compliant controller to peel the food items. We demonstrate the system on 12 food items representing the extremes of different shapes, sizes, skin thickness, surface textures, skin vs flesh colors, and deformability. Check out the Morpheus project at https://emprise.cs.cornell.edu/morpheus/. keywords: {Shape;Instruments;Ergonomics;Manipulators;Skin;Human in the loop;Surface texture},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610050&isnumber=10609862

H. Deguchi, K. Shibata and S. Taguchi, "Language to Map: Topological map generation from natural language path instructions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9556-9562, doi: 10.1109/ICRA57147.2024.10611377.Abstract: In this paper, a method for generating a map from path information described using natural language (textual path) is proposed. In recent years, robotics research mainly focus on vision-and-language navigation (VLN), a navigation task based on images and textual paths. Although VLN is expected to facilitate user instructions to robots, its current implementation requires users to explain the details of the path for each navigation session, which results in high explanation costs for users. To solve this problem, we proposed a method that creates a map as a topological map from a textual path and automatically creates a new path using this map. We believe that large language models (LLMs) can be used to understand textual path. Therefore, we propose and evaluate two methods, one for storing implicit maps in LLMs, and the other for generating explicit maps using LLMs. The implicit map is in the LLM’s memory. It is created using prompts. In the explicit map, a topological map composed of nodes and edges is constructed and the actions at each node are stored. This makes it possible to estimate the path and actions at waypoints on an undescribed path, if enough information is available. Experimental results on path instructions generated in a real environment demonstrate that generating explicit maps achieves significantly higher accuracy than storing implicit maps in the LLMs. keywords: {Costs;Accuracy;Navigation;Large language models;Natural languages;Path planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611377&isnumber=10609862

H. M. Ray, Z. Laouar, Z. Sunberg and N. Ahmed, "Human-Centered Autonomy for UAS Target Search," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9563-9570, doi: 10.1109/ICRA57147.2024.10611708.Abstract: Current methods of deploying robots that operate in dynamic, uncertain environments, such as Uncrewed Aerial Systems in search & rescue missions, require nearly continuous human supervision for vehicle guidance and operation. These methods do not consider high-level mission context resulting in cumbersome manual operation or inefficient exhaustive search patterns. We present a human-centered autonomous frame-work that infers geospatial mission context through dynamic feature sets, which then guides a probabilistic target search planner. Operators provide a set of diverse inputs, including priority definition, spatial semantic information about ad-hoc geographical areas, and reference waypoints, which are probabilistically fused with geographical database information and condensed into a geospatial distribution representing an operator’s preferences over an area. An online, POMDP-based planner, optimized for target searching, is augmented with this reward map to generate an operator-constrained policy. Our results, simulated based on input from five professional rescuers, display effective task mental model alignment, 18% more victim finds, and 15 times more efficient guidance plans then current operational methods. keywords: {Semantics;Pipelines;Probabilistic logic;Spatial databases;Geospatial analysis;Planning;Cognitive science},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611708&isnumber=10609862

S. Choi, Z. A. Al-Sabbag, S. Narasimhan and C. M. Yeum, "Gaze-based Human-Robot Interaction System for Infrastructure Inspections," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9571-9577, doi: 10.1109/ICRA57147.2024.10610684.Abstract: Routine inspections for critical infrastructures such as bridges are required in most jurisdictions worldwide. Such routine inspections are largely visual in nature, which are qualitative, subjective, and not repeatable. Although robotic infrastructure inspections address such limitations, they cannot replace the superior ability of experts to make decisions in complex situations, thus making human-robot interaction systems a promising technology. This study presents a novel gaze-based human-robot interaction system, designed to augment the visual inspection performance through mixed reality. Through holograms from a mixed reality device, gaze can be utilized effectively to estimate the properties of the defect in real-time. Additionally, inspectors can monitor the inspection progress on-line, which enhances the speed of the entire inspection process. Limited controlled experiments demonstrate its effectiveness across various users and defect types. To our knowledge, this is the first demonstration of the real-time application of eye gaze in civil infrastructure inspections. keywords: {Visualization;Accuracy;Tracking;Human-robot interaction;Mixed reality;Virtual reality;Inspection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610684&isnumber=10609862

C. Fernando, D. Olds, S. I. Campbell and P. M. Maffettone, "Facile Integration of Robots into Experimental Orchestration at Scientific User Facilities," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9578-9584, doi: 10.1109/ICRA57147.2024.10611706.Abstract: Integration of robots into scientific user facilities, such as the National Synchrotron Light Source II, improves their efficiency and capacity. Many such facilities use the opensource Bluesky project for experimental control and orchestration. However, there remains an open challenge in deploying robotic solutions at these facilities that are reconfigurable, extensible, and compatible with pre-existing software infrastructure. Herein, we introduce a framework that uses the Robotic Operating System 2 (ROS2) and Bluesky to provide extensible robotic applications, while working under the operational constraints of a large-scale user facility. We demonstrated this framework by integrating a robotic arm to pick and place a sample holder at a beamline, recording a 90% repeatability rate. This provides the groundwork for further new robotics applications at large-scale scientific user facilities that depend on Bluesky. keywords: {Operating systems;Synchrotrons;Manipulators;Software;Recording;Robots;Light sources},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611706&isnumber=10609862

D. Marta, S. Holk, C. Pek and I. Leite, "SEQUEL: Semi-Supervised Preference-based RL with Query Synthesis via Latent Interpolation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9585-9592, doi: 10.1109/ICRA57147.2024.10610534.Abstract: Preference-based reinforcement learning (RL) poses as a recent research direction in robot learning, by allowing humans to teach robots through preferences on pairs of desired behaviours. Nonetheless, to obtain realistic robot policies, an arbitrarily large number of queries is required to be answered by humans. In this work, we approach the sample-efficiency challenge by presenting a technique which synthesizes queries, in a semi-supervised learning perspective. To achieve this, we leverage latent variational autoencoder (VAE) representations of trajectory segments (sequences of state-action pairs). Our approach manages to produce queries which are closely aligned with those labeled by humans, while avoiding excessive uncertainty according to the human preference predictions as determined by reward estimations. Additionally, by introducing variation without deviating from the original human’s intents, more robust reward function representations are achieved. We compare our approach to recent state-of-the-art preference-based RL semi-supervised learning techniques. Our experimental findings reveal that we can enhance the generalization of the estimated reward function without requiring additional human intervention. Lastly, to confirm the practical applicability of our approach, we conduct experiments involving actual human users in a simulated social navigation setting. Videos of the experiments can be found at https://sites.google.com/view/rl-sequel keywords: {Interpolation;Uncertainty;Navigation;Estimation;Reinforcement learning;Semisupervised learning;Robot learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610534&isnumber=10609862

I. Igbinedion and S. Karaman, "Learning When to Ask for Help: Efficient Interactive Navigation via Implicit Uncertainty Estimation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9593-9599, doi: 10.1109/ICRA57147.2024.10610965.Abstract: Robots operating alongside humans often encounter unfamiliar environments that make autonomous task completion challenging. Though improving models and increasing dataset size can enhance a robot’s performance in unseen environments, data collection and model refinement may be impractical in every environment. Approaches that utilize human demonstrations through manual operation can aid in refinement and generalization, but often require significant data collection efforts to generate enough demonstration data to achieve satisfactory task performance. Interactive approaches allow for humans to provide correction to robot action in real time, but intervention policies are often based on explicit factors related to state and task understanding that may be difficult to generalize. Addressing these challenges, we train a lightweight interaction policy that allows robots to decide when to proceed autonomously or request expert assistance at estimated times of uncertainty. An implicit estimate of uncertainty is learned via evaluating the feature extraction capabilities of the robot’s visual navigation policy. By incorporating part-time human interaction, robots recover quickly from their mistakes, significantly improving the odds of task completion. Incorporating part-time interaction yields an increase in success of 0.38 with only a 0.3 expert interaction rate within the Habitat simulation environment using a simulated human expert. We further show success transferring this approach to a new domain with a real human expert, improving success from less than 0.1 with an autonomous agent to 0.92 with a 0.23 human interaction rate. This approach provides a practical means for robots to interact and learn from humans in real-world settings. keywords: {Visualization;Uncertainty;Navigation;Measurement uncertainty;Human factors;Data collection;Real-time systems;human factors and human-in-the-loop;vision-based navigation;reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610965&isnumber=10609862

K. Mukoya, E. Weng, R. Choudhury and K. Kitani, "JaywalkerVR: A VR System for Collecting Safety-Critical Pedestrian-Vehicle Interactions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9600-9607, doi: 10.1109/ICRA57147.2024.10610819.Abstract: Developing autonomous vehicles that can safely interact with pedestrians requires large amounts of pedestrian and vehicle data in order to learn accurate pedestrian-vehicle interaction models. However, gathering data that include crucial but rare scenarios - such as pedestrians jaywalking into heavy traffic - can be costly and unsafe to collect. We propose a virtual reality human-in-the-loop simulator, JaywalkerVR, to obtain vehicle-pedestrian interaction data to address these challenges. Our system enables efficient, affordable, and safe collection of long-tail pedestrian-vehicle interaction data. Using our proposed simulator, we create a high-quality dataset with vehicle-pedestrian interaction data from safety critical scenarios called CARLA-VR. The CARLA-VR dataset addresses the lack of long-tail data samples in commonly used real world autonomous driving datasets. We demonstrate that models trained with CARLA-VR improve displacement error and collision rate by 10.7% and 4.9%, respectively, and are more robust in rare vehicle-pedestrian scenarios. keywords: {Training;Solid modeling;Pedestrians;Accuracy;Virtual reality;Data collection;Human in the loop},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610819&isnumber=10609862

H. Tan, Y. Yuan, H. Yan, S. Zhong and Y. Yang, "Human Preference-aware Rebalancing and Charging for Shared Electric Micromobility Vehicles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9608-9615, doi: 10.1109/ICRA57147.2024.10610713.Abstract: Shared electric micromobility has surged to a popular model of urban transportation due to its efficiency in short-distance trips and environmentally friendly characteristics compared to traditional automobiles. However, managing thousands of shared electric micromobility vehicles including rebalancing and charging to meet users’ travel demands still has been a challenge. Existing methods generally ignore human preferences in vehicle selection and assume all nearby vehicles have an equal chance of being selected, which is unrealistic based on our findings. To address this problem, we design PERCEIVE, a human preference-aware rebalancing and charging framework for shared electric micromobility vehicles. Specifically, we model human preferences in vehicle selection based on vehicle usage history and current status (e.g., energy level) and incorporate the vehicle selection model into a robust adversarial reinforcement learning framework. We further utilize conformal prediction to quantify human preference uncertainty and fuse it with the reinforcement learning framework. We evaluate our framework using two months of real-world electric micromobility operation data in a city. Experimental results show that our method achieves a performance gain of at least 4.02% in the net revenue and offers more robust performance in worst-case scenarios compared to state-of-the-art baselines. keywords: {Uncertainty;Fuses;Urban areas;Transportation;Reinforcement learning;Performance gain;Energy states;Intelligent Transportation Systems;Human Factors and Human-in-the-Loop;Reinforcement Learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610713&isnumber=10609862

J. Huang, X. Wang, C. Xia, H. Liu, M. Shao and B. Liang, "A Planar Compliant Contact Control Applied to Multi-dimensional Elastic Gripper for Unexpected Contact," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9624-9630, doi: 10.1109/ICRA57147.2024.10611428.Abstract: It is difficult to guarantee an empty living environment to prevent unexpected contact between the object being manipulated by the robot and unplanned obstacles. In this paper, we propose a planar compliant contact control method for planar manipulation to cope with unexpected contact. We first use sheet gel as a multi-dimensional passive elastic element and combine it with a two-finger gripper to design a multi-dimensional elastic gripper. Subsequently, we explore the lumped parameter model for the force-displacement relationship of gel deformation and combine the model with the high impedance motion of robots to design an elastic interaction controller. The controller not only actively adjusts the deformation of the gel to provide the desired contact force and torque depending on contact, but also performs avoidance by following the surface of obstacles. Finally, we design and deploy several planar compliant contact experiments to validate the proposed method and demonstrate the unexpected contact response in human-robot co-packing. The results show that our method enables the robot to remain compliant in the face of unexpected contact caused by unplanned obstacles, which provides a guarantee for safe manipulation. Physics experiments can be viewed in the attached video. keywords: {Deformable models;Surface impedance;Torque;Deformation;Force;Safety;Impedance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611428&isnumber=10609862

S. Hjorth, J. Lachner, A. Ajoudani and D. Chrysostomou, "Enabling passivity for Cartesian workspace restrictions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9631-9637, doi: 10.1109/ICRA57147.2024.10610969.Abstract: An emerging trend in the field of human-robot collaboration is the disassembly of end-of-life products. Safety is a crucial requirement of the disassembly process since worn-out or damaged products could break, possibly resulting in dangerous behavior of the robot. To protect the user from such behavior, this work addresses this challenge through the implementation of an energy-aware Cartesian impedance controller, combined with virtual workspace restrictions. Hereby, the passivity of the robotic system is ensured. The paper proposed two approaches to ensure the passivity of the system when subjected to workspace restrictions due to unplanned interactions and contact loss. The first approach employs an augmented energy tank with restricted energy flow. The second approach monitors the overall energy flow, regulating and separating non-passive behavior, caused by workspace restrictions. The approaches are evaluated and compared with each other, by using a KUKA LBR iiwa robot. The results highlight the potential of virtual workspace restrictions in human-robot collaborative disassembly tasks. keywords: {Collaboration;Market research;Safety;Impedance;Task analysis;Robots;Monitoring},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610969&isnumber=10609862

J. Hu, Y. Zhou, Z. Wang, X. Li, Y. Jiang and B. He, "X-Tacformer : Spatio-tempral Attention Model for Tactile Recognition," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9638-9644, doi: 10.1109/ICRA57147.2024.10610365.Abstract: Recently, tactile sensing has attracted great interests in robotics, especially for exploring unstructured objects. Sensor arrays play an important role in the exploration, which generates rich spatio-temporal information. In this work, we propose an efficient tactile recognition model, X-Tacformer. This model pays attention to both spatial and temporal features of tactile sequences from sensor arrays, which is verified by four public datasets, Ev-Objects, Ev-Containers, Augment8000 and BioTac-Dos. Comparative studies show that our model has resulted in a significant improvement of the recognition accuracy by 0.0223, 0.1416, 0.2735 and 0.1592 in these datasets. In order to verify its performances on dataset with rich spatio-temporal features, a self-designed dataset, ALU-Textures, was constructed with 10 fabrics from everyday textiles, aiming to extend the data collection action modes of current datasets by simulating human rubbing movements with the thumb and index fingers of an Allegro hand. Our model also demonstrates efficient salient feature learning capabilities on ALU-Textures, which is further augmented by tactile data augmentation methods. keywords: {Image recognition;Accuracy;Three-dimensional displays;Biological system modeling;Thumb;Robot sensing systems;Data augmentation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610365&isnumber=10609862

Y. Su, G. Li, Y. Deng, I. Sarakoglou, N. G. Tsagarakis and J. Chen, "The Joint-Space Reconstruction of Human Fingers by using a Highly Under-Actuated Exoskeleton," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9645-9651, doi: 10.1109/ICRA57147.2024.10610872.Abstract: Hand motion tracking is essential in many fields, e.g., immersive virtual reality, teleoperation of robotic hand, and hand rehabilitation of stroke patient, as human hand plays a crucial role in our daily life. The highly under-actuated hand exoskeleton, which can track the 6-DoF motions of each fingertip via a highly under-actuated kinematic chain, exhibits many benefits in wearability and portability over other solutions. However, due to the non-anthropomorphic linkage, this hand exoskeleton also encounters difficulties in measuring human-finger’s joint angles. While the joint-space is important in many scenarios, such as teleoperating a robotic hand with anthropomorphic kinematics but with different size to human. Here we proposed a new method to reconstruct the human finger joints by using a highly under-actuated hand exoskeleton. Our key contribution is the arc-fitting algorithm, which is able to calibrate the misalignment between the exoskeleton’s and the human-finger’s base frames and estimate the length of human’s phalanxes, by using the fingertip’s circular motions. With knowing the aforementioned informations, the joint angles can be reconstructed in high precision based on the inverse kinematics models of human fingers. Furthermore, our proposed method is compared with a baseline method, in which the joint angles obtained by a motion capture system are served as ground-truth. The results demonstrate that our proposed method exhibits excellent performance in reconstructing finger’s joint configurations. keywords: {Couplings;Tracking;Exoskeletons;Thumb;Virtual reality;Stroke (medical condition);Motion capture;Hand Motion Tracking;Hand Exoskeleton;Joint Reconstruction;Human-Robot Interaction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610872&isnumber=10609862

A. S. Ivani, F. Barontini, M. G. Catalano, G. Grioli, M. Bianchi and A. Bicchi, "Prosthetic Upper-Limb Sensory Enhancement (PULSE): a Dual Haptic Feedback Device in a Prosthetic Socket," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9652-9658, doi: 10.1109/ICRA57147.2024.10610327.Abstract: This study presents the Prosthetic Upper-Limb Sensory Enhancement (PULSE), a novel dual feedback device completely integrated into a prosthetic socket. The core of the system includes two compact vibrotactile actuators and two silicone chambers in contact with the user’s skin. These components provide high-frequency tactile cues for initial contact and surface information (e.g. texture) as well as pressure stimuli related to grasping force. Ten able-bodied participants and one subject with limb loss validated the system, accomplishing an object discrimination task in two different modalities (with and without the feedback). Standardized questionnaires evaluate users’ satisfaction and workload, enabling a systematic and robust device assessment. The results show that the PULSE device enhanced performance compared to its absence without causing discomfort for a prosthetic user and able-bodied participants. The findings highlight the potential of dual haptic feedback to enhance sensory perception in prosthetic applications and offer valuable insights for future prosthetic design. keywords: {Systematics;Sockets;Force;Robot sensing systems;Skin;Surface texture;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610327&isnumber=10609862

X. Li, R. Zhao, C. Lu, X. Xiao and W. Ding, "Point-Wise Vibration Pattern Production via a Sparse Actuator Array for Surface Tactile Feedback," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9659-9665, doi: 10.1109/ICRA57147.2024.10611362.Abstract: Surface vibration tactile feedback is capable of conveying various semantic information to humans via handheld electronic devices, such as smartphones, touch panels, and game controllers. However, covering the entire contacting surface of the device with a dense arrangement of actuators can affect its normal use. Determining how to produce desired vibration patterns at any contact point with only a few sparse actuators deployed on the surface of the handheld device remains a significant challenge. In this work, we develop a tactile feedback board in the size of a smartphone with only five actuators, and achieve the precise production of vibration patterns that can focus at any desired position on the board. Specifically, we investigate the vibration characteristics of a single passive coil actuator and construct its vibration pattern model for any position on the feedback board surface. Optimal phase and amplitude modulation, determined using the simulated annealing algorithm, is employed with five actuators in a sparse array. The vibration patterns from all actuators are superimposed linearly to synthetically generate different onboard vibration energy distributions for tactile sensing. Experiments demonstrated that point-wise vibration pattern production on our tactile board achieved an average level of about 0.9 in the Structural Similarity Index Measure (SSIM) evaluation, when compared to the ideal single-point-focused target vibration pattern. Four point-wise patterns focused on the top, bottom, left, and right parts of the tactile board were applied, to guide continuous directional movements without visual assistance, which shows significant implications for machine-assisted cognition based on vibration tactile feedback. keywords: {Vibrations;Phased arrays;Actuators;Tactile sensors;Production;Simulated annealing;Vibration measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611362&isnumber=10609862

A. Rota, K. Fan and E. De Momi, "Implementation and Assessment of an Augmented Training Curriculum for Surgical Robotics," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9666-9673, doi: 10.1109/ICRA57147.2024.10610411.Abstract: The integration of high-level assistance algorithms in surgical robotics training curricula may be beneficial in establishing a more comprehensive and robust skillset for aspiring surgeons, improving their clinical performance as a consequence. This work presents the development and validation of a haptic-enhanced Virtual Reality simulator for surgical robotics training, featuring 8 surgical tasks that the trainee can interact with thanks to the embedded physics engine. This virtual simulated environment is augmented by the introduction of high-level haptic interfaces for robotic assistance that aim at re-directing the motion of the trainee’s hands and wrists toward targets or away from obstacles, and providing a quantitative performance score after the execution of each training exercise.An experimental study shows that the introduction of enhanced robotic assistance into a surgical robotics training curriculum improves performance during the training process and, crucially, promotes the transfer of the acquired skills to an unassisted surgical scenario, like the clinical one. keywords: {Training;Wrist;Medical robotics;Surgery;Virtual reality;Haptic interfaces;Task analysis;Surgical Robotics;Enhanced Training;Haptic Assistance;Skill Transfer},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610411&isnumber=10609862

J. Rozsa, S. Costrell, M. O. Martinez and G. K. Fedder, "Fingertip Ultrasonic Array for Tactile Rendering," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9682-9688, doi: 10.1109/ICRA57147.2024.10610307.Abstract: A miniature haptic stimulation device utilizes focused ultrasound to deliver a tactile haptic sensation to the finger. The 1-3 piezocomposite device has a 1 cm2 footprint, which is an order of magnitude smaller than other ultrasonic haptic devices and is a good candidate for wearable tactile rendering systems. The device focuses energy to a 1 mm3 voxel. The current prototype was validated with a small, preliminary human subject study and requires an average input voltage of 68.8 V to elicit tactile sensation. The sensory drive voltage threshold will decrease with future refinement of mechanical impedance matching and focusing. keywords: {Ultrasonic imaging;Focusing;Prototypes;High-voltage techniques;Rendering (computer graphics);Robot sensing systems;Acoustics;haptics;ultrasound;piezocomposite},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610307&isnumber=10609862

J. Ketchum, A. Prabhakar and T. D. Murphey, "Active Exploration for Real-Time Haptic Training," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9689-9695, doi: 10.1109/ICRA57147.2024.10610782.Abstract: Tactile perception is important for robotic systems that interact with the world through touch. Touch is an active sense in which tactile measurements depend on the contact properties of an interaction—e.g., velocity, force, acceleration— as well as properties of the sensor and object under test. These dependencies make training tactile perceptual models challenging. Additionally, the effects of limited sensor life and the near-field nature of tactile sensors preclude the practical collection of exhaustive data sets even for fairly simple objects. Active learning provides a mechanism for focusing on only the most informative aspects of an object during data collection. Here we employ an active learning approach that uses a data-driven model’s entropy as an uncertainty measure and explore relative to that entropy conditioned on the sensor state variables. Using a coverage-based ergodic controller, we train perceptual models in near-real time. We demonstrate our approach using a biomimentic sensor, exploring "tactile scenes" composed of shapes, textures, and objects. Each learned representation provides a perceptual sensor model for a particular tactile scene. Models trained on actively collected data outperform their randomly collected counterparts in real-time training tests. Additionally, we find that the resulting network entropy maps can be used to identify high salience portions of a tactile scene. keywords: {Training;Uncertainty;Biological system modeling;Tactile sensors;Entropy;Real-time systems;Haptic interfaces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610782&isnumber=10609862

W. -S. Law, S. D. Toro Wyetzner, R. Zhen and S. Follmer, "A Multi-Stable Curved Line Shape Display," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9696-9703, doi: 10.1109/ICRA57147.2024.10610902.Abstract: Shape-changing displays enable real-time visualization and haptic exploration of 3D surfaces. However, many shape-changing displays are composed of individually actuated rigid bodies, which makes them both mechanically complex and unable to form smooth surfaces. In this work, we build a multi-stable curved line display inspired by physical splines. By using circular splines to initialize a discrete elastic rods simulator, we can model multiple stable shapes that fit specific boundary conditions. We then generate actuation instructions based on the circular spline initialization to drive the physical display. We demonstrate our display’s ability to create 16 shapes with 8 different boundary conditions. Our display is consistent in shape output, with an average standard deviation in height of 0.75 mm or 0.47% of the display’s maximum vertical range. We also show that our model is consistent with our display, with a mean RMSE of 6.68 mm or 3.85% of the display’s maximum vertical range for shapes we could stably simulate. We then demonstrate potential scalability by simulating a multi-segment version of the system and show the display’s ability to withstand loads during contour following in haptic exploration. keywords: {Visualization;Three-dimensional displays;Shape;Scalability;Boundary conditions;Surface fitting;Haptic interfaces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610902&isnumber=10609862

H. Huang, A. Loquercio, A. Kumar, N. Thakkar, K. Goldberg and J. Malik, "Manipulator as a Tail: Promoting Dynamic Stability for Legged Locomotion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9712-9719, doi: 10.1109/ICRA57147.2024.10610049.Abstract: For locomotion, is an arm on a legged robot a liability or an asset for locomotion? Biological systems evolved additional limbs beyond legs that facilitates postural control. This work shows how a manipulator can be an asset for legged locomotion at high speeds or under external perturbations, where the arm serves beyond manipulation. Since the system has 15 degrees of freedom (twelve for the legged robot and three for the arm), off-the-shelf reinforcement learning (RL) algorithms struggle to learn effective locomotion policies. Inspired by Bernstein’s neurophysiological theory of animal motor learning, we develop an incremental training procedure that initially freezes some degrees of freedom and gradually releases them, using behaviour cloning (BC) from an early learning procedure to guide optimization in later learning. Simulation experiments show that our policy increases the success rate by up to 61 percentage points over the baselines. Simulation and real robot experiments suggest that our policy learns to use the arm as a "tail" to initiate robot turning at high speeds and to stabilize the quadruped under external perturbations. Quantitatively, in simulation experiments, we cut the failure rate up to 43.6% during high-speed turning and up to 31.8% for quadruped under external forces compared to using a locked arm. keywords: {Legged locomotion;Training;Perturbation methods;Tail;Reinforcement learning;Arms;Turning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610049&isnumber=10609862

F. Vezzi, J. Ding, A. Raffin, J. Kober and C. D. Santina, "Two-Stage Learning of Highly Dynamic Motions with Rigid and Articulated Soft Quadrupeds," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9720-9726, doi: 10.1109/ICRA57147.2024.10610561.Abstract: Controlled execution of dynamic motions in quadrupedal robots, especially those with articulated soft bodies, presents a unique set of challenges that traditional methods struggle to address efficiently. In this study, we tackle these issues by relying on a simple yet effective two-stage learning framework to generate dynamic motions for quadrupedal robots. First, a gradient-free evolution strategy is employed to discover simply represented control policies, eliminating the need for a predefined reference motion. Then, we refine these policies using deep reinforcement learning. Our approach enables the acquisition of complex motions like pronking and back-flipping, effectively from scratch. Additionally, our method simplifies the traditionally labour-intensive task of reward shaping, boosting the efficiency of the learning process. Importantly, our framework proves particularly effective for articulated soft quadrupeds, whose inherent compliance and adaptability make them ideal for dynamic tasks but also introduce unique control challenges. keywords: {Law enforcement;Dynamics;Humanoid robots;Soft robotics;Deep reinforcement learning;Trajectory;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610561&isnumber=10609862

G. Christmann, Y. -S. Luo and W. -C. Chen, "Expert Composer Policy: Scalable Skill Repertoire for Quadruped Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9727-9734, doi: 10.1109/ICRA57147.2024.10611039.Abstract: We propose the expert composer policy, a framework to reliably expand the skill repertoire of quadruped agents. The composer policy links pair of experts via transitions to a sampled target state, allowing experts to be composed sequentially. Each expert specializes in a single skill, such as a locomotion gait or a jumping motion. Instead of a hierarchical or mixture-of-experts architecture, we train a single composer policy in an independent process that is not conditioned on the other expert policies. By reusing the same composer policy, our approach enables adding new experts without affecting existing ones, enabling incremental repertoire expansion and preserving original motion quality. We measured the transition success rate of 72 transition pairs and achieved an average success rate of 99.99%, which is over 10% higher than the baseline random approach, and outperforms other state-of-the-art methods. Using domain randomization during training we ensure a successful transfer to the real world, where we achieve an average transition success rate of 97.22% (N=360) in our experiments. keywords: {Training;Quadrupedal robots;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611039&isnumber=10609862

Y. Li, J. Li, W. Fu and Y. Wu, "Learning Agile Bipedal Motions on a Quadrupedal Robot," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9735-9742, doi: 10.1109/ICRA57147.2024.10611442.Abstract: Can a quadrupedal robot perform bipedal motions like humans? Although developing human-like behaviors is more often studied on costly bipedal robot platforms, we present a solution over a lightweight quadrupedal robot that unlocks the agility of the quadruped in an upright standing pose and is capable of a variety of human-like motions. Our framework is with a hierarchical structure. At the low level is a motion-conditioned control policy that allows the quadrupedal robot to track desired base and front limb movements while balancing on two hind feet. The policy is commanded by a high-level motion generator that gives trajectories of parameterized human-like motions to the robot from multiple modalities of human input. We for the first time demonstrate various bipedal motions on a quadrupedal robot, and showcase interesting human-robot interaction modes including mimicking human videos, following natural language instructions, and physical interaction. The video is available at https://sites.google.com/view/bipedal-motions-quadruped. keywords: {Tracking;Natural languages;Propioception;Human-robot interaction;Generators;Trajectory;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611442&isnumber=10609862

S. Xu et al., "LAGOON: Language-Guided Motion Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9743-9750, doi: 10.1109/ICRA57147.2024.10610467.Abstract: We aim to control a robot to physically behave in the real world following any high-level language command like "cartwheel" or "kick". Although human motion datasets exist, this task remains particularly challenging since generative models can produce physically unrealistic motions, which will be more severe for robots due to different body structures and physical properties. Deploying such a motion to a physical robot can cause even greater difficulties due to the sim2real gap. We develop LAnguage-Guided mOtion cONtrol (LAGOON), a multi-phase reinforcement learning (RL) method to generate physically realistic robot motions under language commands. LAGOON first leverages a pretrained model to generate a human motion from a language command. Then an RL phase trains a control policy in simulation to mimic the generated human motion. Finally, with domain randomization, our learned policy can be deployed to a quadrupedal robot, leading to a quadrupedal robot that can take diverse behaviors in the real world under natural language commands. keywords: {Robot motion;Natural languages;Reinforcement learning;Diffusion models;Skeleton;Quadrupedal robots;Motion control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610467&isnumber=10609862

M. Kim, U. Shin and J. -Y. Kim, "Learning Quadrupedal Locomotion with Impaired Joints Using Random Joint Masking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9751-9757, doi: 10.1109/ICRA57147.2024.10610088.Abstract: Quadrupedal robots have played a crucial role in various environments, from structured environments to complex harsh terrains, thanks to their agile locomotion ability. However, these robots can easily lose their locomotion functionality if damaged by external accidents or internal malfunctions. In this paper, we propose a novel deep reinforcement learning framework to enable a quadrupedal robot to walk with impaired joints. The proposed framework consists of three components: 1) a random joint masking strategy for simulating impaired joint scenarios, 2) a joint state estimator to predict an implicit status of current joint condition based on past observation history, and 3) progressive curriculum learning to allow a single network to conduct both normal gait and various joint-impaired gaits. We verify that our framework enables the Unitree’s Go1 robot to walk under various impaired joint conditions in real-world indoor and outdoor environments. keywords: {Legged locomotion;Deep reinforcement learning;Motion capture;Indoor environment;Quadrupedal robots;Reliability;History},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610088&isnumber=10609862

T. Hou, J. Tu, X. Gao, Z. Dong, P. Zhai and L. Zhang, "Multi-Task Learning of Active Fault-Tolerant Controller for Leg Failures in Quadruped robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9758-9764, doi: 10.1109/ICRA57147.2024.10610151.Abstract: Electric quadruped robots used in outdoor exploration are susceptible to leg-related electrical or mechanical failures. Unexpected joint power loss and joint locking can immediately pose a falling threat. Typically, controllers lack the capability to actively sense the condition of their own joints and take proactive actions. Maintaining the original motion patterns could lead to disastrous consequences, as the controller may produce irrational output within a short period of time, further creating the risk of serious physical injuries. This paper presents a hierarchical fault-tolerant control scheme employing a multi-task training architecture capable of actively perceiving and overcoming two types of leg joint faults. The architecture simultaneously trains three joint task policies for health, power loss, and locking scenarios in parallel, introducing a symmetric reflection initialization technique to ensure rapid and stable gait skill transformations. Experiments demonstrate that the control scheme is robust in unexpected scenarios where a single leg experiences concurrent joint faults in two joints. Furthermore, the policy retains the robot’s planar mobility, enabling rough velocity tracking. Finally, zero-shot Sim2Real transfer is achieved on the real-world SOLO8 robot, countering both electrical and mechanical failures. keywords: {Legged locomotion;Training;Fault tolerance;Visualization;Fault tolerant systems;Multitasking;Reflection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610151&isnumber=10609862

N. T. Nguyen, P. Tej Gangavarapu, N. Mandel, R. Bruder and F. Ernst, "Motion planning for 4WS vehicle with autonomous selection of steering modes via an MIQP-MPC controller," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9765-9771, doi: 10.1109/ICRA57147.2024.10610461.Abstract: Navigation in agricultural fields imposes various constraints on manoeuvrability, which can be tackled by using four-wheel steering (4WS) vehicles which are capable of switching between multiple steering mechanisms with distinct kinematic properties. For example, parallel positive steering (PPS) with four wheels in parallel to each other can maintain the vehicle’s heading when moving along a curve. Symmetric negative steering (SNS) with two wheels on each side sharing the same steering angle can turn with a small radius. This paper presents a controller capable of selecting and switching between the two aforementioned modes autonomously for better trajectory tracking performance with special heading requirements for agricultural applications. The controller is implemented as a Model Predictive Control (MPC) controller formulated as a mixed-integer quadratic programming (MIQP) problem for the 4WS vehicle. Practical constraints, such as limits on wheel velocities, steering angles and their rate-of-changes are taken into account. A Python implementation confirms the real-time execution capability of the controller and simulation results highlight its effectiveness. keywords: {Trajectory tracking;Simulation;Wheels;Switches;Programming;Real-time systems;Quadratic programming;Four-wheel steering;trajectory tracking;Model Predictive Control;mixed-integer programming},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610461&isnumber=10609862

J. -E. Lee, A. Bylard, R. Sun and L. Sentis, "On the Performance of Jerk-Constrained Time-Optimal Trajectory Planning for Industrial Manipulators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9772-9778, doi: 10.1109/ICRA57147.2024.10610437.Abstract: Jerk-constrained trajectories offer a wide range of advantages that collectively improve the performance of robotic systems, including increased energy efficiency, durability, and safety. In this paper, we present a novel approach to jerk-constrained time-optimal trajectory planning (TOTP), which follows a specified path while satisfying up to third-order constraints to ensure safety and smooth motion. One significant challenge in jerk-constrained TOTP is a non-convex formulation arising from the inclusion of third-order constraints. Approximating inequality constraints can be particularly challenging because the resulting solutions may violate the actual constraints. We address this problem by leveraging convexity within the proposed formulation to form conservative inequality constraints. We then obtain the desired trajectories by solving an n-dimensional Sequential Linear Program (SLP) iteratively until convergence. Lastly, we evaluate in a real robot the performance of trajectories generated with and without jerk limits in terms of peak power, torque efficiency, and tracking capability. keywords: {Torque;Trajectory planning;Service robots;Manipulators;Energy efficiency;Trajectory;Safety;Time-Optimal Trajectory Planning;Jerk Constraints;Smooth Trajectory Generation;Industrial Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610437&isnumber=10609862

X. Bu and B. Plancher, "Symmetric Stair Preconditioning of Linear Systems for Parallel Trajectory Optimization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9779-9786, doi: 10.1109/ICRA57147.2024.10610386.Abstract: There has been a growing interest in parallel strategies for solving trajectory optimization problems. One key step in many algorithmic approaches to trajectory optimization is the solution of moderately-large and sparse linear systems. Iterative methods are particularly well-suited for parallel solves of such systems. However, fast and stable convergence of iterative methods is reliant on the application of a high-quality preconditioner that reduces the spread and increase the clustering of the eigenvalues of the target matrix. To improve the performance of these approaches, we present a new parallel-friendly symmetric stair preconditioner. We prove that our preconditioner has advantageous theoretical properties when used in conjunction with iterative methods for trajectory optimization such as a more clustered eigenvalue spectrum. Numerical experiments with typical trajectory optimization problems reveal that as compared to the best alternative parallel preconditioner from the literature, our symmetric stair preconditioner provides up to a 34% reduction in condition number and up to a 25% reduction in the number of resulting linear system solver iterations. keywords: {Linear systems;Symmetric matrices;Clustering algorithms;Stairs;Eigenvalues and eigenfunctions;Iterative methods;Sparse matrices},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610386&isnumber=10609862

E. Adabag, M. Atal, W. Gerard and B. Plancher, "MPCGPU: Real-Time Nonlinear Model Predictive Control through Preconditioned Conjugate Gradient on the GPU," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9787-9794, doi: 10.1109/ICRA57147.2024.10611212.Abstract: Nonlinear Model Predictive Control (NMPC) is a state-of-the-art approach for locomotion and manipulation which leverages trajectory optimization at each control step. While the performance of this approach is computationally bounded, implementations of direct trajectory optimization that use iterative methods to solve the underlying moderately-large and sparse linear systems, are a natural fit for parallel hardware acceleration. In this work, we introduce MPCGPU, a GPU-accelerated, real-time NMPC solver that leverages an accelerated preconditioned conjugate gradient (PCG) linear system solver at its core. We show that MPCGPU increases the scalability and real-time performance of NMPC, solving larger problems, at faster rates. In particular, for tracking tasks using the Kuka IIWA manipulator, MPCGPU is able to scale to kilohertz control rates with trajectories as long as 512 knot points. This is driven by a custom PCG solver which outperforms state-of-the-art, CPU-based, linear system solvers by at least 10x for a majority of solves and 3.6x on average. keywords: {Linear systems;Scalability;Manipulators;Real-time systems;Iterative methods;Task analysis;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611212&isnumber=10609862

G. Chen, F. Dellaert and S. Hutchinson, "Generalizing Trajectory Retiming to Quadratic Objective Functions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9823-9829, doi: 10.1109/ICRA57147.2024.10610854.Abstract: Trajectory retiming is the task of computing a feasible time parameterization to traverse a path. It is commonly used in the decoupled approach to trajectory optimization whereby a path is first found, then a retiming algorithm computes a speed profile that satisfies kino-dynamic and other constraints. While trajectory retiming is most often formulated with the minimum-time objective (i.e. traverse the path as fast as possible), it is not always the most desirable objective, particularly when we seek to balance multiple objectives or when bang-bang control is unsuitable. In this paper, we present a novel algorithm based on factor graph variable elimination that can solve for the global optimum of the retiming problem with quadratic objectives as well (e.g. minimize control effort or match a nominal speed by minimizing squared error), which may extend to arbitrary objectives with iteration. Our work extends prior works, which find only solutions on the boundary of the feasible region, while maintaining the same linear time complexity from a single forward-backward pass. We experimentally demonstrate that (1) we achieve better real-world robot performance by using quadratic objectives in place of the minimum-time objective, and (2) our implementation is comparable or faster than state-of-the-art retiming algorithms. keywords: {Bang-bang control;Linear programming;Time complexity;Task analysis;Trajectory optimization;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610854&isnumber=10609862

F. Tassi and A. Ajoudani, "A Distributed Processing Approach for Smooth Task Transitioning in Strict Hierarchical Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9830-9836, doi: 10.1109/ICRA57147.2024.10610227.Abstract: To enhance robots’ applicability in real-world scenarios, it is essential to establish a complex and multi-tasking behaviour, inspired by human nature. To this purpose, from a hardware perspective, a high number of degrees of freedom is necessary, as is the case for humanoids and collaborative mobile manipulators. From a software standpoint instead, complex hierarchical strategies are often used to define a set of behaviours that the robot should reflect in strict hierarchical order. Their main issue however, is related to the lack of continuity when their stack of tasks is changed. Existing works that address this issue clearly present a trade-off between optimality assurance during transition and computational costs. Here, we employ a distributed processing approach that enables not only the minimization of computational costs, but also continuous optimality and constraints feasibility even under sharp transitions. The approach is tested during three task transitions, for different tasks such as constrained trajectory tracking, obstacle avoidance, and postural optimization. Two mobile manipulators are used, each having 10 DoF, and the results confirm the smoothness of the generated solutions. keywords: {Distributed processing;Trajectory tracking;Scalability;Manipulators;Multitasking;Software;Computational efficiency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610227&isnumber=10609862

H. Liu, N. Li, S. Li, G. J. Mancini and J. Tan, "Towards a Novel Soft Magnetic Laparoscope for Single Incision Laparoscopic Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9845-9850, doi: 10.1109/ICRA57147.2024.10611068.Abstract: In single-incision laparoscopic surgery (SILS), magnetic anchoring and guidance system (MAGS) is a promising technique to prevent clutter in the surgical workspace and provide a larger vision field. Existing camera designs mainly rely on rigid structure design, resulting in risks of losing magnetic coupling and impacting tissue during the insertion and coupling procedure. In this paper, we proposed a wireless MAGS consisting of soft material and structure design. The camera can bend at the exit of the trocar and maintain strong coupling with the external actuator. The operation principle and modeling were established to investigate the parameter design. An easier insertion procedure was introduced and demonstrated in the experiment. The bendability was tested showing the camera could reach 20° in bending angle and 16.4mm in displacement. The insertion and deployment took less than 2 minutes on average. keywords: {Couplings;Wireless communication;Laparoscopes;Minimally invasive surgery;Robot vision systems;Bending;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611068&isnumber=10609862

S. Yuan et al., "Magnetic-Guided Flexible Origami Robot toward Long-Term Phototherapy of H. pylori in the Stomach," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9851-9857, doi: 10.1109/ICRA57147.2024.10611027.Abstract: Helicobacter pylori, a pervasive bacterial infection associated with gastrointestinal disorders such as gastritis, peptic ulcer disease, and gastric cancer, impacts approximately 50% of the global population. The efficacy of standard clinical eradication therapies is diminishing due to the rise of antibiotic-resistant strains, necessitating alternative treatment strategies. Photodynamic therapy (PDT) emerges as a promising prospect in this context. This study presents the development and implementation of a magnetically-guided origami robot, incorporating flexible printed circuit units for sustained and stable phototherapy of Helicobacter pylori. Each integrated unit is equipped with wireless charging capabilities, producing an optimal power output that can concurrently illuminate up to 15 LEDs at their maximum intensity. Crucially, these units can be remotely manipulated via a magnetic field, facilitating both translational and rotational movements. We propose an open-loop manual control sequence that allows the formation of a stable, compliant triangular structure through the interaction of internal magnets. This adaptable configuration is uniquely designed to withstand the dynamic squeezing environment prevalent in real-world gastric applications. The research herein represents a significant stride in leveraging technology for innovative medical solutions, particularly in the management of antibiotic-resistant Helicobacter pylori infections. keywords: {Stomach;Microorganisms;Sociology;Medical treatment;Magnetic fields;Statistics;Magnets},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611027&isnumber=10609862

Y. Lu et al., "Simultaneous Estimation of Shape and Force along Highly Deformable Surgical Manipulators Using Sparse FBG Measurement," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9866-9872, doi: 10.1109/ICRA57147.2024.10611403.Abstract: Recently, fiber optic sensors such as fiber Bragg gratings (FBGs) have been widely investigated for shape reconstruction and force estimation of flexible surgical robots. However, most existing approaches need precise model parameters of FBGs inside the fiber and their alignments with the flexible robots for accurate sensing results. Another challenge lies in online acquiring external forces at arbitrary locations along the flexible robots, which is highly required when with large deflections in robotic surgery. In this paper, we propose a novel data-driven paradigm for simultaneous estimation of shape and force along highly deformable flexible robots by using sparse strain measurement from a single-core FBG fiber. A thin-walled soft sensing tube helically embedded with FBG sensors is designed for a robotic-assisted flexible ureteroscope with large deflection up to 270° and a bend radius under 10 mm. We introduce and study three learning models by incorporating spatial strain encoders, and compare their performances in both free space without interactions as well as constrained environments with contact forces at different locations. The experimental results in terms of dynamic shape-force sensing accuracy demonstrate the effectiveness and superiority of the proposed methods. keywords: {Optical fiber sensors;Shape;Soft sensors;Force;Estimation;Robot sensing systems;Fiber gratings},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611403&isnumber=10609862

N. A. Strohmeyer, J. H. Park, B. P. Murphy and F. Alambeigi, "A Semi-Autonomous Data-Driven Shared Control Framework for Robotic Manipulation and Cutting of an Unknown Deformable Tissue," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9881-9886, doi: 10.1109/ICRA57147.2024.10610931.Abstract: In this work, we propose a semi-autonomous scheme to synergistically share the complicated task of manipulation and cutting of an unknown deformable tissue (U-DT) between a remote surgeon and a surgical robot. Particularly, utilizing the da Vinci Research Kit (dVRK) platform, we have designed and successfully demonstrated a fully functional shared control scheme for an autonomous tensioning and tele-cutting of a U-DT. We have shown the system’s ability to cooperate with a remote surgeon by leveraging an online data-driven learning and adaptive control method coupled with a reduced-order trajectory planning module that depends on just two parameters. By performing 25 experiments on custom-designed silicon phantoms and defining a set of success/failure metrics, we have put forward findings that establish a causal relationship between these two important parameters and the success or failure of the performed experiments. keywords: {Measurement;Personal protective equipment;Trajectory planning;Time series analysis;Surgery;Phantoms;Silicon},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610931&isnumber=10609862

J. T. Molina, T. AbuBaker, Y. Huang, X. Cheng, A. Devillard and E. Burdet, "Design and evaluation of a modular robotic system for microsurgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9887-9893, doi: 10.1109/ICRA57147.2024.10610598.Abstract: The manipulation of instruments under a microscope suffers from physiological tremor and human errors, which are inevitable in long microsurgery interventions. Robotic systems developed in recent years for microsurgery are expensive and not flexible, as they cannot use standard instruments, and need the surgeon to modify their operative skills and strategies. In this paper, we introduce a modular robotic system for microsurgery enabling the surgeon to operate using conventional instruments. Our system was implemented using a commercial Kinova robot and a dedicated modular end-effector that uses standard microsurgery instruments. An initial teleoperation validation was carried out by eleven participants, who could successfully control the microsurgery tools to perform basic surgical movements. Furthermore, participants performed a simple anastomosis task with the robot and compared it to manual control. The results showed that robotic control is superior to manual control in simple surgical tasks and the converse in complex tasks. Participants preferred the proposed robotic system due to its user-friendliness and effort reduction. keywords: {Microscopy;Human factors;Microsurgery;Physiology;End effectors;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610598&isnumber=10609862

S. Inagaki, A. Alikhani, N. Navab, M. Maier and M. A. Nasseri, "Analyzing Accessibility in Robot-Assisted Vitreoretinal Surgery: Integrating Eye Posture and Robot Position," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9894-9900, doi: 10.1109/ICRA57147.2024.10611482.Abstract: Several robotic frameworks have been recently developed to assist ophthalmic surgeons in performing complex vitreoretinal procedures such as subretinal injection. However, in order to intuitively integrate robots into the surgical workflow, it is crucial to emphasize that an accessibility analysis framework for vitreoretinal surgery must be considered as an essential component. Such a framework, ideally, considers the comprehensive factors of the eye anatomy and its positioning, the insertion point, and the initial pose and position of the robot. By combining the mobilization of the eyeball and adjusting the pose and position of the robot, the accessibility of such systems is significantly optimized. At the same time, the accessible-visible area is better and faster matched to the working volume of the robot. This paper presents an analysis of an expansion strategy for the robot’s accessibility and visibility area. The outcomes of this method demonstrate the promising potential to enhance the robot’s accessibility, as evidenced in our analytical and experimental findings from 22.4% to 99.0% of the required working area on an adjustable phantom model. keywords: {Analytical models;Medical robotics;Robot kinematics;Simulation;Phantoms;Microsurgery;Anatomy;Medical Robots and Systems;Surgical Robotics: Planning;Robot-Assisted Microsurgery},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611482&isnumber=10609862

H. Zhang, L. Niu, A. Clark and R. Poovendran, "Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9901-9907, doi: 10.1109/ICRA57147.2024.10610491.Abstract: Safety is a fundamental requirement of many robotic systems. Control barrier function (CBF)-based approaches have been proposed to guarantee the safety of robotic systems. However, the effectiveness of these approaches highly relies on the choice of CBFs. Inspired by the universal approximation power of neural networks, there is a growing trend toward representing CBFs using neural networks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however, are trained and deployed in benign environments, making them ineffective for scenarios where robotic systems experience sensor faults and attacks. In this paper, we study safety-critical control synthesis for robotic systems under sensor faults and attacks. Our main contribution is the development and synthesis of a new class of CBFs that we term fault tolerant neural control barrier function (FT-NCBF). We derive the necessary and sufficient conditions for FT-NCBFs to guarantee safety, and develop a data-driven method to learn FT-NCBFs by minimizing a loss function constructed using the derived conditions. Using the learned FT-NCBF, we synthesize a control input and formally prove the safety guarantee provided by our approach. We demonstrate our proposed approach using two case studies: obstacle avoidance problem for an autonomous mobile robot and spacecraft rendezvous problem, with code available via https://github.com/HongchaoZhang-HZ/FTNCBF. keywords: {Space vehicles;Sufficient conditions;Fault tolerance;Neural networks;Fault tolerant systems;Robot sensing systems;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610491&isnumber=10609862

P. Solanki, J. J. van Beers, A. Jamshidnejad and C. C. de Visser, "A novel algorithmic approach to obtaining maneuverable control-invariant sets *," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9916-9922, doi: 10.1109/ICRA57147.2024.10610609.Abstract: Ensuring safety in autonomous systems is essential as they become more integrated with modern society. One way to accomplish this is to identify and maintain a safe operating space. To this end, much effort has been devoted in the field of reachability analysis to obtaining control-invariant sets which ensure that a system inside of these sets can remain in these sets, and are thus essential for guaranteeing a system’s safety. However, control invariance does not imply that a system can move from any state in the control-invariant set to any other state in the control-invariant set, within a given time horizon. In this paper, we develop an algorithm to obtain a control-invariant set that allows a given system to move from any state in the set to any other state in the set within a given time horizon without having to leave the set. We call this the ‘maneuver set’, $\mathcal{M}$. We substantiate the algorithm’s efficacy through mathematical proof, affirming that the maneuver set obtained through the algorithm is indeed control-invariant. Furthermore, we prove that the system is indeed able to move from any state within this set to any other state in the set. To illustrate the use of our algorithm, we provide the numerical example of a Dubins car, utilising Hamilton-Jacobi-Bellman reachability analysis along with the proposed algorithm in order to obtain $\mathcal{M}$. keywords: {Autonomous systems;Aerospace electronics;Probabilistic logic;Safety;Automobiles;Reachability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610609&isnumber=10609862

Y. Zhang, F. Liu, X. Liang and M. Yip, "Achieving Autonomous Cloth Manipulation with Optimal Control via Differentiable Physics-Aware Regularization and Safety Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9931-9938, doi: 10.1109/ICRA57147.2024.10611111.Abstract: Cloth manipulation is a category of deformable object manipulation of great interest to the robotics community, from applications of automated laundry-folding and home organizing to textiles and flexible manufacturing. Despite the desire for automated cloth manipulation, the thin-shell dynamics and under-actuation nature of cloth present significant challenges for robots to effectively interact with them. Many recent works omit explicit modeling in favor of learning-based methods that may yield control policies directly. However, these methods require large training sets that must be collected and curated. In this regard, we create a framework for differentiable modeling of cloth dynamics leveraging an Extended Position-based Dynamics (XPBD) algorithm. Together with the desired control objective, physics-aware regularization terms are designed for better results, including trajectory smoothness and elastic potential energy. In addition, safety constraints, such as avoiding obstacles, can be specified using signed distance functions (SDFs). We formulate the cloth manipulation task with safety constraints as a constrained optimization problem, which can be effectively solved by mainstream gradient-based optimizers thanks to the end-to-end differentiability of our framework. Finally, we assess the framework with various safety thresholds and demonstrate the feasibility of result trajectories on a surgical robot. The effects of the regularization terms are analyzed in an additional ablation study. keywords: {Training;Potential energy;Medical robotics;Heuristic algorithms;Optimal control;Safety;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611111&isnumber=10609862

N. Rober et al., "Online Data-Driven Safety Certification for Systems Subject to Unknown Disturbances," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9939-9945, doi: 10.1109/ICRA57147.2024.10610163.Abstract: Deploying autonomous systems in safety critical settings necessitates methods to verify their safety properties. This is challenging because real-world systems may be subject to disturbances that affect their performance, but are unknown a priori. This work develops a safety-verification strategy wherein data is collected online and incorporated into a reachability analysis approach to check in real-time that the system avoids dangerous regions of the state space. Specifically, we employ an optimization-based moving horizon estimator (MHE) to characterize the disturbance affecting the system, which is incorporated into an online reachability calculation. Reachable sets are calculated using a computational graph analysis tool to predict the possible future states of the system and verify that they satisfy safety constraints. We include theoretical arguments proving our approach generates reachable sets that bound the future states of the system, as well as numerical results demonstrating how it can be used for safety verification. Finally, we present results from hardware experiments demonstrating our approach’s ability to perform online reachability calculations for an unmanned surface vehicle subject to currents and actuator failures. keywords: {System dynamics;Estimation;Real-time systems;Hardware;Safety;Reachability analysis;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610163&isnumber=10609862

B. Weng, G. A. Castillo, Y. -S. Kang and A. Hereid, "Towards Standardized Disturbance Rejection Testing of Legged Robot Locomotion with Linear Impactor: A Preliminary Study, Observations, and Implications," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9946-9952, doi: 10.1109/ICRA57147.2024.10610064.Abstract: Dynamic locomotion in legged robots is close to industrial collaboration, but a lack of standardized testing obstructs commercialization. The issues are not merely political, theoretical, or algorithmic but also physical, indicating limited studies and comprehension regarding standard testing infrastructure and equipment. For decades, the approaches we have been testing legged robots were rarely standardizable with hand-pushing, foot-kicking, rope-dragging, stick-poking, and ball-swinging. This paper aims to bridge the gap by proposing the use of the linear impactor, a well-established tool in other standardized testing disciplines, to serve as an adaptive, repeatable, and fair disturbance rejection testing equipment for legged robots. A pneumatic linear impactor is also adopted for the case study involving the humanoid robot Digit. Three locomotion controllers are examined, including a commercial one, using a walking-in-place task against frontal impacts. The statistically best controller was able to withstand the impact momentum (26.376 kg • m/s) on par with a reported average effective momentum from straight punches by Olympic boxers (26.506kg•m/s). Moreover, the case study highlights other anti-intuitive observations, demonstrations, and implications that, to the best of the authors’ knowledge, are first-of-its-kind revealed in real-world testing of legged robots. keywords: {Legged locomotion;Service robots;Heuristic algorithms;Humanoid robots;Collaboration;Pneumatic systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610064&isnumber=10609862

A. Gupta, K. Chakraborty and S. Bansal, "Detecting and Mitigating System-Level Anomalies of Vision-Based Controllers," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9953-9959, doi: 10.1109/ICRA57147.2024.10611397.Abstract: Autonomous systems, such as self-driving cars and drones, have made significant strides in recent years by leveraging visual inputs and machine learning for decision-making and control. Despite their impressive performance, these vision-based controllers can make erroneous predictions when faced with novel or out-of-distribution inputs. Such errors can cascade to catastrophic system failures and compromise system safety. In this work, we introduce a run-time anomaly monitor to detect and mitigate such closed-loop, system-level failures. Specifically, we leverage a reachability-based framework to stress-test the vision-based controller offline and mine its system-level failures. This data is then used to train a classifier that is leveraged online to flag inputs that might cause system breakdowns. The anomaly detector highlights issues that transcend individual modules and pertain to the safety of the overall system. We also design a fallback controller that robustly handles these detected anomalies to preserve system safety. We validate the proposed approach on an autonomous aircraft taxiing system that uses a vision-based controller for taxiing. Our results show the efficacy of the proposed approach in identifying and handling system-level anomalies, outperforming methods such as prediction error-based detection, and ensembling, thereby enhancing the overall safety and robustness of autonomous systems. Website: phoenixrider12.github.io/FailureMitigation keywords: {Visualization;Autonomous systems;Electric breakdown;Decision making;Machine learning;Detectors;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611397&isnumber=10609862

R. K. Cosner, I. Sadalski, J. K. Woo, P. Culbertson and A. D. Ames, "Generative Modeling of Residuals for Real-Time Risk-Sensitive Safety with Discrete-Time Control Barrier Functions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. i-viii, doi: 10.1109/ICRA57147.2024.10611355.Abstract: A key source of brittleness for robotic systems is the presence of model uncertainty and external disturbances. Most existing approaches to robust control either seek to bound the worst-case disturbance (which results in conservative behavior), or to learn a deterministic dynamics model (which is unable to capture uncertain dynamics or disturbances). This work proposes a different approach: training a state-conditioned generative model to represent the distribution of error residuals between the nominal dynamics and the actual system. In particular we introduce the Online Risk-Informed Optimization controller (ORIO), which uses Discrete-Time Control Barrier Functions, combined with a learned, generative disturbance model, to ensure the safety of the system up to some level of risk. We demonstrate our approach in simulations and hardware, and show that our method can learn a disturbance model that is accurate enough to enable risk-sensitive control of a quadrotor flying aggressively with an unmodelled slung load. We use a conditional variational autoencoder (CVAE) to learn a state-conditioned dynamics residual distribution, and find that the resulting controller can run at 100Hz on an embedded computer and exhibits less conservative behavior while retaining theoretical safety properties. keywords: {Training;Uncertainty;Computational modeling;Stochastic systems;Real-time systems;Data models;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611355&isnumber=10609862

T. Hansen and A. Birk, "An Open-Source Solution for Fast and Accurate Underwater Mapping with a Low-Cost Mechanical Scanning Sonar," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9968-9975, doi: 10.1109/ICRA57147.2024.10609976.Abstract: An open-source software framework is presented that allows real-time underwater mapping with popular marine robotics components, namely a BlueRobotics BlueROV2 with its standard Ping360 Mechanical Scanning Sonar (MSS) and a A50 Doppler Velocity Log (DVL), which are low-cost devices for their respective types - if not even the most affordable ones on the market. The software runs with low computational power on a Raspberry Pi4. The framework builds upon Synthetic Scan Formation (SSF) where single MSS beams or scan-lines are embedded into a pose-graph. The rendering of scans is not only based on navigation, but based on the graph itself. Scans formed from scan-lines can be optimized by online Simultaneous Localization and Mapping (SLAM) and result in improved scans, based on the current state of the graph. In subsequent steps this leads to improved registration results. To this end, a combination of two different types of loop-closures is presented. Namely a consecutive loop closure, and a proximity based loop closure, which reduces the overall drift. The framework is validated in three different test-environments, namely a pool, a test-tank with a gantry for ground truth motion, and the flooded basement of a WW-II submarine bunker. Among others, it is shown that there is an increased accuracy compared to conventional SLAM and that the software is usable in real-time during a mission with the low-cost hardware. keywords: {Simultaneous localization and mapping;Accuracy;Navigation;Sonar;Rendering (computer graphics);Real-time systems;Underwater vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609976&isnumber=10609862

A. T. Espinoza, A. T. Espinoza, J. Folkesson, P. Sigray and J. Kuttenkeuler, "Boundary Factors for Seamless State Estimation between Autonomous Underwater Docking Phases," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9976-9982, doi: 10.1109/ICRA57147.2024.10611552.Abstract: Autonomous underwater docking is of the utmost importance for expanding the capabilities of Autonomous Underwater Vehicles (AUVs). Due to a historical focus on underwater docking to only static targets, the research gap in underwater docking to dynamically active targets has been left relatively untouched. We address the state estimation problem that arises when trying to rendezvous a chaser AUV with a dynamic target by modeling the scenario as a factor graph optimization-based Simultaneous Localization and Mapping problem. We present a set of boundary factors that aid the inference process by seamlessly transitioning the target’s state between the different observability stages, intrinsic to any dynamic docking scenario. We benchmark the performance of our approach using the Stonefish simulated environment. keywords: {Autonomous underwater vehicles;Simultaneous localization and mapping;Benchmark testing;Vehicle dynamics;State estimation;Robotics and automation;Observability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611552&isnumber=10609862

C. H. Schiller, D. Maas, B. Arsenali, J. Peltola, K. Tervo and S. Maranò, "Vision-Based Water Clearance Determination in Maritime Environment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9983-9989, doi: 10.1109/ICRA57147.2024.10610182.Abstract: Determining the distances from the hull of the own ship to obstacles or land, i.e. water clearance, is a fundamental task in navigation. This is particularly relevant during maneuvering in the harbor or navigating in confined waters. We introduce the concepts of area water clearance and line water clearance. Area water clearance is important especially for path planning and obstacle avoidance. Line water clearance is critical for maneuvering when approaching the quay.In this work, we present a vision-based approach to determine the water clearance. A single calibrated camera together with a semantic segmentation network is used to detect the water region in an image, and back-projection to determine the water clearance on the sea surface in world units.We validate the proposed approach on real data collected from two distinct vessels, where the proposed method is able to produce reliable water clearance for distances beyond one kilometer. During harbor maneuvering 90% of the relative water clearance errors were found to be between −2.3% and 3%. keywords: {Sea surface;Navigation;Semantic segmentation;Receivers;Cameras;Path planning;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610182&isnumber=10609862

C. Knutson, Z. Cao and J. Sattar, "Adaptive Landmark Color for AUV Docking in Visually Dynamic Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 9990-9996, doi: 10.1109/ICRA57147.2024.10611083.Abstract: Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the need for human intervention. A docking station (DS) can extend mission times of an AUV by providing a location for the AUV to recharge its batteries and receive updated mission information. Various methods for locating and tracking a DS exist, but most rely on expensive acoustic sensors, or are vision-based, which is significantly affected by water quality. In this paper, we present a vision-based method that utilizes adaptive color LED markers and dynamic color filtering to maximize landmark visibility in varying water conditions. Both AUV and DS utilize cameras to determine the water background color in order to calculate the desired marker color. No communication between AUV and DS is needed to determine marker color. Experiments conducted in a pool and lake show our method performs 10 times better than static color thresholding methods as background color varies. DS detection is possible at a range of 5 meters in clear water with minimal false positives. keywords: {Meters;Image color analysis;Filtering;Prototypes;Water quality;Lakes;Light emitting diodes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611083&isnumber=10609862

M. Singh, M. Dharmadhikari and K. Alexis, "An Online Self-calibrating Refractive Camera Model with Application to Underwater Odometry," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10005-10011, doi: 10.1109/ICRA57147.2024.10610643.Abstract: This work presents a camera model for refractive media such as water and its application in underwater visual-inertial odometry. The model is self-calibrating in real-time and is free of known correspondences or calibration targets. It is separable as a distortion model (dependent on refractive index n and radial pixel coordinate) and a virtual pinhole model (as a function of n). We derive the self-calibration formulation leveraging epipolar constraints to estimate the refractive index and subsequently correct for distortion. Through experimental studies using an underwater robot integrating cameras and inertial sensing, the model is validated regarding the accurate estimation of the refractive index and its benefits for robust odometry estimation in an extended envelope of conditions. Lastly, we show the transition between media and the estimation of the varying refractive index online, thus allowing computer vision tasks across refractive media. keywords: {Autonomous underwater vehicles;Computational modeling;Robot vision systems;Refractive index;Estimation;Media;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610643&isnumber=10609862

B. Joshi and I. Rekleitis, "Enhancing Visual Inertial SLAM with Magnetic Measurements," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10012-10019, doi: 10.1109/ICRA57147.2024.10611341.Abstract: This paper presents an extension to visual inertial odometry (VIO) by introducing tightly-coupled fusion of magnetometer measurements. A sliding window of keyframes is optimized by minimizing re-projection errors, relative inertial errors, and relative magnetometer orientation errors. The results of IMU orientation propagation are used to efficiently transform magnetometer measurements between frames producing relative orientation constraints between consecutive frames. The soft and hard iron effects are calibrated using an ellipsoid fitting algorithm. The introduction of magnetometer data results in significant reductions in the orientation error and also in recovery of the true yaw orientation with respect to the magnetic north. The proposed framework operates in all environments with slow-varying magnetic fields, mainly outdoors and underwater. We have focused our work on the underwater domain, especially in underwater caves, as the narrow passage and turbulent flow make it difficult to perform loop closures and reset the localization drift. The underwater caves present challenges to VIO due to the absence of ambient light and the confined nature of the environment, while also being a crucial source of fresh water and providing valuable historical records. Experimental results from underwater caves demonstrate the improvements in accuracy and robustness introduced by the proposed VIO extension. keywords: {Underwater structures;Visualization;Magnetic field measurement;Accuracy;Magnetometers;Magnetic confinement;Magnetic domains},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611341&isnumber=10609862

A. J. Oliveira, B. M. Ferreira and N. A. Cruz, "Underwater Volumetric Mapping using Imaging Sonar and Free-Space Modeling Approach," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10020-10026, doi: 10.1109/ICRA57147.2024.10611082.Abstract: Lack of information and perceptual ambiguity are key problems in sonar-based mapping applications. We propose a technique for mapping of underwater environments, building on the finite, positive, sonar beamwidth. Our approach models the free-space covered by each emitted acoustic pulse, employing volumetric techniques to create grid-based submaps of the unoccupied water volumes through images collected from imaging sonars. A representation of the occupied space is obtained by exploration of the free-space frontier. Special attention is given to acoustic image preparation and segmentation. Experimental results are provided based on real data collected from a dam shaft scenario. keywords: {Shafts;Solid modeling;Simultaneous localization and mapping;Uncertainty;Dams;Buildings;Sonar},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611082&isnumber=10609862

T. Mitroudas, V. Balaska, A. Psomoulis and A. Gasteratos, "Light-weight approach for safe landing in populated areas," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10027-10032, doi: 10.1109/ICRA57147.2024.10611639.Abstract: Landing safety is a challenge heavily engaging the research community recently, due to the increasing interest in applications availed by aerial vehicles. In this paper, we propose a landing safety pipeline based on state of the art object detectors and OctoMap. First, a point cloud of surface obstacles is generated, which is then inserted in an OctoMap. The unoccupied areas are identified, thus resulting to a sum of safe landing points. Due to the low processing time achieved by state of the art object detectors and the efficient point cloud manipulation using OctoMap, it is feasible for our approach to deploy on low-weight embedded systems. The proposed pipeline has been evaluated in many simulation scenarios, varying in people density, number, and movement. Simulations were executed with an Nvidia Jetson Nano in the loop to confirm the pipeline’s performance and robustness in a low computing power hardware. The experiments yielded promising results with a 87% success rate. keywords: {Point cloud compression;Embedded systems;Pipelines;Detectors;Robustness;Real-time systems;Hardware;Safe Landing;Object Detectors;OctoMap;Point Cloud;PX4;UAV},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611639&isnumber=10609862

Y. S. Shao, Y. Wu, L. Jarin-Lipschitz, P. Chaudhari and V. Kumar, "Design and Evaluation of Motion Planners for Quadrotors in Environments with Varying Complexities," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10033-10039, doi: 10.1109/ICRA57147.2024.10610207.Abstract: Motion planning techniques for quadrotors have advanced significantly over the past decade. Most successful planners have two stages: a front-end that determines a path that incorporates geometric (or kinematic or input) constraints and specifies the homotopy class of the trajectory, and a back-end that optimizes this path to respect dynamics and input constraints. While there are many different choices for each stage, the eventual performance depends critically not only on these choices, but also on the environment. Given a new environment, it is difficult to decide a priori how one should design a motion planner. In this work, we develop (i) a procedure to construct parametrized environments, (ii) metrics that characterize the difficulty of motion planning in these environments, and (iii) an open-source software stack that can be used to combine a wide variety of two-stage planners seamlessly. We perform experiments in simulations and a real platform. We find, somewhat conveniently, that geometric front-ends are sufficient for environments with varying complexities if combined with dynamics-aware backends. The metrics we designed faithfully capture the planning difficulty in a given environment. All code is available at https://github.com/KumarRobotics/kr_mp_design. keywords: {Measurement;Navigation;Software algorithms;Kinematics;Planning;Complexity theory;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610207&isnumber=10609862

F. Panetsos, G. C. Karras and K. J. Kyriakopoulos, "An NMPC Framework for Tracking and Releasing a Cable-suspended Load to a Ground Target Using a Multirotor UAV," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10057-10063, doi: 10.1109/ICRA57147.2024.10610034.Abstract: In this work, we present a nonlinear Model Predictive Control (NMPC) scheme for tracking a ground target using a multirotor with a cable-suspended load. The NMPC framework relies on the dynamic model of the UAV with the suspended load and, hence, an estimate of the load state is obtained by fusing the measurements of a downward-facing camera and a load cell with an Unscented Kalman Filter (UKF). Additionally, since the NMPC relies on the future behavior of the system, the trajectory of the ground target throughout the predicted time horizon of the NMPC, is required. Towards this direction, Bézier curves are employed in order to predict the future trajectory of the target, which moves in an arbitrary way. The ultimate goal of the proposed framework is to release the suspended load to the ground target and, consequently, a condition is checked at each time instant that triggers the opening of a gripper, located at the lower edge of the cable. The performance of the proposed control scheme is experimentally validated using an octorotor. keywords: {Target tracking;Autonomous aerial vehicles;Cameras;Robot sensing systems;Trajectory;Grippers;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610034&isnumber=10609862

M. -N. Stamatopoulos, A. Banerjee and G. Nikolakopoulos, "On Experimental Emulation of Printability and Fleet Aware Generic Mesh Decomposition for Enabling Aerial 3D Printing," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10080-10086, doi: 10.1109/ICRA57147.2024.10610806.Abstract: This article introduces an experimental emulation of a novel chunk-based flexible multi-DoF aerial 3D printing framework. The experimental demonstration of the overall autonomy focuses on precise motion planning and task allocation for a UAV, traversing through a series of planned space-filling paths involved in the aerial 3D printing process without physically depositing the overlaying material. The flexible multi-DoF aerial 3D printing is a newly developed framework and has the potential to strategically distribute the envisioned 3D model to be printed into small, manageable chunks suitable for distributed 3D printing. Moreover, by harnessing the dexterous flexibility due to the 6 DoF motion of UAV, the framework enables the provision of integrating the overall autonomy stack, potentially opening up an entirely new frontier in additive manufacturing. However, it’s essential to note that the feasibility of this pioneering concept is still in its very early stage of development, which yet needs to be experimentally verified. Towards this direction, experimental emulation serves as the crucial stepping stone, providing a pseudo mockup scenario by virtual material deposition, helping to identify technological gaps from simulation to reality. Experimental emulation results, supported by critical analysis and discussion, lay the foundation for addressing the technological and research challenges to significantly push the boundaries of the state-of-the-art 3D printing mechanism. - Full mission video available at https://youtu.be/gfZuYCA8jAw keywords: {Solid modeling;Three-dimensional displays;Coordinate measuring machines;Emulation;Three-dimensional printing;Autonomous aerial vehicles;Motion measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610806&isnumber=10609862

R. Takemura and G. Ishigami, "Perception-and-Energy-aware Motion Planning for UAV using Learning-based Model under Heteroscedastic Uncertainty," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10103-10109, doi: 10.1109/ICRA57147.2024.10610390.Abstract: Global navigation satellite systems (GNSS) denied environments/conditions require unmanned aerial vehicles (UAVs) to energy-efficiently and reliably fly. To this end, this study presents perception-and-energy-aware motion planning for UAVs in GNSS-denied environments. The proposed planner solves the trajectory planning problem by optimizing a cost function consisting of two indices: the total energy consumption of a UAV and the perception quality of light detection and ranging (LiDAR) sensor mounted on the UAV. Before online navigation, a high-fidelity simulator acquires a flight dataset to learn energy consumption for the UAV and heteroscedastic uncertainty associated with LiDAR measurements, both as functions of the horizontal velocity of the UAV. The learned models enable the online planner to estimate energy consumption and perception quality, reducing UAV battery usage and localization errors. Simulation experiments in a photorealistic environment confirm that the proposed planner can address the trade-off between energy efficiency and perception quality under heteroscedastic uncertainty. The open-source code is released at https://gitlab.com/ReI08/perception-energy-planner. keywords: {Energy consumption;Global navigation satellite system;Uncertainty;Laser radar;Accuracy;Measurement uncertainty;Autonomous aerial vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610390&isnumber=10609862

A. Garzelli, K. D. Yadav, A. Scalvini, A. Gonzalez-Morgado, A. Suarez and A. Ollero, "Representing On-Orbit Rendezvous and Proximity Operations with Fully-Actuated Multirotor Aerial Platforms," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10110-10117, doi: 10.1109/ICRA57147.2024.10610964.Abstract: Ground testing is of paramount importance to verify and validate space operations and the associated control algorithms before on-orbit deployment. Although state-of-the-art facilities are capable of reproducing zero-G environment with high degree of fidelity, these infrastructures can be complemented with multi-rotors emulating free flying or free floating conditions, exploiting the similarities and analogies between both domains in terms of floating nature, attitude dynamics, and thrust-wrench relation through the mixer matrix. Furthermore, the effective workspace of the testbed can be extended to the dimensions of the flight area and the coverage of the positioning system. Therefore, this papers introduces a new way to recreate orbital motion within an indoor facility, considering the case study of trajectories derived from the Clohessy–Wiltshire equations. This advancement opens up avenues for replicating close-proximity operations between chaser and target satellites employing fully-actuated multi-rotors that allow decoupling translational and attitude dynamics. keywords: {Space vehicles;Satellites;Heuristic algorithms;Dynamics;Aerospace electronics;Orbits;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610964&isnumber=10609862

E. Cereda, M. Rusci, A. Giusti and D. Palossi, "On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10118-10124, doi: 10.1109/ICRA57147.2024.10610317.Abstract: Sub-50g nano-drones are gaining momentum in both academia and industry. Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (i.e., sub-100mW processor). When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift. To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to self-supervised finetuning of a pre-trained convolutional neural network (CNN). Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the fine-tuning phase along three axes: i) dataset size (more data increases the regression performance but requires more memory and longer computation); ii) methodologies (e.g., fine-tuning all model parameters vs. only a subset); and iii) self-supervision strategy. Our approach demonstrates an improvement in mean absolute error up to 30% compared to the pre-trained baseline, requiring only 22s fine-tuning on an ultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem via on-device learning aboard nano-drones not only marks a novel result for hardware-limited robots but lays the ground for more general advancements for the entire robotics community. keywords: {Industries;Memory management;Training data;Self-supervised learning;Data models;System-on-chip;Convolutional neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610317&isnumber=10609862

J. Kim, G. Koo, H. Park and N. Doh, "Visual Localization in Repetitive and Symmetric Indoor Parking Lots using 3D Key Text Graph," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10185-10191, doi: 10.1109/ICRA57147.2024.10611591.Abstract: Indoor parking lots are the GPS-denied spaces to which vision-based localization approaches have usually been applied to solve localization problems. However, due to the repetitiveness and symmetry of the spaces, visual localization methods commonly confront difficulties in estimating precise 3D poses. In this study, we propose four novel modules that improve localization precision by imposing the existing methods with the spatial discerning ability. The first module constructs a key text graph that represents the topology of key texts in the space and becomes the basis for discerning repetitiveness and symmetry. Next, the orientation filtering module estimates the unknown 3D orientation of the query image and resolves spatial symmetric ambiguity. The similarity scoring module sorts out the top-scored database images, discerning the spatial repetitiveness based on detected key text bounding boxes. Our pose verification module evaluates the pose confidence of top-scored candidates and determines the most reliable pose. Our method has been validated in two real indoor parking lots, achieving new state-of-the-art performance levels. keywords: {Location awareness;Visualization;Three-dimensional displays;Filtering;Image retrieval;Spatial databases;Topology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611591&isnumber=10609862

X. Cai, Y. Wang, Z. Huang, Y. Shao and D. Li, "VOLoc: Visual Place Recognition by Querying Compressed Lidar Map," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10192-10199, doi: 10.1109/ICRA57147.2024.10610530.Abstract: The availability of city-scale Lidar maps enables the potential of city-scale place recognition using mobile cameras. However, the city-scale Lidar maps generally need to be compressed for storage efficiency, which increases the difficulty of direct visual place recognition in compressed Lidar maps. This paper proposes VOLoc, an accurate and efficient visual place recognition method that exploits geometric similarity to directly query the compressed Lidar map via the real-time captured image sequence. In the offline phase, VOLoc compresses the Lidar maps using a Geometry-Preserving Compressor (GPC), in which the compression is reversible, a crucial requirement for the downstream 6DoF pose estimation. In the online phase, VOLoc proposes an online Geometric Recovery Module (GRM), which is composed of online Visual Odometry (VO) and a point cloud optimization module, such that the local scene structure around the camera is online recovered to build the Querying Point Cloud (QPC). Then the QPC is compressed by the same GPC, and is aggregated into a global descriptor by an attentionbased aggregation module, to query the compressed Lidar map in the vector space. A transfer learning mechanism is also proposed to improve the accuracy and the generality of the aggregation network. Extensive evaluations show that VOLoc provides localization accuracy even better than the Lidar-toLidar place recognition, setting up a new record for utilizing the compressed Lidar map by low-end mobile cameras. The code are publicly available at https://github.com/Master-cai/VOLoc. keywords: {Point cloud compression;Visualization;Laser radar;Image coding;Image recognition;Accuracy;Transfer learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610530&isnumber=10609862

A. D. Hines, P. G. Stratton, M. Milford and T. Fischer, "VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10200-10207, doi: 10.1109/ICRA57147.2024.10610918.Abstract: Spiking Neural Networks (SNNs) are at the forefront of neuromorphic computing thanks to their potential energy-efficiency, low latencies, and capacity for continual learning. While these capabilities are well suited for robotics tasks, SNNs have seen limited adaptation in this field thus far. This work introduces a SNN for Visual Place Recognition (VPR) that is both trainable within minutes and queryable in milliseconds, making it well suited for deployment on compute-constrained robotic systems. Our proposed system, VPRTempo, overcomes slow training and inference times using an abstracted SNN that trades biological realism for efficiency. VPRTempo employs a temporal code that determines the timing of a single spike based on a pixel’s intensity, as opposed to prior SNNs relying on rate coding that determined the number of spikes; improving spike efficiency by over 100%. VPRTempo is trained using Spike-Timing Dependent Plasticity and a supervised delta learning rule enforcing that each output spiking neuron responds to just a single place. We evaluate our system on the Nordland and Oxford RobotCar benchmark localization datasets, which include up to 27k places. We found that VPRTempo’s accuracy is comparable to prior SNNs and the popular NetVLAD place recognition algorithm, while being several orders of magnitude faster and suitable for real-time deployment – with inference speeds over 50 Hz on CPU. VPRTempo could be integrated as a loop closure component for online SLAM on resource-constrained systems such as space and underwater robots. keywords: {Training;Visualization;Simultaneous localization and mapping;Neuromorphic engineering;Neurons;Spiking neural networks;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610918&isnumber=10609862

C. Xie, R. Xing, N. Hao and F. He, "17-Point Algorithm Revisited: Toward a More Accurate Way," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10208-10214, doi: 10.1109/ICRA57147.2024.10611149.Abstract: 17-point algorithm is a popular method in relative pose estimation of multi-cameras. However, the role of overlap in 17-point algorithm remains unexplored. And the relaxed way in solving constrained normal equation leads to sub-optimal results. Both of them influence accuracy of the estimated pose. In this paper, we theoretically analyze the influence of overlap and the solvability of 17-point algorithm. In addition, we show that the abuse of overlap can harm accuracy in practice. In light of these findings, we propose an improved 17-point algorithm, which avoids using overlaps and derives a simple way to solve normal equation on manifold. Both simulations and real world data experiments demonstrate the proposed one outperforms the traditional 17-point algorithm in term of accuracy. keywords: {Manifolds;Accuracy;Pose estimation;Cameras;Mathematical models;Data models;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611149&isnumber=10609862

A. Wilhelm and N. Napp, "Lightweight Ground Texture Localization," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10223-10229, doi: 10.1109/ICRA57147.2024.10611712.Abstract: We present a lightweight ground texture based localization algorithm (L-GROUT) that improves the state of the art in performance and can be run in real-time on single board computers without GPU acceleration. Such computers are ubiquitous on small indoor robots and thus this work enables high-precision, millimeter-level localization without instrumenting, marking, or modifying the environment. The key innovations are an improved database feature extraction algorithm, a dimensionality reduction method based on locality preserving projections (LPP) that can accommodate faster-to-compute binary features, and an improved spatial filtering step that better preserves performance when the databases are tuned for lightweight applications. We demonstrate the approach by running the whole system on a low-cost single board computer (Raspberry Pi 4) to produce global localization estimates at greater than 4Hz on an outdoor asphalt dataset. keywords: {Location awareness;Computers;Technological innovation;Asphalt;Accuracy;Instruments;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611712&isnumber=10609862

S. Katragadda et al., "NeRF-VINS: A Real-time Neural Radiance Field Map-based Visual-Inertial Navigation System," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10230-10237, doi: 10.1109/ICRA57147.2024.10610051.Abstract: Achieving efficient and consistent localization with a prior map remains challenging in robotics. Conventional keyframe-based approaches often suffer from sub-optimal viewpoints due to limited field of view (FOV) and/or constrained motion, thus degrading the localization performance. To address this issue, we design a real-time tightly-coupled Neural Radiance Fields (NeRF)-aided visual-inertial navigation system (VINS). In particular, by effectively leveraging the NeRF’s potential to synthesize novel views, the proposed NeRF-VINS overcomes the limitations of traditional keyframe-based maps (with limited views) and optimally fuses IMU, monocular images, and synthetically rendered images within an efficient filter-based framework. This tightly-coupled fusion enables efficient 3D motion tracking with bounded errors. We extensively validate the proposed NeRF-VINS against the state-of-the-art methods that use prior map information, and demonstrate its ability to perform real-time localization, at 15 Hz, on a resource-constrained Jetson AGX Orin embedded platform. keywords: {Location awareness;Three-dimensional displays;Navigation;Tracking;Fuses;Neural radiance field;Information filters},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610051&isnumber=10609862

T. Gao, M. Zhao, C. Xu and H. Kong, "Night-Rider: Nocturnal Vision-aided Localization in Streetlight Maps Using Invariant Extended Kalman Filtering," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10238-10244, doi: 10.1109/ICRA57147.2024.10611408.Abstract: Vision-aided localization for low-cost mobile robots in diverse environments has attracted widespread attention recently. Although many current systems are applicable in daytime environments, nocturnal visual localization is still an open problem owing to the lack of stable visual information. An insight from most nocturnal scenes is that the static and bright streetlights are reliable visual information for localization. Hence we propose a nocturnal vision-aided localization system in streetlight maps with a novel data association and matching scheme using object detection methods. We leverage the Invariant Extended Kalman Filter (InEKF) to fuse IMU, odometer, and camera measurements for consistent state estimation at night. Furthermore, a tracking recovery module is also designed for tracking failures. Experimental results indicate that our proposed system achieves accurate and robust localization with less than 0.2% relative error of trajectory length in four nocturnal environments. keywords: {Location awareness;Visualization;Accuracy;Odometers;Cameras;Sensor systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611408&isnumber=10609862

Y. Zhuge, H. Luo, R. Chen, Y. Chen, J. Yan and Z. Jiang, "ONeK-SLAM: A Robust Object-level Dense SLAM Based on Joint Neural Radiance Fields and Keypoints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10245-10252, doi: 10.1109/ICRA57147.2024.10610068.Abstract: Neural implicit representation has recently achieved significant advancements, especially in the field of SLAM(Simultaneous Localization and Mapping). Previous NeRF-based SLAM methods have difficulties with object-level localization and reconstruction and struggle in dynamic and illumination-varied environments. We propose ONeK-SLAM, a robust object-level SLAM system that effectively combines feature points and neural radiance fields. ONeK-SLAM uses the joint information at the object level to improve localization accuracy and enhance reconstruction details. Moreover, our approach detects and eliminates dynamic objects based on the joint errors, while also harnessing the illumination invariance offered by feature points. Consequently, ONeK-SLAM achieves high-precision localization and detailed object-level mapping, even in dynamic and illumination-varying environments. Our evaluations, conducted on three public datasets that include both dynamic and variable lighting sequences, demonstrate that our method outperforms recent NeRF-based SLAM method in both localization and reconstruction. keywords: {Location awareness;Simultaneous localization and mapping;Accuracy;Lighting;Visual systems;Neural radiance field;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610068&isnumber=10609862

B. Jiang and S. Shen, "A Two-step Nonlinear Factor Sparsification for Scalable Long-term SLAM Backend," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10253-10259, doi: 10.1109/ICRA57147.2024.10610889.Abstract: This paper proposes a new nonlinear factor sparsification paradigm for general feature-based long-term SLAM backend. Given a pose sparsification policy, we aim to scale the SLAM problem with space explored instead of time in a principled way so that the number of time-indexed poses can be limited. At the same time, their influence and the long-lived landmarks are appropriately maintained. To do this, we propose a new two-step sparsification pipeline. Given a pose node to remove, the first step is performed in the Markov blankets of affected landmarks. It transforms pose-landmark constraints into pose-pose constraints while preserving observability and minimizing information loss in the blanket. Moreover, since landmarks are conditionally independent, we can do this in parallel, disconnecting a pose from all the landmarks. The second step marginalizes the pose of interest with pure pose-wise constraints without affecting landmarks. Our method decouples the management of landmarks from pose-only measurements, making it general for any feature-based SLAM. We also give a practical example of how our backend works by concatenating it to a monocular VIO frontend. In simulation and realworld dataset, our sparsified backend is accurate and efficient. We open-source our backend, along with the VIO+Backend example, to contribute to the community’s betterment. keywords: {Location awareness;Simultaneous localization and mapping;Accuracy;Pipelines;Transforms;Space exploration;Topology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610889&isnumber=10609862

S. Gupta, T. Guadagnino, B. Mersch, I. Vizzo and C. Stachniss, "Effectively Detecting Loop Closures using Point Cloud Density Maps," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10260-10266, doi: 10.1109/ICRA57147.2024.10610962.Abstract: The ability to detect loop closures plays an essential role in any SLAM system. Loop closures allow correcting the drifting pose estimates from a sensor odometry pipeline. In this paper, we address the problem of effectively detecting loop closures in LiDAR SLAM systems in various environments with longer lengths of sequences and agnostic of the scanning pattern of the sensor. While many approaches for loop closures using 3D LiDAR sensors rely on individual scans, we propose the usage of local maps generated from locally consistent odometry estimates. Several recent approaches compute the maximum elevation map on a bird’s eye view projection of point clouds to compute feature descriptors. In contrast, we use a density image bird’s eye view representation, which is robust to viewpoint changes. The utilization of dense local maps allows us to reduce the complexity of features describing these maps, as well as the size of the database required to store these features over a long sequence. This yields a real-time application of our approach for a typical robotic 3D LiDAR sensor. We perform extensive experiments to evaluate our approach against other state-of-the-art approaches and show the benefits of our proposed approach. keywords: {Point cloud compression;Simultaneous localization and mapping;Laser radar;Three-dimensional displays;Image coding;Pipelines;Image representation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610962&isnumber=10609862

D. C. Herraez, M. Zeller, L. Chang, I. Vizzo, M. Heidingsfeld and C. Stachniss, "Radar-Only Odometry and Mapping for Autonomous Vehicles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10275-10282, doi: 10.1109/ICRA57147.2024.10610311.Abstract: Odometry and mapping play a pivotal role in the navigation of autonomous vehicles. In this paper, we address the problem of pose estimation and map creation using only radar sensors. We focus on two odometry estimation approaches followed by a mapping step. The first one is a new point-to-point ICP approach that leverages the velocity information provided by 3D radar sensors. The second one is advantageous for 2D radars with a low number of samples, and particularly useful for scenarios where the sensor is being blocked by large dynamic obstacles. It exploits a constant velocity filter and the measured Doppler velocities to estimate the vehicle’s ego-motion. We enrich this with a filtering step to improve the accuracy of the points in the resulting map. We put our work to the test using the View of Delft and NuScenes datasets, which involve 3D and 2D radar sensors. Our findings illustrate state-of-the-art performance of our odometry techniques in terms of accuracy when compared to existing alternatives. Moreover, we demonstrate that our map filtering methodology achieves higher similarity rates than the raw unfiltered map when benchmarked against a corresponding LiDAR map. keywords: {Three-dimensional displays;Accuracy;Laser radar;Radar measurements;Navigation;Doppler radar;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610311&isnumber=10609862

E. Olivastri and A. Pretto, "IPC: Incremental Probabilistic Consensus-based Consistent Set Maximization for SLAM Backends," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10283-10289, doi: 10.1109/ICRA57147.2024.10611214.Abstract: In SLAM (Simultaneous localization and mapping) problems, Pose Graph Optimization (PGO) is a technique to refine an initial estimate of a set of poses (positions and orientations) from a set of pairwise relative measurements. The optimization procedure can be negatively affected even by a single outlier measurement, with possible catastrophic and meaningless results. Although recent works on robust optimization aim to mitigate the presence of outlier measurements, robust solutions capable of handling large numbers of outliers are yet to come. This paper presents IPC, acronym for Incremental Probabilistic Consensus, a method that approximates the solution to the combinatorial problem of finding the maximally consistent set of measurements in an incremental fashion. It evaluates the consistency of each loop closure measurement through a consensus-based procedure, possibly applied to a subset of the global problem, where all previously integrated inlier measurements have veto power. We evaluated IPC on standard benchmarks against several state-of-the-art methods. Although it is simple and relatively easy to implement, IPC competes with or outperforms the other tested methods in handling outliers while providing online performances. We release with this paper an open-source implementation of the proposed method. keywords: {Simultaneous localization and mapping;Power measurement;Terminology;Current measurement;Measurement uncertainty;Position measurement;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611214&isnumber=10609862

Y. Han, Z. Long, Y. Zhang, J. Wu, Z. Fang and R. Fan, "Generalized Correspondence Matching via Flexible Hierarchical Refinement and Patch Descriptor Distillation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10290-10297, doi: 10.1109/ICRA57147.2024.10611456.Abstract: Correspondence matching plays a crucial role in numerous robotics applications. In comparison to conventional hand-crafted methods and recent data-driven approaches, there is significant interest in plug-and-play algorithms that make full use of pre-trained backbone networks for multi-scale feature extraction and leverage hierarchical refinement strategies to generate matched correspondences. The primary focus of this paper is to address the limitations of deep feature matching (DFM), a state-of-the-art (SoTA) plug-and-play correspondence matching approach. First, we eliminate the pre-defined threshold employed in the hierarchical refinement process of DFM by leveraging a more flexible nearest neighbor search strategy, thereby preventing the exclusion of repetitive yet valid matches during the early stages. Our second technical contribution is the integration of a patch descriptor, which extends the applicability of DFM to accommodate a wide range of backbone networks pre-trained across diverse computer vision tasks, including image classification, semantic segmentation, and stereo matching. Taking into account the practical applicability of our method in real-world robotics applications, we also propose a novel patch descriptor distillation strategy to further reduce the computational complexity of correspondence matching. Extensive experiments conducted on three public datasets demonstrate the superior performance of our proposed method. Specifically, it achieves an overall performance in terms of mean matching accuracy of 0.68, 0.92, and 0.95 with respect to the tolerances of 1, 3, and 5 pixels, respectively, on the HPatches dataset, outperforming all other SoTA algorithms. Our source code, demo video, and supplement are publicly available at mias.group/GCM. keywords: {Performance evaluation;Accuracy;Semantic segmentation;Source coding;Image matching;Supervised learning;Pose estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611456&isnumber=10609862

Y. Wang, C. Jiang and X. Chen, "VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10298-10304, doi: 10.1109/ICRA57147.2024.10611684.Abstract: In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency. Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment. However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points. In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment. Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects. It facilitates the creation of a 3D map that closely reflects reality. Next, we use object information to enhance the data association of feature points and consequently update the map. In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects. Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process. Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization. The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git. keywords: {Location awareness;Bundle adjustment;Visualization;Simultaneous localization and mapping;Accuracy;Object oriented modeling;Computational modeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611684&isnumber=10609862

W. Wei, J. Li, K. Huang, J. Li, X. Liu and Y. Zhou, "Lite-SVO: Towards A Lightweight Self-Supervised Semantic Visual Odometry Exploiting Multi-Feature Sharing Architecture," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10305-10311, doi: 10.1109/ICRA57147.2024.10610019.Abstract: Not relying on ground-truth data for training, self-supervised semantic visual odometry (SVO) has recently gained considerable attention. Within self-supervised SVO, feature representation inconsistency between semantic/depth and pose tasks presents a significant challenge, as it may disrupt cross-task feature representations and lead to notable performance degradation. Regrettably, existing self-supervised SVO lacks an effective solution to address this obstacle, for either overlooking this issue or exploiting a too heavy architecture. In response to this challenge, we propose a groundbreaking solution within the Single-Stream architecture, known as Lite-SVO, which is a lightweight yet efficient multi-feature sharing architecture. Lite-SVO is designed to bolster self-supervised SVO, facilitating its adoption on edge devices without compromising accuracy and performance. The crucial innovation lies in the multi-feature sharing architecture, which fuses the semantic and depth maps as pose features, thus significantly reducing the model complexity and boosting the speed in edge devices. Built upon the novel feature sharing framework, Lite-SVO further optimizes the feature sharing representation to improve the performance. Specifically, a cross-feature sharing module alleviates the impact of object boundary in depth estimation, while a multi-feature sharing module focuses on extracting and fusing spatial features to enhance pose estimation. Experimental results demonstrate that our method is at least 84.46% faster than the state-of-the-art Single-Stream approaches, and excitingly, our method’s pose accuracy is about 79.83% higher than theirs. keywords: {Performance evaluation;Training;Technological innovation;Accuracy;Fuses;Semantics;Computer architecture},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610019&isnumber=10609862

I. Mishani, H. Feddock and M. Likhachev, "Constant-time Motion Planning with Anytime Refinement for Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10337-10343, doi: 10.1109/ICRA57147.2024.10611675.Abstract: Robotic manipulators are essential for future autonomous systems, yet limited trust in their autonomy has confined them to rigid, task-specific systems. The intricate configuration space of manipulators, coupled with the challenges of obstacle avoidance and constraint satisfaction, often makes motion planning the bottleneck for achieving reliable and adaptable autonomy. Recently, a class of constant-time motion planners (CTMP) was introduced. These planners employ a preprocessing phase to compute data structures that enable online planning provably guarantee the ability to generate motion plans, potentially sub-optimal, within a user defined time bound. This framework has been demonstrated to be effective in a number of time-critical tasks. However, robotic systems often have more time allotted for planning than the online portion of CTMP requires, time that can be used to improve the solution. To this end, we propose an anytime refinement approach that works in combination with CTMP algorithms. Our proposed framework, as it operates as a constant time algorithm, rapidly generates an initial solution within a user-defined time threshold. Furthermore, functioning as an anytime algorithm, it iteratively refines the solution’s quality within the allocated time budget. This enables our approach to strike a balance between guaranteed fast plan generation and the pursuit of optimization over time. We support our approach by elucidating its analytical properties, showing the convergence of the anytime component towards optimal solutions. Additionally, we provide empirical validation through simulation and real-world demonstrations on a 6 degree-of-freedom robot manipulator, applied to an assembly domain. keywords: {Manipulators;Reliability engineering;Data structures;Real-time systems;Planning;Time factors;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611675&isnumber=10609862

K. Weerakoon, A. J. Sathyamoorthy, M. Elnoor and D. Manocha, "VAPOR: Legged Robot Navigation in Unstructured Outdoor Environments using Offline Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10344-10350, doi: 10.1109/ICRA57147.2024.10610132.Abstract: We present VAPOR, a novel method for autonomous legged robot navigation in unstructured, densely vegetated outdoor environments using offline Reinforcement Learning (RL). Our method trains a novel RL policy using an actor-critic network and arbitrary data collected in real outdoor vegetation. Our policy uses height and intensity-based cost maps derived from 3D LiDAR point clouds, a goal cost map, and processed proprioception data as state inputs, and learns the physical and geometric properties of the surrounding obstacles such as height, density, and solidity/stiffness. The fully-trained policy’s critic network is then used to evaluate the quality of dynamically feasible velocities generated from a novel contextaware planner. Our planner adapts the robot’s velocity space based on the presence of entrapment including vegetation, and narrow passages in dense environments. We demonstrate our method’s capabilities on a Spot robot in complex real-world outdoor scenes, including dense vegetation. We observe that VAPOR’s actions improve success rates by up to 40%, decrease the average current consumption by up to 2.9%, and decrease the normalized trajectory length by up to 11.2% compared to existing end-to-end offline RL and other outdoor navigation methods. keywords: {Legged locomotion;Point cloud compression;Costs;Three-dimensional displays;Laser radar;Navigation;Vegetation mapping},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610132&isnumber=10609862

K. Saha et al., "EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10351-10358, doi: 10.1109/ICRA57147.2024.10610519.Abstract: Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates within a certain time limit. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific costs such as "collision cost" and guide the diffusion to generate valid trajectories that satisfy the scene-specific constraints. Further, instead of a single cost function that may be insufficient in capturing diversity across scenes, we use an ensemble of costs to guide the diffusion process, significantly improving the success rate compared to classical planners. EDMP performs comparably with SOTA deep-learning-based methods while retaining the generalization capabilities primarily associated with classical planners. keywords: {Training;Costs;Diffusion processes;Cost function;Planning;Trajectory;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610519&isnumber=10609862

P. Werner, A. Amice, T. Marcucci, D. Rus and R. Tedrake, "Approximating Robot Configuration Spaces with few Convex Sets using Clique Covers of Visibility Graphs," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10359-10365, doi: 10.1109/ICRA57147.2024.10610005.Abstract: Many computations in robotics can be dramatically accelerated if the robot configuration space is described as a collection of simple sets. For example, recently developed motion planners rely on a convex decomposition of the free space to design collision-free trajectories using fast convex optimization. In this work, we present an efficient method for approximately covering complex configuration spaces with a small number of polytopes. The approach constructs a visibility graph using sampling and generates a clique cover of this graph to find clusters of samples that have mutual line of sight. These clusters are then inflated into large, full-dimensional, polytopes. We evaluate our method on a variety of robotic systems and show that it consistently covers larger portions of free configuration space, with fewer polytopes, and in a fraction of the time compared to previous methods. keywords: {Heuristic algorithms;Approximation algorithms;Convex functions;Trajectory;Reliability;Collision avoidance;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610005&isnumber=10609862

N. M. Stiffler and J. M. O’Kane, "Asymptotically-Optimal Multi-Robot Visibility-Based Pursuit-Evasion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10366-10373, doi: 10.1109/ICRA57147.2024.10610927.Abstract: The multi-robot visibility-based pursuit-evasion problem tasks a team of robots with systematically searching an environment to detect (capture) an evader. Previous techniques to generate search strategies for the pursuit team have shown to be either computationally intractable or permit poor solution quality. This paper presents a novel asymptotically optimal algorithm for generating a joint motion strategy for the pursuers. To explore the space of possible pursuer motion strategies, the algorithm utilizes a trio of hierarchical graph data structures that each capture certain elements of the problem such as connectivity (valid single pursuer motion), coordination (multiple pursuer motion), and tracking information (evaluating where an evader may be). The algorithm is inspired by well-known methods in the motion planning literature and inherits its asymptotic optimality from those techniques. In addition, we describe a method that can improve upon solutions found during the formative stages of the main algorithm, using a "fast-forward" approach that foregoes guarantees of asymptotic optimality, implementing heuristics that concentrate future samples into improving the path quality of the nominal solution. The algorithms were validated in simulation and results are provided. keywords: {Tracking;Fuses;Heuristic algorithms;Robot kinematics;Search problems;Data structures;Space exploration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610927&isnumber=10609862

G. Olivas-Martínez, A. Miranda-Moya, C. Katt and H. Castañeda, "Non-singular Fast Terminal Adaptive Visual Tracking Control with Reduced Tuning Parameters for an Aerial Vehicle Under Perturbations," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10406-10412, doi: 10.1109/ICRA57147.2024.10610345.Abstract: This paper presents a robust image-based visual servoing design for a quad-rotor unmanned aerial vehicle performing a visual target-tracking operation in the presence of turbulent wind. Image information is extracted and processed to control the positioning and heading of the aerial vehicle. A novel adaptive non-singular fast terminal sliding mode strategy is introduced to manage the visual servoing error. Unlike other sliding mode methods, the proposed approach diminishes the complexity of the system due to the reduction of its control parameters while providing practical finite-time convergence, robustness against bounded external disturbances and model uncertainties, non-overestimation of the control gains, and chattering attenuation. Furthermore, the stability of the system in closed loop is guaranteed through Lyapunov theory. Finally, simulation results demonstrate the capabilities and performance of such a controller in a high-fidelity scenario using the Robot Operating System and Gazebo frameworks. keywords: {Visualization;Adaptive systems;Uncertainty;Process control;Autonomous aerial vehicles;Attenuation;Visual servoing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610345&isnumber=10609862

Y. Luo, Q. Sima, T. Ji, F. Sun, H. Liu and J. Zhang, "Smooth Computation without Input Delay: Robust Tube-Based Model Predictive Control for Robot Manipulator Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10429-10435, doi: 10.1109/ICRA57147.2024.10610952.Abstract: Model Predictive Control (MPC) has exhibited remarkable capabilities in optimizing objectives and meeting constraints. However, the substantial computational burden associated with solving the Optimal Control Problem (OCP) at each triggering instant introduces significant delays between state sampling and control application. These delays limit the practicality of MPC in resource-constrained systems when engaging in complex tasks. The intuition to address this issue in this paper is that by predicting the successor state, the controller can solve the OCP one time step ahead of time thus avoiding the delay of the next action. To this end, we compute deviations between real and nominal system states, predicting forthcoming real states as initial conditions for the imminent OCP solution. Anticipatory computation stores optimal control based on current nominal states, thus mitigating the delay effects. Additionally, we establish an upper bound for linearization error, effectively linearizing the nonlinear system, reducing OCP complexity, and enhancing response speed. We provide empirical validation through two numerical simulations and corresponding real-world robot tasks, demonstrating significant performance improvements and augmented response speed (up to 90%) resulting from the seamless integration of our proposed approach compared to conventional time-triggered MPC strategies. keywords: {Upper bound;Optimal control;Stochastic processes;Predictive models;Robustness;Delays;Task analysis;Model Predictive Control;Tube-based Mechanism;Piecewise Linearization;Manipulator Motion Plan},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610952&isnumber=10609862

A. M. Mao, J. L. Moore and L. L. Whitcomb, "Nullspace Adaptive Model-Based Trajectory-Tracking Control for a 6-DOF Underwater Vehicle with Unknown Plant and Actuator Parameters: Theory and Preliminary Simulation Evaluation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10436-10442, doi: 10.1109/ICRA57147.2024.10611318.Abstract: We report a novel model-based nullspace adaptive trajectory-tracking control (NS-ATTC) algorithm for fully-actuated 6-degree-of-freedom (DOF) underwater vehicles which estimates unknown plant and actuator model parameters simultaneously. We provide a stability and convergence analysis with proof of asymptotically stable tracking error convergence, as well as a preliminary simulation study demonstrating 6-DOF trajectory tracking. The NS-ATTC algorithm does not require acceleration instrumentation and provides a stable online parameter estimate, enabling robust model-based autonomy. keywords: {Adaptation models;Actuators;Parameter estimation;Trajectory tracking;Instruments;6-DOF;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611318&isnumber=10609862

D. Kalaria, Q. Lin and J. M. Dolan, "Adaptive Planning and Control with Time-Varying Tire Models for Autonomous Racing Using Extreme Learning Machine," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10443-10449, doi: 10.1109/ICRA57147.2024.10610848.Abstract: Autonomous racing is a challenging problem, as the vehicle needs to operate at the friction or handling limits in order to achieve minimum lap times. Autonomous race cars require highly accurate perception, state estimation, planning, and control. Adding to this complexity is the need to accurately identify vehicle model parameters governing lateral tire slip effects, which can evolve over time due to factors such as tire wear and tear. Current approaches to this problem typically either propose offline model identification methods or rely on initial parameters within a narrow range (typically within 15-20% of the actual values). However, these approaches fall short in accounting for significant changes in tire models that can occur during actual races, particularly when pushing the vehicle to its handling limits. We present a unified framework that not only learns the tire model in real time from collected data but also adapts the model to environmental changes, even when the model parameters exhibit substantial deviations. The friction estimation, obtained as a byproduct from the learning results, facilitates the selection of the optimal racing line from a library for adaptive speed planning. We validate our approach through testing in simulators, encompassing a 1:43 scale race car and a full-size car, and also through experiments with a physical F1/10 autonomous race car. keywords: {Adaptation models;Friction;Tires;Data models;Real-time systems;Planning;Automobiles;Learning-based control;adaptive motion planning and control;extreme learning machine;autonomous racing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610848&isnumber=10609862

A. Jordana, A. Meduri, E. Arlaud, J. Carpentier and L. Righetti, "Risk-Sensitive Extended Kalman Filter," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10450-10456, doi: 10.1109/ICRA57147.2024.10611266.Abstract: Designing robust algorithms in the face of estimation uncertainty is a challenging task. Indeed, controllers seldom consider estimation uncertainty and only rely on the most likely estimated state. Consequently, sudden changes in the environment or the robot’s dynamics can lead to catastrophic behaviors. Leveraging recent results in risk-sensitive optimal control, this paper presents a risk-sensitive Extended Kalman Filter that can adapt its estimation to the control objective, hence allowing safe output-feedback Model Predictive Control (MPC). By taking a pessimistic estimate of the value function resulting from the MPC controller, the filter provides increased robustness to the controller in phases of uncertainty as compared to a standard Extended Kalman Filter (EKF). The filter has the same computational complexity as an EKF and can be used for real-time control. The paper evaluates the risk-sensitive behavior of the proposed filter when used in a nonlinear MPC loop on a planar drone and industrial manipulator in simulation, as well as on an external force estimation task on a real quadruped robot. These experiments demonstrate the ability of the approach to significantly improve performance in face of uncertainties. keywords: {Uncertainty;Service robots;Estimation;Robustness;Real-time systems;Kalman filters;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611266&isnumber=10609862

B. Su, F. Zhang and P. Huang, "Global Terminal Sliding Mode Control of Tethered Satellites Formation with Chattering Reduction via PID Laws," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10457-10463, doi: 10.1109/ICRA57147.2024.10611346.Abstract: This paper researches a novel global terminal sliding mode control(GTSMC) on a tethered satellites system(TSS) under outer disturbances, and the effect of PI/PD compensation in restraining chattering on sliding surface is appended. By taking advantage of the finite-time convergence of traditional terminal sliding surface, the sliding surface with global and terminal sliding motion is proposed, and the convergent time by GTSMC is qualitatively evaluated by the sliding surface. Then the integral/derivative function of the low-pass filtered switching control is appended in GTSMC. By virtue of the accuracy of integral and the damping of derivative, respectively, the persisting on sliding surface is eliminated, such that the chattering effect of the controlled system on the surface is restrained consequently. Finally, simulations of the proposed control on TSS is shown to validate the theoretical analyses. keywords: {Damping;Analytical models;Satellites;Accuracy;Low-pass filters;Switches;Steady-state;global terminal sliding mode;chattering reduction;tethered satellites system;integral/derivative function},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611346&isnumber=10609862

T. Gao et al., "Model Predictive Control for an Autonomous Underwater Robot with Fully Vectored Propulsion," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10482-10488, doi: 10.1109/ICRA57147.2024.10611025.Abstract: Due to the low motion efficiency and maneuver-ability of underwater robots with six degrees of freedom, it is challenging for them to respond quickly to the attitude requirements during underwater autonomous manipulation. This paper presents a novel autonomous underwater robot with fully vectored propulsion and a model predictive control method to achieve more agile and efficient movements autonomously. In detail, we first design a robot with eight vector-distributed thruster layouts for fully vectored propulsion and construct the software architecture based on the robot operating system (ROS). Then, we establish the hydrodynamic model by adopting the Fossen approach and construct a 13-dimensional system state-space equation, which is discretized using the explicit fourth-order Runge-Kutta method. To achieve autonomous manipulation, model predictive control is employed along with physical constraints of the custom-built robot to enable real-time prediction and optimization of the robot’s states for control purposes. Finally, numerical simulations and experiments of the Point-to-Point Motion are conducted to test the robot’s performance. Experimental results reveal that the average error of each direction is 0.0027 m, 0.0031 m, and 0.0368 m in the x-axis, y-axis, and z-axis, respectively, and 0.8502°, 2.1941°, 0.2408° corresponding to three attitude angles, which verify the performance of employing MPC to control an autonomous underwater robot with fully vectored propulsion. keywords: {Autonomous underwater vehicles;Software architecture;Attitude control;Propulsion;Numerical simulation;Mathematical models;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611025&isnumber=10609862

N. Zhao, Y. Luo, C. Qin, X. Luo, R. Chen and Y. Shen, "Attitude Control for Morphing Quadrotor through Model Predictive Control with Constraints*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10489-10495, doi: 10.1109/ICRA57147.2024.10610512.Abstract: Morphing quadrotors that can be potentially applied to confined spaces such as warehouses, tanks, and pipelines have flourished in recent years. Most work has focused on the mechanical feasibility of the morphing systems and high-level flight controller design, with limited discussions on low-level control. In this paper, a constrained model predictive control (MPC) is proposed and applied to solve the attitude control problem of a morphing quadrotor. Prior to controller design, a custom-built morphing quadrotor is introduced with the kinematic and dynamic models established and corresponding issues and challenges presented. In the controller, to eliminate the steady-state error, an embedded integrator is adopted by exploiting the differential variables; then, the constraints of the morphing quadrotor are incorporated into the MPC formulation to simulate real flight conditions, and an orthonormal function is employed to approximate the control input sequences in the controller to alleviate the computational burden. In the comparative studies, several scenarios are considered to demonstrate the effectiveness of the proposed control strategy in attitude control. keywords: {Attitude control;Pipelines;Kinematics;Stability analysis;Steady-state;State-space methods;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610512&isnumber=10609862

R. Liu, S. Li and X. Yin, "NNgTL: Neural Network Guided Optimal Temporal Logic Task Planning for Mobile Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 10496-10502, doi: 10.1109/ICRA57147.2024.10611699.Abstract: In this work, we investigate task planning for mobile robots under linear temporal logic (LTL) specifications. This problem is particularly challenging when robots navigate in continuous workspaces due to the high computational complexity involved. Sampling-based methods have emerged as a promising avenue for addressing this challenge by incrementally constructing random trees, thereby sidestepping the need to explicitly explore the entire state-space. However, the performance of this sampling-based approach hinges crucially on the chosen sampling strategy, and a well-informed heuristic can notably enhance sample efficiency. In this work, we propose a novel neural-network guided (NN-guided) sampling strategy tailored for LTL planning. Specifically, we employ a multi-modal neural network capable of extracting features concurrently from both the workspace and the Büchi automaton. This neural network generates predictions that serve as guidance for random tree construction, directing the sampling process toward more optimal directions. Through numerical experiments, we compare our approach with existing methods and demonstrate its superior efficiency, requiring less than 15% of the time of the existing methods to find a feasible solution. keywords: {Navigation;Buildings;Artificial neural networks;Fasteners;Feature extraction;Probabilistic logic;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611699&isnumber=10609862

