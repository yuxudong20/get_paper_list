Y. Huang et al., "Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3108-3115, doi: 10.1109/ICRA57147.2024.10610240.Abstract: Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline. keywords: {Point cloud compression;Video tracking;Object oriented modeling;Transformers;Cognition;Planning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610240&isnumber=10609862

R. Lee, J. Abou-Chakra, F. Zhang and P. Corke, "Learning Fabric Manipulation in the Real World with Human Videos," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3124-3130, doi: 10.1109/ICRA57147.2024.10610062.Abstract: Fabric manipulation is a long-standing challenge in robotics due to the enormous state space and complex dynamics. Learning approaches stand out as promising for this domain as they allow us to learn behaviours directly from data. Most prior methods however rely heavily on simulation, which is still limited by the large sim-to-real gap of deformable objects or rely on large datasets. A promising alternative is to learn fabric manipulation directly from watching humans perform the task. In this work, we explore how demonstrations for fabric manipulation tasks can be collected directly by humans, providing an extremely natural and fast data collection pipeline. Then, using only a handful of such demonstrations, we show how a pick-and-place policy can be learned and deployed on a real robot, without any robot data collection at all. We demonstrate our approach on a fabric smoothing and folding task, showing that our policy can reliably reach folded states from crumpled initial configurations. Code, video and data are available on the project website: https://sites.google.com/view/foldingbyhand keywords: {Smoothing methods;Pipelines;Data collection;Predictive models;Reliability engineering;Fabrics;Task analysis;Manipulation;Visual Learning;Perception for Grasping and Manipulation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610062&isnumber=10609862

K. -T. Song and H. -H. Chen, "HAGrasp: Hybrid Action Grasp Control in Cluttered Scenes using Deep Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3131-3137, doi: 10.1109/ICRA57147.2024.10610852.Abstract: Robotic autonomous grasp requires the system to perform multiple functions such as gripper and robot control, making it a task with hybrid output nature. Existing methods based on closed-loop deep reinforcement learning rely on external models for termination evaluation. To achieve more effective grasp for novel objects, we propose a new autonomous grasp control scheme termed HAGrasp that considers the complete point cloud of the workspace. It integrates grasp pose estimation, end-effector pose evaluation, and motion planning of the robotic arm into a single model, enhancing the success rate while reducing computational load. We present a closed-loop grasp control system based on deep reinforcement learning. This control system can perform grasp tasks while dynamically adjusting to avoid end-effector collisions. The design of hybrid-action reinforcement learning module is trained with unified latent action space and further improve generalization, achieving real-time autonomous grasp control. Real robot experiments show that our method has 74.2% success rate for grasping 7 unseen objects. Comparative experiments show that the proposed HAGrasp outperforms open-loop baseline Contact-Graspnet in both success rate and inference time. It is demonstrated that with integrated multi-view input and sim-to-real training design, our method improves real-world applications of autonomous grasp. keywords: {Training;Point cloud compression;Robot control;Pose estimation;Deep reinforcement learning;End effectors;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610852&isnumber=10609862

J. Zhong, Y. W. Wong, J. Jin, Y. Song, X. Yuan and X. Chen, "Dual-Critic Deep Reinforcement Learning for Push-Grasping Synergy in Cluttered Environment," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3138-3144, doi: 10.1109/ICRA57147.2024.10610121.Abstract: Robotic push-grasping in densely cluttered environments presents significant challenges due to unbalanced synergy and redundancy between both actions, leading to decreased grasp efficiency. In this paper, a novel double-critic deep reinforcement learning framework is introduced to optimize the push-grasping synergy for robotic manipulation in such environments, aiming to significantly reduce pre-grasping redundancy. This framework incorporates two distinct Deep Q-learning critics: Critic I selects the best course of actions based on the current state derived from visual interpretation, whereas Critic II evaluates the success rate of the current state-action pairing. To further refine the push-grasping synergy, an active double-step learning mechanism is introduced to optimize the training reward function for the pushing action, thereby enhancing its effectiveness through increased intentionality. Simulations show that the proposed framework outperforms contemporary counterparts, notably in grasping success rate and action efficiency. Finally, the framework’s generalization and adaptability are demonstrated by conducting real-world experiments using novel objects without the need of retraining. keywords: {Training;Learning systems;Visualization;Q-learning;Limiting;Redundancy;Grasping},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610121&isnumber=10609862

B. Thach, T. Watts, S. -H. Ho, T. Hermans and A. Kuntz, "DefGoalNet: Contextual Goal Learning from Demonstrations for Deformable Object Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3145-3152, doi: 10.1109/ICRA57147.2024.10610109.Abstract: Shape servoing, a robotic task dedicated to controlling objects to desired goal shapes, is a promising approach to deformable object manipulation. An issue arises, however, with the reliance on the specification of a goal shape. This goal has been obtained either by a laborious domain knowledge engineering process or by manually manipulating the object into the desired shape and capturing the goal shape at that specific moment, both of which are impractical in various robotic applications. In this paper, we solve this problem by developing a novel neural network DefGoalNet, which learns deformable object goal shapes directly from a small number of human demonstrations. We demonstrate our method’s effectiveness on various robotic tasks, both in simulation and on a physical robot. Notably, in the surgical retraction task, even when trained with as few as 10 demonstrations, our method achieves a median success percentage of nearly 90%. These results mark a substantial advancement in enabling shape servoing methods to bring deformable object manipulation closer to practical real-world applications. keywords: {Knowledge engineering;Heart;Shape;Neural networks;Pipelines;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610109&isnumber=10609862

A. Xie, L. Lee, T. Xiao and C. Finn, "Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3153-3160, doi: 10.1109/ICRA57147.2024.10611331.Abstract: What makes generalization hard for imitation learning in visual robotic manipulation? This question is difficult to approach at face value, but the environment from the perspective of a robot can often be decomposed into enumerable factors of variation, such as the lighting conditions or the placement of the camera. Empirically, generalization to some of these factors have presented a greater obstacle than others, but existing work sheds little light on precisely how much each factor contributes to the generalization gap. Towards an answer to this question, we study imitation learning policies in simulation and on a real robot language-conditioned manipulation task to quantify the difficulty of generalization to different (sets of) factors. We design a simulated benchmark of 19 tasks with 11 factors of variation to facilitate more controlled evaluations of generalization. From our study, we determine an ordering of factors based on generalization difficulty, that is consistent across simulation and our real robot setup.1 keywords: {Visualization;Imitation learning;Robot vision systems;Lighting;Benchmark testing;Cameras;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611331&isnumber=10609862

A. Fusco, V. Modugno, D. Kanoulas, A. Rizzo and M. Cognetti, "Transformer-Based Prediction of Human Motions and Contact Forces for Physical Human-Robot Interaction," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3161-3167, doi: 10.1109/ICRA57147.2024.10611211.Abstract: In this paper, we propose a transformer-based architecture for predicting contact forces during a physical human-robot interaction. Our Neural Network is composed of two main parts: a Multi-Layer Perceptron called Transducer and a Transformer. The former estimates, based on the kinematic data from a motion capture suit, the current contact forces. The latter predicts – taking as input the same kinematic data and the output of the Transducer – the human motions and the contact forces over a time window in the future. We validated our approach by testing the network on directions of motions that were not provided in the training set. We also compared our approach to a purely Transformer-based network, showing a better prediction accuracy of the contact forces. keywords: {Training;Robot motion;Transducers;Accuracy;Human-robot interaction;Kinematics;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611211&isnumber=10609862

S. Christen, L. Feng, W. Yang, Y. -W. Chao, O. Hilliges and J. Song, "SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3168-3175, doi: 10.1109/ICRA57147.2024.10610694.Abstract: Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines. keywords: {Training;Human-robot interaction;Morphology;Grasping;Handover;Motion capture;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610694&isnumber=10609862

Z. Rysbek, S. Li, A. M. Shervedani and M. Žefran, "Proactive Robot Control for Collaborative Manipulation Using Human Intent," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3176-3182, doi: 10.1109/ICRA57147.2024.10610909.Abstract: Collaborative manipulation task often requires negotiation using explicit or implicit communication. An important example is determining where to move when the goal destination is not uniquely specified, and who should lead the motion. This work is motivated by the ability of humans to communicate the desired destination of motion through back-and-forth force exchanges. Inherent to these exchanges is also the ability to dynamically assign a role to each participant, either taking the initiative or deferring to the partner’s lead. In this paper, we propose a hierarchical robot control framework that emulates human behavior in communicating a motion destination to a human collaborator and in responding to their actions. At the top level, the controller consists of a set of finite-state machines corresponding to different levels of commitment of the robot to its desired goal configuration. The control architecture is loosely based on the human strategy observed in the human-human experiments, and the key component is a real-time intent recognizer that helps the robot respond to human actions. We describe the details of the control framework, feature engineering and training process of the intent recognition. The proposed controller was implemented on a UR10e robot (Universal Robots) and evaluated through human studies. The experiments show that the robot correctly recognizes and responds to human input, communicates its intent clearly, and resolves conflict. We report success rates and draw comparisons with human-human experiments to demonstrate the effectiveness of the approach. keywords: {Training;Robot control;Force;Collaboration;Process control;Automata;Lead},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610909&isnumber=10609862

J. E. Domínguez-Vidal and A. Sanfeliu, "Exploring Transformers and Visual Transformers for Force Prediction in Human-Robot Collaborative Transportation Tasks," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3191-3197, doi: 10.1109/ICRA57147.2024.10611205.Abstract: In this paper, we analyze the possibilities offered by Deep Learning State-of-the-Art architectures such as Transformers and Visual Transformers in generating a prediction of the human’s force in a Human-Robot collaborative object transportation task at a middle distance. We outperform our previous predictor by achieving a success rate of 93.8% in testset and 90.9% in real experiments with 21 volunteers predicting in both cases the force that the human will exert during the next 1 s. A modification in the architecture allows us to obtain a second output from the model with a velocity prediction, which allows us to improve the capabilities of our predictor if it is used to estimate the trajectory that the human-robot pair will follow. An ablation test is also performed to verify the relative contribution to performance of each input. keywords: {Visualization;Force;Collaboration;Transportation;Predictive models;Transformers;Trajectory;Physical Human-Robot Interaction;Object Transportation;Human-in-the-Loop;Force Prediction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611205&isnumber=10609862

Z. Wang and M. G. Carmichael, "Exploring the Effect of Base Compliance on Physical Human-Robot Collaboration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3198-3204, doi: 10.1109/ICRA57147.2024.10611559.Abstract: Mobile physical human-robot collaboration (pHRC) using collaborative robots (cobots) and mobile robots has attracted much research attention. Many researchers have focused on improving the control performance to comply with human intentions. However, a problem that generally exists with mobile pHRC but often gets neglected is the impact of non-rigid components e.g. deformable tyres, suspension systems and uneven terrain on human interaction experience and task performance. To fullfil this current research gap, we carried out an investigation on the above-mentioned problem by altering a cobot’s base rigidity level (also referred to as base compliance level or BCL) during pHRC experiments. We explored how the task performance is affected by base compliance as well as human operator’s experience and cobot control parameters. Measurements include the human operator’s physical effort, task velocity, and task error. From the experimental results, it is discovered that base compliance has a significant impact on task accuracy as it can easily excite the system if an inadequate control strategy is deployed. Furthermore, through ANOVA, it is discovered that the influence of base compliance can be minimized and system excitation can be avoided by sufficient human operator training and the appropriate selection of cobot’s control parameters. keywords: {Training;Accuracy;Collaborative robots;Tires;Rigidity;Velocity measurement;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611559&isnumber=10609862

A. Fortuna et al., "A Personalizable Controller for the Walking Assistive omNi-Directional Exo-Robot (WANDER)," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3212-3218, doi: 10.1109/ICRA57147.2024.10611368.Abstract: Preserving and encouraging mobility in the elderly and adults with chronic conditions is of paramount importance. However, existing walking aids are either inadequate to provide sufficient support to users’ stability or too bulky and poorly maneuverable to be used outside hospital environments. In addition, they all lack adaptability to individual requirements. To address these challenges, this paper introduces WANDER, a novel Walking Assistive omNi-Directional Exo-Robot. It consists of an omnidirectional platform and a robust aluminum structure mounted on top of it, which provides partial body weight support. A comfortable and minimally restrictive coupling interface embedded with a force/torque sensor allows to detect users’ intentions, which are translated into command velocities by means of a variable admittance controller. An optimization technique based on users’ preferences, i.e., Preference-Based Optimization (PBO) guides the choice of the admittance parameters (i.e., virtual mass and damping) to better fit subject-specific needs and characteristics. Experiments with twelve healthy subjects exhibited a significant decrease in energy consumption and jerk when using WANDER with PBO parameters as well as improved user performance and comfort. The great interpersonal variability in the optimized parameters highlights the importance of personalized control settings when walking with an assistive device, aiming to enhance users’ comfort and mobility while ensuring reliable physical support. keywords: {Legged locomotion;Couplings;Performance evaluation;Process control;Robot sensing systems;Admittance;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611368&isnumber=10609862

M. Dežman, C. Marquardt and T. Asfour, "Ankle Exoskeleton with a Symmetric 3 DoF Structure for Plantarflexion Assistance," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3227-3233, doi: 10.1109/ICRA57147.2024.10609991.Abstract: Ankle exoskeletons can assist the ankle joint and reduce the metabolic cost of walking. However, many existing ankle exoskeletons constrain the natural 3 degrees of freedom (DoF) of the ankle to limit the exoskeleton’s weight and mechanical complexity, thereby compromising comfort and kinematic compatibility with the user.This paper presents a novel ankle exoskeleton frame design that allows for 3 DoF ankle motion using a symmetric parallel frame design principle resulting in a strong frame while weighing 1.8 kg. Furthermore, a cable routing method is proposed to actuate the plantarflexion of the ankle. The kinematic compatibility of the proposed exoskeleton frame is evaluated in straight- and curve-walking scenarios with four users. The study demonstrates that the exoskeleton frame adapts to the natural 3 DoF ankle motion and the range of motion (RoM) during walking. The actuation in plantarflexion is evaluated in a stationary torque experiment demonstrating the ability of the frame to transfer large torque loads of up to 57.4 Nm. This work contributes to the design and development of more flexible and adaptable ankle exoskeletons for walking assistance. keywords: {Ankle;Legged locomotion;Torque;Exoskeletons;3-DOF;Torque control;Kinematics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609991&isnumber=10609862

J. Park et al., "Design of a Front-enveloping Powered Exoskeleton Considering Optimal Distribution of Actuating Torques and Center of Mass," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3234-3240, doi: 10.1109/ICRA57147.2024.10610165.Abstract: Traditionally, powered exoskeletons have predominantly featured a back-enveloping design due to its simplicity in both implementation and user donning. However, this design results in a backward shift of the center of mass (CoM) in the sagittal plane. This paper identifies the limitations of existing design approaches and determines the optimal anterior-posterior (A/P) CoM position considering factors like actuating power, balance in the neutral posture, and user’s hand workspace. Our optimization analysis recommends placing the CoM in front of the user. We address historical constraints on front-enveloping designs and propose solutions. Furthermore, we validate the usability of our designed exoskeleton through testing with a complete paraplegic user. keywords: {Legged locomotion;Design methodology;Exoskeletons;Wearable robots;Usability;Robotics and automation;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610165&isnumber=10609862

Z. Ö. Orhan, A. Dal Prete, A. Bolotnikova, M. Gandolla, A. Ijspeert and M. Bouri, "Real-Time Locomotion Transitions Detection: Maximizing Performances with Minimal Resources," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3241-3247, doi: 10.1109/ICRA57147.2024.10611651.Abstract: Assistive devices, such as exoskeletons and prostheses, have revolutionized the field of rehabilitation and mobility assistance. Efficiently detecting transitions between different activities, such as walking, stair ascending and descending, and sitting, is crucial for ensuring adaptive control and enhancing user experience. We present an approach for real-time transition detection, aimed at optimizing the processing-time performance. By establishing activity-specific threshold values through trained machine learning models, we effectively distinguish motion patterns and we identify transition moments between locomotion modes. This threshold-based method improves real-time embedded processing time performance by up to 11 times compared to machine learning approaches. The efficacy of the developed finite-state machine is validated using data collected from three different measurement systems. Moreover, experiments with healthy participants were conducted on an active pelvis orthosis to validate the robustness and reliability of our approach. The proposed algorithm achieved high accuracy in detecting transitions between activities. These promising results show the robustness and reliability of the method, reinforcing its potential for integration into practical applications. keywords: {Accuracy;Exoskeletons;Automata;Machine learning;Wearable robots;Real-time systems;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611651&isnumber=10609862

Z. Ö. Orhan, M. Shafiee, V. Juillard, J. C. Oliveira, A. Ijspeert and M. Bouri, "ExoRecovery: Push Recovery with a Lower-Limb Exoskeleton Based on Stepping Strategy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3248-3255, doi: 10.1109/ICRA57147.2024.10610027.Abstract: Balance loss is a significant challenge in lower-limb exoskeleton applications, as it can lead to potential falls, thereby impacting user safety and confidence. We introduce a control framework for omnidirectional recovery step planning by online optimization of step duration and position in response to external forces. We map the step duration and position to a human-like foot trajectory, which is then translated into joint trajectories using inverse kinematics. These trajectories are executed via an impedance controller, promoting cooperation between the exoskeleton and the user. Moreover, our framework is based on the concept of the divergent component of motion, also known as the Extrapolated Center of Mass, which has been established as a consistent dynamic for describing human movement. This real-time online optimization framework enhances the adaptability of exoskeleton users under unforeseen forces thereby improving the overall user stability and safety. To validate the effectiveness of our approach, simulations, and experiments were conducted. Our push recovery experiments employing the exoskeleton in zero-torque mode (without assistance) exhibit an alignment with the exoskeleton’s recovery assistance mode, that shows the consistency of the control framework with human intention. To the best of our knowledge, this is the first cooperative push recovery framework for the lower-limb human exoskeleton that relies on the simultaneous adaptation of intra-stride parameters in both frontal and sagittal directions. The proposed control scheme has been validated with human subject experiments. keywords: {Systematics;Exoskeletons;Collaboration;Kinematics;Stability analysis;Real-time systems;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610027&isnumber=10609862

G. M. Bryan, P. W. Franks, S. Song and S. H. Collins, "Pilot comparison of customized and generalized hip-knee-ankle exoskeleton torque profiles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3256-3261, doi: 10.1109/ICRA57147.2024.10611676.Abstract: Optimized assistance patterns have produced the greatest exoskeleton benefits to energy expenditure of any strategy to date. This strategy may be effective due to the customization of the applied torque profiles to the user as well as the locomotion condition; however, it is currently unclear how sensitive participants are to their unique torque profile. To investigate, we applied previously optimized hip-knee-ankle torque profiles to expert users (N=3; 1.25 m/s; 0 deg incline). The participants walked with the profile optimized to them, the two profiles optimized to the other two participants, and the average of the three torque profiles while we measured their energy expenditure. Relative to walking with the device turned off, on average, participants experienced a 47.5% (range 12%) metabolic reduction when walking with the torque profile optimized to them and a 46% (range 15%) reduction when walking with the other profiles. Interestingly, within-subject performance was more consistent than across subjects (P1: 52% range 5%, P2: 49% range 6%, P3: 39% range 3%) suggesting that, for expert users of some devices, there may be a range of nearly equally effective torque profiles to reduce the metabolic cost of walking. The torque timing was remarkably similar across the four torque profiles while the torque magnitude varied; participants may be much more sensitive to torque timing than torque magnitude, and there may be a set of torque timing parameters that are generally effective. keywords: {Legged locomotion;Torque;Sociology;Exoskeletons;Energy measurement;Particle measurements;Timing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611676&isnumber=10609862

D. J. Kelly, R. R. Posh and P. M. Wensing, "Task-space Control of a Powered Ankle Prosthesis," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3262-3268, doi: 10.1109/ICRA57147.2024.10611270.Abstract: Powered lower-limb prostheses have shown promise in helping individuals with amputation regain functionality that passive prostheses cannot provide. However, the best method for controlling these devices in coordination with their users is still an open research topic. While powered devices can replicate normative joint kinematics and kinetics, active control also holds the potential to shape system-level characteristics such as the center of mass (CoM) that play an important role in balance. Controlling the prosthesis based on these system-level, or task-space, variables would further represent a new way of coordinating the user and their device.This paper explores the initial implementation of task-space control for a powered ankle prosthesis, characterizing the emergent outcomes of this new coordination strategy. One able-bodied subject walked using a bypass adapter while prosthesis torques were commanded based on reference ground reaction force (GRF) and CoM trajectories. The subject could walk comfortably and continuously at their preferred walking speed, achieving normative ankle torques and joint trajectories despite not tracking explicit joint-level references in stance. keywords: {Ankle;Legged locomotion;Shape;Force;Kinematics;Trajectory;Kinetic theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611270&isnumber=10609862

J. Xu, H. Zhang, Q. Si, Y. Li, X. Lan and T. Kong, "Towards Unified Interactive Visual Grounding in The Wild," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3288-3295, doi: 10.1109/ICRA57147.2024.10611354.Abstract: Interactive visual grounding in Human-Robot Interaction (HRI) is challenging yet practical due to the inevitable ambiguity in natural languages. It requires robots to disambiguate the user’s input by active information gathering. Previous approaches often rely on predefined templates to ask disambiguation questions, resulting in performance reduction in realistic interactive scenarios. In this paper, we propose TiO, an end-to-end system for interactive visual grounding in human-robot interaction. Benefiting from a unified formulation of visual dialog and grounding, our method can be trained on a joint of extensive public data, and show superior generality to diversified and challenging open-world scenarios. In the experiments, we validate TiO on GuessWhat?! and InViG benchmarks, setting new state-of-the-art performance by a clear margin. Moreover, we conduct HRI experiments on the carefully selected 150 challenging scenes as well as real-robot platforms. Results show that our method demonstrates superior generality to diversified visual and language inputs with a high success rate. Codes and demos are available on https://jxu124.github.io/TiO/. keywords: {Visualization;Grounding;Natural languages;Human-robot interaction;Training data;Benchmark testing;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611354&isnumber=10609862

Y. Dai, R. Peng, S. Li and J. Chai, "Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3296-3303, doi: 10.1109/ICRA57147.2024.10610178.Abstract: Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION)1, which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion and the efficiency of navigation and interaction remains challenging for all methods. We further provide more findings on the impact of diverse user feedback forms on the agents’ performance. keywords: {Navigation;Large language models;Natural languages;Collaboration;Oral communication;Search problems;Complexity theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610178&isnumber=10609862

G. -C. Kang, J. Kim, J. Kim and B. -T. Zhang, "PROGrasp: Pragmatic Human-Robot Communication for Object Grasping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3304-3310, doi: 10.1109/ICRA57147.2024.10610543.Abstract: Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object’s category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., "I am thirsty") is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user’s intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpretation for pragmatic inference. Experimental results show that PROGrasp is effective in offline (i.e., target object discovery) and online (i.e., IOG with a physical robot arm) settings. Code and data are available at https://github.com/gicheonkang/prograsp. keywords: {Visualization;Codes;Grounding;Natural languages;Grasping;Manipulators;Object recognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610543&isnumber=10609862

H. Chen, X. Yang, G. Ma, Y. Wang and X. Wang, "Enhancing Tactile Sensing in Robotics: Dual-Modal Force and Shape Perception with EIT-based Sensors and MM-CNN," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3311-3317, doi: 10.1109/ICRA57147.2024.10610215.Abstract: Electrical Impedance Tomography (EIT)-based tactile sensors offer durability, scalability, and cost-effective manufacturing. However, simultaneously reconstructing force and shape from boundary measurements remains challenging due to EIT’s inherent location dependencies and image artifacts. This study presents a model-driven multimodal convolutional neural network (MM-CNN) for joint EIT-based force and shape sensing. The hybrid approach combines physics-inspired voltage preprocessing with an attention-based network to overcome EIT’s limitations. The preprocessing network applies a linearized one-step inverse solution with Tikhonov regularization to convert raw boundary voltage into a noise-reduced 2D image. The image reconstruction network uses an attention mechanism to focus on salient features, addressing location dependency issues. Quantitative metrics show that MM-CNN outperforms traditional EIT algorithms like NOSER and TV, reducing location dependency and improving shape discrimination. MM-CNN enables unified force and shape modalities, validated through real-contact experiments, enhancing EIT tactile systems for human-robot interaction by incorporating physical knowledge with deep learning. keywords: {Electrical impedance tomography;TV;Shape;Shape measurement;Force;Tactile sensors;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610215&isnumber=10609862

F. Tang et al., "Assisting Group Discussions Using Desktop Robot Haru," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3326-3332, doi: 10.1109/ICRA57147.2024.10611098.Abstract: Socially assistive robots are potentially to be integrated with human daily lives in the near future, and expected to be able to improve group dynamics when interacting with groups of people in social settings. In this paper, we developed a system with desktop robot Haru to assist group discussions. The system consists of three modules: a dialogue assistance module which facilitates Haru to speak to users and answer questions in a free way; a dialogue balance module to encourage participation of users in the discussion with verbal behaviors; an autonomous gazing behavior module trained via deep reinforcement learning in simulation and deployed on physical Haru in reality, which can show politeness during group discussion, e.g., gazing to the speaking member, looking to the middle when both members are talking or silent, looking at the least spoken person when encouraging her. Results of user study with 40 subjects show the significant effectiveness of our system in assisting group discussion. keywords: {Social robots;Personal voice assistants;Deep reinforcement learning;Chatbots;Assistive robots;Text to speech},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611098&isnumber=10609862

O. A. M. F., R. Parameswari, C. Di Natali, D. G. Caldwell and J. Ortiz, "Assessment and Benchmarking of XoNLI: a Natural Language Processing Interface for Industrial Exoskeletons," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3333-3340, doi: 10.1109/ICRA57147.2024.10610451.Abstract: Industrial exoskeletons are a potential solution for reducing work-related musculoskeletal disorders during carrying or lifting tasks. Having sensors, electrical/pneumatic actuators, and control systems, active exoskeletons present a more versatile control system because it is possible to select different assistive strategies based on the performed task. From this perspective, human-machine interaction is required to safely open basic exoskeleton domains to the user and provide an adaptable setup system. This article presents the assessment and benchmarking of the novel XoLab Natural Language Interface, a voice user interface for interaction and configuration of industrial active exoskeletons. The evaluation of the novel interface was performed by 17 participants who completed the setup and operational activities while wearing the XoTrunk exoskeleton. The benchmark consisted of a comparison of the presented device with previous adaptable interfaces for the exoskeleton: the user command interface and the monitor system interface. The results showed that although the novel interface demonstrated a considerable lag in the time response, it was more attractive, stimulating and novel than the standard one. However, the standard interface obtained favourable results over the user command interface and the voice interface perspicuity and efficiency. keywords: {Performance evaluation;Exoskeletons;Benchmark testing;Control systems;Sensor systems;Sensors;Time factors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610451&isnumber=10609862

Z. Zhang et al., "Advancing Virtual Reality Interaction: A Ring-Shaped Controller and Pose Tracking," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3341-3347, doi: 10.1109/ICRA57147.2024.10610313.Abstract: Ensuring robust tracking of controllers’ movement is critical for human-robot interaction in virtual reality (VR) scenarios. This paper proposes a robust tracking algorithm based on a novel wearable ring-shaped controller equipped with an inertial measurement unit (IMU) and a light-emitting diode (LED). This novel controller design allows users to free up their hands for more immersive experiences. To track the controller’s motion accurately and robustly, we resort to various forms of visual measurements, including 6 DoF and 5 DoF pose measurements from hand gesture detection, as well as 3 DoF position measurement and 2 DoF image measurement derived from the LED. We theoretically analyze the performances of these observation models and propose an optimal observation model combination scheme. Moreover, the necessity and rationale of online estimating system gravity are illustrated. The effectiveness of our tracking method is validated through extensive experiments. keywords: {Solid modeling;Analytical models;Visualization;Measurement units;Tracking;Virtual reality;Inertial navigation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610313&isnumber=10609862

Y. Luo, M. Wonsick, J. Hodgins and B. Okorn, "Tactile Embeddings for Multi-Task Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3348-3355, doi: 10.1109/ICRA57147.2024.10611419.Abstract: Tactile sensing plays a pivotal role in human perception and manipulation tasks, allowing us to intuitively understand task dynamics and adapt our actions in real time. Transferring such tactile intelligence to robotic systems would help intelligent agents understand task constraints and accurately interpret the dynamics of both the objects they are interacting with and their own operations. While significant progress has been made in imbuing robots with this tactile intelligence, challenges persist in effectively utilizing tactile information due to the diversity of tactile sensor form factors, manipulation tasks, and learning objectives involved. To address this challenge, we present a unified tactile embedding space capable of predicting a variety of task-centric qualities over multiple manipulation tasks. We collect tactile data from human demonstrations across various tasks and leverage this data to construct a shared latent space for task stage classification, object dynamics estimation, and tactile dynamics prediction. Through experiments and ablation studies, we demonstrate the effectiveness of our shared tactile latent space for more accurate and adaptable tactile networks, showing an improvement of up to 84% over the single-task training. keywords: {Training;Tactile sensors;Estimation;Multitasking;Real-time systems;Sensors;Intelligent agents},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611419&isnumber=10609862

E. T. Chang et al., "An Investigation of Multi-feature Extraction and Super-resolution with Fast Microphone Arrays," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3388-3394, doi: 10.1109/ICRA57147.2024.10611599.Abstract: In this work, we use MEMS microphones as vibration sensors to simultaneously classify texture and estimate contact position and velocity. Vibration sensors are an important facet of both human and robotic tactile sensing, providing fast detection of contact and onset of slip. Microphones are an attractive option for implementing vibration sensing as they offer a fast response and can be sampled quickly, are affordable, and occupy a very small footprint. Our prototype sensor uses only a sparse array (8-9 mm spacing) of distributed MEMS microphones (<$1, 3.76×2.95×1.10 mm) embedded under an elastomer. We use transformer-based architectures for data analysis, taking advantage of the microphones’ high sampling rate to run our models on time-series data as opposed to individual snapshots. This approach allows us to obtain 77.3% average accuracy on 4-class texture classification (84.2% when excluding the slowest drag velocity), 1.8 mm mean error on contact localization, and 5.6 mm/s mean error on contact velocity. We show that the learned texture and localization models are robust to varying velocity and generalize to unseen velocities. We also report that our sensor provides fast contact detection, an important advantage of fast transducers. This investigation illustrates the capabilities one can achieve with a MEMS microphone array alone, leaving valuable sensor real estate available for integration with complementary tactile sensing modalities. keywords: {Micromechanical devices;Vibrations;Location awareness;Training;Transducers;Feature extraction;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611599&isnumber=10609862

A. -H. Shahidzadeh, S. J. Yoo, P. Mantripragada, C. D. Singh, C. Fermüller and Y. Aloimonos, "AcTExplore: Active Tactile Exploration on Unknown Objects," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3411-3418, doi: 10.1109/ICRA57147.2024.10611667.Abstract: Tactile exploration plays a crucial role in understanding object structures for fundamental robotics tasks such as grasping and manipulation. However, efficiently exploring such objects using tactile sensors is challenging, primarily due to the large-scale unknown environments and limited sensing coverage of these sensors. To this end, we present AcTExplore, an active tactile exploration method driven by reinforcement learning for object reconstruction at scales that automatically explores the object surfaces in a limited number of steps. Through sufficient exploration, our algorithm incrementally collects tactile data and reconstructs 3D shapes of the objects as well, which can serve as a representation for higher-level downstream tasks. Our method achieves an average of 95.97% IoU coverage on unseen YCB objects while just being trained on primitive shapes. keywords: {Surface reconstruction;Three-dimensional displays;Shape;Tactile sensors;Reinforcement learning;Grasping;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611667&isnumber=10609862

Y. Wang, J. Kang, Z. Chen and X. Xiong, "Terrestrial Locomotion of PogoX: From Hardware Design to Energy Shaping and Step-to-step Dynamics Based Control," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3419-3425, doi: 10.1109/ICRA57147.2024.10611545.Abstract: We present a novel controller design on a robotic locomotor that combines an aerial vehicle with a spring-loaded leg. The main motivation is to enable the terrestrial locomotion capability on aerial vehicles so that they can carry heavy loads: heavy enough that flying is no longer possible, e.g., when the thrust-to-weight ratio (TWR) is small. The robot is designed with a pogo-stick leg and a quadrotor, and thus it is named as PogoX. We show that with a simple and lightweight spring-loaded leg, the robot is capable of hopping with TWR <1. The control of hopping is realized via two components: a vertical height control via control Lyapunov function-based energy shaping, and a step-to-step (S2S) dynamics based horizontal velocity control that is inspired by the hopping of the Spring-Loaded Inverted Pendulum (SLIP). The controller is successfully realized on the physical robot, showing dynamic terrestrial locomotion of PogoX which can hop at variable heights and different horizontal velocities with robustness to ground height variations and external pushes. keywords: {Legged locomotion;Velocity control;Robustness;Hardware;Vehicle dynamics;Robots;Quadrotors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611545&isnumber=10609862

J. Zhang, S. Heim, S. H. Jeon and S. Kim, "Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3426-3433, doi: 10.1109/ICRA57147.2024.10611045.Abstract: We present a minimal phase oscillator model for learning quadrupedal locomotion. Each of the four oscillators is coupled only to itself and its corresponding leg through local feedback of the ground reaction force, which can be interpreted as an observer feedback gain. We interpret the oscillator itself as a latent contact state-estimator. Through a systematic ablation study, we show that the combination of phase observations, simple phase-based rewards, and the local feedback dynamics induces policies that exhibit emergent gait preferences, while using a reduced set of simple rewards, and without prescribing a specific gait. The code is open-source, and a video synopsis available at https://youtu.be/1NKQ0rSV3jU. keywords: {Legged locomotion;Systematics;Codes;Force;Dynamics;Observers;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611045&isnumber=10609862

Y. Huang, Z. Bing, Z. Zhang, G. Zhuang, K. Huang and A. Knoll, "Optimizing Dynamic Balance in a Rat Robot via the Lateral Flexion of a Soft Actuated Spine," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3442-3448, doi: 10.1109/ICRA57147.2024.10611626.Abstract: Balancing oneself using the spine is a physiological alignment of the body posture in the most efficient manner by the muscular forces for mammals. For this reason, we can see many disabled quadruped animals can still stand or walk even with three limbs. This paper investigates the optimization of dynamic balance during trot gait based on the spatial relationship between the center of mass (CoM) and support area influenced by spinal flexion. During trotting, the robot balance is significantly influenced by the distance of the CoM to the support area formed by diagonal footholds. In this context, lateral spinal flexion, which is able to modify the position of footholds, holds promise for optimizing balance during trotting. This paper explores this phenomenon using a rat robot equipped with a soft actuated spine. Based on the lateral flexion of the spine, we establish a kinematic model to quantify the impact of spinal flexion on robot balance during trot gait. Subsequently, we develop an optimized controller for spinal flexion, designed to enhance balance without altering the leg locomotion. The effectiveness of our proposed controller is evaluated through extensive simulations and physical experiments conducted on a rat robot. Compared to both a non-spine based trot gait controller and a trot gait controller with lateral spinal flexion, our proposed optimized controller effectively improves the dynamic balance of the robot and retains the desired locomotion during trotting. keywords: {Legged locomotion;Spine;Dynamics;Kinematics;Rats;Stability analysis;Physiology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611626&isnumber=10609862

V. Barasuol, S. Emre, V. S. Medeiros, A. Bratta and C. Semini, "Introducing the Carpal-Claw: a Mechanism to Enhance High-Obstacle Negotiation for Quadruped Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3457-3463, doi: 10.1109/ICRA57147.2024.10611337.Abstract: The capability of a quadruped robot to negotiate obstacles is tightly connected to its leg workspace and joint torque limits. When facing terrain where the height of obstacles is close to the leg length, the locomotion robustness and safety are reduced since more dynamic motions are required to traverse it. In this paper, we introduce a new mechanism called the Carpal-Claw, which enables quadruped robots to negotiate higher obstacles and adds safety to the locomotion by allowing the robot to negotiate obstacles under static and quasi-static locomotion and regular joint torque demands. The design of the mechanism is detailed, as well as the methodology to exploit the mechanism in the locomotion control framework. The Carpal-Claw functionality is validated through various experiments on a very high obstacle and stairs-like terrains using an Aliengo robot. We demonstrate how Aliengo can safely descend a step height of 40cm, which is 80% of its leg length. To the best knowledge of the authors, this is the first time a mechanism like the C-Claw is proposed for improving quadruped robot locomotion over high obstacles. keywords: {Legged locomotion;Visualization;Torque;Mechanism design;Reinforcement learning;Stairs;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611337&isnumber=10609862

A. Spiridonov et al., "SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity Celestial Bodies," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3464-3470, doi: 10.1109/ICRA57147.2024.10610057.Abstract: We present SpaceHopper, a three-legged, small-scale robot designed for future mobile exploration of asteroids and moons. The robot weighs 5.2 kg and has a body size of 245 mm while using space-qualifiable components. Furthermore, SpaceHopper’s design and controls make it well-adapted for investigating dynamic locomotion modes with extended flight-phases. Instead of gyroscopes or fly-wheels, the system uses its three legs to reorient the body during flight in preparation for landing. We control the leg motion for reorientation using Deep Reinforcement Learning policies. In a simulation of Ceres’ gravity (0.029 g), the robot can reliably jump to commanded positions up to 6 m away. Our real-world experiments show that SpaceHopper can successfully reorient to a safe landing orientation within 9.7 deg inside a rotational gimbal and jump in a counterweight setup in Earth’s gravity. Overall, we consider SpaceHopper an important step towards controlled jumping locomotion in low-gravity environments. keywords: {Legged locomotion;Moon;Dynamics;Asteroids;Deep reinforcement learning;Gyroscopes;Reliability;Legged Robots;Space Robotics and Automation;Engineering for Robotic Systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610057&isnumber=10609862

M. Shafiee, G. Bellegarda and A. Ijspeert, "ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3471-3477, doi: 10.1109/ICRA57147.2024.10610155.Abstract: Learning a locomotion policy for quadruped robots has traditionally been constrained to a specific robot morphology, mass, and size. The learning process must usually be repeated for every new robot, where hyperparameters and reward function weights must be re-tuned to maximize performance for each new system. Alternatively, attempting to train a single policy to accommodate different robot sizes, while maintaining the same degrees of freedom (DoF) and morphology, requires either complex learning frameworks, or mass, inertia, and dimension randomization, which leads to prolonged training periods. In our study, we show that drawing inspiration from animal motor control allows us to effectively train a single locomotion policy capable of controlling a diverse range of quadruped robots. The robot differences encompass: a variable number of DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass range spanning from 2 kg to 200 kg, and nominal standing heights ranging from 18 cm to 100 cm. Our policy modulates a representation of the Central Pattern Generator (CPG) in the spinal cord, effectively coordinating both frequencies and amplitudes of the CPG to produce rhythmic output (Rhythm Generation), which is then mapped to a Pattern Formation (PF) layer. Across different robots, the only varying component is the PF layer, which adjusts the scaling parameters for the stride height and length. Subsequently, we evaluate the sim-to-real transfer by testing the single policy on both the Unitree Go1 and A1 robots. Remarkably, we observe robust performance, even when adding a 15 kg load, equivalent to 125% of the A1 robot’s nominal mass. keywords: {Training;Motor drives;Robot kinematics;Morphology;Generators;Distance measurement;Biology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610155&isnumber=10609862

J. Kim, J. Lee and A. D. Ames, "Safety-Critical Coordination of Legged Robots via Layered Controllers and Forward Reachable Set based Control Barrier Functions," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3478-3484, doi: 10.1109/ICRA57147.2024.10610589.Abstract: This paper presents a safety-critical approach to the coordination of robots in dynamic environments. To this end, we leverage control barrier functions (CBFs) with the forward reachable set to guarantee the safe coordination of the robots while preserving a desired trajectory via a layered controller. The top-level planner generates a safety-ensured trajectory for each agent, accounting for the dynamic constraints in the environment. This planner leverages high-order CBFs based on the forward reachable set to ensure safety-critical coordination control, i.e., guarantee the safe coordination of the robots during locomotion. The middle-level trajectory planner employs single rigid body (SRB) dynamics to generate optimal ground reaction forces (GRFs) to track the safety-ensured trajectories from the top-level planner. The whole-body motions to adhere to the optimal GRFs while ensuring the friction cone condition at the end of each stance leg are generated from the low-level controller. The effectiveness of the approach is demonstrated through simulation and hardware experiments. keywords: {Legged locomotion;Tracking;Robot kinematics;Friction;Dynamics;Hardware;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610589&isnumber=10609862

J. Lee, J. Kim, W. Ubellacker, T. G. Molnar and A. D. Ames, "Safety-critical Control of Quadrupedal Robots with Rolling Arms for Autonomous Inspection of Complex Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3485-3491, doi: 10.1109/ICRA57147.2024.10610504.Abstract: This paper presents a safety-critical control framework tailored for quadruped robots equipped with a roller arm, particularly when performing locomotive tasks such as autonomous robotic inspection in complex, multi-tiered environments. In this study, we consider the problem of operating a quadrupedal robot in distillation columns, locomoting on column trays and transitioning between these trays with a roller arm. To address this problem, our framework encompasses the following key elements: 1) Trajectory generation for seamless transitions between columns, 2) Foothold re-planning in regions deemed unsafe, 3) Safety-critical control incorporating control barrier functions, 4) Gait transitions based on safety levels, and 5) A low-level controller. Our comprehensive framework, comprising these components, enables autonomous and safe locomotion across multiple layers. We incorporate reduced-order and full-body models to ensure safety, integrating safety-critical control and footstep re-planning approaches. We validate the effectiveness of our proposed framework through practical experiments involving a quadruped robot equipped with a roller arm, successfully navigating and transitioning between different levels within the column tray structure. keywords: {Location awareness;Uncertainty;Service robots;Navigation;Inspection;Manipulators;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610504&isnumber=10609862

L. Jin, K. Liu and M. Liu, "Robust and Remote Center of Cyclic Motion Control for Redundant Robots with Partially Unknown Structure," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3492-3498, doi: 10.1109/ICRA57147.2024.10611145.Abstract: Remote center of motion (RCM) describes a robot with a rod-like end-effector operating through a hole in the interface separating the internal space from the external space. Considering that the control of RCM may be influenced by perturbations (noises) and that the end-effector is frequently replaced to complete different tasks, the structural information related to the robot manipulator and its rod-like end-effector may contain errors. This paper proposes an acceleration-level remote center of cyclic motion (ARC2M) control scheme, which takes into account the cyclic motion index and the physical limitations of robot manipulators to achieve repetitive motion planning and RCM control at the acceleration level. Additionally, a parameter calculation method is proposed to compute unknown parameters of the end-effector under the influence of noise. Kalman filter and a neural dynamics-based method are employed to address noises effects, and related theoretical analyses are given. To validate the proposed ARC2M scheme, simulations and physical experiments are carried out. The source code is available at https://github.com/LongJin-lab/ARCM. keywords: {Trajectory tracking;Source coding;Perturbation methods;Noise;Aerospace electronics;End effectors;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611145&isnumber=10609862

Z. Wang, S. Yuan, M. Dou, J. Yang and B. Liang, "Phase Synthesis for Spatial Locomotion Control of Retractable Worm Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3507-3513, doi: 10.1109/ICRA57147.2024.10610882.Abstract: Retractable worm robots possess hyper-flexibility, allowing them to work in confined spaces that are difficult for humans. However, the spatial locomotion control of these robots remains challenging due to the robots’ large degrees of freedom. To address this challenge, we propose a phase synthesis (PS) scheme for retractable worm robots. The scheme combines an undulating gait inspired by caterpillars with three-dimensional movement commands. We first introduce the kinematics model and real-world prototype of our retractable worm robot, called RW-Robot, and then we introduce footstep phases to express the timing of segments’ spatial movement. According to the length of movement periods, we classify the movement into short-term movements and long-term movements and compress their patterns in the frequency domain. Our PS scheme aligns the patterns according to the footstep phases to generate new gaits of spatial locomotion. We evaluate the scheme in real-world experiments, including steering and climbing a slope. The experimental results indicate that our scheme allows the RW-Robot to perform flexible spatial locomotion from simple user input. keywords: {Frequency-domain analysis;Imitation learning;Prototypes;Kinematics;Aerospace electronics;Motors;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610882&isnumber=10609862

X. Jia, J. Yang, K. Lu, Y. Pan and H. Yu, "Enhanced Robust Motion Control based on Unknown System Dynamics Estimator for Robot Manipulators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3514-3519, doi: 10.1109/ICRA57147.2024.10611460.Abstract: To achieve high-accuracy manipulation in the presence of unknown disturbances, we propose two novel efficient and robust motion control schemes for high-dimensional robot manipulators. Both controllers incorporate an unknown system dynamics estimator (USDE) to estimate disturbances without requiring acceleration signals and the inverse of inertia matrix. Then, based on the USDE framework, an adaptive-gain controller and a super-twisting sliding mode controller are designed to speed up the convergence of tracking errors and strengthen anti-perturbation ability. The former aims to enhance feedback portions through error-driven control gains, while the latter exploits finite-time convergence of discontinuous switching terms. We analyze the boundedness of control signals and the stability of the closed-loop system in theory, and conduct real hardware experiments on a robot manipulator with seven degrees of freedom (DoF). Experimental results verify the effectiveness and improved performance of the proposed controllers, and also show the feasibility of implementation on high-dimensional robots. keywords: {Uncertainty;Torque;System dynamics;Trajectory tracking;Switches;Stability analysis;Motion control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611460&isnumber=10609862

Y. M. Al-Rawashdeh, M. Al Saaideh, M. F. Heertjes and M. Al Janaideh, "Model-Free Control of a Class of High-Precision Scanning Motion Systems with Piezoceramic Actuators," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3520-3525, doi: 10.1109/ICRA57147.2024.10611589.Abstract: To enhance the precision of coarse long-stroke motion axes, complementary short-stroke fine positioning stages are usually introduced. Being mechanically attached, the motion of the combined positioning stages needs to be controlled and synchronized. Therefore, typically suitable model-based controllers of fine stages are designed according to the sophisticated models and identification techniques used. Due to their appealing features, Piezocermamic-based fine positioning stages were successfully utilized in many applications, which recently sparked their use in high-acceleration motion found in wafer scanners, for example, where high-precision motion is required despite the resulting high inertial forces involved. Unfortunately, hard nonlinear behavior is associated with piezoelectric actuators, which adds to the complexity of modeling, control, and synchronization processes. To overcome such a burden, in this study, the design procedure of a model-free control and synchronization technique of piezocermamic-based fine positioning stages is introduced and verified experimentally using a representative precision motion system comprising a planner stage and a uni-axial fine stage under step-and-scan trajectories commonly used in wafer scanners. Despite its simplicity, the herein proposed design procedure can be seamlessly extended to other robotics and automation applications. keywords: {Semiconductor device modeling;Measurement;Tracking;Shape;Process control;Piezoelectric actuators;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611589&isnumber=10609862

J. W. Han, D. Park and M. J. Kim, "Constrained Nonlinear Disturbance Observer for Robotic Systems," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3526-3532, doi: 10.1109/ICRA57147.2024.10611091.Abstract: Disturbance observer (DOB) is a well-known two-loop control structure that imparts robustness to a controller with a simple implementation. As a nonlinear DOB for the robotic systems, we proposed so-called nonlinear robust internal-loop compensator (NRIC) framework in our previous work. In this paper, we further extend the NRIC in such a way that an optimization scheme can be embedded in the control structure. The proposed method is called constrained NRIC (C-NRIC), because the optimization allows us to impose constraints, by which a controller acquires additional properties. As a particular use case of the C-NRIC framework, we design contact-responsive motion controllers that enables a robot to react to unknown interactions while accurately tracking the desired trajectory in free motion. The effectiveness of such designs is validated through the real-world experiments. keywords: {Tracking;Force;Robot sensing systems;Disturbance observers;Robustness;Trajectory;Motion control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611091&isnumber=10609862

X. Xiao et al., "An Integrated Position-velocity-force Method for Safety-enhanced Shared Control in Robot-assisted Surgical Cutting," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3533-3539, doi: 10.1109/ICRA57147.2024.10610466.Abstract: Numerous studies have emphasized the application of autonomous intelligence in human-robot shared control to enhance surgical convenience and efficiency. However, the neglect of human dominance may reduce surgical safety. This paper developed a safety-enhanced human-robot shared control method by intelligently allocating control authority, with the surgeon remaining the leader during the surgical procedure. Three controllers are designed initially, including a master hand position (MP) controller and a master hand velocity (MV) controller related to the surgeon's manipulation, and a planned trajectory tracking (PT) controller related to the robot. In precision surgical manipulation scenarios, precise tracking of the human's operation is achieved by combining MP and MV controllers, while a combination of MV and PT controllers is developed in high-efficiency surgical scenarios, which relaxes the requirement for precise tracking of hand position and enables precise robot assistance guided by the velocity of human hand. The autonomous scenarios and controllers switching are accomplished through a motion fusion mechanism, which is achieved via optimizing evaluation functions that are reliant on future states. Furthermore, a force feedback mechanism is proposed to help human understand the intent of autonomous control to improve safety. The feasibility and effectiveness of this method have been validated through simulations and experiments. keywords: {Medical robotics;Trajectory tracking;Force feedback;Dynamics;Surgery;Switches;Controllability;surgical robot;shared control;motion fusion;safety-enhanced;force feedback},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610466&isnumber=10609862

J. Zhu et al., "Robotic Craniomaxillofacial Osteotomy System Using Acoustic 3D Registration *," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3554-3560, doi: 10.1109/ICRA57147.2024.10610887.Abstract: Osteotomy holds a pivotal position among the fundamental procedures in craniomaxillofacial (CMF) surgery. However, there are inherent challenges and risks associated with ensuring the recuperation of occlusion, safeguarding the facial nerves and blood vessels, as well as preserving facial aesthetics. In this study, a non-invasive image-to-patient registration method for navigation/robotic CMF surgery based on intraoperative freehand ultrasound (US) 3D reconstruction is proposed. Building upon this, a CMF osteotomy robotic system with compliant human-robot interaction and osteotomy trajectory planning was devised. In the freehand US 3D reconstruction and registration experiments, the registration errors for human volunteers and phantoms were consistently less than 1 mm. In robot osteotomy experiments based on the resulting registration, the average osteotomy error was below 1.5 mm. The proposed US 3D reconstruction based registration method is non-invasive and radiation-free, and shows the promising accuracy which is suitable for CMF robotic or navigation systems. keywords: {Three-dimensional displays;Accuracy;Ultrasonic imaging;Navigation;Trajectory planning;Surgery;Phantoms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610887&isnumber=10609862

T. Li, C. Zhao, Y. Wen, F. Chen, Y. Tan and Z. Zhou, "Elliptical torus-based Six-axis FBG Force Sensor with In-situ Calibration for Condition Monitoring of Orthopedic Surgical Robot*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3561-3566, doi: 10.1109/ICRA57147.2024.10611093.Abstract: Six-axis force/moment (6-A F/M) sensors make surgical robots effectively sense intraoperative force feedback and drilling status information, reducing the operating challenges and psychological burden of doctors, which also improves the quality and safety of surgery. However, it is difficult for current commercial electrical 6-A F/M sensors to adapt the electromagnetic environment in the operating room, and status changes after installation can also reduce accuracy. At the same time, there is a strong vibration coupling of low-frequency force information, leading to low identification accuracy and slow response speed in the drilling and milling status. Aiming at these problems, an elliptical torus-based 6-A fiber optic F/M sensor and its in-situ calibration method for orthopedic surgical robot force sensing are proposed. Furthermore, combined with the multichannel one-dimensional convolutional gated recurrent unit (M1-DCGRU), a fast and accurate identification of seven drilling stages was realized. The final force sensing error is less than 7.1%, and the drilling state identification accuracy is at least 93.9%. The designed sensor has higher accuracy, is compatible with magnetic resonance imaging (MRI), and accurately identifies finer drilling stages without relying on other sensors. keywords: {Drilling;Image sensors;Accuracy;Medical robotics;Magnetic sensors;Magnetic resonance imaging;Force},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611093&isnumber=10609862

C. Qian, Z. Li, Q. Ye, P. Ge, J. Zhao and G. -B. Bian, "A Hybrid Admittance Control Algorithm for Automatic Robotic Cranium-Milling," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3575-3581, doi: 10.1109/ICRA57147.2024.10610672.Abstract: Prior robot-assisted cranium-milling studies only considered controlling the force in the skull’s vertical direction and neglected the milling cutter’s feed force. Additionally, achieving stable force control in multiple directions is challenging for robots due to the uneven skull surface. Here a hybrid admittance control algorithm incorporating a model-free adaptive nonlinear force control and fuzzy control algorithms is proposed to accomplish effective automatic cranial-milling tasks. First, a pure data-driven model-free adaptive control method based on partial form dynamic linearization is used to control the feed force. Second, fuzzy control minimizes the total error of both the vertical and feed force by adaptively adjusting the milling cutter’s velocity and position. 42 ex vivo animal skull-milling experiments conducted by the automatic robotic cranium-milling system indicate that when using the proposed control algorithm, the force error percentage can be maintained below 5.0% within 3 s and the maximal root mean square error percentages for vertical and feed force are 1.85% and 1.94%, respectively. Moreover, no instances of dura mater damage are observed and the robotic system exhibits a high level of autonomy as it performs the skull milling task with minimal human involvement throughout the entire experiment. The results suggest the potential for advancing the intelligence level of neurosurgery in the future. keywords: {Fuzzy control;Adaptation models;Force;Milling;Skull;Feeds;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610672&isnumber=10609862

M. Miyasaka, P. Van Esch, A. Morikawa and K. Tadano, "Preliminary Study of Fingertip and Wrist Motion Based Haptic Controller for Robotically Assisted Micro- and Supermicrosurgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3582-3587, doi: 10.1109/ICRA57147.2024.10610859.Abstract: One issue of robotic microsurgery is that compared to manual surgery, the operation time tends to be longer due to high motion scaling. To address this issue, we developed a new controller that can provide the accuracy required for microsurgery without a high scaling factor by utilizing fingertip and wrist motions. Also, for the better outcome of surgery, the proposed controller has a force feedback function which is not available for the existing controllers for microsurgical robots. A challenge of designing such a controller is associated with the size requirement. For conventional microsurgery, surgeons perform surgical procedures while looking at the eyepieces of a surgical microscope and the same applies for robotic microsurgery. The only space available to manipulate controllers is a narrow space between the patient/surgical bed and the surgeon. To satisfy this constraint, the proposed controller is integrated with a handrest and the controller’s DOFs are strategically allocated. In this work, as the first step of addressing the issue of prolonged operational time, we built a prototype controller and evaluated the accuracy and task space with simulations. The results indicated that by using the fingertip and wrist motions with a scaling factor of 3x, 0.5 mm diameter circles could be traced with a mean bidirectional precision of 0.0485 mm. Also, 10.0 mm diameter circles were traceable with the same scaling factor. keywords: {Wrist;Accuracy;Microscopy;Force feedback;Refining;Prototypes;Microsurgery},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610859&isnumber=10609862

H. Ishida et al., "Haptic-Assisted Collaborative Robot Framework for Improved Situational Awareness in Skull Base Surgery," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3588-3594, doi: 10.1109/ICRA57147.2024.10611187.Abstract: Skull base surgery is a demanding field in which surgeons operate in and around the skull while avoiding critical anatomical structures including nerves and vasculature. While image-guided surgical navigation is the prevailing standard, limitation still exists requiring personalized planning and recognizing the irreplaceable role of a skilled surgeon. This paper presents a collaboratively controlled robotic system tailored for assisted drilling in skull base surgery. Our central hypothesis posits that this collaborative system, enriched with haptic assistive modes to enforce virtual fixtures, holds the potential to significantly enhance surgical safety, streamline efficiency, and alleviate the physical demands on the surgeon. The paper describes the intricate system development work required to enable these virtual fixtures through haptic assistive modes. To validate our system’s performance and effectiveness, we conducted initial feasibility experiments involving a medical student and two experienced surgeons. The experiment focused on drilling around critical structures following cortical mastoidectomy, utilizing dental stone phantom and cadaveric models. Our experimental results demonstrate that our proposed haptic feedback mechanism enhances the safety of drilling around critical structures compared to systems lacking haptic assistance. With the aid of our system, surgeons were able to safely skeletonize the critical structures without breaching any critical structure even under obstructed view of the surgical site. keywords: {Drilling;Navigation;Fixtures;Surgery;Phantoms;Skull;Haptic interfaces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611187&isnumber=10609862

Y. Luan et al., "Intelligent Disinfection Robot with High-Touch Surface Detection and Dynamic Pedestrian Avoidance," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3595-3601, doi: 10.1109/ICRA57147.2024.10610836.Abstract: The increasing awareness of public health issues has highlighted the need for effective disinfection of crowded indoor public areas, leading to the development of automated disinfection robots. However, most of the existing robots spray disinfectant in all areas, and they are still immature to navigate in densely populated environments. Hence, in this paper, we design a new disinfection robotic system consisting of a mobile platform, an RGB-D camera, and a robotic arm with a spray disinfection device. To address the above challenges, we propose a vision-based method for accurately detecting high-touch areas in the surroundings, enabling the disinfection robot to achieve superior disinfection efficiency. In addition, we propose a dynamic pedestrian avoidance method, namely Socially Aware APF (SA-APF), which can predict the movement trend of pedestrians and plan the path in real-time. Both simulated and real-world experiments are conducted to demonstrate the effectiveness of our disinfection robot system, especially highlighting the ability to detect high-touch areas and navigate in the environment while avoiding dynamic pedestrians. keywords: {Visualization;Pedestrians;Uncertainty;Navigation;Heuristic algorithms;Robot vision systems;Market research},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610836&isnumber=10609862

Z. Chen et al., "Toward a framework integrating augmented reality and virtual fixtures for safer robot-assisted lymphadenectomy," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3602-3608, doi: 10.1109/ICRA57147.2024.10610099.Abstract: Lymphadenectomy generally accompanies various oncology surgeries to remove infected cancer cells. However, there are two limitations in robot-assisted lymphadenectomy: 1) lymph nodes are not visible during operation since they are hidden by the superficial fat layer; 2) intra-operative bleeding may occur during lymph node removal caused by collisions between surgical instruments and delicate blood vessels (arteries or veins) near the lymph nodes. Therefore, we propose a framework integrating augmented reality and virtual fixtures to address these limitations. Augmented reality intra-operatively visualizes the hidden lymph nodes by projecting the corresponding 3D pre-operative model, and virtual fixtures are used to provide force feedback to surgeons to avoid possible collisions when they operate the surgical instruments to resect the lymph nodes surrounding the blood vessel. Ten human subjects were invited to perform an emulated lymphadenectomy based on the da Vinci robot in a dry lab. Experimental results demonstrated that the proposed framework can keep localizing the hidden lymph nodes, and reduce the number of collisions (21% and 48% reduction rates using two different force models compared to the standard setup, respectively) between the instruments and the delicate blood vessel during lymph node resection. It shows the potential to enhance the safety of robot-assisted lymphadenectomy. keywords: {Visualization;Three-dimensional displays;Instruments;Veins;Fixtures;Surgery;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610099&isnumber=10609862

S. -H. Hyon, R. Ando, E. Sono, S. Sugimoto and Y. Saitou, "Hyblock: Hardware Realization and Control of Modular Hydraulic Robots with Dowel Connectors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3609-3615, doi: 10.1109/ICRA57147.2024.10611336.Abstract: This paper presents the hardware design and development of Hyblock, a modular hydraulic robot for heavy-duty application such as construction. The robot is equipped with a simple docking mechanism called a C-type expansion dowel and a novel hydraulic circuit MHSB that matches the modular structure. In this paper, we first report on the design of the robot hardware including the dowel and hydraulic circuit, then present preliminary experiments on pressure-based torque control and docking control using proximal magnetic sensors. Next, we propose a framework for dynamic reconfiguration and task-space motion control built on the concept of dowel connectors. Simulation results demonstrate that a collective modular robot achieves desired motion tasks while keeping all normal contact forces of the connectors being lower-bound. The results are also explained in the supplementary video. keywords: {Connectors;Simulation;Magnetic sensors;Torque control;Dynamics;Hydraulic systems;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611336&isnumber=10609862

M. Smith, A. Abdel-Rahman and N. Gershenfeld, "Self-Reconfigurable Robots for Collaborative Discrete Lattice Assembly," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3624-3631, doi: 10.1109/ICRA57147.2024.10609866.Abstract: We present a robotic system for the assembly of 3D discrete lattice structures in which the robots are able to self-reproduce, such that the assembly system may scale its own parallelization. Robots and structures are made from a set of compatible building blocks, or voxels, which can be assembled and reassembled into more complex structures. Robotic modules are made by combining actuators with a functional voxel, which routes electrical power and signals. Robotic modules then assemble into reconfigurable robots via a reversible solder joint. The robot assembles higher performance structures using a set of construction voxels, which do not contain electrical features. This paper describes the design, development, and evaluation of this assembly system, including the robotic hardware, lattice material, and planning and controls methods. We demonstrate the system through a set of fundamental assembly tasks: the robot assembling another robot, and the two robots collaborating to assemble a small structure. keywords: {Actuators;Assembly systems;Three-dimensional displays;Lattices;Collaboration;Hardware;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609866&isnumber=10609862

J. Rowell, L. Zhang and M. Fallon, "LiSTA: Geometric Object-Based Change Detection in Cluttered Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3632-3638, doi: 10.1109/ICRA57147.2024.10610102.Abstract: We present LiSTA (LiDAR Spatio-Temporal Analysis), a system to detect probabilistic object-level change over time using multi-mission SLAM. Many applications require such a system, including construction, robotic navigation, long-term autonomy, and environmental monitoring. We focus on the semi-static scenario where objects are added, subtracted, or changed in position over weeks or months. Our system combines multi-mission LiDAR SLAM, volumetric differencing, object instance description, and correspondence grouping using learned descriptors to keep track of an open set of objects. Object correspondences between missions are determined by clustering the object’s learned descriptors. We demonstrate our approach using datasets collected in a simulated environment and a real-world dataset captured using a LiDAR system mounted on a quadruped robot monitoring an industrial facility containing static, semi-static, and dynamic objects. Our method demonstrates superior performance in detecting changes in semi-static environments compared to existing methods. keywords: {Point cloud compression;Laser radar;Simultaneous localization and mapping;Service robots;Navigation;Scalability;Octrees},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610102&isnumber=10609862

S. Lensgraf, A. Sarkar, A. Pediredla, D. Balkcom and A. Q. Li, "Scalable underwater assembly with reconfigurable visual fiducials," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3639-3645, doi: 10.1109/ICRA57147.2024.10611643.Abstract: We present a scalable combined localization infrastructure deployment and task planning algorithm for underwater assembly. Infrastructure is autonomously modified to suit the needs of manipulation tasks based on an uncertainty model on the infrastructure’s positional accuracy. Our uncertainty model can be combined with the noise characteristics from multiple sensors. For the task planning problem, we propose a layer-based clustering approach that completes the manipulation tasks one cluster at a time. We employ movable visual fiducial markers as infrastructure and an autonomous underwater vehicle (AUV) for manipulation tasks. The proposed task planning algorithm is computationally simple, and we implement it on AUV without any offline computation requirements. Combined hardware experiments and simulations over large datasets show that the proposed technique is scalable to large areas. keywords: {Location awareness;Visualization;Uncertainty;Noise;Clustering algorithms;Sensor phenomena and characterization;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611643&isnumber=10609862

D. Eriksson, R. Ghabcheloo and M. Geimer, "Automatic Loading of Unknown Material with a Wheel Loader Using Reinforcement Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3646-3652, doi: 10.1109/ICRA57147.2024.10610221.Abstract: Loading multiple different materials with wheel loaders is a challenging task because various materials require different loading techniques. It’s, therefore, difficult to find a single controller capable of handling them all. One solution is to use a base controller and fine-tune it for different materials. Reinforcement Learning (RL) automates this process without the need for collecting additional human-annotated data. We investigated the feasibility of this approach using a full-size 24-tonnes wheel loader in the real world and demonstrated that it’s possible to fine-tune a neural network controller that was originally trained with imitation learning on blasted rock for use with an unknown gravel material, requiring 20 bucket fillings. Additionally, we showcased the adaptability of a controller pre-trained on woodchips for an unknown gravel material, requiring 40 bucket fillings. We also proposed a novel reward function for the material loading task. Finally, we examined how the sampling time of the reinforcement learning algorithm affects convergence speed and adaptability. Our results demonstrate that it’s optimal to match the sampling time of the RL algorithm to the delays of the wheel loader’s hydraulic actuators. keywords: {Loading;Neural networks;Wheels;Reinforcement learning;Rocks;Filling;Delays},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610221&isnumber=10609862

H. Wang, W. Xu, L. Hou and E. Pan, "Ospreys-inspired Self-takeoff Strategy of An Eagle-scale Flapping-wing Robot: System Design and Flight Experiments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3669-3675, doi: 10.1109/ICRA57147.2024.10610958.Abstract: In this work, we achieved a self-takeoff of an eagle-scale flapping-wing robot for the first time. Inspired by the takeoff process of Ospreys, we propose a bio-inspired takeoff strategy, then discuss the dynamic model and the requirements for self-takeoff. Based on the requirements of flight strategy, we designed a system with two parts, including a flapping-wing aircraft with a wingspan of 1.8m and a take-off weight of 870g, and an auxiliary platform with an initial pitch angle adjustment function. In order to explore the differences in the take-off process under different conditions, we conduct the flight experiments under different time-averaged thrust-to-weight ratios (0.745-0.876) and launch angles (45°-90°). The results of flight experiments confirmed the theoretical analysis that the flapping-wing robot can achieve self-takeoff with no potential energy cost and maintain high maneuverability (The video shows a rapid climb immediately after takeoff) even when the time-averaged thrust-to-weight ratio is smaller than 1. This is significantly different from conventional rotary-wing and vertical take-off and landing (VTOL) UAVs. This work solves the challenge of self-takeoff for large-scale flapping-wing robots using a designable method and demonstrates the superior performance potential of flapping-wing robots compared to conventional UAVs. keywords: {Potential energy;Legged locomotion;Energy consumption;Costs;Aerodynamics;Aircraft;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610958&isnumber=10609862

E. Sanchez-Laulhe, Á. C. Satué Crespo, S. Rafee Nekoo and A. Ollero, "Model-Based Approach for Lateral Maneuvers of Bird-Size Ornithopter," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3684-3690, doi: 10.1109/ICRA57147.2024.10611580.Abstract: A model-based approach for lateral maneuvering of flapping wing UAVs in closed spaces is presented. Bird-size ornithopters do not have asymmetric actuation in the wing due to mechanical complexity, so they rely upon the tail for lateral maneuvering. The prototype E-Flap can deflect the vertical tail to make maneuvers out of the longitudinal plane. This work defines simplified equations for the steady turning maneuver based on the body roll angle. The relation between the velocity of the prototype and the turning radius is also stated. Then, an approach to the attitude is proposed, defining the relation between the deflection of the vertical tail and the roll angle. We prove that, even though this deflection causes a yaw moment, the coupling between yaw and roll dynamics generates also a roll rate. To validate this simplified model, a simple control is presented for continuous circular trajectory tracking inside an indoor flight zone. The objective is to track circular trajectories of a radius 2 times greater than the wingspan at a constant height. Results show a very good agreement between the theoretical and experimental turning radius. In addition, the direct relation between the vertical tail deflection and the roll rate of the ornithopter is identified. Even though the desired radius is not reached, the FWUAV is capable of maintaining a closed turning maneuver for several laps. Therefore, the insight provided by the model proves to be an appropriate approach for aggressive lateral maneuvers of bird-size ornithopters. keywords: {Trajectory tracking;Prototypes;Tail;Turning;Aerodynamics;Trajectory;Transient analysis;Flapping-wing;Circular flight;Ornithopter;Dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611580&isnumber=10609862

S. Li, Y. Zhai, C. Wang and G. Xie, "Real-Time Estimation for the Swimming Direction of Robotic Fish Based on IMU Sensors*," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3721-3727, doi: 10.1109/ICRA57147.2024.10610815.Abstract: An increasing number of underwater robots inspired by Carangidae are developed, which is characterized by high efficiency and flexibility. However, estimating the swimming direction of these robotic fish is challenging due to the constant swinging of the head during movement, which complicates precise control. In this study, we installed two low-cost inertial measurement unit (IMU) sensors separately on the head and tail parts of a double-joint robotic fish and presented a method for accurately and timely estimating the swimming direction. Firstly, we effectively compensated for the yaw angle drift of the IMU sensors through a fused Kalman Filter. Furthermore, we propose the Anti-Shake Estimation (ASE) algorithm to calculate the real-time swimming direction using filtered yaw angles at a high updating rate of 100Hz. Finally, we applied the method to swimming direction feedback control for evaluation and comparison. The results show that our ASE method performs better than other existing methods in straight-line swimming experiments. The experiment of S-curve swimming also demonstrates the effectiveness of our method in complex missions. keywords: {Measurement units;Estimation;Tail;Sensor phenomena and characterization;Sensor fusion;Robot sensing systems;Fish},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610815&isnumber=10609862

J. Li and S. X. Yang, "A Novel Fish-inspired Self-adaptive Approach to Collective Escape of Swarm Robots Based on Neurodynamic Models," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3736-3742, doi: 10.1109/ICRA57147.2024.10610169.Abstract: Fish schools present high-efficiency group behaviors to collective migration and dynamic escape from the predator through simple individual interactions. The purpose of this research is to infuse swarm robots with "fish-like" intelligence that will enable safe navigation and efficient cooperation, and successful completion of escape tasks in changing environments. In this paper, a novel fish-inspired self-adaptive approach is proposed for the collective escape of swarm robots. A bio-inspired neural network (BINN) is introduced to generate collision-free escape trajectories through the dynamics of neural activity and the combination of attractive and repulsive forces. In addition, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in dynamic environments. Similar to fish escape maneuvers, simulations and real-robot experiments show that the swarm robots can collectively leave away from the threat and respond to sudden environmental changes. Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness, efficiency, and flexibility of swarm robots in complex environments. keywords: {Uncertainty;Navigation;Biological system modeling;Dynamics;Neural activity;Swarm robotics;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610169&isnumber=10609862

H. B. Amundsen, T. F. Olsen, M. Xanthidis, M. Føre and E. Kelasidi, "RUMP: Robust Underwater Motion Planning in Dynamic Environments of Fast-moving Obstacles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3743-3750, doi: 10.1109/ICRA57147.2024.10610406.Abstract: Robust underwater motion planning of autonomous underwater vehicles (AUVs) in dynamic cluttered environments is a problem that has yet to be addressed in depth. Due to advances in technology and computational capacity, AUVs are expected to operate safely and autonomously in increasingly challenging environments, necessitating methods that are able to safely navigate robots in real-time. Though, most solutions remain overly cautious and conservative. This paper proposes RUMP, a novel locally-optimal motion planning framework for robust real-time autonomous underwater navigation in 3D cluttered environments consisting of observed static and dynamic obstacles. The problem is modeled using path optimization and can be solved in real-time with a common nonlinear solver. The constructed objective function allows deciding the local goal during optimization to both maximize safety within a planning horizon and minimize the expected distance to the target position. Furthermore, path safety is considered for the entire transition between consecutive states, utilizing a novel approach for continuous spatiotemporal collision checks. The proposed formulation provides safe performance even in environments with obstacles that may move orders of magnitude faster than the AUV itself. Simulation experiments, in different challenging scenarios of obstacles moving up to 100 times faster than the robot, showcase robustness and efficient real-time performance of more than 15 Hz. keywords: {Time-frequency analysis;Dynamics;Real-time systems;Robustness;Underwater navigation;Planning;Spatiotemporal phenomena},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610406&isnumber=10609862

L. Ebner, G. Billings and S. Williams, "Metrically Scaled Monocular Depth Estimation through Sparse Priors for Underwater Robots," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3751-3757, doi: 10.1109/ICRA57147.2024.10611007.Abstract: In this work, we address the problem of real-time dense depth estimation from monocular images for mobile underwater vehicles. We formulate a deep learning model that fuses sparse depth measurements from triangulated features to improve the depth predictions and solve the problem of scale ambiguity. To allow prior inputs of arbitrary sparsity, we apply a dense parameterization method. Our model extends recent state-of-the-art approaches to monocular image based depth estimation, using an efficient encoder-decoder backbone and modern lightweight transformer optimization stage to encode global context. The network is trained in a supervised fashion on the forward-looking underwater dataset, FLSea. Evaluation results on this dataset demonstrate significant improvement in depth prediction accuracy by the fusion of the sparse feature priors. In addition, without any retraining, our method achieves similar depth prediction accuracy on a downward looking dataset we collected with a diver operated camera rig, conducting a survey of a coral reef. The method achieves real-time performance, running at 24 FPS on a NVIDIA Jetson Xavier NX, 160 FPS on a NVIDIA RTX 2080 GPU and 7 FPS on a single Intel i9-9900K CPU core, making it suitable for direct deployment on embedded GPU systems. The implementation of this work is made publicly available at https://github.com/ebnerluca/uw_depth. keywords: {Surveys;Deep learning;Accuracy;Fuses;Graphics processing units;Estimation;Predictive models;Marine Robotics;Computer Vision for Automation;Deep Learning for Visual Perception},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611007&isnumber=10609862

S. Gode, A. Hinduja and M. Kaess, "SONIC: Sonar Image Correspondence using Pose Supervised Learning for Imaging Sonars," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3766-3772, doi: 10.1109/ICRA57147.2024.10611678.Abstract: In this paper, we address the challenging problem of data association for underwater SLAM through a novel method for sonar image correspondence using learned features. We introduce SONIC (SONar Image Correspondence), a pose-supervised network designed to yield robust feature correspondence capable of withstanding viewpoint variations. The inherent complexity of the underwater environment stems from the dynamic and frequently limited visibility conditions, restricting vision to a few meters of often featureless expanses. This makes camera-based systems suboptimal in most open water application scenarios. Consequently, multibeam imaging sonars emerge as the preferred choice for perception sensors. However, they too are not without their limitations. While imaging sonars offer superior long-range visibility compared to cameras, their measurements can appear different from varying viewpoints. This inherent variability presents formidable challenges in data association, particularly for feature-based methods. Our method demonstrates significantly better performance in generating correspondences for sonar images which will pave the way for more accurate loop closure constraints and sonar-based place recognition. Code as well as simulated and real-world datasets are made public on https://github.com/rpl-cmu/sonic to facilitate further development in the field. keywords: {Meters;Image sensors;Simultaneous localization and mapping;Image recognition;Sonar measurements;Supervised learning;Sonar},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611678&isnumber=10609862

A. Abdullah, T. Barua, R. Tibbetts, Z. Chen, M. J. Islam and I. Rekleitis, "CaveSeg: Deep Semantic Segmentation and Scene Parsing for Autonomous Underwater Cave Exploration," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3781-3788, doi: 10.1109/ICRA57147.2024.10611543.Abstract: In this paper, we present CaveSeg - the first visual learning pipeline for semantic segmentation and scene parsing for AUV navigation inside underwater caves. We address the problem of scarce annotated training data by preparing a comprehensive dataset for semantic segmentation of underwater cave scenes. It contains pixel annotations for important navigation markers (e.g. caveline, arrows), obstacles (e.g. ground plain and overhead layers), scuba divers, and open areas for servoing. Through comprehensive benchmark analyses on cave systems in USA, Mexico, and Spain locations, we demonstrate that robust deep visual models can be developed based on CaveSeg for fast semantic scene parsing of underwater cave environments. In particular, we formulate a novel transformer-based model that is computationally light and offers near real-time execution in addition to achieving state-of-the-art performance. Finally, we explore the design choices and implications of semantic segmentation for visual servoing by AUVs inside underwater caves. The proposed model and benchmark dataset open up promising opportunities for future research in autonomous underwater cave exploration and mapping. keywords: {Visualization;Navigation;Semantic segmentation;Computational modeling;Semantics;Training data;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611543&isnumber=10609862

S. McCammon, S. Jamieson, T. A. Mooney and Y. Girdhar, "Discovering Biological Hotspots with a Passively Listening AUV," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3789-3795, doi: 10.1109/ICRA57147.2024.10610917.Abstract: We present a novel system which blends multiple distinct sensing modalities in audio-visual surveys to assist marine biologists in collecting datasets for understanding the ecological relationship of fish and other organisms with their habitats on and around coral reefs. Our system, designed for the CUREE AUV, uses four hydrophones to determine the bearing to biological sound sources through beamforming. These observations are merged in a Bayesian Occupancy Grid to produce a 2D map of the acoustic activity of a coral reef. Simultaneously, the AUV uses unsupervised topic modeling to identify different benthic habitats. Combining these maps allows us to determine the level of acoustic activity within each habitat. We demonstrated the system in field trials on reefs in the U.S. Virgin Islands, where it was able to autonomously discover the favored habitats of snapping shrimp (genus Alpheus). keywords: {Surveys;Temperature measurement;Visualization;Array signal processing;Habitats;Marine vegetation;Fish},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610917&isnumber=10609862

M. Cui, M. Feng, J. Long, D. Hu, S. Zhao and K. Huang, "A Du-Octree based Cross-Attention Model for LiDAR Geometry Compression," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3796-3802, doi: 10.1109/ICRA57147.2024.10610640.Abstract: Point cloud compression is an essential technology for efficient storage and transmission of 3D data. Previous methods usually use hierarchical tree data structures for encoding the spatial sparseness of point clouds. However, the node context within the tree is not fully discovered since the feature space among nodes varies significantly. To address this problem, we innovatively represent the LiDAR points in a two-octree structure instead of using traditional single-octree coding, and then design the cross-attention model to capture the hierarchical features between different octrees, of which each octree incorporates a transformer-based deep entropy model and an arithmetic encoder. Besides, we introduce the untied cross-aware position encoding with principal component analysis and different projection matrices, which enhances the correlations over two octrees’ attention feature embeddings. Experimental results show that our method outperforms the previous state-of-the-art works, achieving up to 8.2% Bpp savings on point cloud benchmark datasets with different lasers. keywords: {Point cloud compression;Geometry;Tree data structures;Laser radar;Three-dimensional displays;Octrees;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610640&isnumber=10609862

J. Sun, X. Zhou, T. Ban, J. Zhao and F. Shuang, "Energy Consumption Modelling of Coaxial-Rotor in Vortex Ring State for Controllable High-speed Descending," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3803-3809, doi: 10.1109/ICRA57147.2024.10610429.Abstract: The ability to fast climb and descend is crucial for Unmanned Aerial Vehicle (UAV) applications in the mountains. The slower descent speed will affect the UAV’s working efficiency in reaching the rescue area. However, during the fast descent of the rotorcraft, a chaotic flow field rampages as the rotorcraft falls into its wake flow. This is known as the vortex ring. Therefore, the safe descent velocity of consumer UAVs is usually limited to approximately 3m/s. This limitation reduces the potential of UAVs to execute tasks in mountainous and plateau regions. To broaden the task capability constrained by the maximum descending speed, it is necessary to jointly analyze the flow field and the energy consumption during descending. Existing research mainly focused on how to avoid entering the vortex ring instead of offering sufficient power to fly with it. In this paper, in order to achieve an efficient rotorcraft for rescuing in mountainous and plateaus, we break through the maximum-descending-speed of a coaxial rotors UAV. Hence, a power consumption managing pipeline is proposed to extend the power tolerance of the UAV. Specifically, a theoretic model for the coaxial rotors is proposed to analyze the induced velocity and energy consumption during vertical descending. Then, the theoretic model is verified to be consistent with the Computational Fluid Dynamics (CFD) and wind tunnel experiment results. Finally, we optimized the tolerance of the power and dynamic system according to the theoretic model. With this pipeline, our real-time flight achieved 8m/s controlled vertical-descent-speed (CVDS), which is a leading result in both quadrotors and coaxial UAVs. keywords: {Energy consumption;Computational modeling;Computational fluid dynamics;Wind tunnels;Pipelines;Rotors;Autonomous aerial vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610429&isnumber=10609862

T. Lan, L. Romanello, M. Kovac, S. F. Armanini and B. Bahadir Kocer, "Aerial Tensile Perching and Disentangling Mechanism for Long-Term Environmental Monitoring," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3827-3833, doi: 10.1109/ICRA57147.2024.10609975.Abstract: Aerial robots show significant potential for forest canopy research and environmental monitoring by providing data collection capabilities at high spatial and temporal resolutions. However, limited flight endurance hinders their application. Inspired by natural perching behaviours, we propose a multi-modal aerial robot system that integrates tensile perching for energy conservation and a suspended actuated pod for data collection. The system consists of a quadrotor drone, a slewing ring mechanism allowing 360° tether rotation, and a streamlined pod with two ducted propellers connected via a tether. Winding and unwinding the tether allows the pod to move within the canopy, and activating the propellers allows the tether to be wrapped around branches for perching or disentangling. We experimentally determined the minimum counterweights required for stable perching under various conditions. Building on this, we devised and evaluated multiple perching and disentangling strategies. Comparisons of perching and disentangling manoeuvres demonstrate energy savings that could be further maximized with the use of the pod or tether winding. These approaches can reduce energy consumption to only 22% and 1.5%, respectively, compared to a drone disentangling manoeuvre. We also calculated the minimum idle time required by the proposed system after the system perching and motor shut down to save energy on a mission, which is 48.9% of the operating time. Overall, the integrated system expands the operational capabilities and enhances the energy efficiency of aerial robots for long-term monitoring tasks. keywords: {Propellers;Energy conservation;Windings;Forestry;Data collection;Autonomous aerial vehicles;Energy efficiency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10609975&isnumber=10609862

L. Wälti and A. Martinoli, "Lumped Drag Model Identification and Real-Time External Force Detection for Rotary-Wing Micro Aerial Vehicles," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3853-3860, doi: 10.1109/ICRA57147.2024.10610862.Abstract: This work focuses on understanding and identifying the drag forces applied to a rotary-wing Micro Aerial Vehicle (MAV). We propose a lumped drag model that concisely describes the aerodynamical forces the MAV is subject to, with a minimal set of parameters. We only rely on commonly available sensor information onboard a MAV, such as accelerometer data, pose estimate, and throttle commands, which makes our method generally applicable. The identification uses an offline gradient-based method on flight data collected over specially designed trajectories. The identified model allows us to predict the aerodynamical forces experienced by the aircraft due to its own motion in real-time and, therefore, will be useful to distinguish them from external perturbations, such as wind or physical contact with the environment. The results show that we are able to identify the drag coefficients of a rotary-wing MAV through onboard flight data and observe the close correlation between the motion of the MAV, the measured external forces, and the predicted drag forces. keywords: {Accelerometers;Force measurement;Drag;Perturbation methods;Atmospheric modeling;Wind tunnels;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610862&isnumber=10609862

K. Murali, E. P. Moreno and L. R. Lustosa, "Flight Validation of a Global Singularity-Free Aerodynamic Model for Flight Control of Tail Sitters," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3861-3867, doi: 10.1109/ICRA57147.2024.10610780.Abstract: This work validates through flight tests a previously developed wide-envelope singularity-free aerodynamic framework, called ϕ-theory, for modeling dual-engine tail-sitting flying-wing vehicles for optimization-based control. The ϕ-theory methodology imposes a specific geometry on aerodynamic coefficients that leads to polynomial differential equations of motion amenable to semidefinite programming optimization. Through ϕ-theory, we illustrate a typical predicted longitudinal and lateral flight envelope of a tail-sitting vehicle, which, while commonplace for fixed-wing aircraft in performance textbooks, is a novel figure that generalizes fixed-wing doghouse plots to tail-sitting vehicles. This flight envelope figure suggests a novel, natural and intuitive remote piloting interface that we validate in flight tests. Furthermore, we further validate ϕ-theory through the computation of flight features in simulation and their subsequent observation in flight tests. keywords: {Semidefinite programming;Transmitters;Computational modeling;Tail;Aerodynamics;Polynomials;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610780&isnumber=10609862

G. Nations, C. L. Nelson and D. S. Drew, "Empirical Study of Ground Proximity Effects for Small-scale Electroaerodynamic Thrusters," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3868-3875, doi: 10.1109/ICRA57147.2024.10610481.Abstract: Electroaerodynamic (EAD) propulsion, where thrust is produced by collisions between electrostatically-accelerated ions and neutral air, is a potentially transformative method for indoor flight owing to its silent and solid-state nature. Like rotors, EAD thrusters exhibit changes in performance based on proximity to surfaces. Unlike rotors, they have no fragile and quickly spinning parts that have to avoid those surfaces; taking advantage of the efficiency benefits from proximity effects may be a route towards longer-duration indoor operation of ion-propelled fliers. This work presents the first empirical study of ground proximity effects for EAD propulsors, both individually and as quad-thruster arrays. It focuses on multi-stage ducted centimeter-scale actuators suitable for use on small robots envisioned for deployment in human-proximal and indoor environments. Three specific effects (ground, suckdown, and fountain lift), each occurring with a different magnitude at a different spacing from the ground plane, are investigated and shown to have strong dependencies on geometric parameters including thruster-to-thruster spacing, thruster protrusion from the fuselage, and inclusion of flanges or strakes. Peak thrust enhancement ranging from 300 to 600% is found for certain configurations operated in close proximity (0.2 mm) to the ground plane and as much as a 20% increase is measured even when operated centimeters away. keywords: {Actuators;Attitude control;Proximity effects;Rotors;Propulsion;Flanges;Market research},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610481&isnumber=10609862

A. Pilko, A. Oakey, M. Ferraro and J. Scanlan, "The Price of a Safe Flight: Risk Cost Based Path Planning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3884-3890, doi: 10.1109/ICRA57147.2024.10610752.Abstract: A risk aware UAS path planning methodology is proposed using monetary value as the sole cost metric. A third party ground risk model is used to generate a non-uniform costmap for a modified A* heuristic search. The Value of a Prevented Fatality provides a basis to convert fatality risk to monetary value terms as a Human Value at Risk (HVaR) measure. Additional operating and UAS Capital Value at Risk (CVaR) costs are modelled to provide a holistic monetary cost model for path cost minimisation. A number of future cost variants are investigated based upon prior work for a realistic urban-rural mix logistics case study in Southern England. Results show increasingly risk averse paths with decreasing future UAS operating costs. keywords: {Measurement;Reactive power;Costs;Probabilistic logic;Minimization;Path planning;Robotics and automation;uas safety;ground risk;path planning;value of a statistical life},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610752&isnumber=10609862

Y. Fan et al., "Tight Fusion of Odometry and Kinematic Constraints for Multiple Aerial Vehicles in Physical Interconnection," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3891-3897, doi: 10.1109/ICRA57147.2024.10610282.Abstract: Integrated aerial Platforms (IAPs), comprising multiple aircrafts, are typically fully actuated and hold significant potential for aerial manipulation tasks. Differing from a multiple aerial swarm, the aircrafts within the IAP are interconnected, presenting promising opportunities for enhancing localization. Incorporating the physical constraints of these multiple aircrafts to improve the accuracy and reliability of integrated aircraft positioning and navigation systems is a challenging yet highly significant problem. In this paper, we introduce a distributed multi-aircraft visual-inertial-range odometry system that analyzes the position, velocity, and attitude constraints within the IAP. Leveraging constraint relationships in the IAP, we propose corresponding methods that tightly fuse visual-inertial-range odometry and kinematic constraints to optimize odometry accuracy. Our system’s performance is validated using a collected dataset, resulting in a notable 28.7% reduction in drift compared to the baseline. keywords: {Location awareness;Accuracy;System performance;Kinematics;Aircraft navigation;Odometry;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610282&isnumber=10609862

Y. Zhang, H. Luo, W. Wang and W. Feng, "Robust Multi-Robot Global Localization with Unknown Initial Pose based on Neighbor Constraints," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3898-3904, doi: 10.1109/ICRA57147.2024.10610066.Abstract: Multi-robot global localization (MR-GL) with unknown initial positions in a large scale environment is a challenging task. The key point is the data association between different robots’ viewpoints. It also makes traditional Appearance-based localization methods unusable. Recently, researchers have utilized the object’s semantic invariance to generate a semantic graph to address this issue. However, previous works lack robustness and are sensitive to overlap rate of maps, resulting in unpredictable performance in real-world environments. In this paper, we propose a data association algorithm based on neighbor constraints to improve the robustness of the system. We demonstrate the effectiveness of our method on three different datasets, indicating a significant improvement in robustness compared to previous works. keywords: {Location awareness;Semantics;Pipelines;Robot sensing systems;Robustness;Real-time systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610066&isnumber=10609862

Y. Zhang, S. Bu, Y. Dong, Y. Zhang, K. Li and L. Chen, "AutoFusion: Autonomous Visual Geolocation and Online Dense Reconstruction for UAV Cluster," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3913-3919, doi: 10.1109/ICRA57147.2024.10611315.Abstract: Real-time dense reconstruction using Unmanned Aerial Vehicle (UAV) is becoming increasingly popular in large-scale rescue and environmental monitoring tasks. However, due to the energy constraints of a single UAV, the efficiency can be greatly improved through the collaboration of multi-UAVs. Nevertheless, when faced with unknown environments or the loss of Global Navigation Satellite System (GNSS) signal, most multi-UAV SLAM systems can’t work, making it hard to construct a global consistent map. In this paper, we propose a real-time dense reconstruction system called AutoFusion for multiple UAVs, which robustly supports scenarios with lost global positioning and weak co-visibility. A method for Visual Geolocation and Matching Network (VGMN) is suggested by constructing a graph convolutional neural network as a feature extractor. It can acquire geographical location information solely through images. We also present a real-time dense reconstruction framework for multi-UAV with autonomous visual geolocation. UAV agents send images and relative positions to the ground server, which processes the data using VGMN for multi-agent geolocation optimization, including initialization, pose graph optimization, and map fusion. Extensive experiments demonstrate that our system can efficiently and stably construct large-scale dense maps in real-time with high accuracy and robustness. keywords: {Visualization;Global navigation satellite system;Simultaneous localization and mapping;Geology;Autonomous aerial vehicles;Real-time systems;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611315&isnumber=10609862

S. Zhong et al., "CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3920-3926, doi: 10.1109/ICRA57147.2024.10611672.Abstract: Collaborative state estimation using different heterogeneous sensors is a fundamental prerequisite for robotic swarms operating in GPS-denied environments, posing a significant research challenge. In this paper, we introduce a centralized system to facilitate collaborative LiDAR-ranging-inertial state estimation, enabling robotic swarms to operate without the need for anchor deployment. The system efficiently distributes computationally intensive tasks to a central server, thereby reducing the computational burden on individual robots for local odometry calculations. The server back-end establishes a global reference by leveraging shared data and refining joint pose graph optimization through place recognition, global optimization techniques, and removal of outlier data to ensure precise and robust collaborative state estimation. Extensive evaluations of our system, utilizing both publicly available datasets and our custom datasets, demonstrate significant enhancements in the accuracy of collaborative SLAM estimates. Moreover, our system exhibits remarkable proficiency in large-scale missions, seamlessly enabling ten robots to collaborate effectively in performing SLAM tasks. In order to contribute to the research community, we will make our code open-source and accessible at https://github.com/PengYu-team/Co-LRIO. keywords: {Simultaneous localization and mapping;Accuracy;Scalability;Collaboration;Computational efficiency;Sensors;Servers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611672&isnumber=10609862

L. Han, Z. Liu and S. Lian, "TP3M: Transformer-based Pseudo 3D Image Matching with Reference Image," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3962-3968, doi: 10.1109/ICRA57147.2024.10610556.Abstract: Image matching is still challenging in such scenes with large viewpoints or illumination changes or with low textures. In this paper, we propose a Transformer-based pseudo 3D image matching method. It upgrades the 2D features extracted from the source image to 3D features with the help of a reference image and matches to the 2D features extracted from the destination image by the coarse-to-fine 3D matching. Our key discovery is that by introducing the reference image, the source image’s fine points are screened and furtherly their feature descriptors are enriched from 2D to 3D, which improves the match performance with the destination image. Experimental results on multiple datasets show that the proposed method achieves the state-of-the-art on the tasks of homography estimation, pose estimation and visual localization especially in challenging scenes. keywords: {Location awareness;Visualization;Three-dimensional displays;Simultaneous localization and mapping;Image matching;Pose estimation;Lighting},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610556&isnumber=10609862

A. Fontan, J. Civera and M. Milford, "Adaptive Outlier Thresholding for Bundle Adjustment in Visual SLAM," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3969-3976, doi: 10.1109/ICRA57147.2024.10610433.Abstract: State-of-the-art V-SLAM pipelines utilize robust cost functions and outlier rejection techniques to remove incorrect correspondences. However, these methods are typically fine-tuned to overfit certain benchmarks and struggle to adapt effectively to changes in the application domain or environmental conditions. This renders them impractical for many robotic applications in which robustness in a wide variety of conditions is essential. In this paper we introduce a novel distribution-based approach for online outlier rejection that reduces the necessity for scene-specific fine-tuning while simultaneously improving the overall SLAM performance. Through experiments across 3 different public datasets, we show that our approach consistently outperforms state-of-the-art methods in various real-world settings. Our code is available at https://github.com/alejandrofontan/ORB_SLAM2_Distribution keywords: {Bundle adjustment;Training;Gamma distribution;Visualization;Simultaneous localization and mapping;Pipelines;Training data},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610433&isnumber=10609862

X. Guo, H. Peng, J. Hu, H. Bao and G. Zhang, "From Satellite to Ground: Satellite Assisted Visual Localization with Cross-view Semantic Matching," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3977-3983, doi: 10.1109/ICRA57147.2024.10611658.Abstract: One of the key challenges of visual Simultaneous Localization and Mapping (SLAM) in large-scale environments is how to effectively use global localization to correct the cumulative errors from long-term tracking. This challenge presents itself in two main aspects: first, the difficulty for robots in revisiting previous locations to perform loop closure, and second, the considerable memory resources required to maintain point-cloud-based global maps. Recent solutions have resorted into neural networks, using satellite images as the references for ground-level localization. However, most of these methods merely provide cross-view patch-matching results, which leads to unfeasible in integration with the SLAM system. To address these issues, we present a semantic-based cross-view localization method. This approach combines semantic information with a reward and penalty mechanism, enabling us to obtain a global probability map and achieve precise 3-degree-of-freedom (3-DoF) localization. Based on that, we develop a SLAM system that capitalizes on satellite imagery for global localization. This strategy effectively bridges the gap between SLAM and real-world coordinates while also substantially reducing accumulated errors. Our experimental results demonstrate that our global localization method significantly outperforms existing satellite-based systems. Moreover, in scenarios where the robot struggles to find loop closures, employing our localization method improves the SLAM accuracy. keywords: {Location awareness;Visualization;Simultaneous localization and mapping;Satellites;Accuracy;Robot kinematics;Semantics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611658&isnumber=10609862

C. Wang, G. Zhang and W. Zhou, "Self-Supervised Learning of Monocular Visual Odometry and Depth with Uncertainty-Aware Scale Consistency," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3984-3990, doi: 10.1109/ICRA57147.2024.10610075.Abstract: The inherent scale ambiguity issue greatly limits the performance of monocular visual odometry. In recent years, a variety of methods have been proposed for self-supervised learning of ego-motion and depth estimation, incorporating specifically designed scale-consistency constraints that utilize estimated depth as a reference. However, these existing methods neglect the influence of the depth uncertainty introduced by the dominant photometric loss, which leads to unreliable depth estimation in difficult regions and detrimentally affects scale alignment. To solve these problems, we introduces a feature-based visual odometry learning system with an effective scale recovery strategy in this paper. Additionally, we propose a learning method to estimate the photometric-sensitive depth uncertainty for guiding the scale recovery. The proposed method is evaluated on KITTI odometry, and the experimental results demonstrate that our system can predict scale-consistent trajectories from monocular videos and achieves state-of-the-art performance. Moreover, the proposed method achieves competitive performance on KITTI depth estimation. keywords: {Uncertainty;Estimation;Self-supervised learning;Trajectory;Odometry;Robotics and automation;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610075&isnumber=10609862

A. García-Hernández, R. Giubilato, K. H. Strobl, J. Civera and R. Triebel, "Unifying Local and Global Multimodal Features for Place Recognition in Aliased and Low-Texture Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3991-3998, doi: 10.1109/ICRA57147.2024.10611563.Abstract: Perceptual aliasing and weak textures pose significant challenges to the task of place recognition, hindering the performance of Simultaneous Localization and Mapping (SLAM) systems. This paper presents a novel model, called UMF (standing for Unifying Local and Global Multimodal Features) that 1) leverages multi-modality by cross-attention blocks between vision and LiDAR features, and 2) includes a re-ranking stage that re-orders based on local feature matching the top-k candidates retrieved using a global representation. Our experiments, particularly on sequences captured on a planetary-analogous environment, show that UMF outperforms significantly previous baselines in those challenging aliased environments. Since our work aims to enhance the reliability of SLAM in all situations, we also explore its performance on the widely used RobotCar dataset, for broader applicability. Code and models are available at https://github.com/DLR-RM/UMF. keywords: {Visualization;Simultaneous localization and mapping;Laser radar;Codes;Fuses;Transformers;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611563&isnumber=10609862

Z. Chen et al., "RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse Environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 3999-4005, doi: 10.1109/ICRA57147.2024.10611074.Abstract: LiDAR-based localization is valuable for applications like mining surveys and underground facility maintenance. However, existing methods can struggle when dealing with uninformative geometric structures in challenging scenarios. This paper presents RELEAD, a LiDAR-centric solution designed to address scan-matching degradation. Our method enables degeneracy-free point cloud registration by solving constrained ESIKF updates in the front end and incorporates multisensor constraints, even when dealing with outlier measurements, through graph optimization based on Graduated Non-Convexity (GNC). Additionally, we propose a robust Incremental Fixed Lag Smoother (rIFL) for efficient GNC-based optimization. RELEAD has undergone extensive evaluation in degenerate scenarios and has outperformed existing state-of-the-art LiDAR-Inertial odometry and LiDAR-Visual-Inertial odometry methods. keywords: {Degradation;Location awareness;Surveys;Laser radar;Current measurement;Pose estimation;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611074&isnumber=10609862

Z. Xu, Z. Ren, Q. Zhang, J. Lou, D. Tao and J. Cheng, "Semantic-focused Patch Tokenizer with Multi-branch Mixer for Visual Place Recognition," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4006-4012, doi: 10.1109/ICRA57147.2024.10610372.Abstract: Visual Place Recognition (VPR) is critical for navigation and loop closure in autonomous driving tasks, mitigating the impact of shift errors caused by dynamic changes in the environment. Due to the limited ability of backbone networks and extreme environmental changes, current methods fail to capture foundational semantic details that include the distinctive attributes for unique place identification. To address this problem, we propose a new visual token-guided VPR framework that contains a semantic-focused patch tokenizer and a multi-branch Mixer. To mitigate the inference from place-unrelated objects, the semantic-focused patch tokenizer exploits attention-based channel selection and spatial partition, which efficiently captures important semantic information within the channels and preserve spatial relationships among the backbone features. To extract abstract features with spatial structure information, the multi-branch Mixer utilizes a multi-branch structure to aggregate local and global position information, improving the robustness of global representations to environmental changes. Experimental results demonstrate that our method outperforms state-of-the-art methods, achieving 85.3% Recall@1 on the MSLS val dataset and 59.1% Recall@1 on the Nordland dataset when using ResNet18 as the backbone. keywords: {Visualization;Navigation;Semantics;Feature extraction;Robustness;Data mining;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610372&isnumber=10609862

J. -J. Chao, S. Engin, N. Chavan-Dafle, B. Lee and V. Isler, "VioLA: Aligning Videos to 2D LiDAR Scans," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4021-4028, doi: 10.1109/ICRA57147.2024.10610757.Abstract: We study the problem of aligning a video that captures a local portion of an environment to the 2D LiDAR scan of the entire environment. We introduce a method (VioLA) that starts with building a semantic map of the local scene from the image sequence, then extracts points at a fixed height for registering to the LiDAR map. Due to reconstruction errors or partial coverage of the camera scan, the reconstructed semantic map may not contain sufficient information for registration. To address this problem, VioLA makes use of a pre-trained text-to-image inpainting model paired with a depth completion model for filling in the missing scene content in a geometrically consistent fashion to support pose registration. We evaluate VioLA on two real-world RGB-D benchmarks, as well as a self-captured dataset of a large office scene. Notably, our proposed scene completion module improves the pose registration performance by up to 20%. keywords: {Geometry;Casting;Laser radar;Three-dimensional displays;Semantics;Buildings;Text to image},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610757&isnumber=10609862

K. Z. Luo et al., "Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4029-4035, doi: 10.1109/ICRA57147.2024.10610276.Abstract: Autonomous driving has traditionally relied heavily on costly and labor-intensive High Definition (HD) maps, hindering scalability. In contrast, Standard Definition (SD) maps are more affordable and have worldwide coverage, offering a scalable alternative. In this work, we systematically explore the effect of SD maps for real-time lane-topology understanding. We propose a novel framework to integrate SD maps into online map prediction and propose a Transformer-based encoder, SD Map Encoder Representations from transFormers, to leverage priors in SD maps for the lane-topology prediction task. This enhancement consistently and significantly boosts (by up to 60%) lane detection and topology prediction on current state-of-the-art online map prediction methods without bells and whistles and can be immediately incorporated into any Transformer-based lane-topology method. Code is available at https://github.com/NVlabs/SMERF. keywords: {Navigation;Lane detection;Scalability;Prediction methods;Transformers;Real-time systems;Topology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610276&isnumber=10609862

S. Sun, M. Mielle, A. J. Lilienthal and M. Magnusson, "3QFP: Efficient neural implicit surface reconstruction using Tri-Quadtrees and Fourier feature Positional encoding," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4036-4044, doi: 10.1109/ICRA57147.2024.10610338.Abstract: Neural implicit surface representations are currently receiving a lot of interest as a means to achieve high-fidelity surface reconstruction at a low memory cost, compared to traditional explicit representations. However, state-of-the-art methods still struggle with excessive memory usage and non-smooth surfaces. This is particularly problematic in large-scale applications with sparse inputs, as is common in robotics use cases. To address these issues, we first introduce a sparse structure, tri-quadtrees, which represents the environment using learnable features stored in three planar quadtree projections. Secondly, we concatenate the learnable features with a Fourier feature positional encoding. The combined features are then decoded into signed distance values through a small multilayer perceptron. We demonstrate that this approach facilitates smoother reconstruction with a higher completion ratio with fewer holes. Compared to two recent baselines, one implicit and one explicit, our approach requires only 10%–50% as much memory, while achieving competitive quality. The code is released on https://github.com/ljjTYJR/3QFP. keywords: {Surface reconstruction;Costs;Codes;Memory management;Multilayer perceptrons;Encoding;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610338&isnumber=10609862

J. Liu and H. Chen, "Towards Large-Scale Incremental Dense Mapping using Robot-centric Implicit Neural Representation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4045-4051, doi: 10.1109/ICRA57147.2024.10611564.Abstract: Large-scale dense mapping is vital in robotics, digital twins, and virtual reality. Recently, implicit neural mapping has shown remarkable reconstruction quality. However, incremental large-scale mapping with implicit neural representations remains problematic due to low efficiency, limited video memory, and the catastrophic forgetting phenomenon. To counter these challenges, we introduce the Robot-centric Implicit Mapping (RIM) technique for large-scale incremental dense mapping. This method employs a hybrid representation, encoding shapes with implicit features via a multi-resolution voxel map and decoding signed distance fields through a shallow MLP. We advocate for a robot-centric local map to boost model training efficiency and curb the catastrophic forgetting issue. A decoupled scalable global map is further developed to archive learned features for reuse and maintain constant video memory consumption. Validation experiments demonstrate our method’s exceptional quality, efficiency, and adaptability across diverse scales and scenes over advanced dense mapping methods using range sensors. Our system’s code will be accessible at https://github.com/HITSZ-NRSL/RIM.git. keywords: {Training;Solid modeling;Adaptation models;Shape;Memory management;Virtual reality;Sensor phenomena and characterization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611564&isnumber=10609862

S. Xu et al., "Camera Relocalization in Shadow-free Neural Radiance Fields," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4052-4059, doi: 10.1109/ICRA57147.2024.10611228.Abstract: Camera relocalization is a crucial problem in computer vision and robotics. Recent advancements in neural radiance fields (NeRFs) have shown promise in synthesizing photo-realistic images. Several works have utilized NeRFs for refining camera poses, but they do not account for lighting changes that can affect scene appearance and shadow regions, causing a degraded pose optimization process. In this paper, we propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization. We implement our scene representation upon a hash-encoded NeRF which significantly boosts up the pose optimization process. To account for the noisy image gradient computing problem in grid-based NeRFs, we further propose a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient averaging technique to smoothen the process. Experimental results on several datasets with varying lighting conditions demonstrate that our method achieves state-of-the-art results in camera relocalization under varying lighting conditions. Code and data will be made publicly available. keywords: {Training;Accuracy;Robot vision systems;Pipelines;Refining;Lighting;Neural radiance field},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611228&isnumber=10609862

J. Wu, H. Yu, W. Yang and G. -S. Xia, "QuadricsNet: Learning Concise Representation for Geometric Primitives in Point Clouds," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4060-4066, doi: 10.1109/ICRA57147.2024.10610722.Abstract: This paper presents a novel framework to learn a concise geometric primitive representation for 3D point clouds. Different from representing each type of primitive individually, we focus on the challenging problem of how to achieve a concise and uniform representation robustly. We employ quadrics to represent diverse primitives with only 10 parameters and propose the first end-to-end learning-based framework, namely QuadricsNet, to parse quadrics in point clouds. The relationships between quadrics mathematical formulation and geometric attributes, including the type, scale and pose, are insightfully integrated for effective supervision of QuaidricsNet. Besides, a novel pattern-comprehensive dataset with quadrics segments and objects is collected for training and evaluation. Experiments demonstrate the effectiveness of our concise representation and the robustness of QuadricsNet. Our code is available at https://github.com/MichaelWu99-lab/QuadricsNet. keywords: {Point cloud compression;Training;Three-dimensional displays;Codes;Robustness;Robotics and automation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610722&isnumber=10609862

J. Zhang and Y. Zhang, "ERASOR++: Height Coding Plus Egocentric Ratio Based Dynamic Object Removal for Static Point Cloud Mapping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4067-4073, doi: 10.1109/ICRA57147.2024.10610396.Abstract: Mapping plays a crucial role in location and navigation within automatic systems. However, the presence of dynamic objects in 3D point cloud maps generated from scan sensors can introduce map distortion and long traces, thereby posing challenges for accurate mapping and navigation. To address this issue, we propose ERASOR++, an enhanced approach based on the Egocentric Ratio of Pseudo Occupancy for effective dynamic object removal. To begin, we introduce the Height Coding Descriptor, which combines height difference and height layer information to encode the point cloud. Subsequently, we propose the Height Stack Test, Ground Layer Test, and Surrounding Point Test methods to precisely and efficiently identify the dynamic bins within point cloud bins, thus overcoming the limitations of prior approaches. Through extensive evaluation on open-source datasets, our approach demonstrates superior performance in terms of precision and efficiency compared to existing methods. Furthermore, the techniques described in our work hold promise for addressing various challenging tasks or aspects through subsequent migration. keywords: {Point cloud compression;Technological innovation;Accuracy;Three-dimensional displays;Image analysis;Navigation;Encoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610396&isnumber=10609862

Z. Liao, J. Yang, J. Qian, A. P. Schoellig and S. L. Waslander, "Uncertainty-aware 3D Object-Level Mapping with Deep Shape Priors," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4082-4089, doi: 10.1109/ICRA57147.2024.10611206.Abstract: 3D object-level mapping is a fundamental problem in robotics, which is especially challenging when object CAD models are unavailable during inference. We propose a framework that can reconstruct high-quality object-level maps for unknown objects. Our approach takes multiple RGB-D images as input and outputs dense 3D shapes and 9-DoF poses (including 3 scale parameters) for detected objects. The core idea is to leverage a learnt generative model for a category of object shapes as priors and to formulate a probabilistic, uncertainty-aware optimization framework for 3D reconstruction. We derive a probabilistic formulation that propagates shape and pose uncertainty through two novel loss functions. Unlike current state-of-the-art approaches, we explicitly model the uncertainty of the object shapes and poses during our optimization, resulting in a high-quality object-level mapping system. Moreover, the estimated shape and pose uncertainties, which we demonstrate can accurately reflect the true errors of our object maps, can be useful for downstream robotics tasks such as active vision. We perform extensive evaluations on indoor and outdoor real-world datasets, achieving substantial improvements over state-of-the-art methods. Our code is available at https://github.com/TRAILab/UncertainShapePose. keywords: {Solid modeling;Uncertainty;Three-dimensional displays;Shape;Robot vision systems;Probabilistic logic;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611206&isnumber=10609862

S. Garg et al., "RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4090-4097, doi: 10.1109/ICRA57147.2024.10610234.Abstract: Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on , which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a continuous sense of a place, defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of hops over segments and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level ‘hopping’ based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/. keywords: {Image segmentation;Visualization;Three-dimensional displays;Navigation;Search problems;Cognition;Robot localization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610234&isnumber=10609862

J. Luo, Y. Liu, H. Wang, M. Ding and X. Lan, "Grasp Manipulation Relationship Detection based on Graph Sample and Aggregation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4098-4104, doi: 10.1109/ICRA57147.2024.10611627.Abstract: In multi-object stacking scenarios, exploring the relationships among objects and determining the correct sequence of operations are crucial for robotic manipulation. However, previous algorithms inefficiently combine global and local information, often focusing solely on the local features of objects or the interactions of object features at a global level. This approach leads to imbalanced distribution of features and the generation of redundant or missing relationships in complex scenes, such as multi-object stacking and partial occlusion. To address this issue, we have developed a grasp manipulation relationship detection algorithm called Graph Sampling Aggregation Network for Visual Manipulation Relationship Detection (GSAGED). This algorithm assists robots in detecting targets in complex scenes and determining the appropriate grasping order. Firstly, the Positional Encoding Module in GSAGED enhances object feature information by considering global contexts. Secondly, the Graph Sampling Aggregation method effectively integrates global and local information, relieving imbalanced distribution of features. Finally, we applied the developed algorithm to a physical robot for grasping. Experimental results on the Visual Manipulation Relationship Dataset (VMRD) and the large-scale relational grasp dataset named REGRAD demonstrate that our method significantly improves the accuracy of relationship detection in complex scenes and exhibits robust generalization capabilities in real-world applications. keywords: {Visualization;Accuracy;Stacking;Focusing;Grasping;Encoding;Detection algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611627&isnumber=10609862

V. R. S, W. Mandil, K. Nazari, S. Parsons and A. G. E, "Acoustic Soft Tactile Skin (AST Skin)," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4105-4111, doi: 10.1109/ICRA57147.2024.10610768.Abstract: This paper presents a novel acoustic soft tactile (AST) skin technology operating with sound waves. In this innovative approach, the sound waves generated by a speaker travel in channels embedded in a soft membrane and get modulated due to a deformation of the channel when pressed by an external force and received by a microphone at the end of the channel. The sensor leverages regression and classification methods for estimating the normal force and its contact location. Our sensor can be affixed to any robot part, e.g., end effectors or arm. We tested several regression and classifier methods to learn the relation between sound wave modulation, the applied force, and its location, respectively and picked the best-performing models for force and location predictions. The best skin configurations yield more than 93% of the force estimation within ±1.5 N tolerances for a range of 0-30+1 N and contact locations with over 96% accuracy. We also demonstrated the performance of AST Skin technology for a real-time gripping force control application. keywords: {Accuracy;Force;Surface acoustic waves;Estimation;Prototypes;Robot sensing systems;Skin},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610768&isnumber=10609862

J. Huber, F. Hélénon, H. Watrelot, F. B. Amar and S. Doncieux, "Domain Randomization for Sim2real Transfer of Automatically Generated Grasping Datasets," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4112-4118, doi: 10.1109/ICRA57147.2024.10610677.Abstract: Robotic grasping refers to making a robotic system pick an object by applying forces and torques on its surface. Many recent studies use data-driven approaches to address grasping, but the sparse reward nature of this task made the learning process challenging to bootstrap. To avoid constraining the operational space, an increasing number of works propose grasping datasets to learn from. But most of them are limited to simulations. The present paper investigates how automatically generated grasps can be exploited in the real world. More than 7000 reach-and-grasp trajectories have been generated with Quality-Diversity (QD) methods on 3 different arms and grippers, including parallel fingers and a dexterous hand, and tested in the real world. Conducted analysis on the collected measure shows correlations between several Domain Randomization-based quality criteria and sim-to-real transferability. Key challenges regarding the reality gap for grasping have been identified, stressing matters on which researchers on grasping should focus in the future. A QD approach has finally been proposed for making grasps more robust to domain randomization, resulting in a transfer ratio of 84% on the Franka Research 3 arm. keywords: {Correlation;Refining;Grasping;Trajectory;Task analysis;Grippers;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610677&isnumber=10609862

J. Starke and T. Asfour, "Kinematic Synergy Primitives for Human-Like Grasp Motion Generation," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4119-4125, doi: 10.1109/ICRA57147.2024.10611490.Abstract: Grasping with five-fingered humanoid hands is a complex control problem. Throughout the entire grasping motion, all finger joints need to be coordinated to achieve a stable grasp. Grasp synergies provide a simplified, low-dimensional representation of grasp postures and motions, that can be used for the description of human grasps as well as the generation of novel, human-like grasps. However, the abstract synergy representation complicates the association of relevant high-level grasp parameters, as for example the grasp type and final posture or the grasp speed. Therefore, it is difficult to control these grasp characteristics in the synergy space. This paper presents an adaptable representation for kinematic grasping motions in synergy space, that allows the generation of novel, human-like grasps under direct control of high-level grasp parameters. It is based on via-point movement primitives trained on synergy trajectories of human grasping motions. The representation using synergy primitives allows for a straightforward adaptation of grasp characteristics while preserving the essential grasping motion learned from human demonstration. The kinematic synergy primitives have a low reproduction error of 3.9% of the maximum finger joint angle and are able to generate successful grasps on a simulated human hand and a real prosthetic hand. keywords: {Shape;Humanoid robots;Grasping;Kinematics;Aerospace electronics;Trajectory;Timing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611490&isnumber=10609862

P. Piacenza, J. Yuan, J. Huh and V. Isler, "VFAS-Grasp: Closed Loop Grasping with Visual Feedback and Adaptive Sampling," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4126-4132, doi: 10.1109/ICRA57147.2024.10611183.Abstract: We consider the problem of closed-loop robotic grasping and present a novel planner which uses Visual Feedback and an uncertainty-aware Adaptive Sampling strategy (VFAS) to close the loop. At each iteration, our method VFAS-Grasp builds a set of candidate grasps by generating random perturbations of a seed grasp. The candidates are then scored using a novel metric which combines a learned grasp-quality estimator, the uncertainty in the estimate and the distance from the seed proposal to promote temporal consistency. Additionally, we present two mechanisms to improve the efficiency of our sampling strategy: We dynamically scale the sampling region size and number of samples in it based on past grasp scores. We also leverage a motion vector field estimator to shift the center of our sampling region. We demonstrate that our algorithm can run in real time (20 Hz) and is capable of improving grasp performance for static scenes by refining the initial grasp proposal. We also show that it can enable grasping of slow moving objects, such as those encountered during human to robot handover. Video: https://youtu.be/8DRe2OFlf7o keywords: {Wrist;Visualization;Uncertainty;Semantics;Grasping;Streaming media;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611183&isnumber=10609862

M. Tisdale and J. W. Burdick, "The Fractal Hand-II: Reviving a Classic Mechanism for Contemporary Grasping Challenges," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4133-4139, doi: 10.1109/ICRA57147.2024.10611267.Abstract: This paper and its companion propose a new fractal robotic gripper, drawing inspiration from the centuryold Fractal Vise. The unusual synergistic properties allow it to passively conform to diverse objects using only one actuator. Designed to be easily integrated with prevailing parallel jaw grippers, it alleviates the complexities tied to perception and grasp planning, especially when dealing with unpredictable object poses and geometries. We build on the foundational principles of the Fractal Vise to a broader class of gripping mechanisms and address the limitations that had led to its obscurity. Two Fractal Fingers, coupled with a closing actuator, can form an adaptive and synergistic Fractal Hand. We articulate a design methodology for low-cost, easy-to-fabricate, large workspace, and compliant Fractal Fingers. The companion paper delves into the kinematics and grasping properties of a specific class of Fractal Fingers and Hands. keywords: {Geometry;Space vehicles;Actuators;Pipelines;Kinematics;Grasping;Fractals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611267&isnumber=10609862

R. Zurbrügg et al., "ICGNet: A Unified Approach for Instance-Centric Grasping," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4140-4146, doi: 10.1109/ICRA57147.2024.10611725.Abstract: Accurate grasping is the key to several robotic tasks including assembly and household robotics. Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding: First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps. These grasps need to be compliant with the local object geometry. Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene. Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry of the target object. Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not capture the composability of the environment. In this paper, we introduce an end-to-end architecture for object-centric grasping. The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an instance-centric representation for each partially observed object in the scene. This representation is further used for object reconstruction and grasp detection in cluttered table-top scenes. We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art methods on synthetic datasets, indicating superior performance for grasping and reconstruction. Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of objects. Videos and Code icgraspnet.github.io. keywords: {Geometry;Grasping;Computer architecture;Trajectory;Collision avoidance;Task analysis;Detection algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611725&isnumber=10609862

K. DuFrene, K. Nave, J. Campbell, R. Balasubramanian and C. Grimm, "The Grasp Reset Mechanism: An Automated Apparatus for Conducting Grasping Trials," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4147-4153, doi: 10.1109/ICRA57147.2024.10610892.Abstract: Advancing robotic grasping and manipulation requires the ability to test algorithms and/or train learning models on large numbers of grasps. Towards the goal of more advanced grasping, we present the Grasp Reset Mechanism (GRM), a fully automated apparatus for conducting large-scale grasping trials. The GRM automates the process of resetting a grasping environment, repeatably placing an object in a fixed location and controllable 1-D orientation. It also collects data and swaps between multiple objects enabling robust dataset collection with no human intervention. We also present a standardized state machine interface for control, which allows for integration of most manipulators with minimal effort. In addition to the physical design and corresponding software, we include a dataset of 1,020 grasps. The grasps were created with a Kinova Gen3 robot arm and Robotiq 2F-85 Adaptive Gripper to enable training of learning models and to demonstrate the capabilities of the GRM. The dataset includes ranges of grasps conducted across four objects and a variety of orientations. Manipulator states, object pose, video, and grasp success data are provided for every trial. keywords: {Training;Adaptation models;Process control;Grasping;Manipulators;Software;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610892&isnumber=10609862

H. Liu, S. Dass, R. Martín-Martín and Y. Zhu, "Model-Based Runtime Monitoring with Interactive Imitation Learning," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4154-4161, doi: 10.1109/ICRA57147.2024.10611038.Abstract: Robot learning methods have recently made great strides, but generalization and robustness challenges still hinder their widespread deployment. Failing to detect and address potential failures renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains. This work aims to endow a robot with the ability to monitor and detect errors during task execution. We introduce a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, our method learns a latent-space dynamics model and a failure classifier, enabling our method to simulate future action outcomes and detect out-of-distribution and high-risk states preemptively. We train our method within an interactive imitation learning framework, where it continually updates the model from the experiences of the human-robot team collected using trustworthy deployments. Consequently, our method reduces the human workload needed over time while ensuring reliable task execution. Our method outperforms the baselines across system-level and unit-test metrics, with 23% and 40% higher success rates in simulation and on physical hardware, respectively. More information at https://ut-austin-rpl.github.io/sirius-runtime-monitor/ keywords: {Training;Measurement;Learning systems;Runtime;Limiting;Imitation learning;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10611038&isnumber=10609862

J. W. Burdick and M. Tisdale, "The Fractal Hand–I: A Non-anthropomorphic, but Synergistic, Adaptable Gripper," 2024 IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024, pp. 4162-4169, doi: 10.1109/ICRA57147.2024.10610687.Abstract: We introduce a novel Fractal Hand robotic gripper. The hand has only 1 actuator, but (2n+1 −1) joints, where a design parameter n defines the depth of the fingers’ tree structures. The hand is synergistic in its operation (because its joint movements are coupled through the hand’s interaction with the grasped object), but it is not anthropomorphic. The basic finger and hand geometry, governing kinematics, and quasi-statics mechanics of a rigid version of the hand are developed. These analyses remarkably show that under mild constraints, the grasped object is compliantly stable at an equilibrium grasp configuration. Thus, the Fractal Hand adapts to a very wide range of planar objects with a single design. Grasp planning is thus simplified. A companion paper [33] introduces a design methodology for this new class of robot hands, and multiple prototypes. keywords: {Shape;Prototypes;Kinematics;Fractals;Stability analysis;Planning;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610687&isnumber=10609862

