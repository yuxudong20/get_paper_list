L. Soum-Fontez, J. -E. Deschaud and F. Goulette, "MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5765-5772, doi: 10.1109/IROS55552.2023.10341614.Abstract: Supervised 3D Object Detection models have been displaying increasingly better performance in single-domain cases where the training data comes from the same environment and sensor as the testing data. However, in real-world scenarios data from the target domain may not be available for finetuning or for domain adaptation methods. Indeed, 3D object detection models trained on a source dataset with a specific point distribution have shown difficulties in generalizing to unseen datasets. Therefore, we decided to leverage the information available from several annotated source datasets with our Multi-Dataset Training for 3D Object Detection (MDT3D) method to increase the robustness of 3D object detection models when tested in a new environment with a different sensor configuration. To tackle the labelling gap between datasets, we used a new label mapping based on coarse labels. Furthermore, we show how we managed the mix of datasets during training and finally introduce a new cross-dataset augmentation method: crossdataset object injection. We demonstrate that this training paradigm shows improvements for different types of 3D object detection models. The source code and additional results for this research project will be publicly available on GitHub for interested parties to access and utilize: https://github.com/LouisSF/MDT3D keywords: {Training;Solid modeling;Adaptation models;Three-dimensional displays;Laser radar;Training data;Object detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341614&isnumber=10341342

G. Guo, Y. Song and F. Sun, "Addressing the Scale Shrinkage Problem in Learning-based Binocular Depth Estimation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5789-5796, doi: 10.1109/IROS55552.2023.10341579.Abstract: Binocular depth estimation is a fundamental problem in computer vision. Learning-based models have achieved significant performance improvements on public datasets in recent years. Our study finds that the performance of the current state-of-the-art deep learning-based models deteriorates significantly in distant areas. We point out that these deep learning-based models suffer from a scale shrinkage problem. Specifically, the predicted depth value ratio to the ground truth decreases as depth increases. Such a phenomenon is not conducive to the path planning and navigation of intelligent agents in outdoor scenes. We analyze the reasons for the scale shrinkage problem and give a simple and effective method. Our method employs a two-stage fine-tuning strategy and appropriately fuses the predictions of the two-stage models. The method does not reduce the prediction accuracy in close areas and significantly improves the accuracy of the models in distant areas. On the KITTI stereo 2015 dataset, our method can reduce the absolute relative difference by about 6% and the root-mean-square error (RMSE) by about 10%. keywords: {Measurement;Computer vision;Navigation;Fuses;Computational modeling;Estimation;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341579&isnumber=10341342

L. Wang et al., "Holistic Parking Slot Detection with Polygon-Shaped Representations," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5797-5803, doi: 10.1109/IROS55552.2023.10342486.Abstract: Current parking slot detection in advanced driver-assistance systems (ADAS) primarily relies on ultrasonic sen-sors. This method has several limitations such as the need to scan the entire parking slot before detecting it, the incapacity of detecting multiple slots in a row, and the difficulty of classifying them. Due to the complex visual environment, vehicles are equipped with surround view camera systems to detect vacant parking slots. Previous research works in this field mostly use image-domain models to solve the problem. These two-stage approaches separate the 2D detection and 3D pose estimation steps using camera calibration. In this paper, we propose one-step Holistic Parking Slot Network (HPS-Net), a tailor-made adaptation of the You Only Look Once (YOLO)v4 algorithm. This camera-based approach directly outputs the four vertex coordinates of the parking slot in topview domain, instead of a bounding box in raw camera images. Several visible points and shapes can be proposed from different angles. A novel regression loss function named polygon-corner Generalized Intersection over Union (GIoU) for polygon vertex position optimization is also proposed to manage the slot orientation and to distinguish the entrance line. Experiments show that HPS-Net can detect various vacant parking slots with a F1-score of 0.92 on our internal Valeo Parking Slots Dataset (VPSD) and 0.99 on the public dataset PS2.0. It provides a satisfying generalization and robustness in various parking scenarios, such as indoor (F1: 0.86) or paved ground (F1: 0.91). Moreover, it achieves a realtime detection speed of 17 FPS on Nvidia Drive AGX Xavier. A demo video can be found at https://streamable.com/75j7sj. keywords: {Visualization;Three-dimensional displays;Shape;Pose estimation;Streaming media;Cameras;Acoustics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342486&isnumber=10341342

Y. Cao, Y. Shi, Z. Cheng and H. Li, "End-to-End Point Cloud Registration via Rotation Equivariant Descriptors," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5804-5811, doi: 10.1109/IROS55552.2023.10342154.Abstract: Point cloud registration (PCR) aims to recover the rigid transformation between two noisy, unordered point sets. This task is typically tackled by establishing point-wise correspondences, and solving the rigid transformation between the two sets. Since descriptor-based methods find correspondences by matching the feature space distance, a powerful and rotation-robust point feature extractor is critical to the success of this task. Existing methods assume soft rotation invariance/equivariance through the means of training augmentation, rotational discretization or pre-alignment of patches. In contrast, this paper proposes a new method which generates fully rotation invariant and equivariant descriptors by construction. For each keypoint patch, our network extracts not only a rotation invariant descriptor for establishing corre-spondences, but also a rotation equivariant one. The rotation equivariant descriptor allows relative transformation to be directly recovered from a single correspondence pair, unlike standard methods that require three correspondences. This design significantly reduces iteration number of RANSAC and guarantees high registration recall when the inlier ratio of estimated correspondences is low. Extensive experiments have demonstrated that the proposed method outperforms state-of-art methods in the same category even after much fewer RANSAC iterations. keywords: {Point cloud compression;Training;Feature extraction;Noise measurement;Task analysis;Standards;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342154&isnumber=10341342

A. Brunetto, S. Hornauer, S. X. Yu and F. Moutarde, "The Audio-Visual BatVision Dataset for Research on Sight and Sound," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341715.Abstract: Vision research showed remarkable success in understanding our world, propelled by datasets of images and videos. Sensor data from radar, LiDAR and cameras supports research in robotics and autonomous driving for at least a decade. However, while visual sensors may fail in some conditions, sound has recently shown potential to complement sensor data. Simulated room impulse responses (RIR) in 3D apartment-models became a benchmark dataset for the community, fostering a range of audiovisual research. In simulation, depth is predictable from sound, by learning bat-like perception with a neural network. Concurrently, the same was achieved in reality by using RGB-D images and echoes of chirping sounds. Biomimicking bat perception is an exciting new direction but needs dedicated datasets to explore the potential. Therefore, we collected the BatVision dataset to provide large-scale echoes in complex real-world scenes to the community. We equipped a robot with a speaker to emit chirps and a binaural microphone to record their echoes. Synchronized RGB-D images from the same perspective provide visual labels of traversed spaces. We sampled modern US office spaces to historic French university grounds, indoor and outdoor with large architectural variety. This dataset will allow research on robot echolocation, general audio-visual tasks and sound phænomena unavailable in simulated data. We show promising results for audio-only depth prediction and show how state-of-the-art work developed for simulated data can also succeed on our dataset. Project page: https://amandinebtto.github.io/Batvision-Dataset/ keywords: {Training;Visualization;Ultrasonic imaging;Three-dimensional displays;Chirp;Robot vision systems;Radar imaging},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341715&isnumber=10341342

J. Xiao, D. Tortei, E. Roura and G. Loianno, "Long-Range UAV Thermal Geo-Localization with Satellite Imagery," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5820-5827, doi: 10.1109/IROS55552.2023.10342068.Abstract: Onboard sensors, such as cameras and thermal sensors, have emerged as effective alternatives to Global Positioning System (GPS) for geo-Iocalization in Unmanned Aerial Vehicle (UAV) navigation. Since GPS can suffer from signal loss and spoofing problems, researchers have explored camera-based techniques such as Visual Geo-Iocalization (VG) using satellite RGB imagery. Additionally, thermal geo-Iocalization (TG) has become crucial for long-range UAV flights in low-illumination environments. This paper proposes a novel thermal geo-Iocalization framework using satellite RGB imagery, which includes multiple domain adaptation methods to address the limited availability of paired thermal and satellite images. The experimental results demonstrate the effectiveness of the proposed approach in achieving reliable thermal geo-Iocalization performance, even in thermal images with indistinct self-similar features. We evaluate our approach on real data collected onboard a UAV. We also release the code and Boson-nighttime, a dataset of paired satellite-thermal and unpaired satellite images for thermal geo-Iocalization with satellite imagery. To the best of our knowledge, this work is the first to propose a thermal geo-Iocalization method using satellite RGB imagery in long-range flights. keywords: {Training;Adaptation models;Visualization;Satellites;Thermal sensors;Autonomous aerial vehicles;Sensor systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342068&isnumber=10341342

N. Jaber, B. Wehbe and F. Kirchner, "Sonar2Depth: Acoustic-Based 3D Reconstruction Using cGANs," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5828-5835, doi: 10.1109/IROS55552.2023.10342251.Abstract: This work proposes the use of conditional Generative Adversarial Networks (cGANs) for acoustic-based 3D reconstruction. Acoustics being the most reliable sensor modality in underwater domains is accompanied with the loss of elevation angle in its images. The challenge of recovering the missing dimension in acoustic images have pushed researchers to try various methods and approaches over the past years. cGANs being an image-to-image translation method makes it possible to learn a desired style, and transforms the data from one modality to another. This was applied here as a way of transforming an acoustic image into another form which contains the elevation characteristics, such as depth images. Depth images are hard to acquire underwater, thus data was generated synthetically and used for training and testing the deep learning model. As a way of performance enhancement, real data was collected for training a Cycle-GAN network in the aim of transferring the realistic style into the synthetically generated images. Simulation experiments were conducted to evaluate the system and find out the best experimental setup, which was then used to carry out the real experiment. The system performed dense 3D reconstruction of the scanned object and proved to be applicable in real environments. keywords: {Training;Meters;Three-dimensional displays;Sonar;Transforms;Acoustics;Reliability;3D reconstruction;imaging sonar;marine robotics;deep-learning;GANs},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342251&isnumber=10341342

G. Minelli, M. Poggi and S. Salti, "Depth Self-Supervision for Single Image Novel View Synthesis," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5836-5843, doi: 10.1109/IROS55552.2023.10342058.Abstract: In this paper, we tackle the problem of generating a novel image from an arbitrary viewpoint given a single frame as input. While existing methods operating in this setup aim at predicting the target view depth map to guide the synthesis, without explicit supervision over such a task, we jointly optimize our framework for both novel view synthesis and depth estimation to unleash the synergy between the two at its best. Specifically, a shared depth decoder is trained in a self-supervised manner to predict depth maps that are consistent across the source and target views. Our results demonstrate the effectiveness of our approach in addressing the challenges of both tasks allowing for higher-quality generated images, as well as more accurate depth for the target viewpoint. keywords: {Image synthesis;Video sequences;Pipelines;Estimation;Predictive models;Decoding;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342058&isnumber=10341342

R. Ma et al., "Skill Generalization with Verbs," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5844-5851, doi: 10.1109/IROS55552.2023.10341472.Abstract: It is imperative that robots can understand natural language commands issued by humans. Such commands typically contain verbs that signify what action should be performed on a given object and that are applicable to many objects. We propose a method for generalizing manipulation skills to novel objects using verbs. Our method learns a probabilistic classifier that determines whether a given object trajectory can be described by a specific verb. We show that this classifier accurately generalizes to novel object categories with an average accuracy of 76.69% across 13 object categories and 14 verbs. We then perform policy search over the object kinematics to find an object trajectory that maximizes classifier prediction for a given verb. Our method allows a robot to generate a trajectory for a novel object based on a verb, which can then be used as input to a motion planner. We show that our model can generate trajectories that are usable for executing five verb commands applied to novel instances of two different object categories on a real robot. keywords: {Natural languages;Kinematics;Search problems;Probabilistic logic;Trajectory;Planning;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341472&isnumber=10341342

S. Matsuzaki, H. Masuzawa and J. Miura, "Multi-Source Soft Pseudo-Label Learning with Domain Similarity-based Weighting for Semantic Segmentation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5852-5857, doi: 10.1109/IROS55552.2023.10342159.Abstract: This paper describes a method of domain adap-tive training for semantic segmentation using multiple source datasets that are not necessarily relevant to the target dataset. We propose a soft pseudo-label generation method by integrating predicted object probabilities from multiple source models. The prediction of each source model is weighted based on the estimated domain similarity between the source and the target datasets to emphasize contribution of a model trained on a source that is more similar to the target and generate reasonable pseudo-labels. We also propose a training method using the soft pseudo-labels considering their entropy to fully exploit information from the source datasets while suppressing the influence of possibly misclassified pixels. The experiments show comparative or better performance than our previous work and another existing multi-source domain adaptation method, and applicability to a variety of target environments. keywords: {Training;Adaptation models;Semantic segmentation;Predictive models;Entropy;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342159&isnumber=10341342

V. Gaudillière, L. Pauly, A. Rathinam, A. G. Sanchez, M. A. Musallam and D. Aouada, "3D-Aware Object Localization using Gaussian Implicit Occupancy Function," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5858-5863, doi: 10.1109/IROS55552.2023.10342399.Abstract: To automatically localize a target object in an image is crucial for many computer vision applications. To represent the 2D object, ellipse labels have recently been identified as a promising alternative to axis-aligned bounding boxes. This paper further considers 3D-aware ellipse labels, i.e., ellipses which are projections of a 3D ellipsoidal approximation of the object, for 2D target localization. Indeed, projected ellipses carry more geometric information about the object geometry and pose (3D awareness) than traditional 3D-agnostic bounding box labels. Moreover, such a generic 3D ellipsoidal model allows for approximating known to coarsely known targets. We then propose to have a new look at ellipse regression and replace the discontinuous geometric ellipse parameters with the parameters of an implicit Gaussian distribution encoding object occupancy in the image. The models are trained to regress the values of this bivariate Gaussian distribution over the image pixels using a statistical loss function. We introduce a novel non-trainable differentiable layer, E-DSNT, to extract the distribution parameters. Also, we describe how to readily generate consistent 3D-aware Gaussian occupancy parameters using only coarse dimensions of the target and relative pose labels. We extend three existing spacecraft pose estimation datasets with 3D-aware Gaussian occupancy labels to validate our hypothesis. Labels and source code are publicly accessible here: https://cvi2.uni.lu/3d-aware-obj-loc/. keywords: {Location awareness;Space vehicles;Solid modeling;Three-dimensional displays;Source coding;Pose estimation;Pipelines},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342399&isnumber=10341342

Z. Yu et al., "TransUPR: A Transformer-based Plug-and-Play Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5864-5869, doi: 10.1109/IROS55552.2023.10342116.Abstract: Common image-based LiDAR point cloud semantic segmentation (LiDAR PCSS) approaches have bottlenecks resulting from the boundary-blurring problem of convolution neural networks (CNNs) and quantitation loss of spherical projection. In this work, we propose a transformer-based plug-and-play uncertain point refiner, i.e., TransUPR, to refine selected uncertain points in a learnable manner, which leads to an improved segmentation performance. Uncertain points are sampled from coarse semantic segmentation results of 2D image segmentation where uncertain points are located close to the object boundaries in the 2D range image representation and 3D spherical projection background points. Following that, the geometry and coarse semantic features of uncertain points are aggregated by neighbor points in 3D space without adding expensive computation and memory footprint. Finally, the transformer-based refiner, which contains four stacked self-attention layers, along with an MLP module, is utilized for uncertain point classification on the concatenated features of self-attention layers. As the proposed refiner is independent of 2D CNNs, our TransUPR can be easily integrated into any existing image-based LiDAR PCSS approaches, e.g., CENet. Our TransUPR with the CENet achieves state-of-the-art performance, i.e., 68.2% mean Intersection over Union (mIoU) on the Semantic KITTI benchmark, which provides a performance improvement of 0.6% on the mIoU compared to the original CENet. keywords: {Point cloud compression;Laser radar;Three-dimensional displays;Semantic segmentation;Semantics;Neural networks;Memory management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342116&isnumber=10341342

J. M. Salt Ducaju, B. Olofsson, A. Robertsson and R. Johansson, "Null-Space Compliance Variation for Safe Human-Robot Collaboration in Redundant Manipulators using Safety Control Barrier Functions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5903-5909, doi: 10.1109/IROS55552.2023.10342181.Abstract: In this paper, Safety Control Barrier Functions (SCBFs) were used to adjust the null-space compliant behavior of a redundant robot to improve safety in Human-Robot Collaboration (HRC) without modifying the robot behavior with respect to its main Cartesian task. A Lyapunov function was included in an energy storage formulation compatible with strict passivity to provide global asymptotic stability guarantees for the null-space compliance variation, and the necessary conditions for stability were formulated as inequality constraints of the optimization problem used for the null-space compliance variation. Experimental validation was performed using a Franka Emika Panda robot for a collaborative assembly application and its results showed that safety can be improved by using SCBFs simultaneously to the optimization of the robot configuration, while employing a single degree of freedom. keywords: {Asymptotic stability;Collaboration;Manipulators;Safety;Behavioral sciences;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342181&isnumber=10341342

A. Mohammad, M. Schappler and T. Ortmaier, "Collision Isolation and Identification Using Proprioceptive Sensing for Parallel Robots to Enable Human-Robot Collaboration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5910-5917, doi: 10.1109/IROS55552.2023.10342345.Abstract: Parallel robots (PRs) allow for higher speeds in human-robot collaboration due to their lower moving masses but are more prone to unintended contact. For a safe reaction, knowledge of the location and force of a collision is useful. A novel algorithm for collision isolation and identification with proprioceptive information for a real PR is the scope of this work. To classify the collided body, the effects of contact forces at the links and platform of the PR are analyzed using a kinetostatic projection. This insight enables the derivation of features from the line of action of the estimated external force. The significance of these features is confirmed in experiments for various load cases. A feedforward neural network (FNN) classifies the collided body based on these physically modeled features. Generalization with the FNN to 300k load cases on the whole robot structure in other joint angle configurations is successfully performed with a collision-body classification accuracy of 84% in the experiments. Platform collisions are isolated and identified with an explicit solution, while a particle filter estimates the location and force of a contact on a kinematic chain. Updating the particle filter with estimated external joint torques leads to an isolation error of less than 3 cm and an identification error of 4 N in a real-world experiment. keywords: {Parallel robots;Simulation;Force;Propioception;Collaboration;Observers;Particle filters},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342345&isnumber=10341342

K. -C. Hsu, K. Leung, Y. Chen, J. F. Fisac and M. Pavone, "Interpretable Trajectory Prediction for Autonomous Vehicles via Counterfactual Responsibility," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5918-5925, doi: 10.1109/IROS55552.2023.10341712.Abstract: The ability to anticipate surrounding agents' behaviors is critical to enable safe and seamless autonomous vehicles (AVs). While phenomenological methods have successfully predicted future trajectories from scene context, these predictions lack interpretability. On the other hand, ontological approaches assume an underlying structure able to describe the interaction dynamics or agents' internal decision processes. Still, they often suffer from poor scalability or cannot reflect diverse human behaviors. This work proposes an interpretability framework for a phenomenological method through responsibility evaluations. We formulate responsibility as a measure of how much an agent takes into account the welfare of other agents through counterfactual reasoning. Additionally, this framework abstracts the computed responsibility sequences into different responsibility levels and grounds these latent levels into reward functions. The proposed responsibility-based interpretability framework is modular and easily integrated into a wide range of prediction models. To demonstrate the utility of the proposed framework in providing added interpretability, we adapt an existing AV prediction model and perform a simulation study on a real-world nuScenes traffic dataset. Experimental results show that we can perform offline ex-post traffic analysis by incorporating the responsibility signal and rendering interpretable but accurate online trajectory predictions. keywords: {Adaptation models;Computational modeling;Scalability;Predictive models;Rendering (computer graphics);Cognition;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341712&isnumber=10341342

S. Fujii and Q. -C. Pham, "Time-Optimal Path Tracking with ISO Safety Guarantees," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5926-5933, doi: 10.1109/IROS55552.2023.10342287.Abstract: One way of ensuring operator's safety during human-robot collaboration is through Speed and Separation Monitoring (SSM), as defined in ISO standard ISO/TS 15066. In general, it is impossible to avoid all human-robot collisions: consider for instance the case when the robot does not move at all, a human operator can still collide with it by hitting it of her own voluntary motion. In the SSM framework, it is possible however to minimize harm by requiring this: if a collision ever occurs, then the robot must be in a stationary state (all links have zero velocity) at the time instant of the collision. In this paper, we propose a time-optimal control policy based on Time-Optimal Path Parameterization (TOPP) to guarantee such a behavior. Specifically, we show that: for any robot motion that is strictly faster than the motion recommended by our policy, there exists a human motion that results in a collision with the robot in a non-stationary state. Correlatively, we show, in simulation, that our policy is strictly less conservative than state-of-the-art safe robot control methods. Additionally, we propose a parallelization method to reduce the computation time of our pre-computation phase (down to about 0.5 sec, practically), which enables the whole pipeline (including the pre-computation) to be executed at runtime, nearly in real-time. Finally, we demonstrate the application of our method in a scenario: time-optimal, safe control of a 6-dof industrial robot. keywords: {Runtime;Service robots;ISO Standards;Graphics processing units;Collaboration;6-DOF;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342287&isnumber=10341342

P. Schlosser, C. Ledermann and T. Asfour, "Upper Bounds for Localization Errors in 2D Human Pose Estimation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5934-5941, doi: 10.1109/IROS55552.2023.10341572.Abstract: Obtaining reliable detections of a human is crucial for many safety-related robotic tasks. This can be done by human pose estimation methods, which predict the position of several different keypoints of the human body. In most cases, recent approaches based on neural networks produce ‘good’ results, i.e. predictions with small localization errors, however, large errors do also occur. For an individual keypoint prediction, the magnitude of the error is unknown, posing a risk to safety. In this work, we extend a neural network architecture for single-person 2D human pose estimation, so that it predicts not only the keypoints of the human body, but also corresponding upper bounds for their localization errors. These upper bounds correspond to the neural network's confidence in its output, and are obtained by one of two general strategies based on (i) a direct estimation of the localization error or (ii) the predicted standard deviations of a 2D Gaussian. We propose several approaches employing these strategies and evaluate them on the MPII Human Pose dataset. In addition, we consider two quality criteria for the results: closeness of the predicted keypoint position to the actual one, and closeness of the predicted upper bound to the localization error. The best results are achieved by a Gaussian-based approach, which predicted correct upper bounds in 94.7% of the cases, while also sufficiently fulfilling the quality criteria. keywords: {Location awareness;Upper bound;Uncertainty;Pose estimation;Neural networks;Safety;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341572&isnumber=10341342

B. Lacevic, A. R. S. E. M. Newishy, A. M. Zanchettin and P. Rocco, "Enhanced Performance of Human-Robot Collaboration Using Braking Surfaces and Trajectory Scaling," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5942-5949, doi: 10.1109/IROS55552.2023.10341408.Abstract: This paper presents an effective approach to enable performance improvement in human-robot collaboration scenarios. The problem is tackled from the perspective of speed and separation monitoring principle, which stems from the recently instituted safety standard. The proposed approach attempts to seek for performance gains, measured by the speed-up of the production cycle, without compromising the safety constraints consistent with the standard. The approach is based on the notion of braking surface - an abstraction of the swept volume described by the manipulator during braking motion. We address two types of braking behavior: general and path-consistent. In both cases, the braking surface can be evaluated in a receding horizon manner. The robot velocity is continuously scaled such that, in case of a controlled stop, the corresponding volume spanned by the robot (braking surface) does not interfere with the surrounding obstacles. The approach is entirely kinematic and does not require the knowledge of the robot's dynamic model. Simulation study indicates that the pro-posed approach offers performance improvements compared to other state of the art methods. Moreover, the experiments demonstrate the real-time applicability of the method with the real robot in human-shared environment. keywords: {Collaboration;Production;Performance gain;Real-time systems;Safety;Trajectory;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341408&isnumber=10341342

M. Zechmair and Y. Morel, "Active Electric Perception-Based Haptic Modality with Applications to Robotics," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5950-5957, doi: 10.1109/IROS55552.2023.10341465.Abstract: This paper describes the hardware implementation and characterization of a capacitive sensor designed to support detection and localization of nearby objects. The sensor can be mounted on the exterior of any given robotic system. The technology is particularly well-suited to detection of capacitive material, such as living tissue. As such, it offers perspectives of facilitating human-robot interactions (cobotics). We exploit experimental data to implement a digital model of the sensor and illustrate its accuracy by emulating experimental results in simulation. The sensor is used in a number of interaction scenarii (following and avoidance), providing examples of the manner in which it can be used to support human-robot interactions. keywords: {Location awareness;Shape;Human-robot interaction;Robot sensing systems;Manipulators;Capacitive sensors;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341465&isnumber=10341342

J. Rozlivek, P. Svarny and M. Hoffmann, "Perirobot Space Representation for HRI: Measuring and Designing Collaborative Workspace Coverage by Diverse Sensors," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5958-5965, doi: 10.1109/IROS55552.2023.10341829.Abstract: Two regimes permitting safe physical human-robot interaction, speed and separation monitoring and safety-rated monitored stop, depend on reliable perception of the space surrounding the robot. This can be accomplished by visual sensors (like cameras, RGB-D cameras, LIDARs), proximity sensors, or dedicated devices used in industrial settings like pads that are activated by the presence of the operator. The deployment of a particular solution is often ad hoc and no unified representation of the interaction space or its coverage by the different sensors exists. In this work, we make first steps in this direction by defining the spaces to be monitored, representing all sensor data as information about occupancy and using occupancy-based metrics to calculate how a particular sensor covers the workspace. We demonstrate our approach in two sensor-placement experiments in three static scenes and one experiment in a dynamic scene. The occupancy representation allow the comparison of the effectiveness of various sensor setups. Therefore, this approach can serve as a prototyping tool to establish the sensor setup that provides the most efficient coverage for the given metrics and sensor representations. keywords: {Visualization;Laser radar;Robot vision systems;Human-robot interaction;Cameras;Sensor systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341829&isnumber=10341342

A. Mohammad, M. Schappler, T. -L. Habich and T. Ortmaier, "Safe Collision and Clamping Reaction for Parallel Robots During Human-Robot Collaboration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5966-5973, doi: 10.1109/IROS55552.2023.10341581.Abstract: Parallel robots (PRs) offer the potential for safe human-robot collaboration because of their low moving masses. Due to the in-parallel kinematic chains, the risk of contact in the form of collisions and clamping at a chain increases. Ensuring safety is investigated in this work through various contact reactions on a real planar PR. External forces are estimated based on proprioceptive information and a dynamics model, which allows contact detection. Retraction along the direction of the estimated line of action provides an instantaneous response to limit the occurring contact forces within the experiment to 70 N at a maximum velocity of 0.4 m/s. A reduction in the stiffness of a Cartesian impedance control is investigated as a further strategy. For clamping, a feedforward neural network (FNN) is trained and tested in different joint angle configurations to classify whether a collision or clamping occurs with an accuracy of 80%. A second FNN classifies the clamping kinematic chain to enable a subsequent kinematic projection of the clamping joint angle onto the rotational platform coordinates. In this way, a structure opening is performed in addition to the softer retraction movement. The reaction strategies are compared in real-world experiments at different velocities and controller stiffnesses to demonstrate their effectiveness. The results show that in all collision and clamping experiments the PR terminates the contact in less than 130 ms. keywords: {Parallel robots;Fuzzy control;Trajectory planning;Propioception;Collaboration;Kinematics;Switches},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341581&isnumber=10341342

S. Rupavatharam et al., "AmbiSense: Acoustic Field Based Blindspot-Free Proximity Detection and Bearing Estimation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5974-5981, doi: 10.1109/IROS55552.2023.10341766.Abstract: In this paper, we present AmbiSense, an acoustic field based sensing system that performs proximity detection and bearing estimation for safer physical human-robot interactions. A single low cost piezoelectric transducer is used to setup this novel acoustic sensing modality to create a blindspot-free sound field engulfing a robot arm. Two detection algorithms leveraging spectral information from reflected audio waves of objects entering the acoustic field are proposed to infer object presence and bearing. We also present a new receiver structure which improves signal to noise ratio (SNR). AmbiSense is paired with a collision avoidance inverse kinematic solver for real world deployment on a Kinova Gen3 robot. Validation is performed using ten test objects generating 2000 proximity and bearing estimation events in real world settings, we show that AmbiSense detects proximity with 93.8% sensitivity and 96.6 % specificity. It estimates bearing and maps it to three zones on a robot link with 100% sensitivity and specificity, while using fewer sensors than state of the art methods for similar coverage. keywords: {Direction-of-arrival estimation;Sensitivity;Receivers;Sensitivity and specificity;Robot sensing systems;Sensors;Acoustic field},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341766&isnumber=10341342

M. Harder, M. Iskandar, J. Lee and A. Dietrich, "Extensions to Dynamically-Consistent Collision Reaction Control for Collaborative Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5982-5988, doi: 10.1109/IROS55552.2023.10341950.Abstract: Since modern robots are supposed to work closely together with humans, physical human-robot interaction is gaining importance. One crucial aspect for safe collaboration is a robust collision reaction strategy that is triggered after an unintentional physical contact. In this work, we propose a dynamically-consistent collision reaction controller, where the reactive motion is performed in one particular desired direction in Cartesian space, without disturbing the remaining ones. This results in more intuitive and more predictable behavior of the end-effector. In addition, the proposed reaction control law is independent of contact and internal observer dynamics used for collision detection. The theoretical claims are validated in simulation and experiments. The proposed reaction controller is experimentally compared with a conventional approach for collision reaction. All experiments have been conducted on a torque controlled KUKA LWR IV + lightweight robot. keywords: {Torque;Uncertainty;Dynamics;Collaboration;Observers;Robustness;End effectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341950&isnumber=10341342

X. Lu, A. Faragasso, A. Yamashita and H. Asama, "All Aware Robot Navigation in Human Environments Using Deep Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5989-5996, doi: 10.1109/IROS55552.2023.10341477.Abstract: Mobile robots functioning in human environments should behave with a secure and socially-compliant manner. Although many studies have revealed the effectiveness of Deep Reinforcement Learning (DRL) in robot navigation, most of them can only handle the presence of human as independent individuals. Failing to consider groups may lead to the robot getting stuck or behaving rudely, and omitting to separately handle obstacles from pedestrians will cause low efficiency. In this work, we present a novel all-aware neural network that utilizes DRL to process groups, obstacles, and individuals simultaneously. The proposed solution employs a new Group–Robot Interaction (GRI) subnetwork to encode the mutual effects between groups and the robot, and a modified Obstacle–Robot Unilateral interaction (ORU) subnetwork is presented to avoid obstacle collisions caused by sensing noises or motion uncertainties. In addition, the influences of a pedestrian, obstacle, and group on other pedestrians or groups, that indirectly affect the robot, are also integrated into the Human–Robot Interaction (HRI) subnetwork or GRI subnetwork respectively by using map tensors. Finally, the GRI, ORU, and HRI subnetworks are aggregated into a planning subnetwork to train and derive an all-aware robot navigation policy based on DRL. Evaluation results in both real-world and simulation experiments show that the proposed approach outperforms the current cutting-edge methods. keywords: {Deep learning;Pedestrians;Uncertainty;Navigation;Human-robot interaction;Reinforcement learning;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341477&isnumber=10341342

J. de Miguel-Fernández et al., "Relationship Between Ankle Assistive Torque and Biomechanical Gait Metrics in Individuals After Stroke," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6054-6061, doi: 10.1109/IROS55552.2023.10341864.Abstract: The effectiveness of robotic exoskeletons for post-stroke gait rehabilitation might be limited as the control parameters of these devices do not adapt to key biomechanical descriptors. The main contribution of this study is to examine post-stroke gait with the aim of finding relationships between exoskeleton control parameters and a comprehensive set of biomechanical metrics. Five stroke survivors walked with the assistance of a wearable ankle exoskeleton (ABLE-S) using different levels of plantarflexion (PF) and dorsiflexion (DF) peak torque, as well as different timings of PF peak torque. We found that DF peak magnitude had significant negative relations with the temporal symmetry index ($\mathrm{p}=0.033$) and the paretic foot absolute angle at heel strike ($\mathrm{p}=0.019$). Changes in the applied PF assistance parameters were significantly correlated with a high variety of temporal and spatial parameters, e.g., walking speed ($\mathrm{p}=0.009$), stride length ($\mathrm{p}=0.011$), non-paretic step length ($\mathrm{p}=0.024$), foot clearance ($\mathrm{p}=0.003$) and hip hiking ($\mathrm{p}=0.038$), and the muscle activation for the non-paretic side, e.g., Tibialis Anterior ($\mathrm{p}=0.049$) and Gastrocnemius Medialis ($\mathrm{p}=0.049$). Based on our results, we propose a set of control laws for adapting the assistance of ankle exoskeletons that will be evaluated in future work. keywords: {Measurement;Biomechanics;Legged locomotion;Torque;Exoskeletons;Timing;Indexes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341864&isnumber=10341342

R. M. Andrade, S. Sapienza, A. Mohebbi, E. E. Fabara and P. Bonato, "Experimental Evaluation of a Transparent Operation Mode for a Lower-Limb Exoskeleton Designed for Children with Cerebral Palsy," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6062-6067, doi: 10.1109/IROS55552.2023.10342182.Abstract: Robot-assisted rehabilitation is expected to reduce locomotor limitations of children and young adults with Cerebral Palsy (CP). However, to achieve this result, it is essential that the robot is transparent, allowing the user to move freely, and generate joint torques only when the exoskeleton joints significantly deviate from physiological gait patterns. Nevertheless, the development of transparent operation in lower-limb exoskeletons is still an open problem with several implementation challenges. In this study, we implemented a transparent operation strategy on the ExoRoboWalker, an overground exoskeleton designed for children and young adults with CP. The approach employs a feedback zero-torque controller with feedforward compensations for the exoskeleton's dynamics and actuators' impedance. We experimented the proposed system in five healthy subjects walking overground with the exoskeleton in transparent mode (ExoTransp) and non-transparent mode (ExoOff). The proposed transparent controller reduced the user-robot interaction torque and improved user gait kinematics relative to ExoOff. This is a significant step toward an overground gait training exoskeleton for CP population. keywords: {Legged locomotion;Actuators;Pediatrics;Torque;Exoskeletons;Sociology;Kinematics;Lower-limb exoskeleton;cerebral palsy;gait training;transparent control;gait kinematics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342182&isnumber=10341342

J. A. Montes-Pérez, G. C. Thomas and R. D. Gregg, "Effects of Personalization on Gait-State Tracking Performance Using Extended Kalman Filters," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6068-6074, doi: 10.1109/IROS55552.2023.10342498.Abstract: Emerging partial-assistance exoskeletons can enhance able-bodied performance and aid people with patho-logical gait or age-related immobility. However, every person walks differently, which makes it difficult to directly compute assistance torques from joint kinematics. Gait-state estimation-based controllers use phase (normalized stride time) and task variables (e.g., stride length and ground inclination) to parameterize the joint torques. Using kinematic models that depend on the gait-state, prior work has used an Extended Kalman filter (EKF) to estimate the gait-state online. However, this EKF suffered from kinematic errors since it used a subject-independent measurement model, and it is still unknown how personalization of this measurement model would reduce gait-state tracking error. This paper quantifies how much gait-state tracking improvement a personalized measurement model can have over a subject-independent measurement model when using an EKF-based gait-state estimator. Since the EKF performance depends on the measurement model covariance matrix, we tested on multiple different tuning parameters. Across reasonable values of tuning parameters that resulted in good performance, personalization improved estimation error on average by 8.5 ± 13.8% for phase (mean ± standard deviation), 27.2 $\pm \ 8.1{\%}$ for stride length, and 10.5 $\pm \ 13.5{\%}$ for ground inclination. These findings support the hypothesis that personalization of the measurement model significantly improves gait-state estimation performance in EKF based gait-state tracking $(P \ll 0.05)$, which could ultimately enable reliable responses to faster human gait changes. keywords: {Measurement uncertainty;Sociology;Kinematics;Robot sensing systems;Kalman filters;Reliability;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342498&isnumber=10341342

S. Eckstein, B. Leudesdorff, C. Maufroy, U. Schneider and C. D. Remy, "State- Based Control for an Actuated Reciprocal Gait Orthosis," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6075-6081, doi: 10.1109/IROS55552.2023.10342037.Abstract: Upright locomotion has many health benefits for patients with spinal cord injury. Passive gait orthoses, such as an isocentric reciprocal gait orthosis (IRGO), allow patients to walk by pushing themselves forward with forearm supports. To move the legs, the IRGO physically couples the motion of stance and swing leg through a linkage. Unfortunately, this locomotion is associated with high metabolic effort. To reduce the metabolic demand while maintaining the advantages of simplicity and low weight of an IRGO, we propose the extension of an IRGO with a single actuator directly integrated into the rocking mechanism and investigate the use of a Hybrid Zero Dynamics based controller to support the user. This is a time-invariant feedback controller, in which a stable reciprocal gait is designed by means of numerically optimized virtual constraints. We evaluate the feasibility of this approach in a sagittal plane model of the dynamic system. For a range of walking speeds and step lengths, we are able to show that the chosen approach has the potential to safely support impaired users walking with IRGOs. The obtained gaits were periodic, stable, maintained a minimal ground clearance and could be implemented with a driving torque of 50 Nm. keywords: {Legged locomotion;Couplings;Actuators;Torque;Numerical models;Spinal cord injury;Dynamical systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342037&isnumber=10341342

K. Walters, G. C. Thomas, J. Lin and R. D. Gregg, "An Energetic Approach to Task-Invariant Ankle Exoskeleton Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6082-6089, doi: 10.1109/IROS55552.2023.10342136.Abstract: Robotic ankle exoskeletons have been shown to reduce human effort during walking. However, existing ankle exoskeleton control approaches are limited in their ability to apply biomimetic torque across diverse tasks outside of the controlled lab environment. Energy shaping control can provide task-invariant assistance without estimating the user's state, classifying task, or reproducing pre-defined torque trajectories. In previous work, we showed that an optimally task-invariant energy shaping controller implemented on a knee-ankle ex-oskeleton reduced the effort of certain muscles for a range of tasks. In this paper, we extend this approach to the sensor suite available at the ankle and present its implementation on a commercially-available, bilateral ankle exoskeleton. An experiment with three healthy subjects walking on a circuit and on a treadmill showed that the controller can approximate biomimetic profiles for varying terrains and task transitions without classifying tasks or switching control modes. keywords: {Legged locomotion;Training;Torque;Biomimetics;Exoskeletons;Stairs;Control systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342136&isnumber=10341342

A. Sochopoulos, T. Poliero, D. Caldwell, J. Ortiz and C. Di Natali, "Human-in-the-Loop Optimization of Active Back-Support Exoskeleton Assistance Via Lumbosacral Joint Torque Estimation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6090-6096, doi: 10.1109/IROS55552.2023.10341810.Abstract: The assistive profile of an active back support exoskeleton is strongly dependent on the manual tuning of controller gains based on previous experience and trial-and-error. Human-in-the-loop (HIL) optimization allows for automatic tuning of assistive profiles to different subjects. Most HIL methods make use of intrusive sensors that could affect out-of-the-lab exoskeleton adoption. Therefore, we propose a HIL-based assistive controller architecture using only one single IMU that can be easily embedded in any exoskeleton system. To validate our algorithm we recruited 3 subjects and asked them to perform a series of successive load liftings. Meanwhile, we analysed the back-muscles activations focusing on cumulative activation (iEMG), and median activation. We also monitored the total torque generated by the exoskeleton. With respect to an assistance-less condition, the proposed controller resulted in up to 19% reduction of the back-muscles activity. Moreover, compared to a state-of-the-art controller that produced up to 15% reduction of the back-muscles activity, the new controller also required generation of 4% less exoskeleton torque. keywords: {Torque;Exoskeletons;Sociology;Estimation;Human in the loop;User experience;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341810&isnumber=10341342

A. Alili, V. Nalam, A. Fleming, M. Liu, J. Dean and H. He Huang, "Closed-Loop Feedback Control of Human Step Width During Walking by Mediolaterally Acting Robotic Hip Exoskeleton," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6097-6102, doi: 10.1109/IROS55552.2023.10342127.Abstract: Maintaining balance during gait in the mediolateral direction requires more active motor control than in the anteroposterior direction. Step width modulation is a key strategy used by healthy individuals to achieve mediolateral walking balance, but it can be disrupted in populations with poor sensorimotor integration and weak hip abductors, such as the elderly, stroke patients, and people with lower limb amputation. Wearable hip exoskeletons have the potential to serve as assistive or rehabilitation devices for these populations, but there has been limited research on their appropriate usage. In this study, we successfully demonstrated the feasibility of controlling step width using a mediolaterally acting robotic hip exoskeleton. We were able to effectively adjust the user's step width by increasing or decreasing it to predefined targets through the regulation of admittance control parameters governing the device. The maximum average error to increase or decrease the step width was 1.2 cm. This research has the potential to facilitate the development of assistive and rehabilitation applications focused on enhancing the mediolateral gait balance of individuals with neurological impairments, elderly individuals, and amputees via the control of step width. keywords: {Legged locomotion;Motor drives;Exoskeletons;Sociology;Stroke (medical condition);Robot sensing systems;Regulation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342127&isnumber=10341342

J. M. Li et al., "Comparing the Effectiveness of Control Methodologies of a Hip-Knee-Ankle Exoskeleton During Squatting," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6103-6109, doi: 10.1109/IROS55552.2023.10341374.Abstract: Manual materials handling occupations often involve repetitive lifting, lowering, and carrying motions, which can lead to muscular fatigue and/or injury. The risk increases when loads must be worn on the body for the entirety of a job shift. Exoskeletons have been developed to assist these types of motions, but require the user to bear the weight of a load through their body. Load carriage exoskeletons have been developed to offload worn mass from the user to the ground through the device structure, but they have had limited success and have not been well studied in manual materials handling tasks. In this paper, we introduce a hip-knee-ankle exoskeleton and two control methods: virtual model control and gravity compensation. We compared the ability of each controller to reduce lower-limb muscle activity during squatting. Because the virtual model controller is tailored to squatting, we hypothesized that it would outperform gravity compensation. Both controllers were able to reduce the activity of major lower-limb muscle groups during squatting when compared to squatting with the exoskeleton turned off. Contrary to our original hypothesis, the gravity compensation controller generally outperformed the virtual model controller, which may have been caused by the gravity compensation controller having more consistent knee torque application and the virtual model controller requiring better per-user tuning and familiarization. These results indicate the efficacy of both controllers in reducing injury risk in the lower limbs during squatting. keywords: {Knee;Torque;Exoskeletons;Dynamics;Materials handling;Manuals;Muscles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341374&isnumber=10341342

K. Ghonasgi, R. Mirsky, A. M. Haith, P. Stone and A. D. Deshpande, "A Novel Control Law for Multi-Joint Human-Robot Interaction Tasks While Maintaining Postural Coordination," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6110-6116, doi: 10.1109/IROS55552.2023.10342501.Abstract: Exoskeleton robots are capable of safe torque-controlled interactions with a wearer while moving their limbs through predefined trajectories. However, affecting and assisting the wearer's movements while incorporating their inputs (effort and movements) effectively during an interaction re-mains an open problem due to the complex and variable nature of human motion. In this paper, we present a control algorithm that leverages task-specific movement behaviors to control robot torques during unstructured interactions by implementing a force field that imposes a desired joint angle coordination behavior. This control law, built by using principal component analysis (PCA), is implemented and tested with the Harmony exoskeleton. We show that the proposed control law is versatile enough to allow for the imposition of different coordination behaviors with varying levels of impedance stiffness. We also test the feasibility of our method for unstructured human-robot interaction. Specifically, we demonstrate that participants in a human-subject experiment are able to effectively perform reaching tasks while the exoskeleton imposes the desired joint coordination under different movement speeds and interaction modes. Survey results further suggest that the proposed control law may offer a reduction in cognitive or motor effort. This control law opens up the possibility of using the exoskeleton for training the participating in accomplishing complex multi-joint motor tasks while maintaining postural coordination. keywords: {Training;Surveys;Robot kinematics;Exoskeletons;Human-robot interaction;Behavioral sciences;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342501&isnumber=10341342

E. A. Bywater, R. Leo and E. J. Rouse, "Investigations into Customizing Bilateral Ankle Exoskeletons to Increase Vertical Jumping Performance," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6117-6124, doi: 10.1109/IROS55552.2023.10341974.Abstract: Exoskeletons have shown great potential to enhance locomotion by augmenting the lower limb. While most research has focused on steady-state ambulatory activities, the ability to assist transient, ballistic tasks is also important for understanding the potential of exoskeletons in mobility enhancement. In this preliminary study (N = 5), we developed an individually-customized control strategy to assist vertical jumping. The control strategy was deployed on bilateral ankle exoskeletons (ExoBoot, Dephy Inc.). We structured the control strategy as a work loop that parameterized the assistance provided during the jump. We show that configuring the controller based on individual biomechanics and user preferences facilitates increased vertical jump height when using exoskeleton assistance. In addition, we demonstrate that a user's squat depth can have a significant (p < 0.05) impact on height achieved, but that this depth does not need to be optimized; rather, the exoskeleton provides the maximum performance assistance from both preferred- and deep-squat conditions. Jump height increased by 7.2% with the exoskeleton at its maximum assistance setting, which is comparable to or greater than previous systems. keywords: {Biomechanics;Exoskeletons;Steady-state;Transient analysis;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341974&isnumber=10341342

Q. Zhang, X. Tu, J. Si, M. D. Lewek and H. Huang, "A Robotic Assistance Personalization Control Approach of Hip Exoskeletons for Gait Symmetry Improvement," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6125-6132, doi: 10.1109/IROS55552.2023.10341440.Abstract: Healthy human locomotion functions with good gait symmetry depend on rhythmic coordination of the left and right legs, which can be deteriorated by neurological disorders like stroke and spinal cord injury. Powered exoskeletons are promising devices to improve impaired people's locomotion functions, like gait symmetry. However, given higher uncertainties and the time-varying nature of human-robot interaction, providing personalized robotic assistance from exoskeletons to achieve the best gait symmetry is challenging, especially for people with neurological disorders. In this paper, we propose a hierarchical control framework for a bilateral hip exoskeleton to provide the adaptive optimal hip joint assistance with a control objective of imposing the desired gait symmetry during walking. Three control levels are included in the hierarchical framework, including the high-level control to tune three control parameters based on a policy iteration reinforcement learning approach, the middle-level control to define the desired assistive torque profile based on a delayed output feedback control method, and the low-level control to achieve a good torque trajectory tracking performance. To evaluate the feasibility of the proposed control framework, five healthy young participants are recruited for treadmill walking experiments, where an artificial gait asymmetry is imitated as the hemiparesis post-stroke, and only the ‘paretic’ hip joint is controlled with the proposed framework. The pilot experimental studies demonstrate that the hierarchical control framework for the hip exoskeleton successfully (asymmetry index from 8.8% to − 0.5%) and efficiently (less than 4 minutes) achieved the desired gait symmetry by providing adaptive optimal assistance on the ‘paretic’ hip joint. keywords: {Legged locomotion;Neurological diseases;Torque;Uncertainty;Trajectory tracking;Robot kinematics;Exoskeletons},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341440&isnumber=10341342

Y. Wang, Y. Ji, C. Wu, H. Tsuchiya, H. Asama and A. Yamashita, "Motion Degeneracy in Self-supervised Learning of Elevation Angle Estimation for 2D Forward-Looking Sonar," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6133-6140, doi: 10.1109/IROS55552.2023.10341601.Abstract: 2D forward-looking sonar is a crucial sensor for underwater robotic perception. A well-known problem in this field is estimating missing information in the elevation direction during sonar imaging. There are demands to estimate 3D information per image for 3D mapping and robot navigation during fly-through missions. Recent learning-based methods have demonstrated their strengths, but there are still drawbacks. Supervised learning methods have achieved high-quality results but may require further efforts to acquire 3D ground-truth labels. The existing self-supervised method requires pretraining using synthetic images with 3D supervision. This study aims to realize stable self-supervised learning of elevation angle estimation without pretraining using synthetic images. Failures during self-supervised learning may be caused by motion degeneracy problems. We first analyze the motion field of 2D forward-looking sonar, which is related to the main supervision signal. We utilize a modern learning framework and prove that if the training dataset is built with effective motions, the network can be trained in a self-supervised manner without the knowledge of synthetic data. Both simulation and real experiments validate the proposed method. keywords: {Training;Point cloud compression;Three-dimensional displays;Simultaneous localization and mapping;Supervised learning;Sonar;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341601&isnumber=10341342

O. Álvarez-Tuñón et al., "MIMIR-UW: A Multipurpose Synthetic Dataset for Underwater Navigation and Inspection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6141-6148, doi: 10.1109/IROS55552.2023.10341436.Abstract: This paper presents MIMIR-UW, a multipurpose underwater synthetic dataset for SLAM, depth estimation, and object segmentation to bridge the gap between theory and application in underwater environments. MIMIR-UW integrates three camera sensors, inertial measurements, and ground truth for robot pose, image depth, and object segmentation. The underwater robot is deployed within a pipe exploration scenario, carrying artificial lights that create uneven lighting, in addition to natural artefacts such as reflections from natural light and backscattering effects. Four environments totalling eleven tracks are provided, with various difficulties regarding light conditions or dynamic elements. Two metrics for dataset evaluation are proposed, allowing MIMIR-UW to be compared with other datasets. State-of-art methods on SLAM, segmentation and depth estimation are deployed and benchmarked on MIMIR-UW. Moreover, the dataset's potential for sim-to-real transfer is demonstrated by leveraging the segmentation and depth estimation models trained on MIMIR-UW in a real pipeline inspection scenario. To the best of the authors' knowledge, this is the first underwater dataset targeted for such a variety of methods. The dataset is publicly available online. https://github.com/remaro-network/MIMIR-UW/ keywords: {Measurement;Simultaneous localization and mapping;Robot vision systems;Pipelines;Estimation;Object segmentation;Inspection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341436&isnumber=10341342

S. Singhal, Y. Ao, D. Maas, B. Arsenali and S. Maranò, "Marine Vessel Attitude Estimation from Coastline and Horizon," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6149-6154, doi: 10.1109/IROS55552.2023.10341522.Abstract: Reliable monitoring of vessel motions is crucial for safe and efficient operation of marine vessels. Pitching and rolling motions are commonly monitored using high-grade inertial measurement units (IMUs). However, such sensors become unreliable in presence of long-lasting accelerations. In this work, we propose a method for attitude estimation of marine vessels relying on an image stream and known world features. Our focus is on the estimation of pitch and roll angles. We employ a semantic segmentation network and process its output for robust extraction of coastlines and horizon. The image features are matched with known world features to estimate the attitude. The proposed method is validated using different metrics on data acquired from a small passenger ferry. The proposed method achieves more than 60% reduction in vertical reprojection error compared to IMU. We show that the proposed method outperforms IMU and can be used to replace it whenever horizon or coastline is visible. keywords: {Coastlines;Measurement units;Semantic segmentation;Estimation;Inertial navigation;Streaming media;Sensors;Attitude estimation;semantic segmentation;marine navigation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341522&isnumber=10341342

Z. Fan, C. Lyu and Z. Zeng, "DANDELION: An ASV Deployed Micro-Profiler Array for Air-Sea Observation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6155-6161, doi: 10.1109/IROS55552.2023.10342079.Abstract: The air-sea interface is vital in studying heat and energy exchange between the sea and air. The field observation technology of the air-sea interface is an effective way to explore the nature of the air-sea interface. This paper presents an observation system called DANDELION for the air-sea interface environment. The system includes an automatic surface vehicle (ASV), a launching device using a pair of high-speed rotating friction wheels, and eight dandelion-like micro profilers. This system can implement the environmental observation of the air-sea interface in an extensive range through micro profilers' rapid and multi-point placement. The DANDELION system was characterized by establishing the friction wheel launching mechanism model and summarizing the effects of different wings on the profiler performance. A series of experiments were conducted in Qiandao Lake, China, to characterize the DANDELION system. We demonstrate the developed system with data from field experiments, which show very high flexibility and feasibility to observe the air-sea interface, implying potential applications in ocean transient phenomena observation. keywords: {Sea surface;Friction;Atmospheric modeling;Wheels;Lakes;Arrays;Vehicle dynamics;Marine Robotics;Environment Monitoring and Management;Air-sea Interface;Micro-profiler},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342079&isnumber=10341342

D. Gregorek, A. Tibebu, E. Caudet, C. Barrera and R. Bachmayer, "Long-Endurance Optical Seafloor Imaging Using Underwater Gliders: Concept, Development and Initial Trials," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6162-6168, doi: 10.1109/IROS55552.2023.10341259.Abstract: In-situ observation of the deep-sea floor is a fundamental need for marine sciences and ecosystem monitoring. This work proposes a novel robotic approach for benthic observations in the deep sea using underwater gliders. The glider is equipped with a downward looking camera system to acquire high resolution optical images of the seafloor. The system works fully autonomous and has the potential for long-endurance missions at low deployment costs. Key factors for long battery lifetime are a low power consumption during idle phases, a timely activation of the main imaging system and an efficient lighting setup. The results of our initial trials at sea using a 1000m glider show the applicability of the approach for marine science applications. We achieved optical image quality utilizable for seafloor classification and 3D reconstruction of underwater objects. Keeping the vertical zig-zag motion of underwater gliders in mind, our findings substantiate the feasibility of multi-week seafloor observation missions at long operating ranges. keywords: {Three-dimensional displays;Power demand;Sea floor;Robot vision systems;Lasers;Lighting;Seaports},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341259&isnumber=10341342

K. L. Walker and F. Giorgio-Serchi, "Disturbance Preview for Non-Linear Model Predictive Trajectory Tracking of Underwater Vehicles in Wave Dominated Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6169-6176, doi: 10.1109/IROS55552.2023.10341695.Abstract: Operating in the near-vicinity of marine energy devices poses significant challenges to the control of underwater vehicles, predominantly due to the presence of large magnitude wave disturbances causing hazardous state perturbations. Approaches to tackle this problem have varied, but one promising solution is to adopt predictive control methods. Given the predictable nature of ocean waves, the potential exists to incorporate disturbance estimations directly within the plant model; this requires inclusion of a wave predictor to provide online preview information. To this end, this paper presents a Non-linear Model Predictive Controller with an integrated Deterministic Sea Wave Predictor for trajectory tracking of underwater vehicles. State information is obtained through an Extended Kalman Filter, forming a complete closed-loop strategy and facilitating online wave load estimations. The strategy is compared to a similar feed-forward disturbance mitigation scheme, showing mean performance improvements of 51% in positional error and 44.5% in attitude error. The preliminary results presented here provide strong evidence of the proposed method's high potential to effectively mitigate disturbances, facilitating accurate tracking performance even in the presence of high wave loading. keywords: {Power demand;Trajectory tracking;Perturbation methods;Estimation;Predictive models;Regulation;Underwater vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341695&isnumber=10341342

T. M. Paine and M. R. Benjamin, "An Ensemble of Online Estimation Methods for One Degree-of-Freedom Models of Unmanned Surface Vehicles: Applied Theory and Preliminary Field Results with Eight Vehicles," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6177-6184, doi: 10.1109/IROS55552.2023.10341433.Abstract: In this paper we report an experimental evaluation of three popular methods for online system identification of unmanned surface vehicles (USVs) which were implemented as an ensemble: certifiably stable shallow recurrent neural network (RNN), adaptive identification (AID), and recursive least squares (RLS). The algorithms were deployed on eight USVs for a total of 30 hours of online estimation. During online training the loss function for the RNN was augmented to include a cost for violating a sufficient condition for the RNN to be stable in the sense of contraction stability. Additionally we described an efficient method to calculate the equilibrium points of the RNN and classify the associated stability properties about these points. We found the AID method had lowest mean absolute error in the online prediction setting, but a weighted ensemble had lower error in offline processing. keywords: {Training;Sufficient conditions;Recurrent neural networks;Linear regression;Estimation;Stability analysis;System identification},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341433&isnumber=10341342

X. Lin, J. McConnell and B. Englot, "Robust Unmanned Surface Vehicle Navigation with Distributional Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6185-6191, doi: 10.1109/IROS55552.2023.10342389.Abstract: Autonomous navigation of Unmanned Surface Vehicles (USV) in marine environments with current flows is challenging, and few prior works have addressed the sensor-based navigation problem in such environments under no prior knowledge of the current flow and obstacles. We propose a Distributional Reinforcement Learning (RL) based local path planner that learns return distributions which capture the uncertainty of action outcomes, and an adaptive algorithm that automatically tunes the level of sensitivity to the risk in the environment. The proposed planner achieves a more stable learning performance and converges to safer policies than a traditional RL based planner. Computational experiments demonstrate that comparing to a traditional RL based planner and classical local planning methods such as Artificial Potential Fields and the Bug Algorithm, the proposed planner is robust against environmental flows, and is able to plan trajectories that are superior in safety, time and energy consumption. keywords: {Training;Energy consumption;Uncertainty;Sensitivity;Navigation;Computer bugs;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342389&isnumber=10341342

N. Bauschmann, D. A. Duecker, T. L. Alff, R. C. Hochdahl and R. Seifried, "Towards Full Actuation: Reconfigurable Micro Underwater Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6192-6199, doi: 10.1109/IROS55552.2023.10341621.Abstract: Exploration and monitoring of hazardous environments, such as legacy nuclear storage ponds, constitute safety-critical missions to be performed by small-scale underwater robots. These monitoring tasks require fully actuated robot platforms in order to allow for hovering while inspecting objects of interest in detail. A severe bottleneck arises from the restricted access points commonly encountered in these surveillance missions that pose strict limitations on the vehicle dimensions. However, small-scale underwater robots usually possess underactuated propulsion systems and are, thus, only partially suitable for these missions. In this work, we investigate and exploit the idea of reconfigurability. Following the idea of the whole is more than the sum of its parts, we daisy-chain multiple small-scale underwater robots with revolute joints to enable shape reconfigurations. In combination with a centralized sliding mode control scheme, the robot platform is able to change its shape depending on the current task, see Fig. 1. While the straight configuration fits well through tight passages such as inspection holes, the robot can reconfigure itself towards a triangular shape that enables the neutrally buoyant robot to hover at areas of interest, e. g. for inspection tasks. Finally, we examine our proposed concept in a series of simulations and experiments. Moreover, we demonstrate the performance of key elements such as reconfiguration and navigation, discuss their limitations, and point out future directions. keywords: {Autonomous underwater vehicles;Three-dimensional displays;Shape;Trajectory tracking;Surveillance;Prototypes;Inspection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341621&isnumber=10341342

Y. He, Y. Zheng and F. Asano, "Water Surface Walking of Six-Legged Robot by Controlling Attitude of Feet When It Enter Water," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6200-6205, doi: 10.1109/IROS55552.2023.10342494.Abstract: This paper presents a water walking robot with 6 feet which consists of a rimless wheel and a flywheel, and has a foot attached to the tip of each leg. First, in order to make the robot walk on water, we propose a foot control method by imitating the legs of some animals that can walk on water. Second, in order to increase the robot's forward speed, we improved the control method. In addition we have analysed the effect of some different physical parameters on the motion of the robot. This research is aimed at building a versatile water walking model for such applications as marine exploration. keywords: {Legged locomotion;Attitude control;Animals;Buildings;Wheels;Flywheels;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342494&isnumber=10341342

N. R. Rypkema, S. Randeni, M. Sacarny, M. Benjamin and M. Triantafyllou, "Perseus AUV: Towards Linear Convoying of Agile A-Sized AUVs Through Acoustic Track-and-Trail," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6206-6213, doi: 10.1109/IROS55552.2023.10341835.Abstract: We present the Perseus autonomous underwater vehicle (AUV) - an A-sized aaA-size stands for the standard sonobuoy [2] form factor, with a maximum diameter of 124 mm and a length of around 0.9 m, ensuring the ability to launch from standard sonobuoy launchers onboard a wide array of fixed wing and rotary wing air crafts, surface ships and submarines [3] micro AUV, outfitted with a low-cost passive inverted ultra-short baseline (piUSBL) acoustic reception system, which allows it to acoustically track-and-trail a leader vehicle that carries an acoustic transmission source. With a long-term goal of linear convoying of multiple A-sized AUVs, in this work, we used an unmanned surface vehicle (USV) towing an acoustic source, as a proxy for a lead AUV, demonstrating that the Perseus AUV is able to successfully track-and-trail a leader vehicle. The AUV was also outfitted with a tuna-inspired morphing fin mechanism that allowed the vehicle to achieve good directional stability as well as good maneuverability; properties that are useful for linear convoying AUVs, but are presently difficult to achieve because they impose contradictory requirements. We demonstrated this system with real-world, in-water experiments in the Charles river, Massachusetts, USA. keywords: {Meters;Autonomous underwater vehicles;Receivers;Acoustics;Rivers;Reliability;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341835&isnumber=10341342

S. An et al., "An Open-Source Robotic Chinese Chess Player," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6238-6245, doi: 10.1109/IROS55552.2023.10341697.Abstract: Consumer robots can accompany children growing up, improving their abilities while playing and entertaining. This paper presents an open-source, practical, low-cost robotic Chinese chess player. The proposed system includes an elaborate mechanical structure, a simple kinematic solution, a novel robot operating system, real-time and accurate chess recognition. Regarding its mechanical design, it combines a magnetism structure and mechanical cam drive, while the overall system has just three servo motors. At the same time, its control strategy is simple and effective. Furthermore, a lightweight robot message communication mechanism, entitled TinyROS, is developed for computing resource-limited embedded chips. Concerning the recognition process, our CNNbased object detector determines chess and achieves accurate identification. As a result, our robotic Chinese chess player is exquisite and easy for large-scale promotion while improving users' chess skills. Aiming to facilitate future consumer robot research and popularize customer robots, the model's mechanical and software design and the TinyROS protocol are open-sourced at https://github.com/Star-Robot/chinese-chess-robot. keywords: {Visualization;Protocols;Software design;Operating systems;Detectors;Real-time systems;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341697&isnumber=10341342

C. -H. Huang, W. -T. Chen, Y. -C. Chang, K. -T. Wu and R. -H. Wang, "ETAUS: An Edge and Trustworthy AI UAV System with Self-Adaptivity for Air Quality Monitoring," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6253-6260, doi: 10.1109/IROS55552.2023.10342087.Abstract: This work presents the ETAUS, an Edge and Trustworthy AI UAV System, as a mobile sensing platform for air quality monitoring. ETAUS employs an FPGA device as the main hardware computing architecture rather than relying solely on a microprocessor or integrating with GPUs to meet real-time processing demands and achieve adaptivity and scalability. ETAUS contains a neural engine that can execute our customized AI model for air quality index (AQI) level classification and a pre-trained model for detecting objects containing private information. ETAUS also incorporates a de-identification process, cryptographic functions, and protection matrices to safeguard information and individuals' privacy. Furthermore, cryptographic functions and protection matrices are implemented as reconfigurable modules, which can accelerate processing, protect data privacy, and be reconfigured as needed. Experiments have demonstrated ETAUS can achieve a speedup of 3.15x to 72.46x for AQI level classification compared to microprocessor-based and GPU-based designs. ETAUS can also enhance energy efficiency by 5.02x compared to embedded GPU solutions such as NVIDIA Jetson Nano. To support all the cryptographic functions and protection matrices, system adaptivity in ETAUS can significantly increase resource utilization while decreasing power consumption by up to 2.79%. keywords: {Privacy;Scalability;Computational modeling;Air quality;Autonomous aerial vehicles;Energy efficiency;Cryptography},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342087&isnumber=10341342

C. Lienen, S. H. Middeke and M. Platzner, "FPGADDS: An Intra-FPGA Data Distribution Service for ROS 2 Robotics Applications," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6261-6266, doi: 10.1109/IROS55552.2023.10341921.Abstract: Modern computing platforms for robotics applications comprise a set of heterogeneous elements, e.g., multi-core CPUs, embedded GPUs, and FPGAs. FPGAs are reprogrammable hardware devices that allow for fast and energy-efficient computation of many relevant tasks in robotics. ROS is the de-facto programming standard for robotics and decomposes an application into a set of communicating nodes. ReconROS is a previous approach that can map complete ROS nodes into hardware for acceleration. Since ReconROS relies on standard ROS communication layers, exchanging data between hardware-mapped nodes can lead to a performance bottleneck. This paper presents fpgaDDS, a lean data distribution service for hardware-mapped ROS 2 nodes. fpgaDDS relies on a customized and statically generated streaming-based communication architecture. We detail this communication architecture with its components and outline its benefits. We evaluate fpgaDDS on a test example and a larger autonomous vehicle case study. Compared to a ROS 2 application in software, we achieve speedups of up to 13.34 and reduce jitter by two orders of magnitude. keywords: {Computer architecture;Programming;Jitter;Hardware;Software;Energy efficiency;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341921&isnumber=10341342

A. Lendinez et al., "Enhancing 5G-Enabled Robots Autonomy by Radio-Aware Semantic Maps," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6267-6272, doi: 10.1109/IROS55552.2023.10342279.Abstract: Future robotics systems aiming for true autonomy must be robust against dynamic and unstructured environments. The 5th generation (5G) mobile network is expected to provide ubiquitous, reliable and low-latency wireless communications to ground robots, especially in outdoor scenarios. Empowered by 5G, the digital transformation of robotics is emerging, enabled by the cloud-native paradigm and the adoption of edge-computing principles for heavy computational task offloading. However, wireless link quality fluctuates due to multiple aspects such as the topography of the deployment area, the presence of obstacles, robots' movement and the configuration of the serving base stations. This directly impacts not only the connectivity to the robots but also the performance of robot operations, resulting in severe challenges when targeting full robot autonomy. To address such challenges, in this paper, we propose a framework to build a semantic map based on radio quality. By means of our proposed approach, mobile robots can gain knowledge on up-to-date radio context map information of the surrounding environment, hence enabling reliable and efficient robotics operations. keywords: {Wireless communication;5G mobile communication;Semantics;Surfaces;Reliability;Mobile robots;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342279&isnumber=10341342

M. Mayr, F. Rovida and V. Krueger, "SkiROS2: A Skill-Based Robot Control Platform for ROS," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6273-6280, doi: 10.1109/IROS55552.2023.10342216.Abstract: The need for autonomous robot systems in both the service and the industrial domain is larger than ever. In the latter, the transition to small batches or even “batch size 1” in production created a need for robot control system architectures that can provide the required flexibility. Such architectures must not only have a sufficient knowledge integration framework. It must also support autonomous mission execution and allow for interchangeability and interoperability between different tasks and robot systems. We introduce SkiROS2, a skill-based robot control platform on top of ROS. SkiROS2 proposes a layered, hybrid control structure for automated task planning, and reactive execution, supported by a knowledge base for reasoning about the world state and entities. The scheduling formulation builds on the extended behavior tree model that merges task-level planning and execution. This allows for a high degree of modularity and a fast reaction to changes in the environment. The skill formulation based on pre-, hold-and post-conditions allows to organize robot programs and to compose diverse skills reaching from perception to low-level control and the incorporation of external tools. We relate SkiROS2 to the field and outline three example use cases that cover task planning, reasoning, multisensory input, integration in a manufacturing execution system and reinforcement learning. keywords: {Robot control;Systems architecture;Tutorials;Reinforcement learning;Production;Organizations;Cognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342216&isnumber=10341342

H. Wu, M. Wu, D. Sadigh and C. Barrett, "Soy: An Efficient MILP Solver for Piecewise-Affine Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6281-6288, doi: 10.1109/IROS55552.2023.10342011.Abstract: Piecewise-affine (PWA) systems are widely used for modeling and control of robotics problems including modeling contact dynamics. A common approach is to encode the control problem of the PWA system as a Mixed-Integer Convex Program (MICP), which can be solved by general-purpose off-the-shelf MICP solvers. To mitigate the scalability challenge of solving these MICP problems, existing work focuses on devising efficient and strong formulations of the problems, while less effort has been spent on exploiting their specific structure to develop specialized solvers. The latter is the theme of our work. We focus on efficiently handling one-hot constraints, which are particularly relevant when encoding PWA dynamics. We have implemented our techniques in a tool, Soy, which organically integrates logical reasoning, arithmetic reasoning, and stochastic local search. For a set of PWA control benchmarks, Soy solves more problems, faster, than two state-of-the-art MICP solvers. keywords: {Scalability;Stochastic processes;Benchmark testing;Cognition;Encoding;Intelligent robots;Arithmetic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342011&isnumber=10341342

C. A. Dimmig, A. Goodridge, G. Baraban, P. Zhu, J. Bhowmick and M. Kobilarov, "A Small Form Factor Aerial Research Vehicle for Pick-and-Place Tasks with Onboard Real-Time Object Detection and Visual Odometry," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6289-6296, doi: 10.1109/IROS55552.2023.10342295.Abstract: This paper introduces a novel, small form-factor, aerial vehicle research platform for agile object detection, classification, tracking, and interaction tasks. General-purpose hardware components were designed to augment a given aerial vehicle and enable it to perform safe and reliable grasping. These components include a custom collision tolerant cage and low-cost Gripper Extension Package, which we call GREP, for object grasping. Small vehicles enable applications in highly constrained environments, but are often limited by computational resources. This work evaluates the challenges of pick-and-place tasks, with entirely onboard computation of object pose and visual odometry based state estimation on a small platform, and demonstrates experiments with enough accuracy to reliably grasp objects. In a total of 70 trials across challenging cases such as cluttered environments, obstructed targets, and multiple instances of the same target, we demonstrated successfully grasping the target in 93 % of trials. Both the hardware component designs and software framework are released as open-source, since our intention is to enable easy reproduction and application on a wide range of small vehicles. keywords: {Grasping;Object detection;Hardware;Visual servoing;Software;Software reliability;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342295&isnumber=10341342

F. De Vincenti and S. Coros, "Ungar - A C++ Framework for Real-Time Optimal Control Using Template Metaprogramming," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6297-6303, doi: 10.1109/IROS55552.2023.10341365.Abstract: We present Ungar, an open-source library to aid the implementation of high-dimensional optimal control problems (OCPs). We adopt modern template metaprogramming techniques to enable the compile-time modeling of complex systems while retaining maximum runtime efficiency. Our framework provides syntactic sugar to allow for expressive formulations of a rich set of structured dynamical systems. While the core modules depend only on the header-only Eigen and Boost.Hana libraries, we bundle our codebase with optional packages and custom wrappers for automatic differentiation, code generation, and nonlinear programming. Finally, we demonstrate the versatility of Ungar in various model predictive control applications, namely, four-legged locomotion and collaborative loco-manipulation with multiple one-armed quadruped robots. Ungar is available under the Apache License 2.0 at https://github.com/fdevinc/ungar. keywords: {Codes;Optimal control;Collaboration;C++ languages;Syntactics;Libraries;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341365&isnumber=10341342

Y. -S. Hsiao et al., "VaPr: Variable-Precision Tensors to Accelerate Robot Motion Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6304-6309, doi: 10.1109/IROS55552.2023.10342109.Abstract: High-dimensional motion generation requires nu-merical precision for smooth, collision-free solutions. Typically, double-precision or single-precision floating-point (FP) formats are utilized. Using these for big tensors imposes a strain on the memory bandwidth provided by the devices and alters the memory footprint, hence limiting their applicability to low-power edge devices needed for mobile robots. The uniform application of reduced precision can be advantageous but severely degrades solutions. Using decreased precision data types for important tensors, we propose to accelerate motion generation by removing memory bottlenecks. We propose variable-precision (VaPr) search optimization to determine the appropriate precision for large tensors from a vast search space of approximately 4 million unique combinations for FP data types across the tensors. To obtain the efficiency gains, we exploit existing platform support for an out-of-the-box GPU speedup and evaluate prospective precision converter units for GPU types that are not currently supported. Our experimental results on 800 planning problems for the Franka Panda robot on the MotionBenchmaker dataset across 8 environments show that a 4-bit FP format is sufficient for the largest set of tensors in the motion generation stack. With the software-only solution, VaPr achieves 6.3% and 6.3% speedups on average for a significant portion of motion generation over the SOTA solution (CuRobo) on Jetson Orin and RTX2080 Ti GPU, respectively, and 9.9%, 17.7% speedups with the FP converter. More details are available at sites.google.com/nvidia.com/vapr. keywords: {Robot motion;Tensors;Limiting;Pipelines;Graphics processing units;Manipulators;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342109&isnumber=10341342

F. Sanches et al., "Scalable. Intuitive Human to Robot Skill Transfer with Wearable Human Machine Interfaces: On Complex, Dexterous Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6318-6325, doi: 10.1109/IROS55552.2023.10341661.Abstract: The advent of collaborative industrial and house-hold robotics has blurred the demarcation between the human and robot workspace. The capability of robots to function efficiently alongside humans requires new research to be conducted in dynamic environments as opposed to the traditional well-structured laboratory. In this work, we propose an efficient skill transfer methodology comprising intuitive interfaces, efficient optical tracking systems, and compliant control of robotic arm-hand systems. The lightweight wearable interfaces mounted with robotic grippers and hands allow the execution of dexterous activities in dynamic environments without restricting human dexterity. The fiducial and reflective markers mounted on the interfaces facilitate the extraction of positional and rotational information allowing efficient trajectory tracking. As the tasks are performed using the mounted grippers and hands, gripper state information can be directly transferred. The hardware-agnostic nature and efficiency of the proposed interfaces and skill transfer methodology are demonstrated through the execution of complex tasks that require increased dexterity, writing and drawing. keywords: {Visualization;Service robots;Optical recording;End effectors;Trajectory;Optical reflection;Optical sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341661&isnumber=10341342

L. Yin et al., "Towards Cooperative Flight Control Using Visual-Attention," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6334-6341, doi: 10.1109/IROS55552.2023.10342229.Abstract: The cooperation of a human pilot with an autonomous agent during flight control realizes parallel autonomy. We propose an air-guardian system that facilitates cooperation between a pilot with eye tracking and a parallel end-to-end neural control system. Our vision-based air-guardian system combines a causal continuous-depth neural network model with a cooperation layer to enable parallel autonomy between a pilot and a control system based on perceived differences in their attention profiles. The attention profiles for neural networks are obtained by computing the networks' saliency maps (feature importance) through the VisualBackProp algorithm, while the attention profiles for humans are either obtained by eye tracking of human pilots or saliency maps of networks trained to imitate human pilots. When the attention profile of the pilot and guardian agents align, the pilot makes control decisions. Otherwise, the air-guardian makes interventions and takes over the control of the aircraft. We show that our attention-based air-guardian system can balance the trade-off between its level of involvement in the flight and the pilot's expertise and attention. The guardian system is particularly effective in situations where the pilot was distracted due to information overload. We demonstrate the effectiveness of our method for navigating flight scenarios in simulation with a fixed-wing aircraft and on hardware with a quadrotor platform. keywords: {Visualization;Neural networks;Gaze tracking;Aircraft navigation;Hardware;Aircraft;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342229&isnumber=10341342

A. Jokic, A. Khazraei, M. Petrovic, Z. Jakovljevic and M. Pajic, "Cyber-Attacks on Wheeled Mobile Robotic Systems with Visual Servoing Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6342-6348, doi: 10.1109/IROS55552.2023.10341376.Abstract: Visual servoing represents a control strategy capable of driving dynamical systems from the current to the desired pose, when the only available information is the images generated at both poses. In this work, we analyze vulnerability of such systems and introduce two types of attacks to deceive visual servoing controller within a wheeled mobile robotic system. The attack goal is to alter the visual servoing procedure in such a way that mobile robot achieves the pose defined by an attacker instead of the desired one. Specifically, the attacks exploit image transformations developed using a methodology based on simulated annealing. The main difference between the attacks is the considered threat model - i.e., how the attacker has infiltrated the system. The first attack assumes the real-time camera feed has been compromised and thus, the images from the current pose are modified (e.g., during the acquisition or communication); for the second, only the desired destination image is potentially altered. Finally, in 3D simulations and real- world experiments, we show the effectiveness of cyber-attacks. keywords: {Threat modeling;Image transformation;Solid modeling;Three-dimensional displays;Runtime;Simulated annealing;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341376&isnumber=10341342

S. N. Aspragkathos, E. Ntouros, G. C. Karras, B. Linares-Barranco, T. Serrano-Gotarredona and K. J. Kyriakopoulos, "An Event-Based Tracking Control Framework for Multirotor Aerial Vehicles Using a Dynamic Vision Sensor and Neuromorphic Hardware," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6349-6355, doi: 10.1109/IROS55552.2023.10342437.Abstract: In this paper, we present an event-based control framework for the efficient tracking of contour-based areas, such as road pavements, using a multirotor aerial vehicle equipped with a bio-inspired Dynamic Vision Sensor (DVS). Concerning the detection part, the DVS camera captures events, which are asynchronously fed into a Neuromorphic Hough Transform algorithm running on a SpiNN-3 board and implemented as a Spiking Neural Network (SNN). Next, the asynchronous output of the detection module is fed into an analytically formulated event-based Partitioned Visual Servoing (PVS) algorithm, running on conventional processing hardware, which allows the multirotor to autonomously track and navigate along the detected contour. The proposed architecture achieves efficient tracking of contour-based areas, while constantly maintaining the latter inside the DVS camera's field of view. A set of real-time experiments in various settings employing an octorotor equipped with a downward-looking DVS and a SpiNN-3 board demonstrate the effectiveness of the suggested framework. keywords: {Neuromorphics;Transforms;Vision sensors;Cameras;Hardware;Real-time systems;Visual servoing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342437&isnumber=10341342

F. Makiyeh, F. Chaumette, M. Marchal and A. Krupa, "Shape Servoing of a Soft Object Using Fourier Series and a Physics-Based Model," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6356-6363, doi: 10.1109/IROS55552.2023.10342354.Abstract: In this paper, we propose a physics-based robot controller to deform a soft object toward a desired 3D shape using a limited number of handling points. For this purpose, the shape of the deformable object is represented using Fourier descriptors. We derive the analytical relation that provides the variation of the Fourier coefficients as a function of the movements of the handling points by considering a mass-spring model (MSM). A control law is then designed from this relation. Since the MSM provides an approximation of the object behavior, which in practice can lead to a drift between the object and its model, an online realignment of the model with the real object is performed by tracking its surface from data provided by a remote RGB-D camera. Simulation results validate the approach for the case where many points interact on a 2D soft object while experimental results obtained with two robotic arms demonstrate the autonomous shaping of a 3D soft object. keywords: {Three-dimensional displays;Shape;Tracking;Simulation;Robot vision systems;Manipulators;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342354&isnumber=10341342

L. Robinson, D. De Martini, M. Gadd and P. Newman, "Visual Servoing on Wheels: Robust Robot Orientation Estimation in Remote Viewpoint Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6364-6370, doi: 10.1109/IROS55552.2023.10341260.Abstract: This work proposes a fast deployment pipeline for visually-servoed robots which does not assume anything about either the robot - e.g. sizes, colour or the presence of markers - or the deployment environment. Specifically, we apply a learning based approach to reliably estimate the pose of a robot in the image frame of a 2D camera upon which a visual servoing control system can be deployed. To alleviate the time-consuming process of labelling image data, we propose a weakly supervised pipeline that can produce a vast amount of data in a small amount of time. We evaluate our approach on a dataset of remote camera images captured in various indoor environments demonstrating high tracking performances when integrated into a fully-autonomous pipeline with a simple controller. With this, we then analyse the data requirement of our approach, showing how it is possible to deploy a new robot in a new environment in fewer than 30.00 min. keywords: {Training;Visualization;Pipelines;Wheels;Cameras;Visual servoing;Mobile robots;Cloud Robotics;Visual Servoing;Deep Learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341260&isnumber=10341342

F. Dursun, B. V. Adorno, S. Watson and W. Pan, "Maintaining Visibility of Dynamic Objects in Cluttered Environments Using Mobile Manipulators and Vector Field Inequalities," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6371-6378, doi: 10.1109/IROS55552.2023.10341763.Abstract: Vision-based perception has become prevalent in robotic applications, especially in those where the control loop relies on visual data, such as visual servoing. For those applications, ensuring that the features or target object remain visible to the camera is critical, necessitating visibility-aware control. In this paper, we propose a method to guarantee the visibility of a dynamic object using a constrained kinematic controller and Vector Field Inequalities (VFIs) to include a linear visibility constraint. Unlike existing methods, we introduce constraints into the kinematic controller to ensure the target's visibility without needing a trajectory optimizer or local planner. Our method maintains the target object in the camera field of view (FoV) by representing the FoV with four infinite planes and maintaining the distance between the target object and each plane higher than a predefined distance. We evaluated the proposed approach using a mobile manipulator in two simulations involving cluttered environments: the first scenario involves a stationary target object, whereas the second scenario presents a more challenging workspace involving a moving target. Our results demonstrate that the proposed approach successfully maintains the target within the FoV while avoiding obstacles in the workspace, showing the potential of our method to improve the safety and reliability of visual-servoing-based robotic systems. keywords: {Visualization;Target tracking;Kinematics;Cameras;End effectors;Visual servoing;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341763&isnumber=10341342

A. Gandhi, S. -S. Chiang, C. D. Onal and B. Calli, "Shape Control of Variable Length Continuum Robots Using Clothoid-Based Visual Servoing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6379-6386, doi: 10.1109/IROS55552.2023.10342057.Abstract: In this paper, we present a novel clothoid-based visual servoing method for controlling the shape of a variable length continuum manipulator. Clothoids are curves with linearly changing curvature. They allow us to obtain a smooth representation of a continuum manipulator's shape in a compact form with few parameters. Using this curve model, we generate image features that are used in an adaptive visual servoing method to drive the robot to a desired shape. The adaptive algorithm estimates and updates a local interaction matrix that maps the rate of change in clothoid features to actuator velocities of the continuum manipulator. As such, the method does not require any robot model or even actuator encoder measurements and only uses the visual clothoid features to control the robot shape. A unique advantage of using our clothoid representation is being able to generate reference shape curves without the need for taking images of the robot at the desired shapes. Experiments demonstrate successful shape and end effector pose convergence for a diverse set of references. Our repeatability tests demonstrate that the system performance is consistent. Notably, we also present the first results in the literature for the vision-based shape control of a variable length continuum robot, extending and contracting to achieve the desired shape. keywords: {Adaptation models;Actuators;Visualization;Shape control;Shape;System performance;Shape measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342057&isnumber=10341342

S. Chatterjee, A. C. Karade, A. Gandhi and B. Calli, "Keypoints-Based Adaptive Visual Servoing for Control of Robotic Manipulators in Configuration Space," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6387-6394, doi: 10.1109/IROS55552.2023.10342503.Abstract: This paper presents a visual servoing method for controlling a robot in the configuration space by purely using its natural features. We first created a data collection pipeline that uses camera intrinsics, extrinsics, and forward kinematics to generate 2D projections of a robot's joint locations (keypoints) in image space. Using this pipeline, we are able to collect large sets of real-robot data, which we use to train realtime keypoint detectors. The inferred keypoints from the trained model are used as control features in an adaptive visual servoing scheme that estimates, in runtime, the Jacobian relating the changes of the keypoints and joint velocities. We compared the 2D configuration control performance of this method to the skeleton-based visual servoing method (the only other algorithm for purely vision-based configuration space visual servoing), and demonstrated that the keypoints provide more robust and less noisy features, which result in better transient response. We also demonstrate the first vision-based 3D configuration space control results in the literature, and discuss its limitations. Our data collection pipeline is available at https://github.com/JaniC-WPI/KPDataGenerator.git which can be utilized to collect image datasets and train realtime keypoint detectors for various robots and environments. keywords: {Transient response;Three-dimensional displays;Runtime;Pipelines;Detectors;Aerospace electronics;Data collection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342503&isnumber=10341342

E. Bauer, B. G. Cangan and R. K. Katzschmann, "Autonomous Marker-Less Rapid Aerial Grasping," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6395-6402, doi: 10.1109/IROS55552.2023.10342033.Abstract: In a future with autonomous robots, visual and spatial perception is of utmost importance for robotic systems. Particularly for aerial robotics, there are many applications where utilizing visual perception is necessary for any real-world scenarios. Robotic aerial grasping using drones promises fast pick-and-place solutions with a large increase in mobility over other robotic solutions. Utilizing Mask R-CNN scene segmentation (detectron2), we propose a vision-based system for autonomous rapid aerial grasping which does not rely on mark-ers for object localization and does not require the appearance of the object to be previously known. Combining segmented images with spatial information from a depth camera, we generate a dense point cloud of the detected objects and perform geometry - based grasp planning to determine grasping points on the objects. In real-world experiments on a dynamically grasping aerial platform, we show that our system can replicate the performance of a motion capture system for object local-ization up to 94.5 % of the baseline grasping success rate. With our results, we show the first use of geometry-based grasping techniques with a flying platform and aim to increase the autonomy of existing aerial manipulation platforms, bringing them further towards real-world applications in warehouses and similar environments.††Code: https://github.com/srl-ethz/detectron-realsense keywords: {Location awareness;Point cloud compression;Image segmentation;Visualization;Shape;Grasping;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342033&isnumber=10341342

J. Akl, Y. Patil, C. Todankar and B. Calli, "Vision-Based Oxy-Fuel Torch Control for Robotic Metal Cutting," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6403-6410, doi: 10.1109/IROS55552.2023.10341532.Abstract: The automation of key processes in metal cutting would substantially benefit many industries such as manufacturing and metal recycling. We present a vision-based control scheme for automated metal cutting with oxy-fuel torches, an established cutting medium in industry. The system consists of a robot equipped with a cutting torch and an eye-in-hand camera observing the scene behind a tinted visor. We develop a vision-based control algorithm to servo the torch's motion by visually observing its effects on the metal surface. As such, the vision system processes the metal surface's heat pool and computes its associated features, specifically pool convexity and intensity, which are then used for control. The operating conditions of the control problem are defined within which the stability is proven. In addition, metal cutting experiments are performed using a physical 1-DOF robot and oxy-fuel cutting equipment. Our results demonstrate the successful cutting of metal plates across three different plate thicknesses, relying purely on visual information without a priori knowledge of the thicknesses. keywords: {Industries;Heating systems;Visualization;Service robots;Metals;Combustion;Steel},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341532&isnumber=10341342

R. Parosi, M. Risiglione, D. G. Caldwell, C. Semini and V. Barasuol, "Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341714.Abstract: We propose a control pipeline for SAG (Searching, Approaching, and Grasping) of objects, based on a decoupled arm kinematic chain and impedance control, which integrates image-based visual servoing (IBVS). The kinematic decoupling allows for fast end-effector motions and recovery that leads to robust visual servoing. The whole approach and pipeline can be generalized for any mobile platform (wheeled or tracked vehicles), but is most suitable for dynamically moving quadruped manipulators thanks to their reactivity against disturbances. The compliance of the impedance controller makes the robot safer for interactions with humans and the environment. We demonstrate the performance and robustness of the proposed approach with various experiments on our 140 kg HyQReal quadruped robot equipped with a 7-DoF manipulator arm. The experiments consider dynamic locomotion, tracking under external disturbances, and fast motions of the target object. keywords: {Wrist;Pipelines;Kinematics;Grasping;Search problems;Visual servoing;Impedance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341714&isnumber=10341342

D. Yu, M. Xu, Z. Liu and H. Wang, "Lyapunov Constrained Safe Reinforcement Learning for Multicopter Visual Servoing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6419-6424, doi: 10.1109/IROS55552.2023.10341561.Abstract: Traditional methods based on Lyapunov analysis and learning-based approaches such as reinforcement learning (RL) are two powerful tools in visual servo tasks. Traditional methods are interpretable and their stability can be guar-anteed by Lyapunov analysis. However, they tend to have a high dependency on an accurate system dynamic model. RL approaches learn to act based on past experiences and thus have higher adaptability on disturbances and errors. However, the training process is long and its safety or stability is generally hard to guarantee, making real-world training risky. In this paper, we propose a residual RL framework for training a multicopter to finish visual servo tasks under disturbances, guided by the system safety in terms of system Lyapunov function. Such an approach compensates for the lack of disturbance-rejection ability of the traditional method, and optimizes stability explicitly so the RL agent makes safer actions both during the training and in the final policy. A comparison between our approach and the baselines is provided in simulation, and real-world experiments on a multicopter are also carried out to show our effectiveness. We believe that this work moves one step toward achieving RL applications on real-world robotic systems. keywords: {Training;Visualization;System dynamics;Reinforcement learning;Stability analysis;Visual servoing;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341561&isnumber=10341342

J. Liang et al., "Employing Multi-Layer, Sensorised Kirigami Grippers for Single-Grasp Based Identification of Objects and Force Exertion Estimation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6433-6440, doi: 10.1109/IROS55552.2023.10341390.Abstract: Soft robotic devices have been popular in handling intricate grasping and dexterous manipulation tasks, serving as an alternative to conventional, rigid end-effectors. These devices are relatively simple, lightweight, and cost-effective. Recently, kirigami based structures have been used to create low-cost and disposable soft robotic grippers and hands. These grippers undergo a complex post-contact reconfiguration and conform to an object's shape and size during grasping. In this paper, we explore this new class of soft robotic grippers by utilising them for single-grasp object classification and grasping force estimation. We install simplistic sensors on both the gripper and the actuation system to estimate the state of the kirigami gripper, and the collected data features are employed to train Random Forest models for identifying the grasped object. The classifier trained exhibits a high accuracy of 98 % in discriminating objects of various shapes. When handling food items, the classifier achieves an accuracy of 94 %, while in classifying transparent objects, the classifier obtained again a high accuracy of 97 %. Finally, object-specific force estimation models are triggered based on the classification decision of the Random Forest model to estimate the grasping force exerted by the gripper. These positive outcomes demonstrate the kirigami based robotic gripper's potential for object classification in a variety of circumstances, particularly where vision systems are not available or not reliable. keywords: {Shape;Force;Estimation;Grasping;Soft robotics;Robot sensing systems;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341390&isnumber=10341342

V. Perez-Sanchez, S. R. Nekoo, B. Arrue and A. Ollero, "A Finite-Time State-Dependent Differential Riccati Equation Control Design for Closed-Loop SMA-Actuated Hip Joint," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6441-6448, doi: 10.1109/IROS55552.2023.10341562.Abstract: This paper presents the modeling and closed loop control of the shape-memory-alloy (SMA)-actuated hip joint of a flapping-wing flying robot (FWFR). Despite the lightweight legs/claw mechanism, a strong force of grasping is needed. The SMAs show high force delivery; however, it is difficult to control (position and temperature) the actuation due to the necessity of high currents for warming up, and time for cooling down process. This paper presents a state-dependent differential Riccati equation (SDDRE) controller taking into account the SMA dynamic and the actuator limits to control the leg/claw system. The use of nonlinear optimal control, specifically, the SDDRE, has been reported for the first time for bio-inspired leg/claw control of FWFR. The dynamics of the SMA actuators and on-off switching of the MOSFETs to provide current for the system demands switching in the design of the controller as a constraint for inputs which was considered in the design. Simulation and experimental results and analysis of different phases of heating of SMAs were discussed and resulted in satisfactory control performance. keywords: {Actuators;Shape memory alloys;Force;Riccati equations;Switches;Nonlinear dynamical systems;Task analysis;SDRE;SMA;Closed-loop control;Bio-inspired claw;Fkapping-wing robots;UAV;Aerial Robot},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341562&isnumber=10341342

Z. Xiong, Z. Guo, L. Yuan, Y. Su, Y. Liu and H. Lipson, "Rapid Grasping of Fabric Using Bionic Soft Grippers with Elastic Instability," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6449-6455, doi: 10.1109/IROS55552.2023.10341428.Abstract: Robot grasping is subject to an inherent tradeoff: Grippers with a large span typically take a longer time to close, and fast grippers usually cover a small span. However, many practical applications of grippers require the ability to close a large distance rapidly. For example, grasping cloth typically requires pressing a wide span of fabric into a graspable cusp. Besides, the ability to perform human-like grasping and ease offabrication are also very important for new soft grippers. Here, we demonstrate a human-finger-inspired snapping gripper that exploits elastic instability to achieve rapid and reversible closing over a wide span. Using prestressed semi-rigid material as the skeleton, the gripper fingers can widely open (86 mm) and rapidly close (46 ms) following a trajectory similar to that of a thumb-index finger pinching, and is 2.7 times and 10.9 times better than the reference gripper in terms of span and speed, respectively. We theoretically give the design principle, simulatively verify the method, and experimentally test this gripper on a variety of rigid, flexible, and limp objects and achieve good mechanical performance. keywords: {Thumb;Grasping;Pressing;Soft robotics;Fabrics;Software;Skeleton},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341428&isnumber=10341342

J. Park et al., "Design of a Cable Driven Wearable Fitness Device for Upper Limb Exercise," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6456-6461, doi: 10.1109/IROS55552.2023.10342373.Abstract: To provide an alternative to conventional large-scale fitness equipment, we previously developed a soft passive wearable device for upper limb resistance exercises that utilized elastic exercise bands. However, the user was required to manually adjust the level of strength. In this paper, we introduce a novel wearable fitness device for upper limb exercise that constitutes cable-driven actuation to control the resistance profiles. Our proposed device allows for the generation of isotonic force trajectories, similar to those produced during dumbbell lifting, utilizing custom cable-driven actuators. The cable path of the device was determined based on the results of user testing aimed at optimizing muscle stimulation levels. In addition, linear resonant actuators were installed at customized haptic handles and upper arm modules to enhance proprioceptive sensitivity and exercise efficacy. The immersive “exer-tainment” user display interface was also redesigned to increase user motivation. The efficacy of our device was assessed by comparing the surface electromyography (sEMG) activities of upper limb muscles during chest press and triceps extension exercises performed with our device versus those using traditional weight training machines and rubber bands. It was found that our device could effectively strengthen during arm exercise, such as triceps extension. keywords: {Resistance;Performance evaluation;Training;Actuators;Presses;Force;Entertainment industry},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342373&isnumber=10341342

C. Suulker et al., "Soft Cap for Vine Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6462-6468, doi: 10.1109/IROS55552.2023.10341377.Abstract: Growing robots based on the eversion principle are known for their ability to extend rapidly, from within, along their longitudinal axis, and, in doing so, reach deep into hitherto inaccessible, remote spaces. Despite many advantages, vine robots also present significant challenges, one of which is maintaining sensory payload at the tip without restricting the eversion process. A variety of tip mechanisms have been proposed by the robotics community, among them rounded caps of relatively complex construction that are not always compatible with functional hardware, such as sensors or navigation pouches, integrated with the main eversion structure. Moreover, many tip designs incorporate rigid materials, reducing the robot's flexibility and consequent ability to navigate through narrow openings. Here, we address these shortcomings and propose a design to overcome them: a soft, entirely fabric based, cylindrical cap that can easily be slipped onto the tip of vine robots. Having created a series of caps of different sizes and materials, an experimental study was conducted to evaluate our new design in terms of four key aspects: vine robot made from multiple layers of everting material, solid objects protruding from the vine robot, squeezability, and navigability. In all scenarios, we can show that our soft, flexible cap is robust in its ability to maintain its position and is capable of transporting payloads such as a camera across long distances. We also demonstrate that the robot's ability to move through restricted aperture openings and indeed its overall flexibility is virtually unhindered by the addition of our cap. The paper discusses the advantages of this design and gives further recommendations in relation to aspects of its engineering. keywords: {Navigation;Robot vision systems;Solids;Cameras;Hardware;Fabrics;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341377&isnumber=10341342

J. Royo-Miquel, M. Hamaya, C. C. Beltran-Hernandez and K. Tanaka, "Learning Robotic Assembly by Leveraging Physical Softness and Tactile Sensing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6469-6476, doi: 10.1109/IROS55552.2023.10341471.Abstract: This study aims to achieve autonomous robotic assembly under uncertain conditions arising from imprecise goal positioning and variations in the angle of the grasped part. Soft robots are suitable for such uncertain and contact-rich environments and are capable of insertion tasks with imprecise goal positions. However, we may also struggle to handle further uncertainty, such as variations in grasping pose. To address the challenge posed by multiple sources of uncertainty, we equipped the soft robot with a tactile sensor. Our key insight is that tactile signal patterns are closely linked to the subtask transitions in an assembly process, specifically from the search to insertion subtasks. We hypothesize soft robots could complete the task by exploring the transition via tactile signals, even in scenarios with imprecise goal positions and grasp misalignment. To this end, we develop an anomaly detection model using a Variational Autoencoder to identify the timing of these transitions. We then employ learning and heuristic-based controllers to navigate the peg tip to the hole and perform the insertion. Our method was validated through real-robot experiments using a soft wrist and a vision-based tactile sensor. The results demonstrate that our method achieves a 100% success rate in scenarios with less uncertain goal pose ($\sigma=2\text{mm}$) and grasp misalignment (up to 5°) and a 70% success rate in scenarios with uncertain goal pose ($\sigma=10\text{mm}$) and grasp misalignment (up to 20°). Moreover, our anomaly detection model can generalize to different peg diameters without additional training. keywords: {Wrist;Robotic assembly;Training;Uncertainty;Tactile sensors;Soft robotics;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341471&isnumber=10341342

D. -G. Lee, N. G. Kim and J. -H. Ryu, "High-Curvature Consecutive Tip Steering of a Soft Growing Robot for Improved Target Reachability," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6477-6483, doi: 10.1109/IROS55552.2023.10341407.Abstract: Over the years, soft growing robots that allow the feeding of new materials at their tips have attracted considerable attention owing to their unique locomotion characteristics. However, accessing targets over highly curved passages by steering compliant continuum bodies remains challenging. To this end, this study proposes a new tip steering mechanism that imparts soft growing robots with consecutive high-curvature steering ability. We place a hyper-redundant rolling contact joint and twisted string actuator (TSA) at the tip of a soft growing robot to provide high-curvature continuous steering with a smaller scale factor, allowing consecutive turns. The proposed small form factor continuous curvature tip steering mechanism allows the steering mechanism to stay at the tip during steering, improving accessibility by enabling consecutive turns. The improved accessibility of the proposed mechanism is demonstrated based on better confined space reachability compared with that of conventional whole-body steering soft growing robots, along with a consecutive high-curvature pipe navigation demonstration. keywords: {Actuators;Navigation;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341407&isnumber=10341342

J. Peters, C. M. Sourkounis, M. Wiese, T. Kwasnitschka and A. Raatz, "Single Channel Soft Robotic Actuator Leveraging Switchable Strain-Limiting Structures for Deep-Sea Suction Sampling," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6484-6490, doi: 10.1109/IROS55552.2023.10341262.Abstract: Soft Robotics has established itself as an integral field in the broader discipline of general robotics through multiple advantages like inherent safety, adaptable morphology, and energy- and weight efficiency. Especially in environments hostile to humans and classical robots like the deep sea, soft robotic structures made out of silicone and actuated by seawater have numerous advantages. An application with a huge scientific and commercial potential for soft robotic solutions is suction sampling for marine geology in depths of up to 6000 m. In this paper, we propose a single channel soft robotic actuator that is able to bend into six directions while absorbing process forces. By embedding a low melting point alloy (LMPA) acting as switchable strain-limiting structures, the actuator is capable of hexa-planar bending of up to 40° and elongation of 30 % with only one valve used for actuation. In addition, the LMPA chambers enable a stiffening factor of 4.1 and locking the actuator in its bending state for energy efficient usage in robotic deep-sea suction sampling. keywords: {Actuators;Wires;Water heating;Switches;Soft robotics;Bending;Valves},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341262&isnumber=10341342

S. Even and Y. Ozkan-Aydin, "Locomotion and Obstacle Avoidance of a Worm-Like Soft Robot," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6491-6496, doi: 10.1109/IROS55552.2023.10341586.Abstract: This paper presents a soft earthworm robot that is capable of both efficient locomotion and obstacle avoidance. The robot is designed to replicate the unique locomotion mechanisms of earthworms, which enable them to move through narrow and complex environments with ease. The robot consists of multiple segments, each with its own set of actuators, that are connected through rigid plastic joints, allowing for increased adaptability and flexibility in navigating different environments. The robot utilizes proprioceptive sensing and control algorithms to detect and avoid obstacles in real-time while maintaining efficient locomotion. The robot uses a pneumatic actuation system to mimic the circumnutation behavior exhibited by plant roots in order to navigate through complex environments. The results demonstrate the capabilities of the robot for navigating through cluttered environments, making this development significant for various fields of robotics, including search and rescue, environmental monitoring, and medical procedures. keywords: {Navigation;Propioception;Pneumatic systems;Soft robotics;Real-time systems;Sensors;Plastics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341586&isnumber=10341342

J. Li, J. Yang, Y. Liu, Z. Li, G. -Z. Yang and Y. Guo, "EasyGaze3D: Towards Effective and Flexible 3D Gaze Estimation from a Single RGB Camera," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6537-6543, doi: 10.1109/IROS55552.2023.10342361.Abstract: Eye gaze can convey rich information of human intentions, which enables the social robots to comprehend the cognition and behavior of human targets. However, the existing 3D gaze estimation methods generally have high requirements either on the dedicated hardware or the quantity and quality of training databases, which largely limits their practical application values. This paper proposes EasyGaze3D, an effective 3D gaze estimation framework using a single RGB camera. First, the framework detects the 2D facial landmarks and recovers the 3D facial shape from the input image, and derives the required camera parameters with these features. Then, without loss of generality, the gaze direction can be regarded as the vector pointing from the eyeball center to the pupil center, which are derived respectively from the detected facial landmarks and the spherical fitting performed on the recovered 3D facial shape. Besides, we propose a flexible yet efficient calibration module, namely Easy-Cali, for deriving the subject-specific 3D facial shape and eyeball centers. The features calibrated by Easy-Cali can further boost the performance of EasyGaze3D. Experimental results show that our proposed method, being plug-and-play and without the need of training on large-scale dataset, can achieve superior performance against the existing methods based on deep models. keywords: {Training;Three-dimensional displays;Shape;Fitting;Social robots;Estimation;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342361&isnumber=10341342

D. Kuzmenko, O. Tsepa, A. G. Kurbis, A. Mihailidis and B. Laschowski, "Efficient Visual Perception of Human-Robot Walking Environments Using Semi-Supervised Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6544-6549, doi: 10.1109/IROS55552.2023.10341654.Abstract: Convolutional neural networks trained using supervised learning can improve visual perception for human-robot walking. These advances have been possible due to largescale datasets like ExoNet and StairNet - the largest open-source image datasets of real-world walking environments. However, these datasets require vast amounts of manually annotated data, the development of which is time consuming and labor intensive. Here we present a novel semi-supervised learning system (ExoNet-SSL) that uses over 1.2 million unlabelled images from ExoNet to improve training efficiency. We developed a deep learning model based on mobile vision transformers and trained the model using semi-supervised learning for image classification. Compared to standard supervised learning (98.4%), our ExoNet-SSL system was able to maintain high prediction accuracy (98.8%) when tested on previously unseen environments, while requiring 35% fewer labelled images during training. These results show that semi-supervised learning can improve training efficiency by leveraging large amounts of unlabelled data and minimize the size requirements for manually annotated images. Future research $\text{will}$ focus on model deployment for onboard real-time inference and control of human-robot walking. keywords: {Legged locomotion;Training;Supervised learning;Semisupervised learning;Transformers;Real-time systems;Standards},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341654&isnumber=10341342

J. Lee, E. Quist, J. Chambers, M. Yip and N. Fisher, "Contactless Weight Estimation of Human Body and Body Parts for Safe Robotics-Assisted Casualty Extraction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6550-6556, doi: 10.1109/IROS55552.2023.10342473.Abstract: Deploying humans in a high-risk environment to extract casualties in order to provide medical attention is an inherently dangerous endeavor. To minimize this risk, Robotics and Autonomous Systems can be deployed in hazardous areas in place of human personnel to limit the exposure of first responders to various life-threatening conditions. The success of robotic extraction of injured persons depends heavily on how safely the human subject is handled. Therefore, the integration of intelligent technologies for secure control and motion planning is crucial in overcoming the dynamic and complex challenges of robotic grasping and manipulation. In this regard, the measurement of the target human subject's weight is an essential factor for safe grasping and maneuvering during robotic interactions with humans. This paper presents a contactless vision-based approach for estimating the weight of the human body. This approach employs visual body perception, 3D body point cloud representation, and a deep learning network for body segmentation to measure specific body parameters. Next, the body parameters are fed into a neural network model to predict the total body weight. This prediction then enables an approximation of the weight of individual body segments to be obtained. keywords: {Weight measurement;Point cloud compression;Visualization;Biological system modeling;Estimation;Grasping;Data collection;Robotic casualty extraction;Human robot interaction;Point cloud segmentation;Vision-based body perception;parameter measurement;weight estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342473&isnumber=10341342

J. Deng, P. Li, K. Dhaliwal, C. X. Lu and M. Khadem, "Feature-based Visual Odometry for Bronchoscopy: A Dataset and Benchmark," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6557-6564, doi: 10.1109/IROS55552.2023.10342034.Abstract: Bronchoscopy is a medical procedure that involves the insertion of a flexible tube with a camera into the airways to survey, diagnose and treat lung diseases. Due to the complex branching anatomical structure of the bronchial tree and the similarity of the inner surfaces of the segmental airways, navigation systems are now being routinely used to guide the operator during procedures to access the lung periphery. Current navigation systems rely on sensor-integrated bronchoscopes to track the position of the bronchoscope in real-time. This approach has limitations, including increased cost and limited use in non-specialized settings. To address this issue, researchers have proposed visual odometry algorithms to track the bronchoscope camera without the need for external sensors. However, due to the lack of publicly available datasets, limited progress is made. To this end, we have developed a database of bronchoscopy videos in a phantom lung model and ex-vivo human lungs. The dataset contains 34 video sequences with over 23,000 frames with odometry ground truth data collected using electromagnetic tracking sensors. With our dataset, we empower the robotics and machine learning community to advance the field. We share our insights on challenges in endoscopic visual odometry. Furthermore, we provide benchmark results for this dataset. State-of-the-art feature extraction algorithms including SIFT, ORB, Superpoint, Shi- Tomasi, and LoFTR are tested on this dataset. The benchmark results demonstrate that the LoFTR algorithm outperforms other approaches, but still has significant errors in the presence of rapid movements and occlusions. keywords: {Bronchoscopy;Machine learning algorithms;Navigation;Lung;Phantoms;Benchmark testing;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342034&isnumber=10341342

D. Huang, Y. Bi, N. Navab and Z. Jiang, "Motion Magnification in Robotic Sonography: Enabling Pulsation-Aware Artery Segmentation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6565-6570, doi: 10.1109/IROS55552.2023.10342220.Abstract: Ultrasound (US) imaging is widely used for diagnosing and monitoring arterial diseases, mainly due to the advantages of being non-invasive, radiation-free, and real-time. In order to provide additional information to assist clinicians in diagnosis, the tubular structures are often segmented from US images. To improve the artery segmentation accuracy and stability during scans, this work presents a novel pulsation-assisted segmentation neural network (PAS-NN) by explicitly taking advantage of the cardiac-induced motions. Motion magnification techniques are employed to amplify the subtle motion within the frequency band of interest to extract the pulsation signals from sequential US images. The extracted real-time pulsation information can help to locate the arteries on cross-section US images; therefore, we explicitly integrated the pulsation into the proposed PAS-NN as attention guidance. Notably, a robotic arm is necessary to provide stable movement during US imaging since magnifying the target motions from the US images captured along a scan path is not manually feasible due to the hand tremor. To validate the proposed robotic US system for imaging arteries, experiments are carried out on volunteers' carotid and radial arteries. The results demonstrated that the PAS-NN could achieve comparable results as state-of-the-art on carotid and can effectively improve the segmentation performance for small vessels (radial artery). The code11Code: https://qithub.com/dianveHuanq/RobPMEPASNN and demonstration video22Video: https://youtu.belc9AM042_lUQ can be publicly accessed. keywords: {Image segmentation;Ultrasonic imaging;Motion segmentation;Neural networks;Imaging;Manipulators;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342220&isnumber=10341342

H. You and J. Chen, "Compact and Accurate Adaptive Width Radial Basis Function Neural Network with Discriminative Features for Thyroid Gland Segmentation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6571-6578, doi: 10.1109/IROS55552.2023.10342090.Abstract: The goal of this work is to develop a fast, compact, accurate, and robust neural network for automatic thyroid gland segmentation in medical ultrasound images when the computational resources are limited, such as portable intelligent medical diagnostic devices or robots. An adaptive width radial basis function neural network model with discriminative features is proposed in this work. The model combines the local characteristics and global context of the entire feature space, which improves the performance of the automatic thyroid gland segmentation, and enhances the data fitting capability and robustness of the neural network. A new feature selection method combining the measurements of area under the curve values and Pearson correlation coefficients is proposed to select six discriminative and low correlated features from a set of handcrafted features, which helps to construct a compact yet accurate radial basis function neural network. The proposed method is experimented on thyroid ultrasound images using five-fold cross validation. The proposed method achieves an average accuracy of 0.9770 and an average IoU of 0.7841, achieving competitive segmentation performance compared to convolutional neural networks, with thousands of times faster training speed on a CPU. keywords: {Training;Performance evaluation;Image segmentation;Ultrasonic imaging;Computational modeling;Fitting;Radial basis function networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342090&isnumber=10341342

F. Abhimanyu, A. L. Orekhov, A. Bal, J. Galeotti and H. Choset, "Unsupervised Deformable Ultrasound Image Registration and Its Application for Vessel Segmentation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6579-6585, doi: 10.1109/IROS55552.2023.10341870.Abstract: This paper presents a deep-learning model for deformable registration of ultrasound images at online rates, which we call U-RAFT. As its name suggests, U-RAFT is based on RAFT, a convolutional neural network for estimating optical flow. U-RAFT, however, can be trained in an unsupervised manner and can generate synthetic images for training vessel segmentation models. We propose and compare the registration quality of different loss functions for training U-RAFT. We also show how our approach, together with a robot performing force-controlled scans, can be used to generate synthetic deformed images to significantly expand the size of a femoral vessel segmentation training dataset without the need for additional manual labeling. We validate our approach on both a silicone human tissue phantom as well as on in-vivo porcine images. We show that U-RAFT generates synthetic ultrasound images with 98% and 81% structural similarity index measure (SSIM) to the real ultrasound images for the phantom and porcine datasets, respectively. We also demonstrate that synthetic deformed images from U-RAFT can be used as a data augmentation technique for vessel segmentation models to improve intersection-over-union (IoU) segmentation performance. keywords: {Training;Deformable models;Optical losses;Image segmentation;Ultrasonic imaging;Ultrasonic variables measurement;Imaging phantoms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341870&isnumber=10341342

Z. Jiang, C. Li, X. Lil and N. Navab, "Thoracic Cartilage Ultrasound-CT Registration Using Dense Skeleton Graph," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6586-6592, doi: 10.1109/IROS55552.2023.10341575.Abstract: Autonomous ultrasound (US) imaging has gained increased interest recently, and it has been seen as a potential solution to overcome the limitations of free-hand US exami-nations, such as inter-operator variations. However, it is still challenging to accurately map planned paths from a generic atlas to individual patients, particularly for thoracic applications with high acoustic-impedance bone structures below the skin. To address this challenge, a dense graph-based non-rigid registration is proposed to transfer planned paths from the atlas to the current setup by explicitly considering subcutaneous bone surface. To this end, the sternum and cartilage branches are segmented using a template matching to assist coarse alignment of US and CT point clouds. Afterward, a directed graph is generated based on the CT template. Then, the self-organizing map using geographical distance is successively performed twice to extract the optimal graph representations for CT and US point clouds, individually. To evaluate the proposed approach, five cartilage point clouds from distinct patients are employed. The results demonstrate that the proposed graph-based registration can effectively map trajectories from CT to the current setup to do US examination through limited intercostal space. The non-rigid registration results in terms of Hausdorff distance (Mean±SD) is $9.48 \pm 0.27$ mm and the path transferring error in terms of Euclidean distance is $2.21\pm 1.11\ mm$. The code11https://github.com/marslicy/Cartilage-graph-based-US-CT-Registration and video22Video: https://www.youtube.com/watch?v=QJz2fkwgbP8 can be publicly accessed. keywords: {Point cloud compression;Self-organizing feature maps;Ultrasonic imaging;Computed tomography;Sternum;Euclidean distance;Bones},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341575&isnumber=10341342

I. Fried, J. Hoelscher, J. A. Akulian, S. Pizer and R. Alterovitz, "Landmark Based Bronchoscope Localization for Needle Insertion Under Respiratory Deformation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6593-6600, doi: 10.1109/IROS55552.2023.10342115.Abstract: Bronchoscopy is currently the least invasive method for definitively diagnosing lung cancer, which kills more people in the United States than any other form of cancer. Successfully diagnosing suspicious lung nodules requires accurate localization of the bronchoscope relative to a planned biopsy site in the airways. This task is challenging because the lung deforms intraoperatively due to respiratory motion, the airways lack photometric features, and the anatomy's appearance is repetitive. In this paper, we introduce a real-time camera-based method for accurately localizing a bronchoscope with respect to a planned needle insertion pose. Our approach uses deep learning and accounts for deformations and overcomes limitations of global pose estimation by estimating pose relative to anatomical landmarks. Specifically, our learned model considers airway bifurcations along the airway wall as landmarks because they are distinct geometric features that do not vary significantly with respiratory motion. We evaluate our method in a simulated dataset of lungs undergoing respiratory motion. The results show that our method generalizes across patients and localizes the bronchoscope with accuracy sufficient to access the smallest clinically-relevant nodules across all levels of respiratory deformation, even in challenging distal airways. Our method could enable physicians to perform more accurate biopsies and serve as a key building block toward accurate autonomous robotic bronchoscopy. keywords: {Location awareness;Bronchoscopy;Deformation;Robot kinematics;Pose estimation;Lung;Medical services},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342115&isnumber=10341342

Z. Li, I. Reyes and H. Alemzadeh, "Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6601-6607, doi: 10.1109/IROS55552.2023.10342013.Abstract: Surgical context inference has recently garnered significant attention in robot-assisted surgery as it can facilitate workflow analysis, skill assessment, and error detection. However, runtime context inference is challenging since it requires timely and accurate detection of the interactions among the tools and objects in the surgical scene based on the segmentation of video data. On the other hand, existing state-of-the-art video segmentation methods are often biased against infrequent classes and fail to provide temporal consistency for segmented masks. This can negatively impact the context inference and accurate detection of critical states. In this study, we propose a solution to these challenges using a Space-Time Correspondence Network (STCN). STCN is a memory network that performs binary segmentation and minimizes the effects of class imbalance. The use of a memory bank in STCN allows for the utilization of past image and segmentation information, thereby ensuring consistency of the masks. Our experiments using the publicly-available JIGSAWS dataset demonstrate that STCN achieves superior segmentation performance for objects that are difficult to segment, such as needle and thread, and improves context inference compared to the state-of-the-art. We also demonstrate that segmentation and context inference can be performed at runtime without compromising performance. keywords: {Context;Training;Image segmentation;Runtime;Instruction sets;Surgery;Needles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342013&isnumber=10341342

Z. Hao, S. Lim and M. K. Jawed, "Modeling, Characterization, and Control of Bacteria-Inspired Bi-Flagellated Mechanism with Tumbling," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6608-6615, doi: 10.1109/IROS55552.2023.10341992.Abstract: Multi-flagellated bacteria utilize the hydrodynamic interaction between their filamentary tails, known as flagella, to swim and change their swimming direction in low Reynolds number flow. Simplified hydrodynamics model, like Resistive Force Theories (RFT), lacks the capability to capture the dynamics of certain interactions known as bundling and tumbling. However, for the development of efficient and steerable robots inspired by bacteria, it becomes crucial to exploit this interaction. In this paper, we present the construction of a macroscopic bio-inspired robot featuring two rigid flagella arranged as right-handed helices, along with a cylindrical head. By rotating the flagella in opposite directions, the robot's body can reorient itself through repeatable and controllable tumbling. To accurately model this bi-flagellated mechanism in low Reynolds flow, we employ a coupling of rigid body dynamics and the method of Regularized Stokeslet Segments (RSS). Unlike RFT, RSS takes into account the hydrodynamic interaction between distant filamentary structures. Furthermore, we delve into the exploration of the parameter space in terms of the flagellum geometry to optimize the propulsion and torque of the system. To achieve the desired reorientation of the robot, we propose a tumble control scheme that involves modulating the rotation direction and speed of the two flagella. The scheme enhance the steerability by enabling the robot to attain the desired heading angle with high accuracy. Notably, the overall scheme boasts a simplified design and control as it only requires two control inputs. With our macroscopic framework serving as a foundation, we envision the eventual miniaturization of this technology to construct mobile and controllable micro-scale bacterial robots. keywords: {Geometry;Microorganisms;Torque;Biological system modeling;Tail;Propulsion;Hydrodynamics;bio-inspired robot;tumbling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341992&isnumber=10341342

X. Chen et al., "Evolving Physical Instinct for Morphology and Control Co-Adaption," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6616-6623, doi: 10.1109/IROS55552.2023.10342243.Abstract: The capability of a robot to perform tasks depends not only on precise motion control, but also on a well-suited body morphology. Adapting both morphology and control of robots to improve their task performance has been a widely studied and long-standing issue. While the bio-inspired bi-level optimization framework has gained popularity in recent years, it suffers from high computation complexity due to the time-consuming and inefficient learning process for each morphology. In fact, in nature, besides the adaptive morphology and the intelligent brain, animals also possess an important gift, which is physical instinct. These instincts allow animals to respond quickly to their surroundings in the neonatal period, facilitating skills acquisition. Inspired by this, we propose an evolvable instinct controller to enhance the morphology-control co-adaption. The instinct controller suggests rough motion inclinations, which require minimal domain knowledge and entail less sophisticated design. Its purpose is to assist the main controller in learning fine-grained and robust control efficiently. We implemented this idea in the context of legged locomotion and designed the instinct controller using phase-based FSMs. We propose the instinct-based co-adaption algorithm and construct GPU parallel simulation experiments on different morphology prototypes. The results indicate that combining the co-adaption process with instinct evolution leads to the development of superior morphologies and robust controllers compared with the conventional co-adaption approach, with minimal additional time cost. keywords: {Robust control;Legged locomotion;Pediatrics;Animals;Morphology;Prototypes;Process control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342243&isnumber=10341342

K. Miyama, K. Kawaharazuka, K. Okada and M. Inaba, "Development of a Five-Fingerd Biomimetic Soft Robotic Hand by 3D Printing the Skin and Skeleton as One Unit," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6624-6630, doi: 10.1109/IROS55552.2023.10341570.Abstract: Robot hands that imitate the shape of the human body have been actively studied, and various materials and mechanisms have been proposed to imitate the human body. Although the use of soft materials is advantageous in that it can imitate the characteristics of the human body's epidermis, it increases the number of parts and makes assembly difficult in order to perform complex movements. In this study, we propose a skin-skeleton integrated robot hand that has 15 degrees of freedom and consists of four parts. The developed robotic hand is mostly composed of a single flexible part produced by a 3D printer, and while it can be easily assembled, it can perform adduction, flexion, and opposition of the thumb, as well as flexion of four fingers. keywords: {Three-dimensional displays;Shape;Thumb;Grasping;Muscles;Soft robotics;Three-dimensional printing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341570&isnumber=10341342

S. Yoshimura et al., "Design Method of a Kangaroo Robot with High Power Legs and an Articulated Soft Tail," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6631-6638, doi: 10.1109/IROS55552.2023.10341756.Abstract: In this paper, we focus on the kangaroo, which has powerful legs capable of jumping and a soft and strong tail. To incorporate these unique structure into a robot for utilization, we propose a design method that takes into account both the feasibility as a robot and the kangaroo-mimetic structure. Based on the kangaroo's musculoskeletal structure, we determine the structure of the robot that enables it to jump by analyzing the muscle arrangement and prior verification in simulation. Also, to realize a tail capable of body support, we use an articulated, elastic structure as a tail. In order to achieve both softness and high power output, the robot is driven by a direct-drive, high-power wire-winding mechanism, and weight of legs and the tail is reduced by placing motors in the torso. The developed kangaroo robot can jump with its hind legs, moving its tail, and supporting its body using its hind legs and tail. keywords: {Legged locomotion;Torso;Design methodology;Wires;Tail;Muscles;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341756&isnumber=10341342

A. Dhole et al., "Hovering Control of Flapping Wings in Tandem with Multi-Rotors," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6639-6644, doi: 10.1109/IROS55552.2023.10341523.Abstract: This work briefly covers our efforts to stabilize the flight dynamics of Northeatern's tailless bat-inspired micro aerial vehicle, Aerobat. Flapping robots are not new. A plethora of examples is mainly dominated by insect-style design paradigms that are passively stable. However, Aerobat, in addition for being tailless, possesses morphing wings that add to the inherent complexity of flight control. The robot can dynamically adjust its wing platform configurations during gaitcycles, increasing its efficiency and agility. We employ a guard design with manifold small thrusters to stabilize Aerobat's position and orientation in hovering, a flapping system in tandem with a multi-rotor. For flight control purposes, we take an approach based on assuming the guard cannot observe Aeroat's states. Then, we propose an observer to estimate the unknown states of the guard which are then used for closed-loop hovering control of the Guard-Aerobat platform. keywords: {Manifolds;Attitude control;Observers;Aerodynamics;Complexity theory;Vehicle dynamics;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341523&isnumber=10341342

H. Deng, C. Nitroy, K. Panta, D. Li, S. Priya and B. Cheng, "Development of an Autonomous Modular Swimming Robot with Disturbance Rejection and Path Tracking," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6645-6651, doi: 10.1109/IROS55552.2023.10341571.Abstract: Here we present the development of an autonomous modular swimming robot. This robot, named µBot 2.0, was upgraded from our previous robot platform µBot and features onboard computing, sensing, and power. Its compact size and modularity render the robot an ideal platform for studying bio-inspired robot swimming. The robot is equipped with a micro controller in its head that communicates with external computers through Bluetooth Low Energy (BLE) and sends motor commands to the body segments via Inter-Integrated Circuit (I2C) protocol. Each body segment has a customized printed circuit board (PCB) that receives commands and controls the electromagnetic actuator for generating body movements. The robot head is also equipped with an Inertial Measurement Unit (IMU) to measure its heading and a battery for power. In this work, a µBot 2.0 with three actuators was assembled and the swimming performance was tested. The robot actuators were activated via rhythmic motor input from a central pattern generator (CPG). Experimental results showed that the swimming speed was highly sensitive to the frequency of the motor input, with a maximum swimming speed of 130 mm/s (equivalent to 0.7 body length per second) at 6 Hz. The robot also had the capability to correct its heading with IMU feedback and follow desired paths using a line-of-sight (LOS) guidance law with an overhead camera. Our results demonstrate the effectiveness of the robot's design and its potential in a variety of aquatic applications. keywords: {Actuators;Measurement units;Aquatic robots;Protocols;Robot vision systems;Printed circuits;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341571&isnumber=10341342

J. -G. Kang, D. Lee and S. Han, "A Highly Maneuverable Flying Squirrel Drone with Controllable Foldable Wings," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6652-6659, doi: 10.1109/IROS55552.2023.10341386.Abstract: Typical drones with multi rotors are generally less maneuverable due to unidirectional thrust, which may be unfavorable to agile flight in very narrow and confined spaces. This paper suggests a new bio-inspired drone that is empowered with high maneuverability in a lightweight and easy-to-carry way. The proposed flying squirrel inspired drone has controllable foldable wings to cover a wider range of flight attitudes and provide more maneuverable flight capability with stable tracking performance. The wings of a drone are fabricated with silicone membranes and sophisticatedly controlled by reinforcement learning based on human-demonstrated data. Specially, such learning based wing control serves to capture even the complex aerodynamics that are often impossible to model mathematically. It is shown through experiment that the proposed flying squirrel drone intentionally induces aerodynamic drag and hence provides the desired additional repulsive force even under saturated mechanical thrust. This work is very meaningful in demonstrating the potential of biomimicry and machine learning for realizing an animal-like agile drone. keywords: {Drag;Biomimetics;Force;Rotors;Reinforcement learning;Aerodynamics;Mathematical models;Flying squirrel;quadrotor;drone;biomimetics;reinforcement learning;learning from demonstration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341386&isnumber=10341342

Y. Li et al., "Programable On-Chip Fabrication of Magnetic Soft Micro-Robot," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6660-6667, doi: 10.1109/IROS55552.2023.10342184.Abstract: In the last decade, researchers have been trying to develop many microrobots that mimic the extraordinary abilities of bionts in complex environments. How to fabricate the biomimetic microrobot with satisfying deformability and complex shapes to realize desired precise motion is the key issue. In this paper, we proposed an efficient programable fabrication method of the magnetic soft micro-robot through an on-chip photopolymerization system. The superparamagnetic nanoparticles were compiled according to the magnetic anisotropy and assembled in the micro-robot. Then these nanoparticles were immobilized by photopolymerization of the hydrogel polymer. With this fabrication method, a joint rotation mechanism was first fabricated to characterize the deformation performance under the magnetic field control. Besides, the snake-like micro-robot were also fabricated, and the desired motions were achieved. The experimental results show that the proposed programable on-chip fabrication of magnetic soft micro-robot has the potential to facilitate the development of magnetic microrobots and their applications in the biomedical field. keywords: {Fabrication;Nanoparticles;Three-dimensional displays;Deformation;Hydrogels;System-on-chip;Soft magnetic materials},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342184&isnumber=10341342

Y. Yoshimitsu, T. Osa and S. Ikemoto, "Forward/Inverse Kinematics Modeling for Tensegrity Manipulator Based on Goal-Conditioned Variational Autoencoder," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6668-6673, doi: 10.1109/IROS55552.2023.10341525.Abstract: This paper uses a data-driven approach to model a highly redundantly driven tensegrity manipulator's forward and inverse kinematics. The tensegrity manipulator is based on a class-1 tensegrity with 20 struts and bends by 40 pneumatic actuators whose internal pressures are independently controlled. Based on the data obtained through random trials with the robot, a VAE-based kinematics model is trained. The forward model, inverse model, and null space of kinematics are simultaneously acquired as subnetworks of the VAE-based kinematics model. Experiments confirmed that the subnetworks representing forward and inverse kinematics could be used for the end position estimation and control, respectively. In addition, the subnetwork representing null space can generate different target pressures that achieve the same end position, which was confirmed to mean variable stiffness properties similar to musculoskeletal robots. keywords: {Training;Pneumatic actuators;Musculoskeletal system;Loading;Null space;Estimation;Kinematics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341525&isnumber=10341342

H. Kim, H. Lee and J. Yoon, "Nematode-Inspired Cable Routing Method for Cable Driven Redundant Manipulator*," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6674-6680, doi: 10.1109/IROS55552.2023.10341593.Abstract: Cable driven redundant manipulator (CDRM) can provide complex movements with high dexterity and singularity reduction. However, traditional CDRMs with universal joints have the disadvantages of requiring a high number of motors and having a narrow joint workspace. Furthermore, there is a limitation in terms of stiffness and payload. Recently, CDRMs composed of Quaternion joints have been developed to address these disadvantages. They require fewer motors and have larger joint workspace due to the Quaternion joints. Yet, their cable routing method is the same as the traditional CDRMs. In this paper, we propose a novel nematode-inspired cable routing method to achieve complex movements and stiffness increase. To achieve the stiffness increase of CDRM, the proposed cable routing method was inspired by the alternately arranged muscle structure of nematodes. Moreover, moving pulley structure was selected to amplify the stiffness and force of CDRM. An 8-DOF CDRM prototype composed of four Quaternion joints was developed to show the effectiveness of the cable routing method. Kinematics simulation was conducted and then, verified by trajectory through experiments. Finally, a joint stiffness simulation was conducted and verified with the developed prototype by stiffness experiments. keywords: {Quaternions;Simulation;Pulleys;Prototypes;Muscles;Routing;Mathematical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341593&isnumber=10341342

B. Yang, M. Yuan, C. Zhang, C. Hong, G. Pan and H. Tang, "Spiking Reinforcement Learning with Memory Ability for Mapless Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341738.Abstract: Our study focuses on mapless navigation in robotics, which involves navigating without an established obstacle map of the environment. Spiking Neural Networks (SNNs) have recently been applied to this task using Deep Reinforcement Learning (DRL), but face challenges in dynamic and partially observable environments, as well as inaccuracies in transmitted data. To overcome these issues, we propose a Multi-Critic DDPG with Spiking Memory (MC-DDPGSM) framework. Our approach introduces a spiking Gate Recurrent Unit layer (Spiking-GRU) to provide memory function and evaluates the state-action value with multi-critic networks. The experimental results demonstrate that our method achieves better performance (success rate, navigation distance, navigation time spent, and power consumption) in complex navigation tasks compared to the state-of-the-art approaches. Furthermore, our model can be transferred to unseen environments without the need for fine-tuning. keywords: {Deep learning;Power demand;Navigation;Neural networks;Reinforcement learning;Logic gates;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341738&isnumber=10341342

J. Li, S. Wang, Z. Chen, Z. Kan and J. Yu, "Lightweight Neural Path Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6713-6718, doi: 10.1109/IROS55552.2023.10342133.Abstract: Learning-based path planning is becoming a promising robot navigation methodology due to its adaptability to various environments. However, the expensive computing and storage associated with networks impose significant challenges for their deployment on low-cost robots. Motivated by this practical challenge, we develop a lightweight neural path planning architecture with a dual input network and a hybrid sampler for resource-constrained robotic systems. Our architecture is designed with efficient task feature extraction and fusion modules to translate the given planning instance into a guidance map. The hybrid sampler is then applied to restrict the planning within the prospective regions indicated by the guide map. To enable the network training, we further construct a publicly available dataset with various successful planning instances. Numerical simulations and physical experiments demonstrate that, compared with baseline approaches, our approach has nearly an order of magnitude fewer model size and five times lower computational while achieving promising performance. Besides, our approach can also accelerate the planning convergence process with fewer planning iterations compared to sample-based methods. keywords: {Training;Navigation;Computer architecture;Numerical simulation;Path planning;Planning;Numerical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342133&isnumber=10341342

Y. Wu, X. Su, D. Salihu, H. Xing, M. Zakour and C. Patsch, "Modeling Action Spatiotemporal Relationships Using Graph-Based Class-Level Attention Network for Long-Term Action Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6719-6726, doi: 10.1109/IROS55552.2023.10341409.Abstract: In recent years, Action Detection has become an active research topic in various fields such as human-robot interaction and assistive robots. Most of the previous methods in this field focus on temporally processing the action representation, without considering the dependencies among the action classes. However, actions that occur in a video are constantly related, and this correlation could offer effective clues for detection tasks. In this work, we propose to exploit the information of related action classes with the help of a graph neural network in conjunction with temporal modeling. We introduce the attention-based temporal class module (ATC), which models the inherent action dependencies on the graph and learns action-specific features among temporal dimensions with a dual-branch attention mechanism. Further, we present the Graph-based Class-level Attention Network (GCAN), which is built upon ATC modules with increasing temporal receptive fields to handle actions instances in complex untrimmed videos. Our network is evaluated on two challenging benchmark datasets with dense annotations: Charades and MultiTHUMOS. Experimental results show that our approach demonstrates highly competitive results with a significantly reduced model complexity. keywords: {Correlation;Human-robot interaction;Benchmark testing;Assistive robots;Multitasking;Spatiotemporal phenomena;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341409&isnumber=10341342

