T. Hiraoka et al., "Whole-Body Torque Control Without Joint Position Control Using Vibration-Suppressed Friction Compensation for Bipedal Locomotion of Gear-Driven Torque Sensorless Humanoid," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8544-8550, doi: 10.1109/IROS55552.2023.10341698.Abstract: Humanoids operate in repeated contact and non-contact with their environment and so the motion of humanoids such as walking on uneven terrain or in a narrow space requires the accurate force and position control. Joint torque control systems are suitable for position and force control, but are prone to friction and other modeling errors. To solve this problem, methods have been proposed to realize torque control in combination with joint position control systems or by improving joint structures such as sensors and actuators, but these methods have problems such as response delay and increased weight and volume. Thus, it is difficult to achieve motion of life-sized humanoids by whole-body torque control. In this paper, we solve challenges not with one specific layer, but rather with multiple layers that complement each other. We propose a hierarchical whole-body torque control method using four layers: friction compensation based on a vibration-suppressed model, whole-body resolved acceleration control using priority, center-of-gravity acceleration control based on foot-guided control, and landing position time modification based on capture point. We verify through walking experiments that the proposed methods can control the life-sized humanoid robot driven by high-reduction ratio joints by whole-body torque control without a torque sensor or joint position control, and that it enables the robot to move and even transport an object on outdoor uneven terrain. keywords: {Legged locomotion;Torque;Friction;Torque control;Humanoid robots;Position control;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341698&isnumber=10341342

N. Rosa, B. Katamish, M. Raff and C. D. Remy, "An Approach for Generating Families of Energetically Optimal Gaits from Passive Dynamic Walking Gaits," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8551-8557, doi: 10.1109/IROS55552.2023.10342322.Abstract: For a class of biped robots with impulsive dynamics and a non-empty set of passive gaits (unactuated, periodic motions of the biped model), we present a method for computing continuous families of locally optimal gaits with respect to a class of commonly used energetic cost functions (e.g., the integral of torque-squared). We compute these families using only the passive gaits of the biped, which are globally optimal gaits with respect to these cost functions. Our approach fills in an important gap in the literature when computing a library of locally optimal gaits, which often do not make use of these globally optimal solutions as seed values. We demonstrate our approach on a well-studied two-link biped model. keywords: {Legged locomotion;Computational modeling;Dynamics;Cost function;Libraries;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342322&isnumber=10341342

O. Dosunmu-Ogunbi, A. Shrivastava, G. Gibson and J. W. Grizzle, "Stair Climbing Using the Angular Momentum Linear Inverted Pendulum Model and Model Predictive Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8558-8565, doi: 10.1109/IROS55552.2023.10342369.Abstract: A new control paradigm using angular momentum and foot placement as state variables in the linear inverted pendulum model has expanded the realm of possibilities for the control of bipedal robots. This new paradigm, known as the ALIP model, has shown effectiveness in cases where a robot's center of mass height can be assumed to be constant or near constant as well as in cases where there are no non-kinematic restrictions on foot placement. Walking up and down stairs violates both of these assumptions, where center of mass height varies significantly within a step and the geometry of the stairs restrict the effectiveness of foot placement. In this paper, we explore a variation of the ALIP model that allows the length of the virtual pendulum formed by the robot's stance foot and center of mass to follow smooth trajectories during a step. We couple this model with a control strategy constructed from a novel combination of virtual constraint-based control and a model predictive control algorithm to stabilize a stair climbing gait that does not soley rely on foot placement. Simulations on a 20-degree of freedom model of the Cassie biped in the SimMechanics simulation environment show that the controller is able to achieve periodic gait. keywords: {Legged locomotion;Geometry;Stairs;Predictive models;Prediction algorithms;Trajectory;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342369&isnumber=10341342

T. Wang, J. White and C. Hubicki, "Real-time Dynamic Bipedal Avoidance," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8566-8573, doi: 10.1109/IROS55552.2023.10341951.Abstract: In real-world settings, bipedal robots must avoid collisions with people and their environment. Further, a biped can choose between modes of avoidance: (1) adjust its pose while standing or (2) step to gain maneuverability. We present a real-time motion planner and multibody control framework for dynamic bipedal robots that avoids multiple moving obstacles and automatically switches between standing and stepping modes as necessary. By leveraging a reduced-order model (i.e. Linear Inverted Pendulum Model) and a half-space relaxation of the safe region, the planner is formulated as a convex optimization problem (i.e. Quadratic Programming) that can be used for real-time application with Model-Predictive-Control (MPC). To facilitate mode switching, we introduce center-of-pressure related slack-variables to the convex planning optimization that both shapes the planning cost function and provides a mode switching criterion for dynamic locomotion. Finally, we implement the proposed algorithm on a 3D Cassie bipedal robot and present hardware experiments showing real-time bipedal standing avoidance, stepping avoidance, and automatic switching of avoidance modes. keywords: {Three-dimensional displays;Shape;Heuristic algorithms;Dynamics;Switches;Real-time systems;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341951&isnumber=10341342

M. Dai, X. Xiong, J. Lee and A. D. Ames, "Data-Driven Adaptation for Robust Bipedal Locomotion with Step-to-Step Dynamics," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8574-8581, doi: 10.1109/IROS55552.2023.10341396.Abstract: This paper presents an online framework for synthesizing agile locomotion for bipedal robots that adapts to unknown environments, modeling errors, and external disturbances. To this end, we leverage step-to-step (S2S) dynamics which has proven effective in realizing dynamic walking on underactuated robots-assuming known dynamics and environments. This paper considers the case of uncertain models and environments and presents a data-driven representation of the S2S dynamics that can be learned via an adaptive control approach that is both data-efficient and easy to implement. The learned S2S controller generates desired discrete foot placement, which is then realized on the full-order dynamics of the bipedal robot by tracking desired outputs synthesized from the given foot placement. The benefits of the proposed approach are twofold. First, it improves the ability of the robot to walk at a given desired velocity when compared to the non-adaptive baseline controller. Second, the data-driven approach enables stable and agile locomotion under the effect of various unknown disturbances: additional unmodeled payload, large robot model errors, external disturbance forces, biased velocity estimation, and sloped terrains. This is demonstrated through in-depth evaluation with a high-fidelity simulation of the bipedal robot Cassie subject to the aforementioned disturbances [1]. keywords: {Legged locomotion;Adaptation models;Dynamics;Estimation;Robustness;Behavioral sciences;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341396&isnumber=10341342

G. A. Castillo, B. Weng, S. Yang, W. Zhang and A. Hereid, "Template Model Inspired Task Space Learning for Robust Bipedal Locomotion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8582-8589, doi: 10.1109/IROS55552.2023.10341263.Abstract: This work presents a hierarchical framework for bipedal locomotion that combines a Reinforcement Learning (RL)-based high-level (HL) planner policy for the online generation of task space commands with a model-based low-level (LL) controller to track the desired task space trajectories. Different from traditional end-to-end learning approaches, our HL policy takes insights from the angular momentum-based linear inverted pendulum (ALIP) to carefully design the observation and action spaces of the Markov Decision Process (MDP). This simple yet effective design creates an insightful mapping between a low-dimensional state that effectively captures the complex dynamics of bipedal locomotion and a set of task space outputs that shape the walking gait of the robot. The HL policy is agnostic to the task space LL controller, which increases the flexibility of the design and generalization of the framework to other bipedal robots. This hierarchical design results in a learning-based framework with improved performance, data efficiency, and robustness compared with the ALIP model-based approach and state-of-the-art learning-based frameworks for bipedal locomotion. The proposed hierarchical controller is tested in three different robots, Rabbit, a five-link underactuated planar biped; Walker2D, a seven-link fully-actuated planar biped; and Digit, a 3D humanoid robot with 20 actuated joints. The trained policy naturally learns human-like locomotion behaviors and is able to effectively track a wide range of walking speeds while preserving the robustness and stability of the walking gait even under adversarial conditions. keywords: {Legged locomotion;Torso;Three-dimensional displays;Aerospace electronics;Stairs;Robustness;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341263&isnumber=10341342

K. S. Narkhede, D. A. Thanki, A. M. Kulkarni and I. Poulakakis, "Overtaking Moving Obstacles with Digit: Path Following for Bipedal Robots via Model Predictive Contouring Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8590-8596, doi: 10.1109/IROS55552.2023.10342209.Abstract: Humanoid robots are expected to navigate in changing environments and perform a variety of tasks. Frequently, these tasks require the robot to make decisions online regarding the speed and precision of following a reference path. For example, a robot may want to decide to temporarily deviate from its path to overtake a slowly moving obstacle that shares the same path and is ahead. In this case, path following performance is compromised in favor of fast path traversal. Available global trajectory tracking approaches typically assume a given-specified in advance-time parametrization of the path and seek to minimize the norm of the Cartesian error. As a result, when the robot should be where on the path is fixed and temporary deviations from the path are strongly discouraged. Given a global path, this paper presents a Model Predictive Contouring Control (MPCC) approach to selecting footsteps that maximize path traversal while simultaneously allowing the robot to decide between faithful versus fast path following. The method is evaluated in high-fidelity simulations of the bipedal robot Digit in terms of tracking performance of curved paths under disturbances and is also applied to the case where Digit overtakes a moving obstacle. keywords: {Legged locomotion;Trajectory tracking;Navigation;Humanoid robots;Predictive models;Trajectory;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342209&isnumber=10341342

H. C. Siu, K. Leahy and M. Mann, "STL: Surprisingly Tricky Logic (for System Validation)," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8613-8620, doi: 10.1109/IROS55552.2023.10342290.Abstract: Much of the recent work developing formal methods techniques to specify or learn the behavior of autonomous systems is predicated on a belief that formal specifications are interpretable and useful for humans when checking systems. Though frequently asserted, this assumption is rarely tested. We performed a human experiment $(\mathbf{N}=62)$ with a mix of people who were and were not familiar with formal methods beforehand, asking them to validate whether a set of signal temporal logic (STL) constraints would keep an agent out of harm and allow it to complete a task in a gridworld capture-the-ftag setting. Validation accuracy was 45% $\pm$ 20% (mean $\pm$ standard deviation). The ground-truth validity of a specification, subjects' familiarity with formal methods, and subjects' level of education were found to be significant factors in determining validation correctness. Participants exhibited an affirmation bias, causing significantly increased accuracy on valid specifications, but significantly decreased accuracy on invalid specifications. Additionally, participants, particularly those familiar with formal methods, tended to be overconfident in their answers, and be similarly confident regardless of actual correctness. Our data do not support the belief that formal specifications are inherently human-interpretable to a meaningful degree for system validation. We recommend ergonomic improvements to data presentation and validation training, which should be tested before claims of interpretability make their way back into the formal methods literature. keywords: {Training;Semantics;Behavioral sciences;Safety;Formal specifications;System validation;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342290&isnumber=10341342

A. Linard, I. Torre, E. Bartoli, A. Sleat, I. Leite and J. Tumova, "Real-Time RRT* with Signal Temporal Logic Preferences," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8621-8627, doi: 10.1109/IROS55552.2023.10341993.Abstract: Signal Temporal Logic (STL) is a rigorous specification language that allows one to express various spatio-temporal requirements and preferences. Its semantics (called robustness) allows quantifying to what extent are the STL specifications met. In this work, we focus on enabling STL constraints and preferences in the Real-Time Rapidly Exploring Random Tree (RT-RRT*) motion planning algorithm in an environment with dynamic obstacles. We propose a cost function that guides the algorithm towards the asymptotically most robust solution, i.e. a plan that maximally adheres to the STL specification. In experiments, we applied our method to a social navigation case, where the STL specification captures spatio-temporal preferences on how a mobile robot should avoid an incoming human in a shared space. Our results show that our approach leads to plans adhering to the STL specification, while ensuring efficient cost computation. keywords: {Navigation;Heuristic algorithms;Semantics;Dynamics;Real-time systems;Robustness;Planning;Signal Temporal Logic;Real-Time Planning;Sampling-based Motion Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341993&isnumber=10341342

R. Phatak and D. A. Shell, "Sensor Selection for Fine-Grained Behavior Verification that Respects Privacy," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8628-8635, doi: 10.1109/IROS55552.2023.10341877.Abstract: A useful capability is that of classifying some agent's behavior using data from a sequence, or trace, of sensor measurements. The sensor selection problem involves choosing a subset of available sensors to ensure that, when generated, observation traces will contain enough information to determine whether the agent's activities match some pattern. In generalizing prior work, this paper studies a formulation in which multiple behavioral itineraries may be supplied, with sensors selected to distinguish between behaviors. This allows one to pose fine-grained questions, e.g., to position the agent's activity on a spectrum. In addition, with multiple itineraries, one can also ask about choices of sensors where some behavior is always plausibly concealed by (or mistaken for) another. Using sensor ambiguity to limit the acquisition of knowledge is a strong privacy guarantee, a form of guarantee which some earlier work examined under formulations distinct from our inter-itinerary conflation approach. By concretely formulating privacy requirements for sensor selection, this paper connects both lines of work in a novel fashion: privacy-where there is a bound from above, and behavior verification-where sensors choices are bounded from below. We examine the worst-case computational complexity that results from both types of bounds, proving that upper bounds are more challenging under standard computational complexity assumptions. The problem is intractable in general, but we introduce an approach to solving this problem that can exploit interrelationships between constraints, and identify opportunities for optimizations. Case studies are presented to demonstrate the usefulness and scalability of our proposed solution, and to assess the impact of the optimizations. keywords: {Privacy;Upper bound;Scalability;Robot sensing systems;Behavioral sciences;Computational complexity;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341877&isnumber=10341342

Y. Chen et al., "An Interactive System for Multiple-Task Linear Temporal Logic Path Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8636-8642, doi: 10.1109/IROS55552.2023.10342309.Abstract: Beyond programming robots to accomplish a single high-level task at a time, people also hope robots follow instructions and complete a series of tasks while meeting their requirements. This paper presents an interactive software system that consists of a multiple-task linear temporal logic (LTL) path planner and a human-machine interface (HMI). The HMI transforms human oral instructions into task commands that can be understood by the machine. The planner grows a rapid random exploring tree to search for solutions for multiple tasks. When switching tasks, the search tree is re-initialized and reconnected to utilize the information gathered during the exploration of the workspace. The feasibility of the improved planner is theoretically guaranteed, and profiling in simulation shows an acceleration in planning. An experiment with a quadcopter is conducted to show that the combination of the multiple-task LTL planner and the HMI results in a synergistic effect in real-world applications. keywords: {Interactive systems;Transforms;Switches;Programming;Software systems;Probabilistic logic;Path planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342309&isnumber=10341342

H. Yoon and S. Sankaranarayanan, "Temporal Logic-Based Intent Monitoring for Mobile Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8643-8650, doi: 10.1109/IROS55552.2023.10341623.Abstract: We propose a framework that uses temporal logic specifications to predict and monitor the intent of a robotic agent through passive observations of its actions over time. Our approach uses a set of possible hypothesized intents specified as Büchi automata, obtained from translating temporal logic formulae. Based on observing the actions of the robot, we update the probabilities of each hypothesis using Bayes rule. Observations of robot actions provide strong evidence for its “immediate” short-term goals, whereas temporal logic specifications describe behaviors over a “never-ending” infinite time horizon. To bridge this gap, we use a two-level hierarchical monitoring approach. At the lower level, we track the immediate short-term goals of the robot which are modeled as atomic propositions in the temporal logic formalism. We apply our approach to predicting intent of human workers and thus their movements in an indoor space based on the publicly available THOR dataset. We show how our approach correctly labels each agent with their appropriate intents after relatively few observations while predicting their future actions accurately over longer time horizons. keywords: {Bridges;Tracking;Automata;Behavioral sciences;Mobile robots;Robots;Monitoring},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341623&isnumber=10341342

A. Badithela, T. Wongpiromsarn and R. M. Murray, "Evaluation Metrics of Object Detection for Quantitative System-Level Analysis of Safety-Critical Autonomous Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8651-8658, doi: 10.1109/IROS55552.2023.10342465.Abstract: This paper proposes two metrics for evaluating learned object detection models: the proposition-labeled and distance-parametrized confusion matrices. These metrics are leveraged to quantitatively analyze the system with respect to its system-level formal specifications via probabilistic model checking. In particular, we derive transition probabilities from these confusion matrices to compute the probability that the closed-loop system satisfies its system-level specifications expressed in temporal logic. Instead of using object class labels, the proposition-labeled confusion matrix uses atomic propositions relevant to the high-level control strategy. Furthermore, unlike the traditional confusion matrix, the proposed distance-parametrized confusion matrix accounts for variations in detection performance with respect to the distance between the ego and the object. Empirically, these evaluation metrics, chosen by considering system-level specifications and control module design, result in less conservative system-level evaluations than those from traditional confusion matrices. We demonstrate this framework on a car-pedestrian example by computing the satisfaction probabilities for safety requirements formalized in Linear Temporal Logic. keywords: {Measurement;Autonomous systems;Computational modeling;Object detection;Model checking;Probabilistic logic;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342465&isnumber=10341342

A. T. Buyukkocak, D. Aksaray and Y. Yazıcıoğlu, "Energy-Aware Planning of Heterogeneous Multi-Agent Systems for Serving Cooperative Tasks with Temporal Logic Specifications," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8659-8665, doi: 10.1109/IROS55552.2023.10342064.Abstract: We address a coordination problem for a team of heterogeneous and energy-limited agents to achieve cooperative tasks given as team-level spatio-temporal specifications. We assume that agents have stochastic energy dynamics and do not have identical capabilities. We define the team-level specification using Signal Temporal Logic (STL) with integral predicates, which can express tasks that can be completed collectively in an asynchronous way. We first abstract the environment as a graph using sampling-based methods. This abstraction includes the possible paths of different types of agents and ensures the availability of recharging within a certain distance. Then, we formulate a mixed-integer program over this abstraction to find the high-level paths of the agents. Finally, we steer the agents in the environment according to the nominal plan under stochastic energy consumption models and a recharging policy. Such stochastic energy dynamics cause deviations from the nominal plan and delays in completing the tasks. Accordingly, we define and evaluate the expected delay (temporal relaxation) in achieving the STL specification under the proposed solution. keywords: {Energy consumption;Green products;Stochastic processes;Charging stations;Planning;Delays;State of charge},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342064&isnumber=10341342

K. Muvvala and M. Lahijanian, "Efficient Symbolic Approaches for Quantitative Reactive Synthesis with Finite Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8666-8672, doi: 10.1109/IROS55552.2023.10342496.Abstract: This work introduces efficient symbolic algorithms for quantitative reactive synthesis. We consider resource-constrained robotic manipulators that need to interact with a human to achieve a complex task expressed in linear temporal logic. Our framework generates reactive strategies that not only guarantee task completion but also seek cooperation with the human when possible. We model the interaction as a two-player game and consider regret-minimizing strategies to encourage cooperation. We use symbolic representation of the game to enable scalability. For synthesis, we first introduce value iteration algorithms for such games with min-max objectives. Then, we extend our method to the regret-minimizing objectives. Our benchmarks reveal that our the symbolic framework not only significantly improves computation time (up to an order of magnitude) but also can scale up to much larger instances of manipulation problems with up to 2× number of objects and locations than the state of the art. keywords: {Scalability;Games;Benchmark testing;Manipulators;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342496&isnumber=10341342

A. Upadhyay, M. Ghosh and C. Ekenna, "Minimal Path Violation Problem with Application to Fault Tolerant Motion Planning of Manipulators," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8673-8679, doi: 10.1109/IROS55552.2023.10342242.Abstract: Failure of any component in a robotic system during operation is a critical concern, and it is essential to address such incidents promptly. This work investigates a novel technique to recover from failures or changes in the configuration space while avoiding expensive re-computation or re-planning. We propose the Minimal Path Violation (MPV) concept to find the best feasible path with minimal re-configurations. The algorithm ranks pathways based on visibility, expansiveness, and cost. We perform experiments with articulated 3 DOF to 28 DOF robots ranging from serial linkage robots, Kuka YouBots, and PR2 robots. Our results show that our method outperforms existing optimal planners in computation time, total nodes, and path cost while preserving path feasibility in changed configuration space. keywords: {Fault tolerance;Costs;Limiting;Heuristic algorithms;Fault tolerant systems;Dynamics;Distance measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342242&isnumber=10341342

X. Lin, A. Koochakzadeh, Y. Yazıcıoğlu and D. Aksaray, "Reinforcement Learning Under Probabilistic Spatio-Temporal Constraints with Time Windows," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8680-8686, doi: 10.1109/IROS55552.2023.10342259.Abstract: We propose an automata-theoretic approach for reinforcement learning (RL) under complex spatio-temporal constraints with time windows. The problem is formulated using a Markov decision process under a bounded temporal logic constraint. Different from existing RL methods that can eventually learn optimal policies satisfying such constraints, our proposed approach enforces a desired probability of constraint satisfaction throughout learning. This is achieved by translating the bounded temporal logic constraint into a total automaton and avoiding “unsafe” actions based on the available prior information regarding the transition probabilities, i.e., a pair of upper and lower bounds for each transition probability. We provide theoretical guarantees on the resulting probability of constraint satisfaction. We also provide numerical results in a scenario where a robot explores the environment to discover high-reward regions while fulfilling some periodic pick-up and delivery tasks that are encoded as temporal logic constraints. keywords: {Learning automata;Reinforcement learning;Markov processes;Probabilistic logic;Time factors;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342259&isnumber=10341342

C. Zhou et al., "A Unified Trajectory Generation Algorithm for Dynamic Dexterous Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8712-8719, doi: 10.1109/IROS55552.2023.10342095.Abstract: This paper proposes a novel efficient multi-phase trajectory generation algorithm for dynamic dexterous manipulation tasks, such as throwing, catching, dynamic regrasping, and dynamic handover, which can be decomposed into multiple manipulation primitives, including sticking, rolling, approaching, separating, colliding, and grasping. Each manipulation primitive is formulate as a free-terminal optimal control problem (OCP), aimed at computing the optimal pose (position and orientation) trajectories of the object and the robot subject to the pose and force linkage constraints between them and the expected force maintenance at contact. A single-arm regrasping task and a dual-arm dynamic handover task are conducted to demonstrate the effectiveness of the proposed algorithm. keywords: {Couplings;Heuristic algorithms;Dynamics;Force;Optimal control;Tactile sensors;Handover},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342095&isnumber=10341342

R. S. Zarrin, R. Jitosho and K. Yamane, "Hybrid Learning- and Model-Based Planning and Control of In-Hand Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8720-8726, doi: 10.1109/IROS55552.2023.10342153.Abstract: This paper presents a hierarchical framework for planning and control of in-hand manipulation of a rigid object involving grasp changes using fully-actuated multifin-gered robotic hands. While the framework can be applied to the general dexterous manipulation, we focus on a more complex definition of in-hand manipulation, where at the goal pose the hand has to reach a grasp suitable for using the object as a tool. The high level planner determines the object trajectory as well as the grasp changes, i.e. adding, removing, or sliding fingers, to be executed by the low-level controller. While the grasp sequence is planned online by a learning-based policy to adapt to variations, the trajectory planner and the low-level controller for object tracking and contact force control are exclusively model-based to robustly realize the plan. By infusing the knowledge about the physics of the problem and the low-level controller into the grasp planner, it learns to successfully generate grasps similar to those generated by model-based optimization approaches, obviating the high computation cost of online running of such methods to account for variations. By performing experiments in physics simulation for realistic tool use scenarios, we show the success of our method on different tool-use tasks and dexterous hand models. Additionally, we show that this hybrid method offers more robustness to trajectory and task variations compared to a model-based method. keywords: {Computational modeling;Robustness;Hardware;Trajectory;Planning;Object tracking;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342153&isnumber=10341342

Y. Isobe, S. Kang, T. Shimamoto, Y. Matsuyama, S. Pathak and K. Umeda, "Vision-Based In-Hand Manipulation of Variously Shaped Objects via Contact Point Prediction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8727-8734, doi: 10.1109/IROS55552.2023.10341968.Abstract: In-hand manipulation (IHM) is an important ability for robotic hands. This ability refers to changing the position and orientation of a grasped object without dropping it from the hand workspace. One major challenge of IHM is to achieve a large range of manipulation (especially rotation), regardless of the shape, size, and the orientation during manipulation of the grasped object. There are two main challenges - the manipulation range (due to the range of motion of the hand) and keeping the object grasped under all shapes and orientations. Specifically, even when the contact points between the hand and the object switch and the positions of these points change due to its shape and changing orientation, constant grasp of the object is required. This paper presents an IHM method for a robotic hand with belts, based on the prediction of the contact-point changes via image information. The focus is on a robotic hand that has a two-fingered parallel gripper with conveyor belts which can continuously manipulate an object through a large range. A stereo camera is attached to the hand. First, the contour of the grasped object is acquired from the camera. From the contour, the switching of the contact points between the surfaces of the belts and the object is predicted. Then, the positions of the contact points in the next frame are estimated by rotating the contour. The velocities of the belts are calculated based on the prediction of the switching. The fingers are controlled to follow the estimated positions of the contact points, via a feed-forward control. The effectiveness of the proposed method is verified through in-hand manipulation experiments for 22 objects of various shapes and sizes. keywords: {Shape;Contacts;Robot vision systems;Switches;Grasping;Belts;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341968&isnumber=10341342

O. Taylor, N. Doshi and A. Rodriguez, "Object Manipulation Through Contact Configuration Regulation: Multiple and Intermittent Contacts," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8735-8743, doi: 10.1109/IROS55552.2023.10341362.Abstract: In this work, we build on our method for manipulating unknown objects via contact configuration regulation: the estimation and control of the location, geometry, and mode of all contacts between the robot, object, and environment. We further develop our estimator and controller to enable manipulation through more complex contact interactions, including intermittent contact between the robot/object, and multiple contacts between the object/environment. In addition, we support a larger set of contact geometries at each interface. This is accomplished through a factor graph based estimation framework that reasons about the complementary kinematic and wrench constraints of contact to predict the current contact configuration. We are aided by the incorporation of a limited amount of visual feedback; which when combined with the available F/T sensing and robot proprioception, allows us to differentiate contact modes that were previously indistinguishable. We implement this revamped framework on our manipulation platform, and demonstrate that it allows the robot to perform a wider set of manipulation tasks. This includes, using a wall as a support to re-orient an object, or regulating the contact geometry between the object and the ground. Finally, we conduct ablation studies to understand the contributions from visual and tactile feedback in our manipulation framework. Our code can be found at: https://github.com/mcubelab/pbal. keywords: {Geometry;Visualization;Estimation;Tactile sensors;Kinematics;Regulation;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341362&isnumber=10341342

P. Chanrungmaneekul, K. Ren, J. T. Grace, A. M. Dollar and K. Hang, "Non-Parametric Self-Identification and Model Predictive Control of Dexterous In-Hand Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8743-8750, doi: 10.1109/IROS55552.2023.10341520.Abstract: Building hand-object models for dexterous in-hand manipulation remains a crucial and open problem. Major challenges include the difficulty of obtaining the geometric and dynamical models of the hand, object, and time-varying contacts, as well as the inevitable physical and perception uncertainties. Instead of building accurate models to map between the actuation inputs and the object motions, this work proposes to enable the hand-object systems to continuously approximate their local models via a self-identification process where an underlying manipulation model is estimated through a small number of exploratory actions and non-parametric learning. With a very small number of data points, as opposed to most data-driven methods, our system self-identifies the underlying manipulation models online through exploratory actions and non-parametric learning. By integrating the self-identified hand-object model into a model predictive control framework, the proposed system closes the control loop to provide high accuracy in-hand manipulation. Furthermore, the proposed self-identification is able to adaptively trigger online updates through additional exploratory actions, as soon as the self-identified local models render large discrepancies against the observed manipulation outcomes. We implemented the proposed approach on a sensorless underactuated Yale Model O hand with a single external camera to observe the object's motion. With extensive experiments, we show that the proposed self-identification approach can enable accurate and robust dexterous manipulation without requiring an accurate system model nor a large amount of data for offline training. keywords: {Training;Robust control;Adaptation models;Uncertainty;Buildings;Predictive models;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341520&isnumber=10341342

S. Patidar, A. Sieler and O. Brock, "In-Hand Cube Reconfiguration: Simplified," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8751-8756, doi: 10.1109/IROS55552.2023.10341521.Abstract: We present a simple approach to in-hand cube reconfiguration. By simplifying planning, control, and perception as much as possible, while maintaining robust and general performance, we gain insights into the inherent complexity of in-hand cube reconfiguration. We also demonstrate the effectiveness of combining GOFAI-based planning with the exploitation of environmental constraints and inherently compliant end-effectors in the context of dexterous manipulation. The proposed system outperforms a substantially more complex system for cube reconfiguration based on deep learning and accurate physical simulation, contributing arguments to the discussion about what the most promising approach to general manipulation might be. Project website: https://rbo.gitlab-pages.tu-berlin.de/robotics/simpleIHM/ keywords: {Deep learning;Dynamics;Robustness;End effectors;Planning;Complexity theory;Complex systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341521&isnumber=10341342

A. Sieler and O. Brock, "Dexterous Soft Hands Linearize Feedback-Control for In-Hand Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8757-8764, doi: 10.1109/IROS55552.2023.10341438.Abstract: This paper presents a feedback-control framework for in-hand manipulation (IHM) with dexterous soft hands that enables the acquisition of manipulation skills in the real-world within minutes. We choose the deformation state of the soft hand as the control variable. To control for a desired deformation state, we use coarsley approximated Jacobians of the actuation-deformation dynamics. These Jacobian are obtained via explorative actions. This is enabled by the self-stabilizing properties of compliant hands, which allow us to use linear feedback control in the presence of complex contact dynamics. To evaluate the effectiveness of our approach, we show the generalization capabilities for a learned manipulation skill to variations in object size by 100 %, 360 degree changes in palm inclination and to disabling up to 50 % of the involved actuators. In addition, complex manipulations can be obtained by sequencing such feedback-skills. keywords: {Jacobian matrices;Actuators;Sequential analysis;Deformation;Feedback control;Behavioral sciences;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341438&isnumber=10341342

C. Pan, M. Lepert, S. Yuan, R. Antonova and J. Bohg, "In-Hand Manipulation of Unknown Objects with Tactile Sensing for Insertion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8765-8771, doi: 10.1109/IROS55552.2023.10341456.Abstract: In this paper, we present a method to manipulate unknown objects in-hand using tactile sensing without relying on a known object model. In many cases, vision-only approaches may not be feasible; for example, due to occlusion in cluttered spaces. We address this limitation by introducing a method to reorient unknown objects using tactile sensing. It incrementally builds a probabilistic estimate of the object shape and pose during task-driven manipulation. Our approach uses Bayesian optimization to balance exploration of the global object shape with efficient task completion. To demonstrate the effectiveness of our method, we apply it to a simulated Tactile-Enabled Roller Grasper, a gripper that rolls objects in hand while collecting tactile data. We evaluate our method on an insertion task with randomly generated objects and find that it reliably reorients objects while significantly reducing the exploration time. keywords: {Three-dimensional displays;Shape;Robot sensing systems;Probabilistic logic;Sensors;Bayes methods;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341456&isnumber=10341342

H. Luo and Y. Demiris, "Bi-Manual Robot Shoe Lacing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8772-8775, doi: 10.1109/IROS55552.2023.10341934.Abstract: Shoe lacing (SL) is a challenging sensorimotor task in daily life and a complex engineering problem in the shoe-making industry. In this paper, we propose a system for autonomous SL. It contains a mathematical definition of the SL task and searches for the best lacing pattern corresponding to the shoe configuration and the user preferences. We propose a set of action primitives and generate plans of action sequences according to the designed pattern. Our system plans the trajectories based on the perceived position of the eyelets and aglets with an active perception strategy, and deploys the trajectories on a bi-manual robot. Experiments demonstrate that the proposed system can successfully lace 3 different shoes in different configurations, with a completion rate of 92.0%, 91.6% and 77.5% for 6, 8 and 10-eyelet patterns respectively. To the best of our knowledge, this is the first demonstration of autonomous SL using a bi-manual robot. keywords: {Industries;Active perception;Footwear;Robot sensing systems;Trajectory;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341934&isnumber=10341342

K. Nave, K. DuFrene, N. Swenson, R. Balasubramanian and C. Grimm, "Hand Design Approach for Planar Fully Actuated Manipulators," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8778-8783, doi: 10.1109/IROS55552.2023.10342077.Abstract: Robotic in-hand manipulation increases the capability of robotic hands to interact with the world. The amount of manipulation that a robot is capable of is highly dependent on the design of the robot hand, and previous works have shown success in designing hands to improve performance for different types of grasping and manipulation. In this paper we present a method for designing a fully-actuated planar manipulator that optimizes for specific in-hand motions. We demonstrate that, with the Asterisk Benchmark and a light-weight IK controller, we can translate our results from simulation to the real world with minimal effort and high-fidelity. Using the simulated data (over 4,000 simulated hand-designs) we begin to analyze which features contribute to improved planar manipulation. keywords: {Grasping;Benchmark testing;Manipulators;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342077&isnumber=10341342

Y. Xue, L. Tang and Y. -B. Jia, "Dynamic Finger Gaits via Pivoting and Adapting Contact Forces," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8784-8791, doi: 10.1109/IROS55552.2023.10342156.Abstract: For over three decades, finger gaiting has remained largely a subject for theoretical inquiries. Successful execution of a sequence of finger gaits does not simply reduce to planning collision-free paths for the involved fingers. A major issue is how to move the gaiting finger without losing the finger contacts with the object, which will most likely undergo a motion as the contact forces need to be adapted during the gait. This paper focuses on a single finger gait executed on a tool by an anthropomorphic hand driven by an arm. To improve stability, the tool's tip is leveraged as a pivot on the supporting plane. The gait consists of three stages: removal, during which the contact force on the gaiting finger gradually decreases to zero; relocation, during which the finger follows a pre-planned path (relative to the moving object) to establish a new contact; and addition, during which the contact force on the relocated finger increases to some desired level. Hybrid position/impedance control employs reference finger forces that satisfy the friction cone constraints and are dynamically consistent with the object's motion, which in turn provides reference poses for the fingertips to maintain their contacts during the gait. Finger gaits have been demonstrated on a kitchen knife and a screwdriver with an Adept SCARA robot and a Shadow Dexterous Hand. keywords: {Friction;Force;Dynamics;Planning;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342156&isnumber=10341342

S. Xu, T. Liu, M. Wong, D. Kulić and A. Cosgun, "Rotating Objects via in-Hand Pivoting Using Vision, Force and Touch," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8792-8799, doi: 10.1109/IROS55552.2023.10341505.Abstract: We propose a robotic manipulation method that can pivot objects on a surface using vision, wrist force and tactile sensing. We aim to control the rotation of an object around the grip point of a parallel gripper by allowing rotational slip, while maintaining a desired wrist force profile. Our approach runs an end-effector position controller and a gripper width controller concurrently in a closed loop. The position controller maintains a desired force using vision and wrist force. The gripper controller uses tactile sensing to keep the grip firm enough to prevent translational slip, but loose enough to allow rotational slip. Our sensor-based control approach relies on matching a desired force profile derived from object dimensions and weight, as well as vision-based monitoring of the object pose. The gripper controller uses tactile sensors to detect and prevent translational slip by tightening the grip when needed. Experimental results where the robot was tasked with rotating cuboid objects 90 degrees show that the multi-modal pivoting approach was able to rotate the objects without causing lift or translational slip, and was more energy-efficient compared to using a single sensor modality or pick-and-place. keywords: {Wrist;Force;Tactile sensors;Robot sensing systems;Energy efficiency;Sensors;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341505&isnumber=10341342

J. Huang, F. Wang and T. Hu, "CoFlyers: A Universal Platform for Collective Flying of Swarm Drones," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8808-8813, doi: 10.1109/IROS55552.2023.10342485.Abstract: Swarm drones flying is a very attractive field of robotics research, motivated by natural bird flocking or other animal collective behaviors. In this paper, we propose and develop an open-source11https://github.com/micros-uav/CoFlyers universal platform CoFlyers for end-to-end whole-chain development from flocking-inspired models to real-drone swarm flying. In particular, CoFlyers is more user-friendly with only a unified programming language of MATLAB&Simulink, rather than several existing platforms with mixed programming languages or more efforts on raw functional modules. The prototype simulator of CoFlyers is implemented in MATLAB, allowing users to quickly develop and prototype swarm flying algorithms, and to conduct task-oriented parameter auto-tuning and batch processing within reproducible scenarios. Moreover, a real-world verification module of swarm drones is developed in Simulink as well, which directly calls the prototype simulator modules for code reuse. It connects the external platforms via a standardized user-datagram-protocol communication in-terface. As a case study, CoFlyers is utilized into a multi-drone collective flying scenario in confined environments, by implementing ROS&PX4&Gazebo for high-fidelity simulation and Optitrack&Tello-drones for experiments. Eventually, both simulation and experimental results have demonstrated and validated the user-friendly practicability of CoFlyers. keywords: {Computer languages;Codes;Software packages;Prototypes;Birds;Behavioral sciences;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342485&isnumber=10341342

K. Pfister and H. Hamann, "Collective Decision-Making and Change Detection with Bayesian Robots in Dynamic Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8814-8819, doi: 10.1109/IROS55552.2023.10341649.Abstract: Solving complex problems collectively with simple entities is a challenging task for swarm robotics. For the task of collective decision-making, robots decide based on local observations on the microscopic level to achieve consensus on the macroscopic level. We study this problem for a common benchmark of classifying distributed features in a binary dynamic environment. Our special focus is on environmental features that are dynamic as they change during the experiment. We present a control algorithm that uses sophisticated statistical change detection in combination with Bayesian robots to classify dynamic environments. The main profit is to reduce false positives allowing for improved speed and accuracy in decision-making. Supported by results from various simulated experiments, we introduce three feedback loops to balance speed and accuracy. In our benchmarks, we show the superiority of our new approach over previous works on Bayesian robots. Our approach of using change detection shows a more reliable detection of environmental changes. This enables the swarm to successfully classify even difficult environments (i.e., hard to detect differences between the binary features), while achieving faster and more accurate results in simpler environments. keywords: {Heuristic algorithms;Microscopy;Decision making;Swarm robotics;Benchmark testing;Software;Bayes methods},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341649&isnumber=10341342

H. Tang, H. Zhang, Z. Shi, X. Chen, W. Ding and X. -P. Zhang, "Autonomous Swarm Robot Coordination via Mean-Field Control Embedding Multi-Agent Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8820-8826, doi: 10.1109/IROS55552.2023.10341749.Abstract: The learning approaches of designing a controller to guide the collective behavior of swarm robots have gained significant attention in recent years. However, the scalability of swarm robots and their inherent stochasticity complicate the control problem due to increasing complexity, unpredictability, and non-linearity. Despite considerable progress made in swarm robotics, addressing these challenges remains a significant issue. In this work, we model the stochastic dynamics of a swarm robot system and then propose a novel control framework based on a mean-field control (MFC) embedding multi-agent reinforcement learning (MARL) approach named MF-MARL to deal with these challenges. While MARL is able to deal with stochasticity statistically, we integrate MFC, allowing MF-MARL to cope with large-scale robots. Moreover, we apply statistical moments of robots' state and control action to discretize continuous input and enable MF-MARL to be applied in continuous scenarios. To demonstrate the effectiveness of MF-MARL, we evaluate the performance of the robots on a specific swarm simulation platform. The experimental results show that our algorithm outperforms the traditional algorithms both in navigation and manipulation tasks. Finally, we demonstrate the adaptability of the proposed algorithm through the component failure test. keywords: {Navigation;Robot kinematics;Scalability;Swarm robotics;Stochastic processes;Reinforcement learning;Complexity theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341749&isnumber=10341342

A. Krischanski, Y. K. Lopes, A. B. Leal, R. F. Martins and R. S. U. Rosso, "Multi-Instance Task in Swarm Robotics: Sorting Groups of Robots or Objects into Clusters with Minimalist Controllers," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8827-8832, doi: 10.1109/IROS55552.2023.10341775.Abstract: Relying only on behaviors that emerge from simple responsive controllers; swarms of robots have been shown capable of autonomously aggregate themselves or objects into clusters without any form of communication. We push these controllers to the limit, requiring robots to sort themselves or objects into different clusters. Based on a responsive controller that maps the current reading of a line-of-sight sensor to a pair of speeds for the robots' differential wheels, we demonstrate how multiple tasks instances can be accomplished by a robotic swarm. Using the dividing rectangles approach and physics simulation, a training step optimizes the parameters of the controller guided by a fitness function. We conducted a series of systematic trials in physics-based simulation and evaluate the performance in terms of dispersion and the ratio of clustered robots/objects. Across 20 trials where 30 robots cluster themselves into 3 groups, an average of 99.83% of them were correctly clustered into their group after 300 s. Across 50 trials where 15 robots cluster 30 objects into 3 groups, an average of 61.20%, 82.87%, and 97.73% of objects were correctly clustered into their group after 600 s, 900 s, and 1800 s, respectively. The object cluster behavior scales well while the aggregation does not, the latter due to the requirement of control tuning based on the number of robots. keywords: {Training;Scalability;Line-of-sight propagation;Wheels;Robot sensing systems;Behavioral sciences;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341775&isnumber=10341342

M. Verdoucq, C. Sire, R. Escobedo, G. Theraulaz and G. Hattenberger, "Bio-Inspired 3D Flocking Algorithm with Minimal Information Transfer for Drones Swarms," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8833-8838, doi: 10.1109/IROS55552.2023.10341413.Abstract: This article introduces a bio-inspired 3D flocking algorithm for a drone swarm, built upon a previously established 2D model, which has proven to be effective in promoting stability, alignment, and distance variation between agents within large groups of agents. The study highlights how the incorporation of a vertical interaction between agents and the acquisition by each agent of a minimal amount of information about their most influential neighbor impacts the collective behavior of the swarm. Additionally, we present a comprehensive investigation of the impacts of the intensity of alignment and attraction interactions on the collective motion patterns that emerge at the group level. These results, mostly conducted in a validated simulator, have significant implications for designing efficient UAV swarm systems and using collective patterns, or phases, in operational contexts such as corridor tracking, surveillance, and exploration. Further research will explore the effectiveness and efficiency of this UAV swarm flocking algorithm, as well as its ability to ensure safe transitions between collective phases in different operational contexts. keywords: {Solid modeling;Three-dimensional displays;Tracking;Biological system modeling;Surveillance;Autonomous aerial vehicles;Stability analysis;3D Flocking Algorithm;Collective Motion;Distributed Control;Drone Swarm;Unmanned Aerial Vehicle (UAV)},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341413&isnumber=10341342

H. Zhao et al., "A Generic Framework for Byzantine-Tolerant Consensus Achievement in Robot Swarms," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8839-8846, doi: 10.1109/IROS55552.2023.10341423.Abstract: Recent studies show that some security features that blockchains grant to decentralized networks on the internet can be ported to swarm robotics. Although the integration of blockchain technology and swarm robotics shows great promise, thus far, research has been limited to proof-of-concept scenarios where the blockchain-based mechanisms are tailored to a particular swarm task and operating environment. In this study, we propose a generic framework based on a blockchain smart contract that enables robot swarms to achieve secure consensus in an arbitrary observation space. This means that our framework can be customized to fit different swarm robotics missions, while providing methods to identify and neutralize Byzantine robots, that is, robots which exhibit detrimental behaviours stemming from faults or malicious tampering. keywords: {Measurement;Smart contracts;Swarm robotics;Robot sensing systems;Robustness;Blockchains;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341423&isnumber=10341342

G. Miyauchi, Y. K. Lopes and R. Groß, "Sharing the Control of Robot Swarms Among Multiple Human Operators: A User Study," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8847-8853, doi: 10.1109/IROS55552.2023.10342457.Abstract: Simultaneously controlling multiple robot swarms is challenging for a single human operator. When involving multiple operators, however, they can each focus on controlling a specific robot swarm, which helps distribute the cognitive workload. They could also exchange some robots with each other in response to the requirements of the tasks they discover. This paper investigates the ability of multiple operators to dynamically share the control of robot swarms and the effects of different communication types on performance and human factors. A total of 52 participants completed an experiment in which they were randomly paired to form a team. In a $2\times 2$ mixed factorial study, participants were split into two groups by communication type (direct vs. indirect). Both groups experienced different robot-sharing conditions (robot-sharing vs. no-robot-sharing). Results show that although the ability to share robots did not necessarily increase task scores, it allowed the operators to switch between working independently and collaboratively, reduced the total energy consumed by the swarm, and was considered useful by the participants. keywords: {Training;Atmospheric measurements;Human factors;Switches;Particle measurements;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342457&isnumber=10341342

J. Bloom, P. Paliwal, A. Mukherjee and C. Pinciroli, "Decentralized Multi-Agent Reinforcement Learning with Global State Prediction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8854-8861, doi: 10.1109/IROS55552.2023.10341563.Abstract: Deep reinforcement learning (DRL) has seen re-markable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second approach, we introduce Global State Prediction (GSP), a network trained to form a belief over the swarm as a whole and predict its future states. We provide a comprehensive study over four well-known deep reinforcement learning algorithms in environments with obstacles, measuring performance as the successful transport of the object to a goal location within a desired time-frame. Through an ablation study, we show that including GSP boosts performance and increases robustness when compared with methods that use global knowledge. keywords: {Training;Deep learning;Robot kinematics;Message passing;Reinforcement learning;Markov processes;Prediction algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341563&isnumber=10341342

K. Y. Chin, Y. Khaluf and C. Pinciroli, "Minimalistic Collective Perception with Imperfect Sensors," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8862-8868, doi: 10.1109/IROS55552.2023.10341384.Abstract: Collective perception is a foundational problem in swarm robotics, in which the swarm must reach consensus on a coherent representation of the environment. An important variant of collective perception casts it as a best-of-n decision-making process, in which the swarm must identify the most likely representation out of a set of alternatives. Past work on this variant primarily focused on characterizing how different algorithms navigate the speed-vs-accuracy tradeoff in a scenario where the swarm must decide on the most frequent environmental feature. Crucially, past work on best-of-n decision-making assumes the robot sensors to be perfect (noise- and fault-less), limiting the real-world applicability of these algorithms. In this paper, we apply optimal estimation techniques and a decentralized Kalman filter to derive, from first principles, a probabilistic framework for minimalistic swarm robots equipped with flawed sensors. Then, we validate our approach in a scenario where the swarm collectively decides the frequency of a certain environmental feature. We study the speed and accuracy of the decision-making process with respect to several parameters of interest. Our approach can provide timely and accurate frequency estimates even in presence of severe sensory noise. keywords: {Time-frequency analysis;Limiting;Navigation;Decision making;Swarm robotics;Sensor phenomena and characterization;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341384&isnumber=10341342

G. Önür, M. Şahin, E. E. Keyvan, A. E. Turgut and E. Şahin, "Onboard Predictive Flocking of Quadcopter Swarm in the Presence of Obstacles and Faulty Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8869-8874, doi: 10.1109/IROS55552.2023.10341354.Abstract: Achieving fluent flocking, similar to those observed in birds and fish, on robotic swarms in a desired direction while avoiding obstacles using onboard sensing and computation remains a challenge. In a previous study (Önür et al, Proc. of ANTS'2022), we proposed a predictive flocking model as a computationally efficient method to generate smoother and more robust motion of the swarm. In this study, we extend this model to achieve safe flocking in cluttered environments in the presence of faulty robots that get immobilized during flocking. Systematical evaluation of the model in simulation with different swarm sizes and different faulty robot ratios has shown that safe flocking can be achieved even when 40% of the robots malfunction during flocking. Finally, we validate the model on a swarm of five micro quadcopters using only onboard range and bearing sensors and computation in a distributed manner without any communication11Videos of the experiments are available here: https://www.youtube.com/playlist?list=PLY04vZs6xGr8U5Y8KWMD056ZRjaUVMh63.. keywords: {Filtering;Computational modeling;Predictive models;Robot sensing systems;Fish;Sensor systems;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341354&isnumber=10341342

S. Tan et al., "OA-Bug: An Olfactory-Auditory Augmented Bug Algorithm for Swarm Robots in a Denied Environment," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8875-8881, doi: 10.1109/IROS55552.2023.10341510.Abstract: Searching in a denied environment is challenging for swarm robots as no assistance from GNSS, mapping, data sharing, and central processing is allowed. However, using olfactory and auditory signals to cooperate like animals could be an important way to improve the collaboration of swarm robots. In this paper, an Olfactory-Auditory augmented Bug algorithm (OA-Bug) is proposed for a swarm of autonomous robots to explore a denied environment. A simulation environment is built to measure the performance of OA-Bug. The coverage of the search task can reach 96.93% using OA-Bug, which is significantly improved compared with a similar algorithm, SGBA [1]. Furthermore, experiments are conducted on real swarm robots to prove the validity of OA-Bug. Results show that OA-Bug can improve the performance of swarm robots in a denied environment. Video: https://youtu.be/vj9cRiSmgeM. keywords: {Global navigation satellite system;Animals;Computer bugs;Olfactory;Swarm robotics;Collaboration;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341510&isnumber=10341342

L. -N. Douce et al., "Agent Prioritization and Virtual Drag Minimization in Dynamical System Modulation For Obstacle Avoidance of Decentralized Swarms," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-7, doi: 10.1109/IROS55552.2023.10341717.Abstract: Efficient and safe multi-agent swarm coordination in environments where humans operate, such as warehouses, assistive living rooms, or automated hospitals, is crucial for adopting automation. In this paper, we augment the obstacle avoidance algorithm based on dynamical system modulation for a swarm of heterogeneous holonomic mobile agents. A smooth prioritization is proposed to change the reactivity of the swarm towards the specific agents. Further, a soft decoupling of the initial agent's kinematics is used to design an independent rotation control to ensure the agent reaches the desired position and orientation simultaneously. This decoupling allowed the introduction of a novel heuristic, the virtual drag. It minimizes the disturbance influence an agent has when moving through its surrounding. Additionally, the safety module adapts the velocity commands from the dynamical system modulation to avoid colliding trajectories between agents. The evaluation was performed in simulated assisted living and hospital environments. The prioritization successfully increased the minimum distance relative to a moving agent. The safety module is observed to create collision-free dynamics where alternative methods fail. Additionally, the repulsive nature of the safety module augments the convergence rate, thus making the proposed method better applicable to dense real-world scenarios. keywords: {Hospitals;Mobile agents;Modulation;Kinematics;Minimization;Safety;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341717&isnumber=10341342

R. Kovenburg, C. George, R. Gale and B. Aksak, "How the Fingerprint Effect Applies to Digitized Fingerprint-Like Structures," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8912-8919, doi: 10.1109/IROS55552.2023.10342396.Abstract: The fingerprint effect describes the relationship between slip speed, fingerprint ridge spacing, and the frequency of vibrations created by the movement of a fingerprint across a surface. We have previously shown that the spacing between straight, parallel, evenly spaced ridges in fingerprint-like structures, and thus the vibrations produced by the fingerprint effect, are dependent on the orientation of the ridges with respect to the direction of movement. We also showed that, when ridge orientation is known, the fingerprint effect can be used to estimate slip speed in real-time. The physical processes behind the fingerprint effect also apply to the interaction between a surface and other, non-ridge, microstructures. It is, therefore, theoretically possible to use the fingerprint effect generated by these structures to estimate slip speed. However, it is first necessary to understand the nature of the fingerprint effect generated by these non-ridge structures. In this paper, we show that digitized structures, evenly spaced in columns and rows, have a more complex relationship to the fingerprint effect than ridges do. At most orientations, these structures produce vibrations amplified around four frequencies, each determined by a set of virtual ridges defined by the digitized structures. A sensor with $\mathbf{100}\ \boldsymbol{\mu} \mathbf{m}$ tall, $\mathbf{150}\ \boldsymbol{\mu} \mathbf{m}$ wide micropillars in evenly spaced rows and columns, with a spacing of $\mathbf{300}\ \boldsymbol{\mu} \mathbf{m}$ center-to-center, is fabricated. This sensor is tested at angles between 0° and 90° by 15° increments. The results support our theoretical analysis. keywords: {Vibrations;Visualization;Shape;Friction;Estimation;Fingerprint recognition;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342396&isnumber=10341342

Z. Liu, Z. Li and L. Cheng, "A Two-Dimensional Reticular Core Optical Waveguide Sensor for Tactile and Positioning Sensing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8937-8942, doi: 10.1109/IROS55552.2023.10342366.Abstract: Tactile sensors based on optical waveguides are highly sensitive to pressure, possess good chemical inertness and electromagnetic resistance, and are unaffected by temperature changes in the surrounding environment. Researchers have developed various waveguide structures with multi-level cores to simultaneously measure tactile forces and positions. However, these designs result in thicker waveguides and reduced sensitivity in the lower levels. This study introduces a two-dimensional reticular core optical waveguide for tactile force and positioning sensing, where vertical waveguides intersect each other. The reticular core reduces waveguide thickness and simplifies fabrication processes. The simulation investigates the characteristics of light propagation and geometric parameters. Experimental results confirm the proposed reticular waveguide's force-sensing capability, with an average sensitivity of 0.36 dB/N. Compared to the split-level structure, the reticular waveguide demonstrates more consistent sensitivities along the two shear directions. Utilizing a deep neural network, the spatial resolution achieves approximately 0.72 mm along the X-axis and 1.14 mm along the Y-axis, outperforming the split-level structure. keywords: {Temperature measurement;Temperature sensors;Resistance;Sensitivity;Optical device fabrication;Tactile sensors;Electromagnetic waveguides},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342366&isnumber=10341342

Y. Chen, A. E. Tekden, M. P. Deisenroth and Y. Bekiroglu, "Sliding Touch-Based Exploration for Modeling Unknown Object Shape with Multi-Fingered Hands," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8943-8950, doi: 10.1109/IROS55552.2023.10342303.Abstract: Efficient and accurate 3D object shape reconstruction contributes significantly to the success of a robot's physical interaction with its environment. Acquiring accurate shape information about unknown objects is challenging, especially in unstructured environments, e.g. the vision sensors may only be able to provide a partial view. To address this issue, tactile sensors could be employed to extract local surface information for more robust unknown object shape estimation. In this paper, we propose a novel approach for efficient unknown 3D object shape exploration and reconstruction using a multi-fingered hand equipped with tactile sensors and a depth camera only providing a partial view. We present a multi-finger sliding touch strategy for efficient shape exploration using a Bayesian Optimization approach and a single-leader-multi-follower strategy for multi-finger smooth local surface perception. We evaluate our proposed method by estimating the 3D shape of objects from the YCB and OCRTOC datasets based on simulation and real robot experiments. The proposed approach yields successful reconstruction results relying on only a few continuous sliding touches. Experimental results demonstrate that our method is able to model unknown objects in an efficient and accurate way. keywords: {Surface reconstruction;Three-dimensional displays;Sensitivity;Shape;Tactile sensors;Vision sensors;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342303&isnumber=10341342

F. Zhang and P. Corke, "Re-Evaluating Parallel Finger-Tip Tactile Sensing for Inferring Object Adjectives: An Empirical Study," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8951-8957, doi: 10.1109/IROS55552.2023.10342262.Abstract: Finger-tip tactile sensors are increasingly used for robotic sensing to establish stable grasps and to infer object properties. Promising performance has been shown in a number of works for inferring adjectives that describe the object, but there remains a question about how each taxel contributes to the performance. This paper explores this question with empirical experiments, leading insights for future finger-tip tactile sensor usage and design: one tactile sensor instead of a pair of sensors is sufficient for symmetric objects and interaction motions; dense taxels are beneficial for texture-related adjectives, but can be distracting to non-texture-related ones; and a frame-rate much lower than the BioTac sensor can satisfy the demand of inferring object adjectives in the PHAC-2 dataset. keywords: {Tactile sensors;Sensors;Biosensors;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342262&isnumber=10341342

Y. -E. Liu, C. -Y. Chai, Y. -T. Chen and S. -L. Tsao, "Content Estimation Through Tactile Interactions with Deformable Containers," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8958-8963, doi: 10.1109/IROS55552.2023.10342436.Abstract: Pouring snacks and moving containers with beverages are challenging for a service robot. To obtain accurate content properties for planning robotic motion, tactile sensing can provide information about the pressure distribution of the contact surface, which is not obvious by visual observation. In this work, we focus on estimating the content properties of various content materials in distinct deformable containers through tactile interactions. We propose a learning-based model that can estimate content properties by using the tactile data collected by slightly squeezing a container with the content of interest. We analyzed an uncalibrated tactile sensor and collected a dataset consisting of 1125 sets of tactile sequences, which are combinations of five types of deformable containers and eleven types of content materials in different content heights. Experiments were conducted on content estimation with known contents and containers, unknown contents, and unknown containers. For unknown contents, our model can still achieve 8.5% height relative error and 79.7% state of matter accuracy. Furthermore, we analyzed that the tactile features of contents with similar content properties are close in the latent snace to show the effectiveness of our model. keywords: {Deformable models;Robot motion;Visualization;Service robots;Estimation;Tactile sensors;Containers;Content Estimation;Deformable Containers;Tactile Sensing;Robotic Interaction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342436&isnumber=10341342

L. Lach et al., "Placing by Touching: An Empirical Study on the Importance of Tactile Sensing for Precise Object Placing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8964-8971, doi: 10.1109/IROS55552.2023.10342340.Abstract: This work deals with a practical everyday problem: stable object placement on flat surfaces starting from unknown initial poses. Common object-placing approaches require either complete scene specifications or extrinsic sensor measurements, e.g., cameras, that occasionally suffer from occlusions. We propose a novel approach for stable object placing that combines tactile feedback and proprioceptive sensing. We devise a neural architecture called PlaceNet that estimates a rotation matrix, resulting in a corrective gripper movement that aligns the object with the placing surface for the subsequent object manipulation. We compare models with different sensing modalities, such as force-torque, an external motion capture system, and two classical baseline models in real-world object placing tasks with different objects. The experimental evaluation of our placing policies with a set of unseen everyday objects reveals significant generalization of our proposed pipeline, suggesting that tactile sensing plays a vital role in the intrinsic understanding of robotic dexterous object manipulation. Code, models, and supplementary videos are available on https://sites.google.com/view/placing-by-touching. keywords: {Training;Supervised learning;Pipelines;Tactile sensors;Propioception;Robot sensing systems;Motion capture},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342340&isnumber=10341342

D. C. Bulens, N. F. Lepora, S. J. Redmond and B. Ward-Cherrier, "Incipient Slip Detection with a Biomimetic Skin Morphology," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8972-8978, doi: 10.1109/IROS55552.2023.10341807.Abstract: Incipient slip is defined as the slippage of part, but not all, of the contact surface between a sensor and an object. Reliably detecting incipient slip in artificial tactile sensors would benefit autonomous robot handling capabilities by helping prevent object slippage during manipulation. Here, we present a biomimetic skin morphology based on the human fingerprint with application to marker-based tactile sensors such as the TacTip biomimetic optical tactile sensor. We modify the 3D-printed outer membrane of the TacTip to mimic glabrous skin morphology with the inclusion of external ridges (fingerprint) and internal markers (intermediate ridges), allowing localised shear deformation of the sensor's skin prior to the onset of gross slip. To validate the performance of this skin morphology, we train a random forest classifier (RFC) to identify incipient slip based on the extracted marker displacements from the sensor when it is compressed against an acrylic plate and moved laterally. The RFC model achieves 97.46% accuracy on incipient slip prediction, and is then validated on an unseen pouring task, in which gravity-induced incipient slip is detected on average within $418\pm 753\ \text{ms}$ of its onset, and before gross slip in all trials. This accurate detection of incipient slip enables corrective actions prior to the onset of gross slip, a key capability in robotic manipulation and upper-limb prosthetics. keywords: {Biomimetics;Morphology;Tactile sensors;Surface morphology;Fingerprint recognition;Skin;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341807&isnumber=10341342

J. Zhao and E. H. Adelson, "GelSight Svelte: A Human Finger-Shaped Single-Camera Tactile Robot Finger with Large Sensing Coverage and Proprioceptive Sensing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8979-8984, doi: 10.1109/IROS55552.2023.10341646.Abstract: Camera-based tactile sensing is a low-cost, popular approach to obtain highly detailed contact geometry information. However, most existing camera-based tactile sensors are fingertip sensors, and longer fingers often require extraneous elements to obtain an extended sensing area similar to the full length of a human finger. Moreover, existing methods to estimate proprioceptive information such as total forces and torques applied on the finger from camera-based tactile sensors are not effective when the contact geometry is complex. We introduce GelSight Svelte, a curved, human finger-sized, single-camera tactile sensor that is capable of both tactile and proprioceptive sensing over a large area. GelSight Svelte uses curved mirrors to achieve the desired shape and sensing coverage. Proprioceptive information, such as the total bending and twisting torques applied on the finger, is reflected as deformations on the flexible backbone of GelSight Svelte, which are also captured by the camera. We train a convolutional neural network to estimate the bending and twisting torques from the captured images. We conduct gel deformation experiments at various locations of the finger to evaluate the tactile sensing capability and proprioceptive sensing accuracy. To demonstrate the capability and potential uses of GelSight Svelte, we conduct an object holding task with three different grasping modes that utilize different areas of the finger. keywords: {Geometry;Deformation;Fingers;Propioception;Tactile sensors;Bending;Tools},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341646&isnumber=10341342

X. Guo, H. -J. Huang and W. Yuan, "Estimating Properties of Solid Particles Inside Container Using Touch Sensing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8985-8992, doi: 10.1109/IROS55552.2023.10341880.Abstract: Solid particles, such as rice and coffee beans, are commonly stored in containers and are ubiquitous in our daily lives. Understanding those particles' properties could help us make later decisions or perform later manipulation tasks such as pouring. Humans typically interact with the containers to get an understanding of the particles inside them, but it is still a challenge for robots to achieve that. This work utilizes tactile sensing to estimate multiple properties of solid particles enclosed in the container, specifically, content mass, content volume, particle size, and particle shape. We design a sequence of robot actions to interact with the container. Based on physical understanding, we extract static force/torque value from the F/T sensor, vibration-related features and topple-related features from the newly designed high-speed GelSight tactile sensor to estimate those four particle properties. We test our method on 37 very different daily particles, including powder, rice, beans, tablets, etc. Experiments show that our approach is able to estimate content mass with an error of 1.8 g, content volume with an error of 6.1 ml, particle size with an error of 1.1 mm, and achieves an accuracy of 75.6% for particle shape estimation. In addition, our method can generalize to unseen particles with unknown volumes. By estimating these particle properties, our method can help robots to better perceive the granular media and help with different manipulation tasks in daily life and industry. keywords: {Shape;Estimation;Tactile sensors;Containers;Robot sensing systems;Solids;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341880&isnumber=10341342

H. Xue, P. Liu, Z. Ju and F. Sun, "Acquisition and Prediction of High-Density Tactile Field Data for Rigid and Flexible Objects," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341734.Abstract: Obtaining high-density tactile field information is a critical aspect of research in the field of robotic haptics, as it plays a decisive role in determining the precision of robot manipulations. Vision-based tactile sensors have unique high-resolution features, which make them promising for related research. However, previous studies have mainly focused on reconstructing the shape of rigid objects or predicting the three-dimensional force of rigid objects, neglecting the analysis of flexible objects. Moreover, due to the resolution limitations of existing commercial sensors, the performance evaluation of previous force prediction models relied solely on the total force. To overcome these limitations and in order to explore the tactile field information of objects with more attributes, this paper presents a detailed high-density tactile field data acquisition method based on a mechanical simulation environment. Additionally, we constructed a network to learn the mapping relationship between tactile images and six-dimensional tactile field information. Our results demonstrate that the proposed method can predict the three-dimensional force and displacement information of the object. Notably, the prediction error is within the tolerance range for fine manipulation by robots. keywords: {Performance evaluation;Mechanical sensors;Shape;Force;Data acquisition;Tactile sensors;Sensor phenomena and characterization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341734&isnumber=10341342

S. Fan, Z. Wang, X. Huo, Y. Wang and J. Liu, "Calibration-Free BEV Representation for Infrastructure Perception," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9008-9013, doi: 10.1109/IROS55552.2023.10341916.Abstract: Effective BEV object detection on infrastructure can greatly improve traffic scene understanding and vehicle-to-infrastructure (V2I) cooperative perception. However, cameras installed on infrastructure have various postures, and previous BEV detection methods rely on accurate calibration, which is difficult for practical applications due to inevitable natural factors (e.g., wind and snow). In this paper, we propose a Calibration-free BEV Representation (CBR) network, which achieves 3D detection based on BEV representation without calibration parameters and additional depth supervision. Specifically, we utilize two multi-layer perceptrons for decoupling the features from perspective view to front view and bird-eye view under boxes-induced foreground supervision. Then, a cross-view feature fusion module matches features from orthogonal views according to similarity and conducts BEV feature enhancement with front-view features. Experimental results on DAIR-V2X demonstrate that CBR achieves acceptable performance without any camera parameters and is naturally not affected by calibration noises. We hope CBR can serve as a baseline for future research addressing practical challenges of infrastructure perception. keywords: {Three-dimensional displays;Error analysis;Vehicle-to-infrastructure;Snow;Robot vision systems;Object detection;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341916&isnumber=10341342

C. Zhu, Y. Yang, H. Liang, Z. Dong and M. Fu, "UVSS: Unified Video Stabilization and Stitching for Surround View of Tractor-Trailer Vehicles," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9014-9020, doi: 10.1109/IROS55552.2023.10342264.Abstract: Automotive surround-view camera systems have been commonly employed in automated driving to aid in near-field sensing and other perception tasks. Due to the large size of the body and the presence of multiple blind spots, panoramic surround-view systems are particularly crucial for tractor-trailer vehicles. However, the non-rigid body of tractor-trailer vehicles introduces pose changes between cameras, rendering traditional calibration-based methods inadequate. Additionally, cameras mounted separately on the tractor and the trailer will experience independent vibrations, resulting in undesirable shakiness in captured videos. In this paper, we propose a unified video stabilization and stitching method to address these challenges, which can smooth the unsteady frames and align the images from moving cameras. Delving into video stabilization techniques, we extend mesh-based motion model for unified stitching and leverage deep-learning based modules to handle complex real-world scenarios. Moreover, we design a new optimization framework to estimate the optimal displacements of mesh vertices, enabling simultaneous stabilization and stitching of frames. The experimental results, obtained by public datasets and videos captured from a model tractor-trailer vehicle, demonstrate that our approach outperforms previous methods and is highly effective in real-world applications. keywords: {Vibrations;Robot vision systems;Parallel processing;Cameras;Rendering (computer graphics);Robustness;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342264&isnumber=10341342

M. Hirano and Y. Yamakawa, "Falcon: A Wide-and-Deep Onboard Active Vision System," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9021-9026, doi: 10.1109/IROS55552.2023.10342192.Abstract: The tradeoff between the field-of-view and resolution of conventional onboard vision systems primarily results from their fixed optical components. We propose a novel active vision system, Falcon, as an optimal solution. This system comprises an electric zoom lens connected to a high-speed camera with a pair of galvanometer mirrors, enabling high-resolution imaging of a moving object across a wide range, from near to far. To ensure accurate calibration of the Falcon system, we introduce a novel mapping-based calibration method using external cameras. We also present a robust and lightweight visual feedback method that utilizes this mapping-based calibration for effective object tracking. The effectiveness of the Falcon system is verified by constructing a prototype and conducting tracking experiments in an indoor setting, which demonstrated the superior performance of our method. Additionally, we successfully achieved continuous and high-resolution imaging of a curved mirror on public roads while the vehicle was moving. keywords: {Visualization;Machine vision;Roads;Robot vision systems;High-resolution imaging;Cameras;Calibration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342192&isnumber=10341342

H. -Z. Shen and H. -Y. Lin, "Driver Distraction Detection for Daytime and Nighttime with Unpaired Visible and Infrared Image Translation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9027-9034, doi: 10.1109/IROS55552.2023.10342206.Abstract: Driver distraction detection is an important function of driver monitoring systems and intelligent vehicles. Most previous research only focuses on the system development for daytime operations. In this paper, we propose a network model, V2IA-Net, which is able to use the daytime visible and nighttime infrared images for the driver distraction detection task. With the visible-infrared image translation, driver action recognition and head pose detection, the driver distraction behavior can be analyzed in real-time performance. To provide realistic driving scenes for network training and testing, a visible-infrared image dataset, VID, is created. The proposed V2IA-Net is trained on the unpaired images, and capable of common feature extraction for visible-infrared image conversion. In the experiments, our technique is compared with various driver distraction detection models. The results have demonstrated the effectiveness of the proposed method. Source code and datasets are available at https://github.com/kk2487/V2IA-Net. keywords: {Training;Image recognition;Head;Source coding;Network architecture;Real-time systems;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342206&isnumber=10341342

Y. Bai, X. Song, W. Li, S. Zhang and S. Jiang, "Long-Short Term Policy for Visual Object Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9035-9042, doi: 10.1109/IROS55552.2023.10341652.Abstract: The goal of visual object navigation for an agent is to find the target objects accurately. Recent works mainly focus on the feature of embedding, attempting to learn better features with different variants, such as object distribution and graph representations. However, some typical navigation problems in complex environments, such as partially known and obstacle problems, may not be effectively addressed by previous feature embedding methods. In this paper, we propose a framework with a long-short objective policy, where the hidden states are classified according to the navigation objectives at that moment and separately rewarded. Specifically, we consider two objectives: the long-term objective is to go closer to the target, and the short-term objective is for obstacle avoidance and exploration. To alleviate the effect of long-term and short-term alternation, we build a state memory and propose an adjustment gate to update the state memory. Finally, all past hidden states are reweighted and combined for action prediction with an action-boosting gate. Experimental results on RoboTHOR show that the proposed method can significantly outperform the state-of-the-art. keywords: {Visualization;Navigation;Reinforcement learning;Logic gates;Boosting;Space exploration;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341652&isnumber=10341342

R. -T. Ho, C. -C. Wang and W. -C. Lin, "Lidar-Based Multiple Object Tracking with Occlusion Handling," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9043-9048, doi: 10.1109/IROS55552.2023.10342278.Abstract: Occlusion remains an issue in multiple object tracking, which could cause ambiguity in object detection, such as incorrect or missing detection. Under occlusion, a track could experience an early termination, resulting in identity switches and/or fragmentation. To recover from different lengths of occlusions, the track should be maintained by considering its occlusion status. To address the issues mentioned above, we propose an indicator that can model the track's occlusion extent via geometric information provided by LiDAR data. Through incorporating the indicator into the track management and data association process, it is feasible to prevent tracks from premature termination. The proposed method is evaluated on the collected dataset which undergoes frequent and severe occlusions. Compared to the state-of-the-art probabilistic tracking approach, our approach achieves improvements of 3.26% in MOTA and 5.36% in IDF1. Additionally, we obtain 9.89% improvements in IDF1 specifically for objects experiencing severe occlusions. keywords: {Laser radar;Object detection;Probabilistic logic;Data models;Trajectory;Object tracking;Intelligent robots;Object Tracking;Occlusion;Fragmentation;Autonomous Driving},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342278&isnumber=10341342

M. Brucker, A. Cramariuc, C. Von Einem, R. Siegwart and C. Cadena, "Local and Global Information in Obstacle Detection on Railway Tracks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9049-9056, doi: 10.1109/IROS55552.2023.10342174.Abstract: Reliable obstacle detection on railways could help prevent collisions that result in injuries and potentially damage or derail the train. Unfortunately, generic object detectors do not have enough classes to account for all possible scenarios, and datasets featuring objects on railways are challenging to obtain. We propose utilizing a shallow network to learn railway segmentation from normal railway images. The limited receptive field of the network prevents overconfident predictions and allows the network to focus on the locally very distinct and repetitive patterns of the railway environment. Additionally, we explore the controlled inclusion of global information by learning to hallucinate obstacle-free images. We evaluate our method on a custom dataset featuring railway images with artificially augmented obstacles. Our proposed method outperforms other learning-based baseline methods. keywords: {Training;Image segmentation;Image color analysis;Detectors;Rail transportation;Reliability;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342174&isnumber=10341342

Z. Li et al., "Hybrid Object Tracking with Events and Frames," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9057-9064, doi: 10.1109/IROS55552.2023.10342300.Abstract: Robust object pose tracking plays an important role in robot manipulation, but it is still an open issue for quickly moving targets as motion blur and low frequency detection can reduce pose estimation accuracy even for state-of-the-art RGB-D-based methods. An event-camera is a low-latency vision sensor that can act complementary to RGB-D. Specifically, its sub-millisecond temporal resolution can be exploited to correct for pose estimation inaccuracies due to low frequency RGB-D based detection. To do so, we propose a dual Kalman filter: the first filter estimates an object's velocity from the spatiotemporal patterns of “events”, the second filter fuses the tracked object velocity with a low-frequency object pose estimated from a deep neural network using RGB-D data. The full system outputs high frequency, accurate object poses also for fast moving objects. The proposed method works towards low-power robotics by replacing high-cost GPU-based optical flow used in prior work with event-cameras that inherently extract the required signal without costly processing. The proposed algorithm achieves comparable or better performance when compared to two state-of-the-art 6-DoF object pose estimation algorithms and one hybrid event/RGB-D algorithm on benchmarks with simulated and real data. We discuss the benefits and tradeoffs for using the event-camera and contribute algorithm, code, and datasets to the community. The code and datasets are available at https://github.com/event-driven-robotics/Hybrid-object-tracking-with-events-and-frames. keywords: {Optical filters;Training;Codes;Heuristic algorithms;Pose estimation;Vision sensors;6-DOF},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342300&isnumber=10341342

K. Wu et al., "Semantic Segmentation Based on Multiple Granularity Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9065-9070, doi: 10.1109/IROS55552.2023.10341585.Abstract: Accurate and robust coarse semantic segmentation plays a key role in the pursuit of autonomous driving. We present an algorithm that regularizes the representation space of Semantic Segmentation by Multiple Granularity Learning (SSMGL). This approach explores multiple levels of semantic knowledge in an unified framework, where the fine-grained semantic information can be either labeled or unlabeled. In our experiments, we find that SSMGL can achieve better results (1) on both on-road and off-road benchmarks, (2) under different segmentation architectures, or (3) with different backbones. The method is plug-and-play, not specialized for autonomous driving applications, and can be easily extended to any other segmentation scenario. Moreover, our SSMGL approach does not increase the computational overhead in the inference stage. keywords: {Representation learning;Semantic segmentation;Semantics;Computer architecture;Boosting;Real-time systems;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341585&isnumber=10341342

H. Deguchi, S. Taguchi, K. Shibata and S. Koide, "Enhanced Robot Navigation with Human Geometric Instruction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9071-9078, doi: 10.1109/IROS55552.2023.10342107.Abstract: Recently, robot navigation methods using human instructions have been actively studied, including visual language navigation. Although language is one of the most promising forms of instruction, words often contain ambiguities. To complement this problem, we propose to use geometric instruction as a clue to the task goal. Specifically, in our proposed system, we assume that the robot receives a rough position of the target from human gesture. The robot adaptively estimates the reliability of this geometric instruction, and switches between exploration and instruction-following modes depending on the reliability value. We conducted evaluation of our method using a 3D simulation environment, and show that the task success rate and other metrics improve compared with the baseline methods. keywords: {Measurement;Visualization;Three-dimensional displays;Navigation;Search methods;Reinforcement learning;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342107&isnumber=10341342

Y. Shao, Q. Ye, W. Luo, K. Zhang and J. Chen, "InterTracker: Discovering and Tracking General Objects Interacting with Hands in the Wild," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9079-9085, doi: 10.1109/IROS55552.2023.10341690.Abstract: Understanding human interaction with objects is an important research topic for embodied Artificial Intelligence and identifying the objects that humans are interacting with is a primary problem for interaction understanding. Existing methods rely on frame-based detectors to locate interacting objects. However, this approach is subjected to heavy occlusions, background clutter, and distracting objects. To address the limitations, in this paper, we propose to leverage spatio-temporal information of hand-object interaction to track interactive objects under these challenging cases. Without prior knowledge of the general objects to be tracked like object tracking problems, we first utilize the spatial relation between hands and objects to adaptively discover the interacting objects from the scene. Second, the consistency and continuity of the appearance of objects between successive frames are exploited to track the objects. With this tracking formulation, our method also benefits from training on large-scale general object-tracking datasets. We further curate a video-level hand-object interaction dataset for testing and evaluation from 100DOH. The quantitative results demonstrate that our proposed method outperforms the state-of-the-art methods. Specifically, in scenes with continuous interaction with different objects, we achieve an impressive improvement of about 10% as evaluated using the Average Precision (AP) metric. Our qualitative findings also illustrate that our method can produce more continuous trajectories for interacting objects. keywords: {Training;Measurement;Location awareness;Detectors;Trajectory;Object tracking;Object recognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341690&isnumber=10341342

X. Zheng and J. Zhu, "ECTLO: Effective Continuous-Time Odometry Using Range Image for LiDAR with Small FoV," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9102-9109, doi: 10.1109/IROS55552.2023.10341592.Abstract: Prism-based LiDARs are more compact and cheaper than the conventional mechanical multi-line spinning LiDARs, which have become increasingly popular in robotics, recently. However, there are several challenges for these new LiDAR sensors, including small field of view, severe motion distortions, and irregular patterns. These difficulties hinder them from being widely used in LiDAR odometry, practically. To tackle these problems, we present an effective continuous-time LiDAR odometry (ECTLO) method for the Risley-prism-based LiDARs with non-repetitive scanning patterns. A single range image covering historical points in LiDAR's small FoV is adopted for efficient map representation. To account for the noisy data from occlusions after map updating, a filter-based point-to-plane Gaussian Mixture Model is used for robust registration. Moreover, a LiDAR-only continuous-time motion model is employed to relieve the inevitable distortions. Extensive experiments have been conducted on various testbeds using the prism-based LiDARs with different scanning patterns, whose promising results demonstrate the efficacy of our proposed approach. keywords: {Point cloud compression;Laser radar;Distortion;Robot sensing systems;Sensor systems;Odometry;Noise measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341592&isnumber=10341342

M. Gonzalez, E. Marchand, A. Kacete and J. Royan, "TwistSLAM++: Fusing Multiple Modalities for Accurate Dynamic Semantic SLAM," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9126-9132, doi: 10.1109/IROS55552.2023.10341786.Abstract: Most classical SLAM systems rely on the static scene assumption, which limits their applicability in real world scenarios. Recent SLAM frameworks have been proposed to simultaneously track the camera and moving objects. However they are often unable to estimate the canonical pose of the objects and exhibit a low object tracking accuracy. To solve this problem we propose TwistSLAM++, a semantic, dynamic, SLAM system that fuses stereo images and LiDAR information. Using semantic information, we track potentially moving objects and associate them to 3D object detections in LiDAR scans to obtain their pose and size. Then, we perform registration on consecutive object scans to refine object pose estimation. Finally, object scans are used to estimate the shape of the object and constrain map points to lie on the estimated surface within the bundle adjustment. We show on classical benchmarks that this fusion approach based on multimodal information improves the accuracy of object tracking. keywords: {Simultaneous localization and mapping;Laser radar;Shape;Semantics;Pose estimation;Pipelines;Object detection;Slam;Localization;Mapping},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341786&isnumber=10341342

M. A. Karimi, D. C. Bonham, E. Lopez, A. Srivastava and M. Spenko, "SLAM and Shape Estimation for Soft Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9133-9138, doi: 10.1109/IROS55552.2023.10342213.Abstract: This paper describes Simultaneous Localization and Mapping (SLAM) techniques for mobile soft robots using on-board local sensors. The paper focuses on planar boundary-constrained swarms, which are comprised of identical modular sub-units, each flexibly connected to its neighbor. The sub-units themselves are not necessarily soft, but as the robot's size increases with respect to the size of the sub-units, the robot as a whole approaches a continuous system that exhibits the characteristics and behavior of a soft robot. Previous versions of this system have demonstrated grasping, shape formation, and tunneling; however, all prior embodiments have relied on external sensing for pose estimation. This paper is the first to demonstrate a fully self-sufficient boundary constrained swarm soft robot that does not rely on external pose estimation. The robot successfully navigates a maze-like environment while localizing and mapping the environment. keywords: {Simultaneous localization and mapping;Shape;Navigation;Pose estimation;Robot vision systems;Soft robotics;Tunneling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342213&isnumber=10341342

Y. Chen et al., "Trajectory-Based SLAM for Indoor Mobile Robots with Limited Sensing Capabilities," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9147-9154, doi: 10.1109/IROS55552.2023.10341518.Abstract: In this paper we introduce a novel SLAM system for 2-D indoor environments that relies only on limited sensing. Our fully autonomous system uses only the trajectory of the robot around walls and objects in the environment as landmarks and is capable of robust and long-term exploration and mapping of a broad range of household floor plans. Rank-deficient and full-rank factors are created when the robot observes existing trajectory-based landmarks, and they are filtered and added in a pose graph, which is optimized periodically. The mission space is mapped by efficient adaptive local mapping algorithms. The proposed SLAM system has been extensively tested in various scenarios, and experimental results show its robustness and accuracy. keywords: {Location awareness;Simultaneous localization and mapping;Space missions;Robustness;Sensors;Trajectory;Indoor environment},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341518&isnumber=10341342

M. Shaheer, J. A. Millan-Romera, H. Bavle, J. L. Sanchez-Lopez, J. Civera and H. Voos, "Graph-Based Global Robot Localization Informing Situational Graphs with Architectural Graphs," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9155-9162, doi: 10.1109/IROS55552.2023.10341373.Abstract: In this paper, we propose a solution for legged robot localization using architectural plans. Our specific contributions towards this goal are several. Firstly, we develop a method for converting the plan of a building into what we denote as an architectural graph (A-Graph). When the robot starts moving in an environment, we assume it has no knowledge about it, and it estimates an online situational graph representation (S-Graph) of its surroundings. We develop a novel graph-to-graph matching method, in order to relate the S-Graph estimated online from the robot sensors and the A-Graph extracted from the building plans. Note the challenge in this, as the S-Graph may show a partial view of the full A-Graph, their nodes are heterogeneous and their reference frames are different. After the matching, both graphs are aligned and merged, resulting in what we denote as an informed Situational Graph (is-Graph), with which we achieve global robot localization and exploitation of prior knowledge from the building plans. Our experiments show that our pipeline shows a higher robustness and a significantly lower pose error than several LiDAR localization baselines. Paper Video: https://youtu.be/3Pv7y8aOsUY keywords: {Location awareness;Uncertainty;Navigation;Buildings;Pipelines;Merging;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341373&isnumber=10341342

Y. Tang, M. Wang, Y. Deng, Y. Yang and Y. Yue, "SSGM: Spatial Semantic Graph Matching for Loop Closure Detection in Indoor Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9163-9168, doi: 10.1109/IROS55552.2023.10342317.Abstract: Capturing the semantics of objects and the topological relationship allows the robot to describe the scene more intelligently like a human and measure the similarity between scenes (loop closure detection) more accurately. However, many current semantic graph matching methods are based on walk descriptors, which only extract adjacency relations between objects. In such way, the comprehensive information in the semantic graph is not fully exploited, which may lead to false closed-loop detection. This paper proposes a novel spatial semantic graph matching method (SSGM) in indoor environments, which considers multifaceted information of the semantic graphs. Firstly, two semantic graphs are aligned in the same coordinate space contributed by the second-order spatial compatibility metric between objects and local graph features of objects in semantic graphs. Secondly, the similarity of the spatial distribution of overall semantic graphs is further evaluated. The proposed algorithm is validated on public datasets and compared with the latest semantic graph matching methods, demonstrating improved accuracy and efficiency in loop closure detection. The code is available at https://github.com/BIT-TYJ/SSGM. keywords: {Measurement;Graphical models;Codes;Robot kinematics;Current measurement;Semantics;Indoor environment},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342317&isnumber=10341342

D. Zhang, M. Wu and S. -K. Lam, "Training-Free Attentive-Patch Selection for Visual Place Recognition," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9169-9174, doi: 10.1109/IROS55552.2023.10342347.Abstract: Visual Place Recognition (VPR) utilizing patch descriptors from Convolutional Neural Networks (CNNs) has shown impressive performance in recent years. Existing works either perform exhaustive matching of all patch descriptors, or employ complex networks to select good candidate patches for further geometric verification. In this work, we develop a novel two-step training-free patch selection method that is fast, while being robust to large occlusions and extreme viewpoint variations. In the first step, a self-attention mechanism is used to select sparse and evenly distributed discriminative patches in the query image. Next, a novel spatial-matching method is used to rapidly select corresponding patches with high similar appearances between the query and each reference image. The proposed method is inspired by how humans perform place recognition by first identifying prominent regions in the query image, and then relying on back-and-forth visual inspection of the query and reference image to attentively identify similar regions while ignoring dissimilar ones. Extensive experiment results show that our proposed method outperforms state-of-the-art (SOTA) methods in both place recognition precision and runtime, on various challenging conditions. keywords: {Visualization;Image recognition;Runtime;Complex networks;Inspection;Robustness;Convolutional neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342347&isnumber=10341342

K. Koide, S. Oishi, M. Yokozuka and A. Banno, "Exact Point Cloud Downsampling for Fast and Accurate Global Trajectory Optimization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9175-9182, doi: 10.1109/IROS55552.2023.10342403.Abstract: This paper presents a point cloud downsampling algorithm for fast and accurate trajectory optimization based on global registration error minimization. The proposed algorithm selects a weighted subset of residuals of the input point cloud such that the subset yields exactly the same quadratic point cloud registration error function as that of the original point cloud at the evaluation point. This method accurately approximates the original registration error function with only a small subset of input points (29 residuals at a minimum). Experimental results using the KITTI dataset demonstrate that the proposed algorithm significantly reduces processing time (by 87%) and memory consumption (by 99%) for global registration error minimization while retaining accuracy. keywords: {Point cloud compression;Visualization;Simultaneous localization and mapping;Costs;Electric breakdown;Memory management;Minimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342403&isnumber=10341342

Q. Huang and J. J. Leonard, "GAPSLAM: Blending Gaussian Approximation and Particle Filters for Real-Time Non-Gaussian SLAM," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9183-9190, doi: 10.1109/IROS55552.2023.10341889.Abstract: Inferring the posterior distribution in SLAM is critical for evaluating the uncertainty in localization and mapping, as well as supporting subsequent planning tasks aiming to reduce uncertainty for safe navigation. However, real-time full posterior inference techniques, such as Gaussian approximation and particle filters, either lack expressiveness for representing non-Gaussian posteriors or suffer from performance degeneracy when estimating high-dimensional posteriors. Inspired by the complementary strengths of Gaussian approximation and particle filters-scalability and non-Gaussian estimation, respectively-we blend these two approaches to infer marginal posteriors in SLAM. Specifically, Gaussian approximation provides robot pose distributions on which particle filters are conditioned to sample landmark marginals. In return, the maximum a posteriori point among these samples can be used to reset linearization points in the nonlinear optimization solver of the Gaussian approximation, facilitating the pursuit of global optima. We demonstrate the scalability, generalizability, and accuracy of our algorithm for real-time full posterior inference on realworld range-only SLAM and object-based bearing-only SLAM datasets. keywords: {Simultaneous localization and mapping;Uncertainty;Approximation algorithms;Real-time systems;Particle filters;Inference algorithms;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341889&isnumber=10341342

Z. Tang, H. Ye and H. Zhang, "Multi-Scale Point Octree Encoding Network for Point Cloud Based Place Recognition," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9191-9197, doi: 10.1109/IROS55552.2023.10341943.Abstract: Over the past decades, point cloud-based place recognition has garnered significant attention. This research paper presents a pioneering approach, denoted as the Multi-scale Point Octree Encoding Network (MPOE-Net), designed to acquire a discriminative global descriptor for efficient retrieval of places. The key element of the MPOE-Net is the point octree encoding module, which adeptly captures local information for each point by considering its nearest and farthest neighbors. Further enhancing local relationships, a multi-transformer network is introduced, utilizing a novel grouped offset-attention mechanism. To amalgamate the multi-scale attention maps into a comprehensive global descriptor, a multi-NetVLAD layer is incorporated. Through rigorous experimentation across diverse benchmark datasets, our proposed method unequivocally outperforms existing techniques in the realm of point cloud-based place recognition tasks, achieving state-of-the-art results. Our code is released publicly at https://github.com/Zhilong-Tang/MPOE-Net. keywords: {Point cloud compression;Codes;Octrees;Benchmark testing;Encoding;Data mining;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341943&isnumber=10341342

K. Botashev and G. Ferrer, "Analytical Jacobian Approximation for Direct Optimization of a Trajectory of Interpolated Poses on SE(3)," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9198-9204, doi: 10.1109/IROS55552.2023.10342321.Abstract: This paper relates to time-continuous trajectory representation using direct linear interpolation on SE(3). Our approach focuses on a novel analytical Jacobian approximation of a sequence of linearly interpolated poses on SE(3). This paper shows a derivation of the proposed analytical Jacobian using retraction mapping and an approximation to the commutativity property of infinitesimal group elements. We provide plenty of evaluations for 3 different optimization problems. For the synthetic point cloud alignment problem, our proposed Jacobian is compared with a numerical one. For the synthetic pose graph optimization problem, the proposed Jacobian approximation allows us to reduce by x7 factor the state dimensions while keeping a similar magnitude of resulting error compared to the full discrete-time trajectory. Finally, we show the validity of our approach in a time-continuous approach for real-world LIDAR odometry problem. keywords: {Jacobian matrices;Point cloud compression;Interpolation;Three-dimensional displays;Laser radar;Trajectory;Odometry},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342321&isnumber=10341342

Y. M. Al-Rawashdeh and M. Al Janaideh, "On Cyber-Attacks Mitigation for Distributed Trajectory Generators," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9205-9210, doi: 10.1109/IROS55552.2023.10342286.Abstract: In this paper, an immune average consensus behavior of distributed trajectory generators given in the form of a multi-agent system is presented. Starting with the well-known results of linear consensus protocols, we propose a decomposition of the invariant consensus value to enable a distributed cyber-attacks detection and mitigation mechanism among the connected agents over mainly undirected communication links. This decomposition suggests one preferred propagation of the invariant quantity along communication links of the multi-agent systems under study. Despite its simplicity, the effectiveness of this mechanism in detecting and mitigating various types of cyber-attacks is evident through a numerical simulation. Interestingly, the resulting defense mechanism will not be passive, rather it can initiate its counter-attack measures by pretending that the attack process was a success. Moreover, the trajectory generators can operate under stealth mode where the communication links get silenced or totally disconnected without affecting the intended behavior after having the consensus value locked. keywords: {Numerical simulation;Generators;Trajectory;Behavioral sciences;Consensus protocol;Cyberattack;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342286&isnumber=10341342

J. le Fevre Sejersen and E. Kayacan, "CAMETA: Conflict-Aware Multi-Agent Estimated Time of Arrival Prediction for Mobile Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9254-9261, doi: 10.1109/IROS55552.2023.10341937.Abstract: This study presents the conflict-aware multi-agent estimated time of arrival (CAMETA) framework, a novel approach for predicting the arrival times of multiple agents in unstructured environments without predefined road infrastructure. The CAMETA framework consists of three components: a path planning layer generating potential path suggestions, a multi-agent ETA prediction layer predicting the arrival times for all agents based on the paths, and lastly, a path selection layer that calculates the accumulated cost and selects the best path. The novelty of the CAMETA framework lies in the heterogeneous map representation and the heterogeneous graph neural network architecture. As a result of the proposed novel structure, CAMETA improves the generalization capability compared to the state-of-the-art methods that rely on structured road infrastructure and historical data. The simulation results demonstrate the efficiency and efficacy of the multi-agent ETA prediction layer, with a mean average percentage error improvement of 29.5% and 44% when compared to a traditional path planning method (A *) which does not consider conflicts. The performance of the CAMETA framework shows significant improvements in terms of robustness to noise and conflicts as well as determining proficient routes compared to state-of-the-art multi-agent path planners. keywords: {Costs;Roads;Simulation;Path planning;Robustness;Graph neural networks;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341937&isnumber=10341342

O. Dagan, T. L. Cinquini and N. R. Ahmed, "Non-Linear Heterogeneous Bayesian Decentralized Data Fusion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9262-9268, doi: 10.1109/IROS55552.2023.10342177.Abstract: The factor graph decentralized data fusion (FG-DDF) framework was developed for the analysis and exploitation of conditional independence in heterogeneous Bayesian decentralized fusion problems, in which robots update and fuse pdfs over different, but overlapping subsets of random states. This allows robots to efficiently use smaller probabilistic models and sparse message passing to accurately and scalably fuse relevant local parts of a larger global joint state pdf while accounting for data dependencies between robots. Whereas prior work required limiting assumptions about network connectivity and model linearity, this paper relaxes these to explore the applicability and robustness of FG-DDF in more general settings. We develop a new heterogeneous fusion rule which generalizes the homogeneous covariance intersection algorithm for such cases and test it in multi-robot tracking and localization scenarios with non-linear motion/observation models under communication dropouts. Simulation and hardware experiments show that, in practice, the FG-DDF continues to provide consistent filtered estimates under these more practical operating conditions, while reducing computation and communication costs by more than 99%, thus enabling the design of scalable real-world multi-robot systems. keywords: {Fuses;Tracking;Computational modeling;Simulation;Data integration;Probabilistic logic;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342177&isnumber=10341342

M. T. Hossain, H. M. La, S. Badsha and A. Netchaev, "BRNES: Enabling Security and Privacy-Aware Experience Sharing in Multiagent Robotic and Autonomous Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9269-9276, doi: 10.1109/IROS55552.2023.10341559.Abstract: Although experience sharing (ES) accelerates multiagent reinforcement learning (MARL) in an advisor-advisee framework, attempts to apply ES to decentralized multiagent systems have so far relied on trusted environments and over-looked the possibility of adversarial manipulation and inference. Nevertheless, in a real-world setting, some Byzantine attackers, disguised as advisors, may provide false advice to the advisee and catastrophically degrade the overall learning performance. Also, an inference attacker, disguised as an advisee, may conduct several queries to infer the advisors' private information and make the entire ES process questionable in terms of privacy leakage. To address and tackle these issues, we propose a novel MARL framework (BRNES) that heuristically selects a dynamic neighbor zone for each advisee at each learning step and adopts a weighted experience aggregation technique to reduce Byzantine attack impact. Furthermore, to keep the agent's private information safe from adversarial inference attacks, we leverage the local differential privacy (LDP)-induced noise during the ES process. Our experiments show that our framework outperforms the state-of-the-art in terms of the steps to goal, obtained reward, and time to goal metrics. Particularly, our evaluation shows that the proposed framework is 8.32x faster than the current non-private frameworks and 1.41x faster than the private frameworks in an adversarial setting. keywords: {Measurement;Privacy;Differential privacy;Autonomous systems;Reinforcement learning;Security;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341559&isnumber=10341342

T. Mina, W. Jo, S. S. Kannan and B. -C. Min, "Beacon-Based Distributed Structure Formation in Multi-Agent Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9277-9284, doi: 10.1109/IROS55552.2023.10341782.Abstract: Autonomous shape and structure formation is an important problem in the domain of large-scale multi-agent systems. In this paper, we propose a 3D structure representation method and a distributed structure formation strategy where settled agents guide free moving agents to a prescribed location to settle in the structure. Agents at the structure formation frontier looking for neighbors to settle act as beacons, generating a surface gradient throughout the formed structure propagated by settled agents. Free-moving agents follow the surface gradient along the formed structure surface to the formation frontier, where they eventually reach the closest beacon and settle to continue the structure formation following a local bidding process. Agent behavior is governed by a finite state machine implementation, along with potential field-based motion control laws. We also discuss appropriate rules for recovering from stagnation points. Simulation experiments are presented to show planar and 3D structure formations with continuous and discontinuous boundary/surfaces, which validate the proposed strategy, followed by a scalability analysis. keywords: {Location awareness;Three-dimensional displays;Shape;Simulation;Scalability;Robot sensing systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341782&isnumber=10341342

L. Yin, F. Zhu, Y. Ren, F. Kong and F. Zhang, "Decentralized Swarm Trajectory Generation for LiDAR-based Aerial Tracking in Cluttered Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9285-9292, doi: 10.1109/IROS55552.2023.10341567.Abstract: Aerial tracking with multiple unmanned aerial vehicles (UAVs) has wide potential in various applications. However, the existing works for swarm tracking typically lack the capability of maintaining high target visibility in cluttered environments. To address this deficiency, we present a decentralized planner that maximizes target visibility while ensuring collision-free maneuvers for swarm tracking. In this paper, each drone's tracking performance is first analyzed by a decentralized kinodynamic searching front-end, which renders an optimal guiding path to initialize safe flight corridors and visible sectors. Afterwards, a polynomial trajectory satisfying the corridor constraints is generated by a spatial-temporal optimizer. Inter-vehicle collision and occlusion avoidance are also incorporated into the optimization objectives. The advantages of our methods are verified by extensive benchmark comparisons against other cutting-edge works. Integrated with an autonomous LiDAR-based swarm system, the proposed planner demonstrates its efficiency and robustness in real-world experiments with unknown cluttered surroundings. keywords: {Target tracking;Benchmark testing;Autonomous aerial vehicles;Robustness;Trajectory;Optimization;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341567&isnumber=10341342

C. Ma et al., "Decentralized Planning for Car-Like Robotic Swarm in Cluttered Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9293-9300, doi: 10.1109/IROS55552.2023.10342360.Abstract: Robot swarm is a hot spot in robotic research community. In this paper, we propose a decentralized framework for car-like robotic swarm which is capable of real-time planning in cluttered environments. In this system, path finding is guided by environmental topology information to avoid frequent topological change, and search-based speed planning is leveraged to escape from infeasible initial value's local minima. Then spatial-temporal optimization is employed to generate a safe, smooth and dynamically feasible trajectory. During optimization, the trajectory is discretized by fixed time steps. Penalty is imposed on the signed distance between agents to realize collision avoidance, and differential flatness cooperated with limitation on front steer angle satisfies the non-holonomic constraints. With trajectories broadcast to the wireless network, agents are able to check and prevent potential collisions. We validate the robustness of our system in simulation and real-world experiments. Code will be released as open-source packages. keywords: {Robot kinematics;Wireless networks;System recovery;Real-time systems;Robustness;Planning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342360&isnumber=10341342

Y. Wang, B. Xiang, S. Huang and G. Sartoretti, "SCRIMP: Scalable Communication for Reinforcement- and Imitation-Learning-Based Multi-Agent Pathfinding," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9301-9308, doi: 10.1109/IROS55552.2023.10342305.Abstract: Trading off performance guarantees in favor of scalability, the Multi-Agent Path Finding (MAPF) community has recently started to embrace Multi-Agent Reinforcement Learning (MARL), where agents learn to collaboratively generate individual, collision-free (but often suboptimal) paths. Scalability is usually achieved by assuming a local field of view (FOV) around the agents, helping scale to arbitrary world sizes. However, this assumption significantly limits the amount of information available to the agents, making it difficult for them to enact the type of joint maneuvers needed in denser MAPF tasks. In this paper, we propose SCRIMP, where agents learn individual policies from even very small (down to $3\mathrm{x}3$) FOVs, by relying on a highly-scalable global communication mechanism based on a modified transformer. We further equip agents with a state-value-based tie-breaking strategy to further improve performance in symmetric situations, and introduce intrinsic rewards to encourage exploration while mitigating the long-term credit assignment problem. Empirical evaluations on a set of experiments indicate that SCRIMP can achieve higher performance with improved scalability compared to other state-of-the-art learning-based MAPF planners with larger FOVs, and even yields similar performance as a classical centralized planner in many cases. Ablation studies further validate the effectiveness of our proposed techniques. Finally, we show that our trained model can be directly implemented on real robots for online MAPF through high-fidelity simulations in gazebo. keywords: {Learning systems;Scalability;Stochastic processes;Information sharing;Reinforcement learning;Transformers;Global communication},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342305&isnumber=10341342

S. Satir, Y. F. Aktaş, S. Atasoy, M. Ankarali and E. Şahin, "Distributed Model Predictive Formation Control of Robots with Sampled Trajectory Sharing in Cluttered Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9309-9315, doi: 10.1109/IROS55552.2023.10341414.Abstract: In this paper, we propose a Model Predictive Control (MPC) based distributed formation control method for a multi-robot system (MRS) that would move them among dynamic obstacles to a desired goal position. Specifically, after formulating the formation control, as a distributed version of MPC, we propose and evaluate three information-sharing schemes within the MRS; namely sharing (i) positions, (ii) complete predicted trajectories, and (iii) exponentially-sampled predicted trajectories. Using a simplified kinematic model for robots, we conducted systematic simulation experiments in (a) scenarios, where the robots are instructed to switch places, as one of the most challenging forms of formation changes, and in (b) scenarios where robots are instructed to reach a goal, within environments containing dynamic obstacles. In a set of systematic experiments conducted in simulation and with mini quadcopters, we have shown that sharing of exponentially-sampled trajectories (as opposed to positions, or complete trajectories) among the robots provides near-optimal paths while decreasing the required computation cost and communication bandwidth. Surprisingly, in the presence of noise, sharing exponentially-sampled trajectories among the robots decreased the variance in the final paths. The proposed method is demonstrated on a group of Crazyflie quadcopters. keywords: {Systematics;Costs;Bandwidth;Switches;Predictive models;Formation control;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341414&isnumber=10341342

D. Lawson and A. H. Qureshi, "Control Transformer: Robot Navigation in Unknown Environments Through PRM-Guided Return-Conditioned Sequence Modeling," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9324-9331, doi: 10.1109/IROS55552.2023.10341628.Abstract: Learning long-horizon tasks such as navigation has presented difficult challenges for successfully applying reinforcement learning to robotics. From another perspective, under known environments, sampling-based planning can robustly find collision-free paths in environments without learning. In this work, we propose Control Transformer that models return-conditioned sequences from low-level policies guided by a sampling-based Probabilistic Roadmap (PRM) planner. We demonstrate that our framework can solve long-horizon navigation tasks using only local information. We evaluate our approach on partially-observed maze navigation with MuJoCo robots, including Ant, Point, and Humanoid. We show that Control Transformer can successfully navigate through mazes and transfer to unknown environments. Additionally, we apply our method to a differential drive robot (Turtlebot3) and show zero-shot sim2real transfer under noisy observations. keywords: {Navigation;Humanoid robots;Reinforcement learning;Transformers;Probabilistic logic;Planning;Noise measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341628&isnumber=10341342

J. Fu, Y. Shen, Z. Jian, S. Chen, J. Xin and N. Zheng, "InteractionNet: Joint Planning and Prediction for Autonomous Driving with Transformers," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9332-9339, doi: 10.1109/IROS55552.2023.10342367.Abstract: Planning and prediction are two important modules of autonomous driving and have experienced tremendous advancement recently. Nevertheless, most existing methods regard planning and prediction as independent and ignore the correlation between them, leading to the lack of consideration for interaction and dynamic changes of traffic scenarios. To address this challenge, we propose InteractionNet, which leverages transformer to share global contextual reasoning among all traffic participants to capture interaction and interconnect planning and prediction to achieve joint. Besides, InteractionNet deploys another transformer to help the model pay extra attention to the perceived region containing critical or unseen vehicles. InteractionNet outperforms other baselines in several benchmarks, especially in terms of safety, which benefits from the joint consideration of planning and forecasting. The code will be available at https://github.com/fujiawei0724/InteractionNet. keywords: {Correlation;Collaboration;Benchmark testing;Transformers;Planning;Safety;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342367&isnumber=10341342

A. Khalil and J. Kwon, "ANEC: Adaptive Neural Ensemble Controller for Mitigating Latency Problems in Vision-Based Autonomous Driving," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9340-9346, doi: 10.1109/IROS55552.2023.10342520.Abstract: Humans have latency in their visual perception system between observation and action. Any action we take is based on an earlier observation since, by the time we act, the state has already changed, and we got a new observation. In autonomous driving, this latency is also present, determined by the amount of time the control algorithm needs to process information before acting. This algorithmic perception latency can be reduced by massive computing power via GPUs and FPGAs, which is improbable in automobile platforms. Thus, it is a reasonable assumption that the algorithmic perception latency is inevitable. Many researchers have developed different neural network driving models without consideration of the algorithmic perception latency. This paper studies the latency effect on vision-based neural network autonomous driving in the lane-keeping task and proposes a vision-based novel neural network controller, the Adaptive Neural Ensemble Controller (ANEC) that is inspired by the near/far gaze distribution of human drivers during lane-keeping. ANEC was tested in Gazebo 3D simulation environment with Robot Operating System (ROS) which showed the effectiveness of ANEC in dealing with algorithmic latency. The source code is available at https://github.com/jrkwon/oscar/tree/devel_anec. keywords: {Adaptive systems;Three-dimensional displays;Source coding;Operating systems;Neural networks;Process control;Task analysis;Autonomous Vehicle Navigation;Machine Learning for Robot Control;Imitation Learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342520&isnumber=10341342

T. -C. Au, "A Dynamic Programming Algorithm for Grid-Based Formation Planning of Multiple Vehicles," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9347-9353, doi: 10.1109/IROS55552.2023.10341481.Abstract: A common operation in multirobot systems is to generate a motion plan for multiple robots such that the robots can move in formation to achieve some desired effects. For example, in autonomous parking lots, a group of vehicles can be asked to move to another location when they block another vehicle that needs to leave the parking lot. In this paper, we present a novel grid-based planning approach for motion planning that minimizes the makespan of moving multiple vehicles from one location to another in a safe manner. Unlike most existing multirobot planning algorithms, our algorithm uses dynamic programming to compute a nearly-optimal motion plan for a large group of vehicles in polynomial time with the help of a given set of intermediate vehicle patterns. Our experimental results show that our algorithm is much faster than an exact algorithm but does not increase the minimum makespans tremendously. keywords: {Heuristic algorithms;Planning;Dynamic programming;Multi-robot systems;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341481&isnumber=10341342

J. Zhang et al., "LB-L2L-Calib 2.0: A Novel Online Extrinsic Calibration Method for Multiple Long Baseline 3D LiDARs Using Objects," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9354-9359, doi: 10.1109/IROS55552.2023.10342245.Abstract: In V2X (Vehicle-to-Everything), one important work is to extrinsically calibrate multiple 3D LiDARs, which are mounted with a long baseline and large viewpoint-difference at the road-side. Current solutions either require a specific target being set up (e.g., a sphere), or require specific features existing in the environment (e.g., mutually orthogonal planes). However, it is time-consuming, sometimes even inconvenient, to set up specific targets, e.g., at busy intersections and highways. Furthermore, specific features do not always exist in the traffic scenario. Thus, the current solutions are not feasible. To address this problem, a novel extrinsic calibration method is proposed in this paper, namely LB-L2L-Calib 2.0. It is the 2.0 version of our previous work. The novelties are: 1) We propose to use the easily accessible objects on the road as features for calibration (i.e., the vehicles). Thus, it is not necessary to set up any specific targets and we do not need to worry whether specific features exist or not. The key point is we observed that the 3D bounding box centers of the vehicles are viewpoint-invariant from different viewpoints, which makes them ideal features for long baseline and large viewpoint-difference calibration. 2) To establish correct correspondence between the bounding box centers detected from different LiDARs, we propose an exhaustive searching strategy. It can robustly output correct correspondence. Extensive experiments are performed in three scenarios (simulation: intersection, real: carpark and highway), with two types of LiDAR (Velodyne and Livox), demonstrating that LB-L2L-Calib 2.0 is robust, effective, and accurate. keywords: {Laser radar;Three-dimensional displays;Roads;Feature extraction;Calibration;Vehicle-to-everything;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342245&isnumber=10341342

R. A. T. Perera, M. Jeong, A. Q. Li and P. Stegagno, "A GM-PHD Filter with Estimation of Probability of Detection and Survival for Individual Targets," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9360-9366, doi: 10.1109/IROS55552.2023.10342438.Abstract: This paper proposes a modification of the Gaussian mixture probability hypothesis density (GM-PHD) filter to compute online the probability of detection $(P_{D})$ and probability of survival $(P_{S})$ of targets. This eliminates the need for predetermined and/or constant $P_{D}$ and $P_{S}$ values, that may degrade the estimation. The proposed filter estimates the $P_{D}$ and $P_{S}$ values for each individual target based on newly introduced parameters, which are updated during the measurement update process. The effectiveness of the proposed filter was validated through an in-lab experiment using four unmanned ground robots with varying $P_{D}$ values and a real-world lidar-based obstacle tracking system implemented on an Automated Surface Vehicle operating in a lake with real-time boat traffic. The results of the experiments demonstrate that the proposed filter outperforms the standard PHD filter with incorrect $P_{D}$ and $P_{S}$ values. These findings highlight the potential benefits of the proposed filter in improving target tracking performance in complex environments. keywords: {Target tracking;Estimation;Boats;Lakes;Real-time systems;Collision avoidance;Standards},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342438&isnumber=10341342

E. U. Samani, F. Tao, H. R. Dasari, S. Ding and A. G. Banerjee, "F2BEV: Bird's Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9367-9374, doi: 10.1109/IROS55552.2023.10341862.Abstract: Bird's Eye View (BEV) representations are tremendously useful for perception-related automated driving tasks. However, generating BEVs from surround-view fisheye camera images is challenging due to the strong distortions introduced by such wide-angle lenses. We take the first step in addressing this challenge and introduce a baseline, F2BEV, to generate discretized BEV height maps and BEV semantic segmentation maps from fisheye images. F2BEV consists of a distortion-aware spatial cross attention module for querying and consolidating spatial information from fisheye image features in a transformer-style architecture followed by a task-specific head. We evaluate single-task and multi-task variants of F2BEV on our synthetic FB-SSEM dataset, all of which generate better BEV height and segmentation maps (in terms of the IoU) than a state-of-the-art BEV generation method operating on undistorted fisheye images. We also demonstrate discretized height map generation from real-world fisheye images using F2BEV. Our dataset is publicly available at https://github.com/volvo-cars/FB-SSEM-dataset keywords: {Head;Semantic segmentation;Robot vision systems;Cameras;Transformers;Multitasking;Distortion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341862&isnumber=10341342

S. Morin, M. Saavedra-Ruiz and L. Paull, "One-4-All: Neural Potential Fields for Embodied Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9375-9382, doi: 10.1109/IROS55552.2023.10342302.Abstract: A fundamental task in robotics is to navigate between two locations. In particular, real-world navigation can require long-horizon planning using high-dimensional RGB images, which poses a substantial challenge for end-to-end learning-based approaches. Current semi-parametric methods instead achieve long-horizon navigation by combining learned modules with a topological memory of the environment, often represented as a graph over previously collected images. However, using these graphs in practice requires tuning a number of pruning heuristics. These heuristics are necessary to avoid spurious edges, limit runtime memory usage and maintain reasonably fast graph queries in large environments. In this work, we present One-4-All (O4A), a method leveraging self-supervised and manifold learning to obtain a graph-free, end-to-end navigation pipeline in which the goal is specified as an image. Navigation is achieved by greedily minimizing a potential function defined continuously over image embeddings. Our system is trained offline on non-expert exploration sequences of RGB data and controls, and does not require any depth or pose measurements. We show that 04A can reach long-range goals in 8 simulated Gibson indoor environments and that resulting embeddings are topologically similar to ground truth maps, even if no pose is observed. We further demonstrate successful real-world navigation using a Jackal UGV platform.aaProject page https://montrealrobotics.ca/o4a/. keywords: {Runtime;Navigation;Image edge detection;Pipelines;Planning;Manifold learning;Indoor environment},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342302&isnumber=10341342

W. -B. Kou et al., "Communication Resources Constrained Hierarchical Federated Learning for End-to-End Autonomous Driving," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9383-9390, doi: 10.1109/IROS55552.2023.10342134.Abstract: While federated learning (FL) improves the generalization of end-to-end autonomous driving by model aggregation, the conventional single-hop FL (SFL) suffers from slow convergence rate due to long-range communications among vehicles and cloud server. Hierarchical federated learning (HFL) overcomes such drawbacks via introduction of mid-point edge servers. However, the orchestration between constrained communication resources and HFL performance becomes an urgent problem. This paper proposes an optimization-based Communication Resource Constrained Hierarchical Federated Learning (CRCHFL) framework to minimize the generalization error of the autonomous driving model using hybrid data and model aggregation. The effectiveness of the proposed CRCHFL is evaluated in the Car Learning to Act (CARLA) simulation platform. Results show that the proposed CRCHFL both accelerates the convergence rate and enhances the generalization of federated learning autonomous driving model. Moreover, under the same communication resource budget, it outperforms the HFL by 10.33% and the SFL by 12.44%. keywords: {Fading channels;Federated learning;Communication channels;Benchmark testing;Data models;Servers;Automobiles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342134&isnumber=10341342

X. Li et al., "Poly-MOT: A Polyhedral Framework For 3D Multi-Object Tracking," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9391-9398, doi: 10.1109/IROS55552.2023.10341778.Abstract: 3D Multi-object tracking (MOT) empowers mobile robots to accomplish well-informed motion planning and navigation tasks by providing motion trajectories of surrounding objects. However, existing 3D MOT methods typically employ a single similarity metric and physical model to perform data association and state estimation for all objects. With large-scale modern datasets and real scenes, there are a variety of object categories that commonly exhibit distinctive geometric properties and motion patterns. In this way, such distinctions would enable various object categories to behave differently under the same standard, resulting in erroneous matches between trajectories and detections, and jeopardizing the reliability of downstream tasks (navigation, etc.). Towards this end, we propose Poly-MOT, an efficient 3D MOT method based on the Tracking-By-Detection framework that enables the tracker to choose the most appropriate tracking criteria for each object category. Specifically, Poly-MOT leverages different motion models for various object categories to characterize distinct types of motion accurately. We also introduce the constraint of the rigid structure of objects into a specific motion model to accurately describe the highly nonlinear motion of the object. Additionally, we introduce a two-stage data association strategy to ensure that objects can find the optimal similarity metric from three custom metrics for their categories and reduce missing matches. On the NuScenes dataset, our proposed method achieves state-of-the-art performance with 75.4% AMOTA. The code is available at https://github.com/lixiaoyu20001P0Iy-MOT. keywords: {Measurement;Training;Three-dimensional displays;Tracking;Navigation;Detectors;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341778&isnumber=10341342

Z. Zhou, J. Lu, Y. Zeng, H. Xu and L. Zhang, "SUIT: Learning Significance-Guided Information for 3D Temporal Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9399-9406, doi: 10.1109/IROS55552.2023.10342350.Abstract: 3D object detection from LiDAR point cloud is of critical importance for autonomous driving and robotics. While sequential point cloud has the potential to enhance 3D perception through temporal information, utilizing these temporal features effectively and efficiently remains a challenging problem. Based on the observation that the foreground information is sparsely distributed in LiDAR scenes, we believe sufficient knowledge can be provided by sparse format rather than dense maps. To this end, we propose to learn Significance-gUided Information for 3D Temporal detection (SUIT), which simplifies temporal information as sparse features for information fusion across frames. Specifically, we first introduce a significant sampling mechanism that extracts information-rich yet sparse features based on predicted object centroids. On top of that, we present an explicit geometric transformation learning technique, which learns the object-centric transformations among sparse features across frames. We evaluate our method on large-scale nuScenes and Waymo dataset, where our SUIT not only significantly reduces the memory and computation cost of temporal fusion, but also performs well over the state-of-the-art baselines. keywords: {Point cloud compression;Three-dimensional displays;Laser radar;Costs;Object detection;Detectors;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342350&isnumber=10341342

K. Y. Chee, T. C. Silva, M. A. Hsieh and G. J. Pappas, "Enhancing Sample Efficiency and Uncertainty Compensation in Learning-Based Model Predictive Control for Aerial Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9435-9441, doi: 10.1109/IROS55552.2023.10341774.Abstract: The recent increase in data availability and reliability has led to a surge in the development of learning-based model predictive control (MPC) frameworks for robot systems. Despite attaining substantial performance improvements over their non-learning counterparts, many of these frameworks rely on an offline learning procedure to synthesize a dynamics model. This implies that uncertainties encountered by the robot during deployment are not accounted for in the learning process. On the other hand, learning-based MPC methods that learn dynamics models online are computationally expensive and often require a significant amount of data. To alleviate these shortcomings, we propose a novel learning-enhanced MPC framework that incorporates components from C1 adaptive control into learning-based MPC. This integration enables the accurate compensation of both matched and unmatched uncertainties in a sample-efficient way, enhancing the control performance during deployment. In our proposed framework, we present two variants and apply them to the control of a quadrotor system. Through simulations and physical experiments, we demonstrate that the proposed framework not only allows the synthesis of an accurate dynamics model on-the-fly, but also significantly improves the closed-loop control performance under a wide range of spatio-temporal uncertainties. keywords: {Adaptation models;Uncertainty;Fuses;Computational modeling;Reliability;Adaptive control;Surges},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341774&isnumber=10341342

A. Joglekar et al., "Data-Driven Modeling and Experimental Validation of Autonomous Vehicles Using Koopman Operator," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9442-9447, doi: 10.1109/IROS55552.2023.10341797.Abstract: This paper presents a data-driven framework to discover underlying dynamics on a scaled F1TENTH vehicle using the Koopman operator linear predictor. Traditionally, a range of white, gray, or black-box models are used to develop controllers for vehicle path tracking. However, these models are constrained to either linearized operational domains, unable to handle significant variability or lose explainability through end-2-end operational settings. The Koopman Extended Dynamic Mode Decomposition (EDMD) linear predictor seeks to utilize data-driven model learning whilst providing benefits like explainability, model analysis and the ability to utilize linear model-based control techniques. Consider a trajectory-tracking problem for our scaled vehicle platform. We collect pose measurements of our F1TENTH car undergoing standard vehicle dynamics benchmark maneuvers with an OptiTrack indoor localization system. Utilizing these uniformly spaced temporal snapshots of the states and control inputs, a data-driven Koopman EDMD model is identified. This model serves as a linear predictor for state propagation, upon which an MPC feedback law is designed to enable trajectory tracking. The prediction and control capabilities of our framework are highlighted through real-time deployment on our scaled vehicle. keywords: {Location awareness;Analytical models;Trajectory tracking;Computational modeling;Predictive models;Vehicle dynamics;Standards},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341797&isnumber=10341342

G. P. Kontoudis and M. Otte, "Adaptive Exploration-Exploitation Active Learning of Gaussian Processes," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9448-9455, doi: 10.1109/IROS55552.2023.10342130.Abstract: Active Learning of Gaussian process (GP) surrogates is an efficient way to model unknown environments in various applications. In this paper, we propose an adaptive exploration-exploitation active learning method (ALX) that can be executed rapidly to facilitate real-time decision making. For the exploration phase, we formulate an acquisition function that maximizes the approximated, expected Fisher information. For the exploitation phase, we employ a closed-form acquisition function that maximizes the total expected variance reduction of the search space. The determination of each phase is established with an exploration condition that measures the predictive accuracy of GP surrogates. Extensive numerical experiments in multiple input spaces validate the efficiency of our method. keywords: {Learning systems;Adaptation models;Phase measurement;Decision making;Focusing;Gaussian processes;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342130&isnumber=10341342

M. Kotb, C. Weber and S. Wermter, "Sample-Efficient Real-Time Planning with Curiosity Cross-Entropy Method and Contrastive Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9456-9463, doi: 10.1109/IROS55552.2023.10342018.Abstract: Model-based reinforcement learning (MBRL) with real-time planning has shown great potential in locomotion and manipulation control tasks. However, the existing planning methods, such as the Cross-Entropy Method (CEM), do not scale well to complex high-dimensional environments. One of the key reasons for underperformance is the lack of exploration, as these planning methods only aim to maximize the cumulative extrinsic reward over the planning horizon. Furthermore, planning inside the compact latent space in the absence of observations makes it challenging to use curiosity-based intrinsic motivation. We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for encouraging exploration via curiosity. Our proposed method maximizes the sum of state-action $Q$ values over the planning horizon, in which these $Q$ values estimate the future extrinsic and intrinsic reward, hence encouraging to reach novel observations. In addition, our model uses contrastive representation learning to efficiently learn latent representations. Experiments on image-based continuous control tasks from the DeepMind Control suite show that CCEM is by a large margin more sample-efficient than previous MBRL algorithms and compares favorably with the best model-free RL methods. keywords: {Representation learning;Reinforcement learning;Real-time systems;Planning;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342018&isnumber=10341342

D. Boase, W. Gueaieb and M. S. Miah, "Underactuated MIMO Airship Control Based on Online Data-Driven Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9464-9471, doi: 10.1109/IROS55552.2023.10341752.Abstract: In this work, a novel online model-free controller for an underactuated dirigible is developed based on reinforcement learning and optimal control theory. A reinforcement learning structure is used while overcoming the dependence of the value function on future values by introducing a neural network that is adapted using input-output data. The suboptimal critic neural network is structured such that optimality is guaranteed over the interval from which the data is valid. The system performance is validated using a highly realistic physics engine, Gazebo, with the robot operating system (ROS) interface and the results are compared to the performance of a model-based controller specifically designed to control the airship model. It is emphasized that the proposed formulation does not leverage any knowledge of vehicle dynamics and thus is considered a vehicle agnostic control strategy. keywords: {Training;Atmospheric modeling;System performance;Operating systems;Neural networks;Optimal control;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341752&isnumber=10341342

Z. Zhang, Z. Zhou, H. Wang, Z. Zhang, H. Huang and Q. Cao, "Grasp Stability Assessment Through Attention-Guided Cross-Modality Fusion and Transfer Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9472-9479, doi: 10.1109/IROS55552.2023.10342411.Abstract: Extensive research has been conducted on assessing grasp stability, a crucial prerequisite for achieving optimal grasping strategies, including the minimum force grasping policy. However, existing works employ basic feature-level fusion techniques to combine visual and tactile modalities, resulting in the inadequate utilization of complementary information and the inability to model interactions between unimodal features. This work proposes an attention-guided cross-modality fusion architecture to comprehensively integrate visual and tactile features. This model mainly comprises convolutional neural networks (CNNs), self-attention, and cross-attention mechanisms. In addition, most existing methods collect datasets from real-world systems, which is time-consuming and high-cost, and the datasets collected are comparatively limited in size. This work establishes a robotic grasping system through physics simulation to collect a multimodal dataset. To address the sim-to-real transfer gap, we propose a migration strategy encompassing domain randomization and domain adaptation techniques. The experimental results demonstrate that the proposed fusion framework achieves markedly enhanced prediction performance (approximately 10%) compared to other baselines. Moreover, our findings suggest that the trained model can be reliably transferred to real robotic systems, indicating its potential to address real-world challenges. keywords: {Visualization;Adaptation models;Force;Transfer learning;Grasping;Reinforcement learning;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342411&isnumber=10341342

H. Nguyen, S. Katt, Y. Xiao and C. Amato, "On-Robot Bayesian Reinforcement Learning for POMDPs," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9480-9487, doi: 10.1109/IROS55552.2023.10342114.Abstract: Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach can, for example, utilize typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment. We empirically demonstrate its efficiency by performing on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes. A video of learned policies is at https://youtu.be/H9xp60ngOes. keywords: {Uncertainty;Monte Carlo methods;Shape;Heuristic algorithms;Reinforcement learning;Robot learning;Bayes methods},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342114&isnumber=10341342

