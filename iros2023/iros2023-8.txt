M. Nakamura, J. Svegliato, S. B. Nashed, S. Zilberstein and S. Russell, "Formal Composition of Robotic Systems as Contract Programs," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6727-6732, doi: 10.1109/IROS55552.2023.10342341.Abstract: Robotic systems are often composed of modular algorithms that each perform a specific function within a larger architecture, ranging from state estimation and task planning to trajectory optimization and object recognition. Existing work for specifying these systems as a formal composition of contract algorithms has limited expressiveness compared to the variety of sophisticated architectures that are commonly used in practice. Therefore, in this paper, we (1) propose a novel metareasoning framework for formally composing robotic systems as a contract program with programming constructs for functional, conditional, and looping semantics and (2) introduce a recursive hill climbing algorithm that finds a locally optimal time allocation of a contract program. In our experiments, we demonstrate that our approach outperforms baseline techniques in a simulated pick-and-place robot domain. keywords: {Semantics;Programming;Planning;Resource management;Object recognition;Task analysis;State estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342341&isnumber=10341342

A. Shats, M. Amir and N. Agmon, "Competitive Ant Coverage: The Value of Pursuit," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6733-6740, doi: 10.1109/IROS55552.2023.10342063.Abstract: This paper studies the problem of Competitive Ant Coverage, in which two ant-like robots with very limited capabilities in terms of sensing range, computational power, and knowledge of the world compete in an area coverage task. We examine two variants of the problem that differ in the robot's objective: either being the First to Cover a Cell (FCC), or being the Last to Cover a Cell (LCC). Each robot's goal is to acquire (by visiting first or last, respectively) more cells than the opposing robot, and by that win the game. We examine the problem both theoretically and empirically, and show that the main strategy for dominance revolves around the ability to pursue: in LCC, we wish to pursue the opposing robot, whereas in FCC, we wish to create a scenario wherein the opposing robot pursues us. We find that this ability relies more heavily on knowledge of the opponent's strategy than on the robot's sensing capabilities. Moreover, given the robot's limited capabilities, we find that this knowledge-gap cannot be easily mitigated by learning. keywords: {Heuristic algorithms;FCC;Games;Robot sensing systems;Sensors;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342063&isnumber=10341342

C. Medina-Sánchez, S. Janzon, M. Zella, J. Capitán and P. J. Marrón, "Human-Aware Navigation in Crowded Environments Using Adaptive Proxemic Area and Group Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6741-6748, doi: 10.1109/IROS55552.2023.10342385.Abstract: Navigation is an essential task for social robots. However, certain rules must be followed to allow them to move without causing distraction or discomfort to people. Considering that the context surrounding robots and persons affects the expected behavior, this work defines a social area around a person that adapts to the real situation. In addition, the social context of a person is extended to identify groups of people, which the robot should take into account while navigating. With this understanding of the surrounding of the robot together with the ability to predict the trajectory of individuals as well as groups, the proposed solution not only effectively addresses collision avoidance while promoting socially acceptable behavior but also outperforms the majority of recent works in terms of accuracy. Furthermore, a dedicated policy is introduced to react to social navigation conflicts. The evaluation performed in a simulated environment shows that the computation of our proposed solution is at least 8 times faster than the best state-of-the-art approach while preserving comparable social conduct. Also, the results of realistic experiments performed using Gazebo and a real robot are reported. keywords: {Costs;Navigation;Social robots;Trajectory;Behavioral sciences;Resource management;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342385&isnumber=10341342

T. Guo and J. Yu, "Efficient Heuristics for Multi-Robot Path Planning in Crowded Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6749-6756, doi: 10.1109/IROS55552.2023.10341800.Abstract: Optimal Multi-Robot Path Planning (MRPP) has garnered significant attention due to its many applications in domains including warehouse automation, transportation, and swarm robotics. Current MRPP solvers can be divided into reduction-based, search-based, and rule-based categories, each with their strengths and limitations. Regardless of the methodology, however, the issue of handling dense MRPP instances remains a significant challenge, where existing approaches generally demonstrate a dichotomy regarding solution optimality and efficiency. This study seeks to bridge the gap in optimal MRPP resolution for dense, highly-entangled scenarios, with potential applications to high-density storage systems and traffic congestion control. Toward that goal, we analyze the behaviors of SOTA MRPP algorithms in dense settings and develop two hybrid algorithms leveraging the strengths of existing SOTA algorithms: DCBS (database-accelerated enhanced conflict-based search) and SCBS (sparsified enhanced conflict-based search). Experimental validations demonstrate that DCBS and SCBS deliver a significant reduction in computational time compared to existing bounded-suboptimal methods and improve solution quality compared to existing rule-based methods, achieving a desirable balance between computational efficiency and solution optimality. As a result, DCBS and SCBS are particularly suitable for quickly computing good-quality solutions for multi-robot routing in dense settings. Simulation video https://youtu.be/dZxMPUr7Bqg Upon the publication of the manuscript source code and data will be released at https://github.com/arc-l/dcbs keywords: {Automation;Source coding;Transportation;Swarm robotics;Routing;Path planning;Computational efficiency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341800&isnumber=10341342

J. de Heuvel, N. Corral, B. Kreis, J. Conradi, A. Driemel and M. Bennewitz, "Learning Depth Vision-Based Personalized Robot Navigation From Dynamic Demonstrations in Virtual Reality," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6757-6764, doi: 10.1109/IROS55552.2023.10341370.Abstract: For the best human-robot interaction experience, the robot's navigation policy should take into account personal preferences of the user. In this paper, we present a learning framework complemented by a perception pipeline to train a depth vision-based, personalized navigation controller from user demonstrations. Our virtual reality interface enables the demonstration of robot navigation trajectories under motion of the user for dynamic interaction scenarios. The novel perception pipeline enrolls a variational autoencoder in combination with a motion predictor. It compresses the perceived depth images to a latent state representation to enable efficient reasoning of the learning agent about the robot's dynamic environment. In a detailed analysis and ablation study, we evaluate different configurations of the perception pipeline. To further quantify the navigation controller's quality of personalization, we develop and apply a novel metric to measure preference reflection based on the Frechet Distance. We discuss the robot's navigation performance in various virtual scenes and demonstrate the first personalized robot navigation controller that solely relies on depth images. A supplemental video highlighting our approach is available online11Full video: hrl.uni-bonn.de/publications/deheuve123iros_learning.mp4. keywords: {Measurement;Navigation;Pipelines;Dynamics;Virtual reality;Vision sensors;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341370&isnumber=10341342

S. Poddar, C. Mavrogiannis and S. S. Srinivasa, "From Crowd Motion Prediction to Robot Navigation in Crowds," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6765-6772, doi: 10.1109/IROS55552.2023.10341464.Abstract: We focus on robot navigation in crowded environments. To navigate safely and efficiently within crowds, robots need models for crowd motion prediction. Building such models is hard due to the high dimensionality of multiagent domains and the challenge of collecting or simulating interaction-rich crowd-robot demonstrations. While there has been important progress on models for offline pedestrian motion forecasting, transferring their performance on real robots is nontrivial due to close interaction settings and novelty effects on users. In this paper, we investigate the utility of a recent state-of-the-art motion prediction model (S-GAN) for crowd navigation tasks. We incorporate this model into a model predictive controller (MPC) and deploy it on a self-balancing robot which we subject to a diverse range of crowd behaviors in the lab. We demonstrate that while S-GAN motion prediction accuracy transfers to the real world, its value is not reflected on navigation performance, measured with respect to safety and efficiency; in fact, the MPC performs indistinguishably even when using a simple constant-velocity prediction model, suggesting that substantial model improvements might be needed to yield significant gains for crowd navigation tasks. Footage from our experiments can be found at https://youtu.be/mzFiXgSKsZ0. keywords: {Pedestrians;Navigation;Buildings;Predictive models;Gain measurement;Safety;Motion measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341464&isnumber=10341342

K. Kedia, P. Dan and S. Choudhury, "A Game-Theoretic Framework for Joint Forecasting and Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6773-6778, doi: 10.1109/IROS55552.2023.10341265.Abstract: Planning safe robot motions in the presence of humans requires reliable forecasts of future human motion. However, simply predicting the most likely motion from prior interactions does not guarantee safety. Such forecasts fail to model the long tail of possible events, which are rarely observed in limited datasets. On the other hand, planning for worst-case motions leads to overtly conservative behavior and a “frozen robot”. Instead, we aim to learn forecasts that predict counterfactuals that humans guard against. We propose a novel game-theoretic framework for joint planning and forecasting with the payoff being the performance of the planner against the demonstrator, and present practical algorithms to train models in an end-to-end fashion. We demonstrate that our proposed algorithm results in safer plans in a crowd navigation simulator and real-world datasets of pedestrian motion. We release our code at https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning. keywords: {Robot motion;Pedestrians;Navigation;Tail;Predictive models;Prediction algorithms;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341265&isnumber=10341342

D. Pushp, J. Xu and L. Liu, "Coordination of Bounded Rational Drones Through Informed Prior Policy," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6779-6786, doi: 10.1109/IROS55552.2023.10342006.Abstract: Biological agents, such as humans and animals, are capable of making decisions out of a very large number of choices in a limited time. They can do so because they use their prior knowledge to find a solution that is not necessarily optimal but good enough for the given task. In this work, we study the motion coordination of multiple drones under the above-mentioned paradigm, Bounded Rationality (BR), to achieve cooperative motion planning tasks. Specifically, we design a prior policy that provides useful goal-directed navigation heuristics in familiar environments and is adaptive in unfamiliar ones via Reinforcement Learning augmented with an environment-dependent exploration noise. Integrating this prior policy in the game-theoretic bounded rationality framework allows agents to quickly make decisions in a group considering other agents' computational constraints. Our investigation assures that agents with a well-informed prior policy increase the efficiency of the collective decision-making capability of the group. We have conducted rigorous experiments in simulation and in the real world to demonstrate that the ability of informed agents to navigate to the goal safely can guide the group to coordinate efficiently under the BR framework. keywords: {Uncertainty;Navigation;Decision making;Reinforcement learning;Robustness;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342006&isnumber=10341342

S. Buckeridge, P. Carreno-Medrano, A. Cosgun, E. Croft and W. P. Chan, "Mapless Urban Robot Navigation by Following Pedestrians," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6787-6792, doi: 10.1109/IROS55552.2023.10341843.Abstract: Navigating effectively and safely in unknown urban environments is a crucial ability for service robot applications such as last-mile package delivery. To reach the entrance of its target destination, the robot must make informed local and global path planning decisions. We present a mapless global planning strategy based on pedestrian following. Our method allows the robot to exploit natural routes taken by surrounding pedestrians to make informed and efficient path planning decisions for reaching its goal. The algorithm also includes a recovery system to assist the robot when insufficient progress is made (i.e. robot stuck in dead end). Once the robot is within the vicinity of the target building, a wall following behaviour is used to reach the entrance of the target building. Simulated experiments and a proof-of-concept demonstration on a real robot were shown to validate the approach. keywords: {Pedestrians;Navigation;Service robots;Buildings;Urban areas;Path planning;Product delivery},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341843&isnumber=10341342

S. Aguilera and S. Hutchinson, "Control of Cart-Like Nonholonomic Systems Using a Mobile Manipulator," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6801-6808, doi: 10.1109/IROS55552.2023.10342088.Abstract: This work focuses on the capability of Mobile Manipulators to effectively control and maneuver cart-like non-holonomic systems. These cart-like systems are passive-wheeled objects with nonholonomic constraints with varying inertial parameters. We derive the dynamic equations of the cart-like system using a constrained Euler-Lagrange formulation and propose a Linear Quadratic Regulator controller to move the cart along a desired trajectory using external forces (applied by the MM) at a given contact point. For the MM, we present a control architecture to i) control the mobile base to keep the cart inside the workspace of the manipulator and ii) a control Lyapunov function formulation to control the manipulator in torque control, while decoupling the motion of the base from the arm and applying the required wrench onto the object. We validate our approach experimentally, using a MM to push a shopping cart and track desired trajectories. These experiments show the accuracy of the control architecture to track the desired trajectories for carts with different inertial parameters and improve the controllability of the system by changing the contact point on the cart. keywords: {Torque;Regulators;Tracking;Dynamics;Torque control;Computer architecture;Mathematical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342088&isnumber=10341342

A. H. Li, P. Culbertson, J. W. Burdick and A. D. Ames, "FRoGGeR: Fast Robust Grasp Generation via the Min-Weight Metric," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6809-6816, doi: 10.1109/IROS55552.2023.10341806.Abstract: Many approaches to grasp synthesis optimize analytic quality metrics that measure grasp robustness based on finger placements and local surface geometry. However, generating feasible dexterous grasps by optimizing these metrics is slow, often taking minutes. To address this issue, this paper presents FRoGGeR: a method that quickly generates robust precision grasps using the min-weight metric, a novel, almost-everywhere differentiable approximation of the classical $\epsilon$ grasp metric. The min-weight metric is simple and interpretable, provides a reasonable measure of grasp robustness, and admits numerically efficient gradients for smooth optimization. We leverage these properties to rapidly synthesize collision-free robust grasps-typically in less than a second. FRoGGeR can refine the candidate grasps generated by other methods (heuristic, data-driven, etc.) and is compatible with many object representations (SDFs, meshes, etc.), We study FRoGGeR's performance on over 40 objects drawn from the YCB dataset, outperforming a competitive baseline in computation time, feasibility rate of grasp synthesis, and picking success in simulation. We conclude that FRoGGeR is fast: it has a median synthesis time of 0.834s over hundreds of experiments. keywords: {Measurement;Geometry;Fingers;Robustness;Optimization;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341806&isnumber=10341342

C. Chen, S. Yan, M. Yuan, C. Tay, D. Choi and Q. Dan Le, "A Minimal Collision Strategy of Synergy Between Pushing and Grasping for Large Clusters of Objects," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6817-6822, doi: 10.1109/IROS55552.2023.10341452.Abstract: Grasping and moving objects in a large cluster is a common real scenario. In such scenarios, tens of objects are adjacent to each other, even stacked layer by layer, so that simple grasp would not work due to obstruction. In this paper, we propose a well-designed strategy to use synergy of pushing and grasping to automatically push and grasp objects in a large tightly packed cluster of objects. Our strategy is to detect and grasp isolated graspable objects first before other actions. We then use a smart strategy that pushes objects at the narrowest edge of the clusters. For push action, the robot pushes the edge at the perpendicular direction relative to the cluster, thus improving the performance of isolation and minimizing collisions. We have conducted experiments in both simulation and real-world environments with more than 20 cluttered objects and demonstrated that our solution outperforms existing deep learning based methods, especially in challenging cases, and achieves significantly higher completion rate, grasp success rate, picked rate and efficiency. keywords: {Deep learning;Image edge detection;Grasping;Collision avoidance;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341452&isnumber=10341342

H. Zwirnmann, D. Knobbe, U. Culha and S. Haddadin, "Towards Flexible Biolaboratory Automation: Container Taxonomy-Based, 3D-Printed Gripper Fingers*," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6823-6830, doi: 10.1109/IROS55552.2023.10342218.Abstract: Automation in the life science research laboratory is a paradigm that has gained increasing relevance in recent years. Current robotic solutions often have a limited scope, which reduces their acceptance and prevents the realization of complex workflows. The transport and manipulation of laboratory supplies with a robot is a particular case where this limitation manifests. In this paper, we deduce a taxonomy of biolaboratory liquid containers that clarifies the need for a flexible grasping solution. Using the taxonomy as a guideline, we design fingers for a parallel robotic gripper which are developed with a monolithic dual-extrusion 3D print that integrates rigid and soft materials to optimize gripping properties. We design fine-tuned fingertips that provide stable grasps of the containers in question. A simple actuation system and a low weight are maintained by adopting a passive compliant mechanism. The ability to resist chemicals and high temperatures and the integration with a tool exchange system render the fingers usable for daily laboratory use and complex workflows. We present the task suitability of the fingers in experiments that show the wide range of vessels that can be handled as well as their tolerance against displacements and their grasp stability. keywords: {Three-dimensional displays;Automation;Temperature;Taxonomy;Grasping;Containers;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342218&isnumber=10341342

F. Guo, S. Xie, D. Wang, C. Fang, J. Zou and D. Song, "A Pretouch Perception Algorithm for Object Material and Structure Mapping to Assist Grasp and Manipulation Using a DMDSM Sensor," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6831-6838, doi: 10.1109/IROS55552.2023.10341560.Abstract: We report a new material and structure mapping (MSM) algorithm to assist robotic grasping and manipulation. Building on our new sensor development, the algorithm has four main components: 1) detection of time-of-flight (ToF) durations for the dual modalities of optoacoustic (OA) and pulse-echo ultrasound (US), 2) contour reconstruction by fusing OA and US signals, 3) local noise filtering by checking local consistency of material and structure label (MSL), and 4) medium boundary searching that identifies class boundaries through two-staged clustering and boundary establishment using support vector machine (SVM) hyperplanes. We have implemented our algorithm and tested it with multiple common household items. The experimental results have successfully validated our algorithm design which shows that the average error of contour reconstruction is 0.05 mm and the true positive rate of MSL is over 98%. keywords: {Support vector machines;Ultrasonic imaging;Filtering;Buildings;Clustering algorithms;Grasping;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341560&isnumber=10341342

Y. Tu, J. Jiang, S. Li, N. Hendrich, M. Li and J. Zhang, "PoseFusion: Robust Object-in-Hand Pose Estimation with SelectLSTM," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6839-6846, doi: 10.1109/IROS55552.2023.10341688.Abstract: Accurate estimation of the relative pose between an object and a robot hand is critical for many manipulation tasks. However, most of the existing object-in-hand pose datasets use two-finger grippers and also assume that the object remains fixed in the hand without any relative movements, which is not representative of real-world scenarios. To address this issue, a 6D object-in-hand pose dataset is proposed using a teleoperation method with an anthropomorphic Shadow Dexterous hand. Our dataset comprises RGB-D images, proprioception and tactile data, covering diverse grasping poses, finger contact states, and object occlusions. To overcome the significant hand occlusion and limited tactile sensor contact in real-world scenarios, we propose PoseFusion, a hybrid multi-modal fusion approach that integrates the information from visual and tactile perception channels. PoseFusion generates three candidate object poses from three estimators (tactile only, visual only, and visuo-tactile fusion), which are then filtered by a SelectLSTM network to select the optimal pose, avoiding inferior fusion poses resulting from modality collapse. Extensive experiments demonstrate the robustness and advantages of our framework. All data and codes are available on the project website: https://elevenjiang1.github.io/ObjectlnHand-Dataset/. keywords: {Training;Visualization;Pose estimation;Tactile sensors;Grasping;Information filters;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341688&isnumber=10341342

T. Yang et al., "MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6847-6852, doi: 10.1109/IROS55552.2023.10342371.Abstract: In this paper, we present a novel method for mobile manipulators to perform multiple contact-rich manipulation tasks. While learning-based methods have the potential to generate actions in an end-to-end manner, they often suffer from insufficient action accuracy and robustness against noise. On the other hand, classical control-based methods can enhance system robustness, but at the cost of extensive parameter tuning. To address these challenges, we present MOMA-Force, a visual-force imitation method that seamlessly combines representation learning for perception, imitation learning for complex motion generation, and admittance whole-body control for system robustness and controllability. MOMA-Force enables a mobile manipulator to learn multiple complex contact-rich tasks with high success rates and small contact forces. In a real household setting, our method outperforms baseline methods in terms of task success rates. Moreover, our method achieves smaller contact forces and smaller force variances compared to baseline methods without force imitation. Overall, we offer a promising approach for efficient and robust mobile manipulation in the real world. Videos and more details can be found on https://visual-force-imitation.github.io. keywords: {Learning systems;Visualization;Force;Manipulators;Robustness;Trajectory;Admittance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342371&isnumber=10341342

A. Patankar, K. Phi, D. Mahalingam, N. Chakraborty and I. Ramakrishnan, "Task-Oriented Grasping with Point Cloud Representation of Objects," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6853-6860, doi: 10.1109/IROS55552.2023.10342318.Abstract: In this paper, we study the problem of task-oriented grasp synthesis from partial point cloud data using an eye-in-hand camera configuration. In task-oriented grasp synthesis, a grasp has to be selected so that the object is not lost during manipulation, and it is also ensured that adequate force/moment can be applied to perform the task. We formalize the notion of a gross manipulation task as a constant screw motion (or a sequence of constant screw motions) to be applied to the object after grasping. Using this notion of task, and a corresponding grasp quality metric developed in our prior work, we use a neural network to approximate a function for predicting the grasp quality metric on a cuboid shape. We show that by using a bounding box obtained from the partial point cloud of an object, and the grasp quality metric mentioned above, we can generate a good grasping region on the bounding box that can be used to compute an antipodal grasp on the actual object. Our algorithm does not use any manually labeled data or grasping simulator, thus making it very efficient to implement and integrate with screw linear interpolation-based motion planners. We present simulation as well as experimental results that show the effectiveness of our approach. Website: https://irsl-sbu.github.io/Task-Oriented-Grasping-from-Point-Cloud-Representation/. keywords: {Point cloud compression;Measurement;Shape;Neural networks;Grasping;Fasteners;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342318&isnumber=10341342

T. Taunyazov, H. Zhang, J. P. Eala, N. Zhao and H. Soh, "Refining 6-DoF Grasps with Context-Specific Classifiers," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6861-6867, doi: 10.1109/IROS55552.2023.10341671.Abstract: In this work, we present GraspFlow, a refinement approach for generating context-specific grasps. We formulate the problem of grasp synthesis as a sampling problem: we seek to sample from a context-conditioned probability distribution of successful grasps. However, this target distribution is unknown. As a solution, we devise a discriminator gradient-flow method to evolve grasps obtained from a simpler distribution in a manner that mimics sampling from the desired target distribution. Unlike existing approaches, GraspFlow is modular, allowing grasps that satisfy multiple criteria to be obtained simply by incorporating the relevant discriminators. It is also simple to implement, requiring minimal code given existing auto-differentiation libraries and suitable discriminators. Experiments show that GraspFlow generates stable and executable grasps on a real-world Panda robot for a diverse range of objects. In particular, in 60 trials on 20 different household objects, the first attempted grasp was successful 94% of the time, and 100% grasp success was achieved by the second grasp. Moreover, incorporating a functional discriminator for robot-human handover improved the functional aspect of the grasp by up to 33%. keywords: {Codes;Refining;MIMICs;Handover;Probability distribution;Libraries;6-DOF},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341671&isnumber=10341342

Y. Su et al., "Sequential Manipulation Planning for Over-Actuated Unmanned Aerial Manipulators," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6905-6911, doi: 10.1109/IROS55552.2023.10341441.Abstract: We investigate the sequential manipulation planning problem for unmanned aerial manipulators (UAMs). Unlike prior work that primarily focuses on one-step manipulation tasks, sequential manipulations require coordinated motions of a UAM's floating base, the manipulator, and the object being manipulated, entailing a unified kinematics and dynamics model for motion planning under designated constraints. By leveraging a virtual kinematic chain (VKC)-based motion planning framework that consolidates components' kinematics into one chain, the sequential manipulation task of a UAM can be planned as a whole, yielding more coordinated motions. Integrating the kinematics and dynamics models with a hierarchical control framework, we demonstrate, for the first time, an over-actuated UAM achieves a series of new sequential manipulation capabilities in both simulation and experiment. keywords: {Wireless communication;Tracking;Toy manufacturing industry;Dynamics;Kinematics;Planning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341441&isnumber=10341342

C. Gabellieri, M. Tognon, D. Sanalitro and A. Franchi, "Force-Based Pose Regulation of a Cable-Suspended Load Using UAVs with Force Bias," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6920-6926, doi: 10.1109/IROS55552.2023.10342240.Abstract: This work studies how force measurement/estimation biases affect the force-based cooperative manipulation of a beam-like load suspended with cables by two aerial robots. Indeed, force biases are especially relevant in a force-based manipulation scenario in which direct communication is not relied upon. First, we compute the equilibrium configurations of the system. Then, we show that inducing an internal force in the load augments the robustness of the load attitude error and its sensitivity to force-bias variations. Eventually, we propose a method for zeroing the load position error. The results are validated through numerical simulations and experiments. keywords: {Sensitivity;Force measurement;Force;Robot sensing systems;Numerical simulation;Autonomous aerial vehicles;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342240&isnumber=10341342

Z. Zheng et al., "Roller-Quadrotor: A Novel Hybrid Terrestrial/Aerial Quadrotor with Unicycle-Driven and Rotor-Assisted Turning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6927-6934, doi: 10.1109/IROS55552.2023.10341703.Abstract: The Roller-Quadrotor is a novel quadrotor that combines the maneuverability of aerial drones with the endurance of ground vehicles. This work focuses on the design, modeling, and experimental validation of the Roller-Quadrotor. Flight capabilities are achieved through a quadrotor config-uration, with four thrust-providing actuators. Additionally, rolling motion is facilitated by a unicycle-driven and rotor-assisted turning structure. By utilizing terrestrial locomotion, the vehicle can overcome rolling and turning resistance, thereby conserving energy compared to its flight mode. This innovative approach not only tackles the inherent challenges of traditional rotorcraft but also enables the vehicle to roll through narrow gaps and overcome obstacles by taking advantage of its aerial mobility. We develop comprehensive models and controllers for the Roller-Quadrotor and validate their performance through experiments. The results demonstrate its seamless transition between aerial and terrestrial locomotion, as well as its ability to safely roll through gaps half the size of its diameter. Moreover, the terrestrial range of the vehicle is approximately 2.8 times greater, while the operating time is about 41.2 times longer compared to its aerial capabilities. These findings underscore the feasibility and effectiveness of the proposed structure and control mechanisms for efficient rolling through challenging terrains while conserving energy. keywords: {Resistance;Actuators;Energy conservation;Rotors;Turning;Land vehicles;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341703&isnumber=10341342

H. Li et al., "HALO: A Safe, Coaxial, and Dual-Ducted UAV Without Servo," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6935-6941, doi: 10.1109/IROS55552.2023.10341923.Abstract: This paper presents a novel uncrewed aerial vehicle (UAV) design named HALO, which stands for “harmless aerial limber robot”. HALO uses a swashplateless mechanism to generate a moment for pitch and roll control without requiring additional actuators such as servo, reducing the number of components needed for control and enhancing reliability. Its reduced weight and number of actuators improve payload capacity and maneuverability. Meanwhile, HALO's coaxial duct design improves safety and aerodynamic efficiency. Experimental tests, including figure-of-eight trajectory tracking, wind gust and stick poking disturbances, hover efficiency comparison, and actual flight with collision is conducted to confirm HALO's robustness and exceptional safety characteristics, suggesting it as a promising design for various applications. keywords: {Actuators;Three-dimensional displays;Trajectory tracking;Ducts;Autonomous aerial vehicles;Robustness;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341923&isnumber=10341342

E. Cuniato, I. Geles, W. Zhang, O. Andersson, M. Tognon and R. Siegwart, "Learning to Open Doors with an Aerial Manipulator," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6942-6948, doi: 10.1109/IROS55552.2023.10342289.Abstract: The field of aerial manipulation has seen rapid advances, transitioning from push-and-slide tasks to interaction with articulated objects. The motion trajectory of these complex actions is usually hand-crafted or a result of online optimization methods like Model Predictive Control (MPC) or Model Predictive Path Integral (MPPI) control. However, these methods rely on heuristics or model simplifications to efficiently run on onboard hardware, limiting their robustness, and making them sensitive to disturbances and differences between the real environment and its model. In this work, we propose a Reinforcement Learning (RL) approach to learn reactive motion behaviors for a manipulation task while producing policies that are robust to disturbances and modeling errors. Specifically, we train a policy to perform a door-opening task with an Omnidirectional Micro Aerial Vehicle (OMAV). The policy is trained in a physics simulator and shown in the real world, where it is able to generalize also to door closing tasks never seen in training. We also compare our method against a state-of-the-art MPPI solution in simulation, showing a considerable increase in robustness and speed. keywords: {Training;Optimization methods;Reinforcement learning;Predictive models;Robustness;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342289&isnumber=10341342

G. Perrotta et al., "Planning and Control for a Dynamic Morphing-Wing UAV Using a Vortex Particle Model," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6949-6954, doi: 10.1109/IROS55552.2023.10342191.Abstract: Achieving precise, highly-dynamic maneuvers with Unmanned Aerial Vehicles (UAVs) is a major challenge due to the complexity of the associated aerodynamics. In particular, unsteady effectsas might be experienced in post-stall regimes or during sudden vehicle morphing-can have an adverse impact on the performance of modern flight control systems. In this paper, we present a vortex particle model and associated model-based controller capable of reasoning about the unsteady aerodynamics during aggressive maneuvers. We evaluate our approach in hardware on a morphing-wing UAV executing post-stall perching maneuvers. Our results show that the use of the unsteady aerodynamics model improves performance during both fixed-wing and dynamic-wing perching, while the use of wing-morphing planned with quasi-steady aerodynamics results in reduced performance. While the focus of this paper is a pre-computed control policy, we believe that, with sufficient computational resources, our approach could enable online planning in the future. keywords: {Computational modeling;Aerodynamics;Autonomous aerial vehicles;Robot sensing systems;Real-time systems;Planning;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342191&isnumber=10341342

T. Kominami, Z. Liang, R. R. Martinez, H. Paul and K. Shimonomura, "Physical Contact with Wall using a Multirotor UAV Equipped with Add-On Thruster for Inspection Work," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6955-6961, doi: 10.1109/IROS55552.2023.10341576.Abstract: Inspection and maintenance work at heights car-ries significant risks and is time consuming for human workers. Therefore, aerial manipulators are expected to replace these tasks. This paper presents a multirotor UAV equipped with a single horizontal thruster. This minimal configuration en-ables physical contact while keeping the airframe's attitude horizontal for non-destructive inspection work on vertical wall surfaces. The thrust required to move forward and backward during a physical contact task is independent of the thrust required for hovering, simplifying control of the UAV. Utilizing onboard sensors, the UAV automatically maintains a forward-facing posture against the wall, initiates and sustains contact, and disengages when necessary. Additionally, the UAV in this study incorporates an ultrasonic thickness measurement device, allowing for the verification of automated measurements while in flight. keywords: {Attitude control;Ultrasonic variables measurement;Inspection;Position measurement;Maintenance engineering;Autonomous aerial vehicles;Acoustics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341576&isnumber=10341342

M. Cao, X. Xu, S. Yuan, K. Cao, K. Liu and L. Xie, "DoubleBee: A Hybrid Aerial-Ground Robot with Two Active Wheels," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6962-6969, doi: 10.1109/IROS55552.2023.10341984.Abstract: In this paper, we present the dynamic model and control of DoubleBee, a novel hybrid aerial-ground vehicle consisting of two propellers mounted on tilting servo motors and two motor-driven wheels. DoubleBee exploits the high energy efficiency of a bicopter configuration in aerial mode, and enjoys the low power consumption of a two-wheel self-balancing robot on the ground. Furthermore, the propeller thrusts act as additional control inputs on the ground, enabling a novel decoupled control scheme where the attitude of the robot is controlled using thrusts and the translational motion is realized using wheels. A prototype of DoubleBee is constructed using commercially available components. The power efficiency and the control performance of the robot are verified through comprehensive experiments. Challenging tasks in indoor and outdoor environments demonstrate the capability of DoubleBee to traverse unstructured environments, fly over and move under barriers, and climb steep and rough terrains. keywords: {Propellers;Attitude control;Wheels;Energy efficiency;Stability analysis;Mobile robots;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341984&isnumber=10341342

N. Chen et al., "Swashplateless-Elevon Actuation for a Dual-Rotor Tail-Sitter VTOL UAV," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6970-6976, doi: 10.1109/IROS55552.2023.10341861.Abstract: In this paper, we propose a novel swashplateless-elevon actuation (SEA) for dual-rotor tail-sitter vertical takeoff and landing (VTOL) unmanned aerial vehicles (UAVs). In contrast to the conventional elevon actuation (CEA) which controls both pitch and yaw using elevons, the SEA adopts swash-plateless mechanisms to generate an extra moment through motor speed modulation to control pitch and uses elevons solely for controlling yaw, without requiring additional actuators. This decoupled control strategy mitigates the saturation of elevons' deflection needed for large pitch and yaw control actions, thus improving the UAV's control performance on trajectory tracking and disturbance rejection performance in the presence of large external disturbances. Furthermore, the SEA overcomes the actuation degradation issues experienced by the CEA when the UAV is in close proximity to the ground, leading to a smoother and more stable take-off process. We validate and compare the performances of the SEA and the CEA in various real-world flight conditions, including take-off, trajectory tracking, and hover flight and position steps under external disturbance. Experimental results demonstrate that the SEA has better performances than the CEA. Moreover, we verify the SEA's feasibility in the attitude transition process and fixed-wing-mode flight of the VTOL UAV. The results indicate that the SEA can accurately control pitch in the presence of high-speed incoming airflow and maintain a stable attitude during fixed-wing mode flight. Video of all experiments can be found in youtube.com/watch?v=Sx9Rk4Zf7sQ keywords: {Degradation;Actuators;Fans;Three-dimensional displays;Trajectory tracking;Attitude control;Modulation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341861&isnumber=10341342

J. Xu, D. S. D'Antonio, D. J. Ammirato and D. Saldaña, "SBlimp: Design, Model, and Translational Motion Control for a Swing-Blimp," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6977-6982, doi: 10.1109/IROS55552.2023.10341796.Abstract: We present an aerial vehicle composed of a custom quadrotor with tilted rotors and a helium balloon, called SBlimp. We propose a novel control strategy that takes advantage of the natural stable attitude of the blimp to control translational motion. Different from cascade controllers in the literature that controls attitude to achieve desired translational motion, our approach directly controls the linear velocity regardless of the heading orientation of the vehicle. As a result, the vehicle swings during the translational motion. We provide a planar analysis of the dynamic model, demonstrating stability for our controller. Our design is evaluated in numerical simulations with different physical factors and validated with experiments using a real-world prototype, showing that the SBlimp is able to achieve stable translation regardless of its orientation. keywords: {Attitude control;Rotors;Prototypes;Numerical simulation;Stability analysis;Numerical models;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341796&isnumber=10341342

B. Guan, R. V. Godoy, F. Sanches, A. Dwivedi and M. Liarokapis, "On Semi-Autonomous Robotic Telemanipulation Employing Electromyography Based Motion Decoding and Potential Fields," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6991-6997, doi: 10.1109/IROS55552.2023.10342155.Abstract: Telemanipulation is widely used in robotics applications, ranging from maintenance of various industrial systems to search and rescue response in remote and/or hazardous environments. Human operators are often responsible for the control of such robotic systems. However, these remote interactions require highly trained and experienced operators owing to their complex nature. Semi-autonomous systems are presented as an alternative to complex and counter-intuitive manual systems, combining decoded user intentions with autonomous control modules. This paper proposes a semi-autonomous framework for robotic telemanipulation that employs Electromyography (EMG) based motion decoding and potential fields to execute complex object stacking tasks with a dexterous robot arm-hand system. Even though simple EMG-based teleoperation is promising, the signals are often noisy leading to induced randomness and control errors. To assist the user during task executions, potential fields are utilized to avoid obstacles and guide the robot end-effector toward the objects of interest, thus reducing the cognitive load on the user and the need for accurate predictions. The user's motion is decoded from the myoelectric activations of the human upper arm and upper torso using a Random Forest-based regression methodology. The objects are detected in the environment with an external camera that provides their goal poses to the potential fields scheme. EMG control and potential fields work in a synergistic manner simplifying the system's operation. The framework performance is experimentally validated in real-time experiments involving complex cube and cylinder stacking tasks. keywords: {Electric potential;Visualization;Service robots;Stacking;Robot vision systems;Cameras;Electromyography},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342155&isnumber=10341342

R. V. Godoy, B. Guan, A. Dwivedi and M. Liarokapis, "An Affordances and Electromyography Based Telemanipulation Framework for Control of Robotic Arm-Hand Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 6998-7004, doi: 10.1109/IROS55552.2023.10341955.Abstract: Over the last decades, significant research effort has been put into creating Electromyography (EMG) based controllers for intuitive, hands-free control of robotic arms and hands. To achieve this, machine learning models have been employed to decode human motion and intention using EMG signals as input and to deliver several applications, such as prosthesis control using gesture classification. Despite the advances introduced by new deep learning techniques, real-time control of robot arms and hands using EMG signals as input still lacks accuracy, especially when a plethora of gestures are included as labels in the case of classification. This has been observed to be due to the noise and non-stationarity of the EMG signals and the increased dimensionality of the problem. In this paper, we propose an intuitive, affordances-oriented EMG-based telemanipulation framework for a robot arm-hand system that allows for dexterous control of the device. An external camera is utilized to perform scene understanding and object detection and recognition, providing grasping and manipulation assistance to the user and simplifying control. Object-specific Transformer-based classifiers are employed based on the affordances of the object of interest, reducing the number of possible gesture outputs, dividing and conquering the problem, and resulting in a more robust and accurate gesture decoding system when compared to a single generic classification model. The performance of the proposed system is experimentally validated in a remote telemanipulation setting, where the user successfully performs a set of dexterous manipulation tasks. keywords: {Affordances;Robot vision systems;Transformers;Manipulators;Cameras;Electromyography;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341955&isnumber=10341342

S. Kuitert, J. Hofland, C. J. M. Heemskerk, D. A. Abbink and L. Peternel, "Orbital Head-Mounted Display: A Novel Interface for Viewpoint Control during Robot Teleoperation in Cluttered Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-7, doi: 10.1109/IROS55552.2023.10341733.Abstract: Robotic teleoperation is used in various applications, including the nuclear industry, where the experience and intelligence of a human operator are necessary for making complex decisions that are beyond the autonomy of robots. Human-robot interfaces that help strengthen an operators situational awareness without inducing excessive cognitive load are crucial to the success of teleoperation. This paper presents a novel visual interface that allows operators to simultaneously control a 6-DoF camera platform and a robotic manipulator whilst experiencing the remote environment through a virtual reality head-mounted display (HMD). The proposed system, Orbital Head-Mounted Display (OHMD), utilizes head rotation tracking to command camera movement in azimuth and elevation directions around a fixation point located at a robot's end-effector. A human factor study was conducted to compare the interface acceptance, perceived workload, and task performance of OHMD with a conventional interface utilizing multiple fixed cameras (Array) and a standard head-mounted display implementation (HMD). Results show that both the OHMD and HMD interfaces significantly improve task performance, reduce perceived workload and increase interface acceptance compared to the Array interface. Participants reported they preferred OHMD due to the increased assistance and freedom in viewpoint selection. Whilst OHMD excelled in usefulness, the standard HMD interface allowed operators to perform robotic welding tasks significantly faster. keywords: {Visualization;Head-mounted displays;Welding;Robot vision systems;Resists;Cameras;Orbits},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341733&isnumber=10341342

Y. Wang, C. Sifferman and M. Gleicher, "Exploiting Task Tolerances in Mimicry-Based Telemanipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7012-7019, doi: 10.1109/IROS55552.2023.10342536.Abstract: We explore task tolerances, i.e., allowable position or rotation inaccuracy, as an important resource to facilitate smooth and effective telemanipulation. Task tolerances provide a robot flexibility to generate smooth and feasible motions; however, in teleoperation, this flexibility may make the user's control less direct. In this work, we implemented a telema-nipulation system that allows a robot to autonomously adjust its configuration within task tolerances. We conducted a user study comparing a telemanipulation paradigm that exploits task tolerances (functional mimicry) to a paradigm that requires the robot to exactly mimic its human operator (exact mimicry), and assess how the choice in paradigm shapes user experience and task performance. Our results show that autonomous adjustments within task tolerances can lead to performance improvements without sacrificing perceived control of the robot. Additionally, we find that users perceive the robot to be more under control, predictable, fluent, and trustworthy in functional mimicry than in exact mimicry. keywords: {Shape;User experience;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342536&isnumber=10341342

T. Morimoto et al., "Light-Field Visual System for the Remote Robot Operation Interface," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7020-7025, doi: 10.1109/IROS55552.2023.10342529.Abstract: Robotic automation is expected to be applicable in various fields. The utilization of robots requires human-robot interaction (HRI) for prolonged direct manipulation or learning. Recently, numerous studies on HRI have been conducted in virtual space using virtual, augmented, and mixed reality (VAM-HRI). In the future, VAM-HRI applications are expected to involve users wearing head-mounted displays (HMDs). However, HMDs present various problems, such as vergence and accommodation conflict (VAC) caused by eye strain, and thus cannot be used for long periods. In this study, a remote robot operation interface is developed to solve this problem. Experiments are conducted from the viewpoint of ophthalmology to determine whether “removal of VAC” and “improvement of depth perception” by Light Field HMDs (LFHMD), and “reduction of eye strain” by TransRay are feasible. The results reveal that the LFHMD is superior to the general HMDs for all items. keywords: {Visualization;Human-robot interaction;Mixed reality;Imaging;Visual systems;Fatigue;Light fields},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342529&isnumber=10341342

F. Zorić, A. Suarez, G. Vasiljević, M. Orsag, Z. Kovačić and A. Ollero, "Performance Comparison of Teleoperation Interfaces for Ultra-Lightweight Anthropomorphic Arms," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7026-7033, doi: 10.1109/IROS55552.2023.10342484.Abstract: This paper presents a comparative performance evaluation of three different teleoperation interfaces for very low weight (<3 kg) anthropomorphic dual arms intended to conduct complex manipulation tasks involving a certain level of dexterity, accuracy and agility, either in ground service or in aerial manipulation applications. A visual human pose estimation system is developed to obtain the Cartesian and joint values of the user, which are mapped to the corresponding pose of the dual arm manipulator exploiting the equivalent human-robot kinematics. A leader-follower scheme is also presented, using a reduced scale dual arm that can replicate directly the joint positions of the leader arms to the follower arms. A 6-DOF (degrees of freedom) joystick is proposed to generate linear motions more accurately. A total of 60 ground tests were conducted involving 10 participants to determine the accuracy and time performance in two benchmarks (box edges and S contour tracking). Finally, the visual and leader-follower interfaces were evaluated with the dual arm aerial manipulator on flight tests, reporting several findings derived from the system evaluation. keywords: {Performance evaluation;Visualization;Tracking;Pose estimation;Robot vision systems;Benchmark testing;Position measurement;teleoperation interfaces;human pose estimation;anthropomorphic robotic arms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342484&isnumber=10341342

A. U. Krishnan, T. -C. Lin and Z. Li, "Human Preferred Augmented Reality Visual Cues for Remote Robot Manipulation Assistance: from Direct to Supervisory Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7034-7039, doi: 10.1109/IROS55552.2023.10341969.Abstract: When humans control or supervise remote robot manipulation, augmented reality (AR) visual cues overlaid on the remote camera video stream can effectively enhance human's remote perception of task and robot states, and comprehension of the robot autonomy's capability and intent. In this work, we conducted a user study (N=18) to investigate: (RQ1) what AR cues humans prefer when controlling the robot with various levels of autonomy, and (RQ2) whether this preference can be influenced by the way humans learn to use the interface. We provided AR visual cues of various types (e.g., motion guidance, obstacle indicator, target hint, autonomy activation and intent) to assist humans to pick and place an object around an obstacle on a counter workspace. We found that: 1) Participants prefer different types of AR cues based on the level of robot autonomy; 2) The AR cues the participants prefer to use after hands-on robot operation converged to the recommendation of experienced users, and may largely differ from their initial selection based on video instruction. keywords: {Visualization;Robot control;Stacking;Robot vision systems;Supervisory control;Streaming media;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341969&isnumber=10341342

N. Boguslavskii, Z. Zhong, L. M. Genua and Z. Li, "A Shared Autonomous Nursing Robot Assistant with Dynamic Workspace for Versatile Mobile Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7040-7045, doi: 10.1109/IROS55552.2023.10342401.Abstract: This paper presents a novel integration of a shared autonomous mobile humanoid robot for remote nursing assistance. The proposed nursing robot has a motorized versatile supporting structure to allow flexible integration of the system components, autonomously adjust its mobile manipulation workspace and improve its reachability and manipulability to operate in a cluttered environment. The robot also provides a novel integration of robot autonomy to reduce the human effort to coordinate the motorized chest and arm motion, control the precise manipulation of objects and camera viewpoint, and handle complex collision avoidance in human-guided gross manipulation. Moreover, we developed an open-source virtual testbed that integrates ROS- and Unity-based robot simulation and benchmark mobile manipulation nursing tasks and scenarios in a realistic simulation of a hospital environment. The virtual testbed supports various contemporary gaming and AR/VR interfaces to control the virtual human and robots, and provides autonomy for navigation, manipulation, and remote active perception assistance. We conducted a user study (N=9) to validate that the versatile supporting structure and shared autonomy of the physical testbed can effectively reduce the human effort to control unstructured manipulation, and improve the robot's reachability and manipulability. In addition, we conducted a pilot study (N=8) to test the usability of the virtual testbed and collect feedback from representative users. keywords: {Navigation;Robot kinematics;Robot vision systems;Humanoid robots;Collision avoidance;Usability;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342401&isnumber=10341342

L. McCutcheon and S. Fallah, "Adaptive PD Control Using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7046-7053, doi: 10.1109/IROS55552.2023.10341953.Abstract: Local-remote systems allow robots to execute complex tasks in hazardous environments such as space and nuclear power stations. However, establishing accurate positional mapping between local and remote devices can be difficult due to time delays that can compromise system performance and stability. Enhancing the synchronicity and stability of localremote systems is vital for enabling robots to interact with environments at greater distances and under highly challenging network conditions, including time delays. We introduce an adaptive control method employing reinforcement learning to tackle the time-delayed control problem. By adjusting controller parameters in real-time, this adaptive controller compensates for stochastic delays and improves synchronicity between local and remote robotic manipulators. To improve the adaptive PD controller's performance, we devise a model-based reinforcement learning approach that effectively incorporates multi-step delays into the learning framework. Utilizing this proposed technique, the local-remote system's performance is stabilized for stochastic communication time-delays of up to 290ms. Our results demonstrate that the suggested model-based reinforcement learning method surpasses the Soft-Actor Critic and augmented state Soft-Actor Critic techniques. Access the code at: https://github.com/CAV-Research-Lab/Predictive-Model-Delay-Correction keywords: {Adaptation models;Delay effects;System performance;Computational modeling;Stochastic processes;Reinforcement learning;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341953&isnumber=10341342

R. Luo et al., "Team Northeastern's Approach to ANA XPRIZE Avatar Final Testing: A Holistic Approach to Telepresence and Lessons Learned," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7054-7060, doi: 10.1109/IROS55552.2023.10341475.Abstract: This paper reports on Team Northeastern's Avatar system for telepresence, and our holistic approach to meet the ANA Avatar XPRIZE Final testing task requirements. The system features a dual-arm configuration with hydraulically actuated glove-gripper pair for haptic force feedback. Our proposed Avatar system was evaluated in the ANA Avatar XPRIZE Finals and completed all 10 tasks, scored 14.5 points out of 15.0, and received the 3rd Place Award. We provide the details of improvements over our first generation Avatar, covering manipulation, perception, locomotion, power, network, and controller design. We also extensively discuss the major lessons learned during our participation in the competition. keywords: {Telepresence;Avatars;Force feedback;System improvement;Task analysis;Intelligent robots;Testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341475&isnumber=10341342

R. Tejwani, C. Ma, P. Bonato and H. H. Asada, "An Avatar Robot Overlaid with the 3D Human Model of a Remote Operator," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7061-7068, doi: 10.1109/IROS55552.2023.10341890.Abstract: Although telepresence assistive robots have made significant progress, they still lack the sense of realism and physical presence of the remote operator. This results in a lack of trust and adoption of such robots. In this paper, we introduce an Avatar Robot System which is a mixed real/virtual robotic system that physically interacts with a person in proximity of the robot. The robot structure is overlaid with the 3D model of the remote caregiver and visualized through Augmented Reality (AR). In this way, the person receives haptic feedback as the robot touches him/her. We further present an Optimal Non-Iterative Alignment solver that solves for the optimally aligned pose of 3D Human model to the robot (shoulder to the wrist non-iteratively). The proposed alignment solver is stateless, achieves optimal alignment and faster than the baseline solvers (demonstrated in our evaluations). We also propose an evaluation framework that quantifies the alignment quality of the solvers through multifaceted metrics. We show that our solver can consistently produce poses with similar or superior alignments as IK-based baselines without their potential drawbacks. keywords: {Wrist;Solid modeling;Visualization;Three-dimensional displays;Telepresence;Sensitivity;Avatars},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341890&isnumber=10341342

F. A. Almasalmah, H. Omran, C. Liu and B. Bayle, "Adaptive Robust Model Predictive Control for Bilateral Teleoperation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7069-7074, doi: 10.1109/IROS55552.2023.10341847.Abstract: In this work, we use recent developments in the field of adaptive robust Model Predictive Control (MPC) to build a controller for bilateral teleoperation systems. To guarantee robust constraint satisfaction, we incorporate polytopic tube controllers in the MPC design. In addition, we use online learning methods to learn the environment model. Namely, we use set membership learning to learn the parametric uncertainty bounds and reduce the conservatism of the robust controller, and we combine it with least mean square method to learn a point estimate of the model parameters, which enhances the controller performance. Our simulation demonstrates the effectiveness of the proposed approach in maintaining robust constraint satisfaction and enhancing performance by learning during teleoperation tasks. keywords: {Learning systems;Adaptation models;Adaptive systems;Uncertainty;Electron tubes;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341847&isnumber=10341342

M. Haruna, M. Ogino, S. Tagashira and S. Morita, "Augmented Avatar Toward Both Remote Communication and Manipulation Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7075-7081, doi: 10.1109/IROS55552.2023.10342284.Abstract: A human being can communicate while working at the same time. However, teleoperated humanoid robots that can work and communicate simultaneously are currently too complex and expensive. We propose an “Augmented Avatar” that can perform both work and communication simultaneously at a low cost. The authors have developed two types of Avatars: a Manipulation Avatar with minimal functions tailored for work, and a Communication Avatar that mimics the human body structure to facilitate communication. The aim of this project is to develop an AVATAR system that enables operators to seamlessly operate the two avatars without using a wearable control interface. In this paper, we report on the construction and operation test results of this prototype. The prototype system has been well received in an international AVATAR competition in which 820 teams participated, placing 12th. This paper also discusses operability based on the experience gained at this competition. keywords: {Avatars;Sociology;MIMICs;Prototypes;User interfaces;Manipulators;Remote working},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342284&isnumber=10341342

C. Wu, J. Sun, Z. Shen and L. Zhang, "MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7082-7088, doi: 10.1109/IROS55552.2023.10341681.Abstract: Simulating camera sensors is a crucial task in autonomous driving. Although neural radiance fields are exceptional at synthesizing photorealistic views in driving simulations, they still fail to generate extrapolated views. This paper proposes to incorporate map priors into neural radiance fields to synthesize out-of-trajectory driving views with semantic road consistency. The key insight is that map information can be utilized as a prior to guiding the training of the radiance fields with uncertainty. Specifically, we utilize the coarse ground surface as uncertain information to supervise the density field and warp depth with uncertainty from unknown camera poses to ensure multi-view consistency. Experimental results demonstrate that our approach can produce semantic consistency in deviated views for vehicle camera simulation. The supplementary video can be viewed at https://youtu.be/jEQWr-Rfh3A. keywords: {Training;Uncertainty;Roads;Semantics;Robot vision systems;Cameras;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341681&isnumber=10341342

S. Chen, T. Chabal, I. Laptev and C. Schmid, "Object Goal Navigation with Recursive Implicit Maps," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7089-7096, doi: 10.1109/IROS55552.2023.10341827.Abstract: Object goal navigation aims to navigate an agent to locations of a given object category in unseen environments. Classical methods explicitly build maps of environments and require extensive engineering while lacking semantic information for object-oriented exploration. On the other hand, end-to-end learning methods alleviate manual map design and predict actions using implicit representations. Such methods, however, lack an explicit notion of geometry and may have limited ability to encode navigation history. In this work, we propose an implicit spatial map for object goal navigation. Our implicit map is recursively updated with new observations at each step using a transformer. To encourage spatial reasoning, we introduce auxiliary tasks and train our model to reconstruct explicit maps as well as to predict visual features, semantic labels and actions. Our method significantly outperforms the state of the art on the challenging MP3D dataset and generalizes well to the HM3D dataset. We successfully deploy our model on a real robot and achieve encouraging object goal navigation results in real scenes using only a few real-world demonstrations. Code, trained models and videos are available at https://www.di.ens.fr/willow/research/onav_rim/. keywords: {Learning systems;Visualization;Navigation;Object oriented modeling;Semantics;Manuals;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341827&isnumber=10341342

M. Ahmed et al., "Vision-Based Autonomous Navigation for Unmanned Surface Vessel in Extreme Marine Conditions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7097-7103, doi: 10.1109/IROS55552.2023.10341867.Abstract: Visual perception is an important component for autonomous navigation of unmanned surface vessels (USV), particularly for the tasks related to autonomous inspection and tracking. These tasks involve vision-based navigation techniques to identify the target for navigation. Reduced visibility under extreme weather conditions in marine environments makes it difficult for vision-based approaches to work properly. To overcome these issues, this paper presents an autonomous vision-based navigation framework for tracking target objects in extreme marine conditions. The proposed framework consists of an integrated perception pipeline that uses a generative adversarial network (GAN) to remove noise and highlight the object features before passing them to the object detector (i.e., YOLOv5). The detected visual features are then used by the USV to track the target. The proposed framework has been thoroughly tested in simulation under extremely reduced visibility due to sandstorms and fog. The results are compared with state-of-the-art de-hazing methods across the benchmarked MBZIRC simulation dataset, on which the proposed scheme has outperformed the existing methods across various metrics. keywords: {YOLO;Visualization;Target tracking;Navigation;Pipelines;Generative adversarial networks;Feature extraction;Navigation;Marine Robotics;Visual Control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341867&isnumber=10341342

Y. Shukla, B. Kesari, S. Goel, R. Wright and J. Sinapov, "A Framework for Few-Shot Policy Transfer Through Observation Mapping and Behavior Cloning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7104-7110, doi: 10.1109/IROS55552.2023.10342477.Abstract: Despite recent progress in Reinforcement Learning for robotics applications, many tasks remain prohibitively difficult to solve because of the expensive interaction cost. Transfer learning helps reduce the training time in the target domain by transferring knowledge learned in a source domain. Sim2Real transfer helps transfer knowledge from a simulated robotic domain to a physical target domain. Knowledge transfer reduces the time required to train a task in the physical world, where the cost of interactions is high. However, most existing approaches assume exact correspondence in the task structure and the physical properties of the two domains. This work proposes a framework for Few-Shot Policy Transfer between two domains through Observation Mapping and Behavior Cloning. We use Generative Adversarial Networks (GANs) along with a cycle-consistency loss to map the observations between the source and target domains and later use this learned mapping to clone the successful source task behavior policy to the target domain. We observe successful behavior policy transfer with limited target task interactions and in cases where the source and target task are semantically dissimilar. keywords: {Training;Costs;Transfer learning;Cloning;Reinforcement learning;Generative adversarial networks;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342477&isnumber=10341342

G. Delama, F. Shamsfakhr, S. Weiss, D. Fontanelli and A. Fomasier, "UVIO: An UWB-Aided Visual-Inertial Odometry Framework with Bias-Compensated Anchors Initialization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7111-7118, doi: 10.1109/IROS55552.2023.10342012.Abstract: This paper introduces UVIO, a multi-sensor framework that leverages Ultra Wide Band (UWB) technology and Visual-Inertial Odometry (VIO) to provide robust and low-drift localization. In order to include range measurements in state estimation, the position of the UWB anchors must be known. This study proposes a multi-step initialization procedure to map multiple unknown anchors by an Unmanned Aerial Vehicle (UAV), in a fully autonomous fashion. To address the limitations of initializing UWB anchors via a random trajectory, this paper uses the Geometric Dilution of Precision (GDOP) as a measure of optimality in anchor position estimation, to compute a set of optimal waypoints and synthesize a trajectory that minimizes the mapping uncertainty. After the initialization is complete, the range measurements from multiple anchors, including measurement biases, are tightly integrated into the VIO system. While in range of the initialized anchors, the VIO drift in position and heading is eliminated. The effectiveness of UVIO and our initialization procedure has been validated through a series of simulations and real-world experiments. keywords: {Location awareness;Uncertainty;Autonomous aerial vehicles;Real-time systems;Trajectory;Odometry;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342012&isnumber=10341342

S. Y. Min et al., "Self-Supervised Object Goal Navigation with In-Situ Finetuning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7119-7126, doi: 10.1109/IROS55552.2023.10341959.Abstract: A household robot should be able to navigate to target objects without requiring users to first annotate everything in their home. Most current approaches to object navigation do not test on real robots and rely solely on reconstructed scans of houses and their expensively labeled semantic 3D meshes. In this work, our goal is to build an agent that builds self-supervised models of the world via exploration, the same as a child might - thus we (1) eschew the expense of labeled 3D mesh and (2) enable self-supervised in-situ finetuning in the real world. We identify a strong source of self-supervision (Location Consistency - LocCon) that can train all components of an ObjectNav agent, using unannotated simulated houses. Our key insight is that embodied agents can leverage location consistency as a self-supervision signal - collecting images from different views/angles and applying contrastive learning. We show that our agent can perform competitively in the real world and simulation. Our results also indicate that supervised training with 3D mesh annotations causes models to learn simulation artifacts, which are not transferrable to the real world. In contrast, our LocCon shows the most robust transfer in the real world among the set of models we compare to, and that the real-world performance of all models can be further improved with self-supervised LocCon in-situ training. keywords: {Training;Solid modeling;Three-dimensional displays;Navigation;Annotations;Semantics;Object recognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341959&isnumber=10341342

L. Zhao, M. Zhou and B. Loose, "Tightly-Coupled Visual- DVL- Inertial Odometry for Robot-Based Ice-Water Boundary Exploration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7127-7134, doi: 10.1109/IROS55552.2023.10342024.Abstract: Underwater robots, like Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs), are promising tools for the exploration and study of the under-ice environment and the ecosystems that thrive there. However, state estimation is a well-known problem for robotic systems, especially, for the ones that travel underwater. In this paper, $w$e present a tightly-coupled multi-sensors fusion framework to increase localization accuracy that is robust to sensor failure. Visual images, Doppler Velocity Log (DVL), Inertial Measurement Unit (IMU) and Pressure sensor are integrated using a Multi-State Constraint Kalman Filter (MSCKF) for state estimation. Besides, a modified keyframe-based clone marginalization and a new DVL-aided feature enhancement method are presented to further improve the localization performance. The proposed method is validated in the under-ice environment on Lake Michigan, USA, and the results are cross-compared with 10 other different sensor fusion setups. Overall, the integration of keyframe enabled and DVL-aided feature enhancement yielded the best performance with a Root-mean-square error of less than 2 m compared to the ground truth path over a total traveling distance of about 200 m. keywords: {Location awareness;Pressure sensors;Autonomous underwater vehicles;Visualization;Remotely guided vehicles;Measurement units;Sensor fusion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342024&isnumber=10341342

V. D. Sharma, J. Chen and P. Tokekar, "ProxMaP: Proximal Occupancy Map Prediction for Efficient Indoor Robot Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7135-7140, doi: 10.1109/IROS55552.2023.10341435.Abstract: Planning a path for a mobile robot typically requires building a map (e.g., an occupancy grid) of the environment as the robot moves around. While navigating in an unknown environment, the map built by the robot online may have many as-yet-unknown regions. A conservative planner may avoid such regions taking a longer time to navigate to the goal. Instead, if a robot is able to correctly predict the occupancy in the occluded regions, the robot may navigate efficiently. We present a self-supervised occupancy prediction technique, ProxMaP, to predict the occupancy within the proximity of the robot to enable faster navigation. We show that ProxMaP generalizes well across realistic and real domains, and improves the robot navigation efficiency in simulation by 12.40% against a traditional navigation method. We share our findings and code at https://raaslab.org/projects/ProxMaP. keywords: {Codes;Navigation;Semantics;Robot vision systems;Buildings;Cameras;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341435&isnumber=10341342

B. Bolte, A. Wang, J. Yang, M. Mukadam, M. Kalakrishnan and C. Paxton, "USA-Net: Unified Semantic and Affordance Representations for Robot Memory," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341737.Abstract: In order for robots to follow open-ended instructions like “go open the brown cabinet over the sink,” they require an understanding of both the scene geometry and the semantics of their environment. Robotic systems often handle these through separate pipelines, sometimes using very different representation spaces, which can be suboptimal when the two objectives conflict. In this work, we present USA-Net, a simple method for constructing a world representation that encodes both the semantics and spatial affordances of a scene in a differentiable map. This allows us to build a gradient-based planner which can navigate to locations in the scene specified using open-ended vocabulary. We use this planner to consistently generate trajectories which are both shorter 5-10% shorter and 10-30% closer to our goal query in CLIP embedding space than paths from comparable grid-based planners which don't leverage gradient information. To our knowledge, this is the first end-to-end differentiable planner optimizes for both semantics and affordance in a single implicit map. Code and visuals are available at our website: usa.bolte.cc keywords: {Geometry;Vocabulary;Visualization;Codes;Navigation;Affordances;Semantics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341737&isnumber=10341342

H. Dhami, V. D. Sharma and P. Tokekar, "Pred-NBV: Prediction-Guided Next-Best-View Planning for 3D Object Reconstruction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7149-7154, doi: 10.1109/IROS55552.2023.10341650.Abstract: Prediction-based active perception has shown the potential to improve the navigation efficiency and safety of the robot by anticipating the uncertainty in the unknown environment. The existing works for 3D shape prediction make an implicit assumption about the partial observations and therefore cannot be used for real-world planning and do not consider the control effort for next-best-view planning. We present Pred-NBV, a realistic object shape reconstruction method consisting of PoinTr-C, an enhanced 3D prediction model trained on the ShapeNet dataset, and an information and control effort-based next-best-view method to address these issues. Pred-NBV shows an improvement of 25.46% in object coverage over the traditional methods in the AirSim simulator, and performs better shape completion than PoinTr, the state-of-the-art shape completion model, even on real data obtained from a Velodyne 3D LiDAR mounted on DJI M600 Pro. keywords: {Point cloud compression;Solid modeling;Three-dimensional displays;Uncertainty;Shape;Measurement uncertainty;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341650&isnumber=10341342

W. Chen, W. Li, A. Yang and Y. Hu, "Active Visual SLAM Based on Hierarchical Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7155-7162, doi: 10.1109/IROS55552.2023.10341780.Abstract: We present AVS-HRL, a modular Active visual SLAM system based on hierarchical reinforcement learning. The reward function explicitly considers the efficiency of exploration and the accuracy of mapping by utilizing the internal variables of SLAM, such as feature points distribution and loop-closure signal. Compared to end-to-end active SLAM methods, we designed a map reconstruction module that can correct the cumulative error in the incremental mapping process. Furthermore, the inputs of all neural network modules use more abstract and general information, such as grid maps, rather than raw sensor observations. We conducted experiments in two different simulators and real-world environments. In the noisy setting of Habitat environments, our method improves the accuracy of the mapped areas by 68.48% as an average of Gibson and MP3D datasets. Moreover, our method's generalization performance was demonstrated through direct transfer across different simulators and real-world environments. keywords: {Visualization;Simultaneous localization and mapping;Neural networks;Habitats;Reinforcement learning;Noise measurement;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341780&isnumber=10341342

J. Lin, M. Rickert, L. Wen, Y. Hu and A. Knoll, "Robust Point Cloud Registration with Geometry-based Transformation Invariant Descriptor," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7163-7170, doi: 10.1109/IROS55552.2023.10342244.Abstract: This work presents a novel method for point registration in 3D space. The proposed algorithm utilizes transformation-invariant geometry information to estimate the pose of objects based on correspondences between points in two sets. Conventional methods use geometry descriptors to find these correspondences, which can result in a large number of outliers. Most existing algorithms are error-prone when outliers are present. Instead of formulating point registration as a non-convex optimization problem, we propose an intuitive method that filters out spurious correspondences. This is achieved by evaluating three different geometry-based transformation-invariant descriptors for outlier removal. We construct fully connected graphs with the proposed descriptors on correspondences, and convert the outlier removal problem into a subgraph isomorphism problem that is solved using a binary clustering approach. The resulting inlier clustering is used to estimate the transformation between the two point sets. The effectiveness of the proposed approach is evaluated on standard 3D data and the 3DMatch scan matching dataset, and compared against existing state-of-the-art methods. Results show that our method effectively reduces outliers and performs similarly to these methods. keywords: {Point cloud compression;Geometry;Three-dimensional displays;Buildings;Filtering algorithms;Complexity theory;Standards},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342244&isnumber=10341342

Z. Qiao, Z. Yu, H. Yin and S. Shen, "Online Monocular Lane Mapping Using Catmull-Rom Spline," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7179-7186, doi: 10.1109/IROS55552.2023.10341707.Abstract: In this study, we introduce an online monocular lane mapping approach that solely relies on a single camera and odometry for generating spline-based maps. Our proposed technique models the lane association process as an assignment issue utilizing a bipartite graph, and assigns weights to the edges by incorporating Chamfer distance, pose uncertainty, and lateral sequence consistency. Furthermore, we meticulously design control point initialization, spline parameterization, and optimization to progressively create, expand, and refine splines. In contrast to prior research that assessed performance using self-constructed datasets, our experiments are conducted on the openly accessible OpenLane dataset. The experimental outcomes reveal that our suggested approach enhances lane association and odometry precision, as well as overall lane map quality. We have open-sourced out code11https://github.com/HKUST-Aerial-Robotics/MonoLaneMapping for this project. keywords: {Uncertainty;Cameras;Bipartite graph;Odometry;Splines (mathematics);Optimization;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341707&isnumber=10341342

Y. Bai, Z. Miao, X. Wang, Y. Liu, H. Wang and Y. Wang, "VDBblox: Accurate and Efficient Distance Fields for Path Planning and Mesh Reconstruction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7187-7194, doi: 10.1109/IROS55552.2023.10342123.Abstract: Highly accurate and efficient map in unknown and complex environments is essential for robotics navigation. Traditionally, mobile robot platforms are often computationally constrained when using multiple sensors to process large amounts of input data. In previous works, some of them have been deployed to embedded platforms in real-time. However, how to balance accuracy and efficiency while reducing the computational resources and the memory footprint is still the bottleneck. Motivated by these challenges, we proposed a mapping framework called VDBblox to incrementally build Euclidean Signed Distance Fields (ESDFs) map from Truncated Signed Distance Fields (TSDFs) mapping. We use a novel weight function to update the non-projective TSDFs, thus improving the quality of the mesh reconstruction with higher accuracy than up-to-date methods. Meanwhile, the generated ESDFs map is maintained by the least recently used (LRU) cache to dynamically handle the obstacle changes with less runtime than state-of-the-art. We show VDBblox performance in terms of accuracy and efficiency by benchmark comparison on RGB-D and LiDAR public datasets. Moreover, we demonstrate that VDBblox can be integrated into a completed quadrotor system as a sub-module. Then we validate it through online obstacle avoidance and high-quality mesh reconstruction in real-world experiments. Finally, we release our method as open-source code to the community11Code - https://github.com/yinloonga/vdbblox. keywords: {Runtime;Laser radar;Navigation;Memory management;Robot sensing systems;Real-time systems;Path planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342123&isnumber=10341342

S. Hong, C. Zheng, H. Yin and S. Shen, "Rollvox: Real-Time and High-Quality LiDAR Colorization with Rolling Shutter Camera," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7195-7201, doi: 10.1109/IROS55552.2023.10342172.Abstract: In this study, we propose a novel system for real-time coloring LiDAR point clouds with a low-cost RS camera. The main challenges are dealing with the motion distortion of the RS camera and the multi-sensor time synchronization. To tackle these challenges, we carefully design a hardware synchronizer to ensure the strict alignment of the LiDAR, inertial measurement unit, and RS camera. With accurate timestamps, we first use LiDAR-inertial odometry (LIO) for pose estimation, and the poses of image line exposure are calculated by forward propagation based on a constant velocity motion model. Then, we propose our method based on the RS constraint for colorizing the LiDAR point cloud. For comparison, we colorize the LiDAR point cloud with conventional rolling shutter image undistortion. In the real-world tests, The results show that our proposed method produces more accurate and efficient colorization of point clouds. Besides, considering the situation of readout time not being provided, we propose a method to calibrate the readout time by minimizing the reprojection error of LIO's inter-frame pose and image optical flows. We release our code and self-collected datasets on Github33https://github.com/sheng00125/Rollvox to benefit the community. keywords: {Point cloud compression;Laser radar;Robot vision systems;Pose estimation;Cameras;Real-time systems;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342172&isnumber=10341342

S. Kulkarni, P. Yin and S. Scherer, "360FusionNeRF: Panoramic Neural Radiance Fields with Joint Guidance," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7202-7209, doi: 10.1109/IROS55552.2023.10341346.Abstract: Based on the neural radiance fields (NeRF), we present a pipeline for generating novel views from a single 360° panoramic image. Prior research relied on the neighborhood interpolation capability of multi-layer perceptions to complete missing regions caused by occlusion. This resulted in artifacts in their predictions. We propose 360FusionNeRF, a semi-supervised learning framework that employs geometric supervision and semantic consistency to guide the progressive training process. Firstly, the input image is reprojected to 360° images, and depth maps are extracted at different camera positions. In addition to the NeRF color guidance, the depth supervision enhances the geometry of the synthesized views. Furthermore, we include a semantic consistency loss that encourages realistic renderings of novel views. We extract these semantic features using a pre-trained visual encoder CLIP, a Vision Transformer (ViT) trained on hundreds of millions of diverse 2D photographs mined from the web with natural language supervision. Experiments indicate that our proposed method is capable of producing realistic completions of unobserved regions while preserving the features of the scene. 360FusionNeRF consistently delivers state-of-the-art performance when transferring to synthetic Structured3D dataset (PSNR ~ 5%, SSIM ~3% LPIPS ~13%), real-world Matterport3D dataset (PSNR ~3%, SSIM ~3% LPIPS ~9%) and Replica360 dataset (PSNR ~8%, SSIM ~2% LPIPS ~18%). We provide the source code at https://github.com/MetaSLAM/360FusionNeRF. keywords: {Training;Visualization;Image color analysis;Source coding;Semantics;Semisupervised learning;Feature extraction;Scene representation;View synthesis;Neural Radiance Field;360° image;3D deep learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341346&isnumber=10341342

Z. Yu, Z. Qiao, L. Qiu, H. Yin and S. Shen, "Multi-Session, Localization-Oriented and Lightweight LiDAR Mapping Using Semantic Lines and Planes," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7210-7217, doi: 10.1109/IROS55552.2023.10341462.Abstract: In this paper, we present a centralized framework for multi-session LiDAR mapping in urban environments, by utilizing lightweight line and plane map representations instead of widely used point clouds. The proposed framework achieves consistent mapping in a coarse-to-fine manner. Global place recognition is achieved by associating lines and planes on the Grassmannian manifold, followed by an outlier rejection-aided pose graph optimization for map merging. Then a novel bundle adjustment is also designed to improve the local consistency of lines and planes. In the experimental section, both public and self-collected datasets are used to demonstrate efficiency and effectiveness. Extensive results validate that our LiDAR mapping framework could merge multi-session maps globally, optimize maps incrementally, and is applicable for lightweight robot localization. keywords: {Point cloud compression;Manifolds;Laser radar;Urban areas;Semantics;Merging;Pipelines},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341462&isnumber=10341342

H. Xie, X. Zhong, B. Chen, P. Peng, H. Hu and Q. Liu, "Real-Time Elevation Mapping with Bayesian Ground Filling and Traversability Analysis for UGV Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7218-7225, doi: 10.1109/IROS55552.2023.10341662.Abstract: Unmanned ground vehicles (UGVs) require effective perception and analysis of their surrounding terrain for safe operation. This paper presents a novel approach to their local elevation mapping and traversability analysis using sparse data from a single LiDAR sensor, which can generate a dense local traversability map in real-time. By preserving ground height information, our method can differentiate between vertical obstacles, suspended objects and other terrains in the elevation map. The modified Bayesian generalized kernel elevation inference is utilized to predict and fill in sparse elevation maps to achieve local dense terrain traversability mapping. The system uses GPU parallel processing to accelerate calculations, ensuring real-time perception and dynamic processing. The proposed system was tested in both structured and unstructured environments, and achieved better performances in map filling and handling of suspended and vertical objects compared to other existing approaches. keywords: {Point cloud compression;Visualization;Navigation;Parallel processing;Robot sensing systems;Real-time systems;Filling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341662&isnumber=10341342

M. Mohrat, A. Berkaev, A. Burkov and S. Kolyubin, "Geometrically Consistent Monocular Metric-Semantic 3D Mapping for Indoor Environments with Transparent and Reflecting Objects," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7226-7232, doi: 10.1109/IROS55552.2023.10342329.Abstract: 3D mapping is crucial for many applications in robotics and related industries. To build dense high-quality point clouds accurate depth estimation or completion is needed. This paper presents the development of a metric-semantic mapping pipeline based on Deep Neural Networks (DNN) which assures geometrical consistency with enhancements for chal-lenging environments with transparent and reflecting objects like glass walls, doors, and mirrors. The suggested approach uses camera ego-motion alongside its sparse visual features to avoid the scale ambiguity issue caused by monocular depth affine-invariant estimations and to able to restore metric consistent depth information. Visual-inertial odometry data is used for camera pose graph optimization with no need to use RGB-D cameras. The proposed pipeline incorporates semantic segmentation and robust filtering to refine point clouds by removing outliers associated with mirrors and glass surfaces. Latency-aware performance and quality evaluation of 3D scene reconstruction were carried out on both a specially prepared dataset that reflects office-like scenes with multiple transparent objects and a public ScanNet dataset. The quantitative and qualitative results show that the proposed solution outperforms other state-of-art DNN-based models and algorithms as well as RGB-D cameras in terms of metric depth geometric consistency, 3D reconstruction accuracy, and the ability to preserve mesh quality in challenging scenarios with transparent and reflective surfaces. keywords: {Measurement;Point cloud compression;Surface reconstruction;Three-dimensional displays;Pipelines;Estimation;Glass},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342329&isnumber=10341342

D. Seichter, B. Stephan, S. B. Fischedick, S. Müller, L. Rabes and H. -M. Gross, "PanopticNDT: Efficient and Robust Panoptic Mapping," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7233-7240, doi: 10.1109/IROS55552.2023.10342137.Abstract: As the application scenarios of mobile robots are getting more complex and challenging, scene understanding becomes increasingly crucial. A mobile robot that is supposed to operate autonomously in indoor environments must have precise knowledge about what objects are present, where they are, what their spatial extent is, and how they can be reached; i.e., information about free space is also crucial. Panoptic mapping is a powerful instrument providing such information. However, building 3D panoptic maps with high spatial resolution is challenging on mobile robots, given their limited computing capabilities. In this paper, we propose PanopticNDT – an efficient and robust panoptic mapping approach based on occupancy normal distribution transform (NDT) mapping. We evaluate our approach on the publicly available datasets Hypersim and ScanNetV2. The results reveal that our approach can represent panoptic information at a higher level of detail than other state-of-the-art approaches while enabling real-time panoptic mapping on mobile robots. Finally, we prove the real-world applicability of PanopticNDT with qualitative results in a domestic application. keywords: {Three-dimensional displays;Instruments;Buildings;Transforms;Gaussian distribution;Real-time systems;Indoor environment},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342137&isnumber=10341342

M. Hansen and D. Wettergreen, "Range-based GP Maps: Local Surface Mapping for Mobile Robots using Gaussian Process Regression in Range Space," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7241-7248, doi: 10.1109/IROS55552.2023.10341949.Abstract: This work introduces range-based GP maps, which directly represent terrain by modeling the range from a LiDAR sensor as a Gaussian process (GP) in spherical space. Such a model aligns the predicted uncertainty from the GP regression with the uncertainty in the underlying sensor observations. Experimental evaluation on simulated natural terrain indicates that local range-based GP maps perform comparably to elevation-based methods when predicting terrain height, with the former producing more stable parameters and providing a better uncertainty representation. An aggregation method is proposed using the pose as an additional input to the GP. Unlike their elevation-based counterparts, range-based GP maps are capable of modeling overhangs and vertical obstacles with ease, demonstrated with examples of maps built on real-world data from a fully 3D subterranean environment. keywords: {Location awareness;Solid modeling;Uncertainty;Laser radar;Three-dimensional displays;Runtime;Navigation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341949&isnumber=10341342

L. Wu, C. L. Gentil and T. Vidal-Calleja, "Pseudo Inputs Optimisation for Efficient Gaussian Process Distance Fields," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7249-7255, doi: 10.1109/IROS55552.2023.10342483.Abstract: Robots reason about the environment through dedicated representations. Despite the fact that Gaussian Process (GP)-based representations are appealing due to their probabilistic and continuous nature, the cubic computational complexity is a concern. In this paper, we present a novel efficient GP-based representation that has the ability to produce accurate distance fields and is parameterised by the optimal locations of pseudo inputs. When applying the proposed method together with a kernel approximation approach, we show it outperforms well-established sparse GP frameworks in efficiency and accuracy. Moreover, we extend the proposed method to work in a dynamic setting, where a map is built iteratively and the scene dynamics are accounted for by adding or removing objects from the environment representation. In a nutshell, our method provides the ability to infer dynamic distance fields and achieve state-of-the-art reconstruction efficiently. keywords: {Uncertainty;Optimization methods;Gaussian processes;Probabilistic logic;Computational efficiency;Kernel;Computational complexity;Gaussian Process;Euclidean Distance Fields;Mapping},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342483&isnumber=10341342

P. Blumenberg, A. Schmidt and A. T. Becker, "Computing Motion Plans for Assembling Particles with Global Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7296-7302, doi: 10.1109/IROS55552.2023.10341556.Abstract: We investigate motion planning algorithms for the assembly of shapes in the tilt model in which unit-square tiles move in a grid world under the influence of uniform external forces and self-assemble according to certain rules. We provide several heuristics and experimental evaluation of their success rate, solution length, and runtime. keywords: {Runtime;Shape;Computational modeling;Planning;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341556&isnumber=10341342

M. Bhatt, Y. Jia and N. Mehr, "Efficient Constrained Multi-Agent Trajectory Optimization Using Dynamic Potential Games," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7303-7310, doi: 10.1109/IROS55552.2023.10342328.Abstract: Although dynamic games provide a rich paradigm for modeling agents' interactions, solving these games for real-world applications is often challenging. Many real-world interactive settings involve general nonlinear state and input constraints that couple agents' decisions with one another. In this work, we develop an efficient and fast planner for interactive trajectory optimization in constrained setups using a constrained game-theoretical framework. Our key insight is to leverage the special structure of agents' objective and constraint functions that are common in multi-agent interactions for fast and reliable planning. More precisely, we identify the structure of agents' cost and constraint functions under which the resulting dynamic game is an instance of a constrained dynamic potential game. Constrained dynamic potential games are a class of games for which instead of solving a set of coupled constrained optimal control problems, a constrained Nash equilibrium, i.e. a Generalized Nash equilibrium, can be found by solving a single constrained optimal control problem. This simplifies constrained interactive trajectory optimization significantly. We compare the performance of our method in a navigation setup involving four planar agents and show that our method is on average 20 times faster than the state-of-the-art. We further provide experimental validation of our proposed method in a navigation setup involving two quadrotors carrying a rigid object while avoiding collisions with two humans. keywords: {Costs;Navigation;Optimal control;Games;Nash equilibrium;Planning;Reliability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342328&isnumber=10341342

C. Li, H. Ma, J. Wang and M. Q. . -H. Meng, "Bidirectional Search Strategy for Incremental Search-based Path Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7311-7317, doi: 10.1109/IROS55552.2023.10342039.Abstract: Planning a collision-free path efficiently among obstacles is crucial in robotics. Conventional one-shot unidirectional path planning algorithms work well in the static environment, but cannot respond to the environment changes timely in the dynamic environment. To tackle this issue and improve the search efficiency, we propose a bidirectional incremental search method, Bidirectional Lifelong Planning A* (BLPA*), which searches in the forward and backward directions and performs incremental search bidirectionally when the environment changes. Furthermore, inspired by the robot perception range limitation and BLPA*, we propose the fractional bidirectional D* Lite (fBD* Lite(dp)), which constraints the forward search to the robot perception range and uses the backward search to expand the rest area. Our simulation results demonstrate BLPA* and mD* Lite(dp) can achieve superior performance in the dynamic environment. It reveals that the bidirectional incremental search strategy can be a general and efficient technique for graph-search-based robot path planning methods. keywords: {Heuristic algorithms;Simulation;Search problems;Path planning;Planning;Collision avoidance;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342039&isnumber=10341342

Z. Song, R. Zhang and X. Cheng, "HELSA: Hierarchical Reinforcement Learning with Spatiotemporal Abstraction for Large-Scale Multi-Agent Path Finding," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7318-7325, doi: 10.1109/IROS55552.2023.10342261.Abstract: The Multi-Agent Path Finding (MAPF) problem is a critical challenge in dynamic multi-robot systems. Recent studies have revealed that multi-agent reinforcement learning (MARL) is a promising approach to solving MAPF problems in a fully decentralized manner. However, as the size of the multi-robot system increases, sample inefficiency becomes a major impediment to learning-based methods. This paper presents a hierarchical reinforcement learning (HRL) framework for large-scale multi-agent path finding, featuring applying spatial and temporal abstraction to capture intermediate reward and thus encourage efficient exploration. Specifically, we introduce a meta controller that partitions the map into interconnected regions and optimizes agents' region-wise paths towards globally better solutions. Additionally, we design a lower-level controller that efficiently solves each sub-problem by incorporating heuristic guidance and an inter-agent communication mechanism with RL-based policies. Our empirical results on test instances of various scales demonstrate that our method outperforms existing approaches in terms of both success rate and makespan. keywords: {Learning systems;Reinforcement learning;Routing;Spatiotemporal phenomena;Multi-robot systems;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342261&isnumber=10341342

K. Ahn, H. Park and J. Park, "Learning to Schedule in Multi-Agent Pathfinding," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7326-7332, doi: 10.1109/IROS55552.2023.10342073.Abstract: In this work, we consider the problem of allocating tasks and planning paths for multiple robots to operate cooper-atively. We formulate the problem as a bi-level optimization that involves optimizing the scheduling of robots and the collision-free path for each robot. To address the complexity of the environment with obstacles, we introduce a congestion-aware state representation technique with the aid of graph neural networks. We also derive a joint scheduling policy using multi-agent reinforcement learning, and we propose an additional auxiliary loss that promotes coordination among the robots. Through empirical evaluation, we demonstrate the effectiveness of our approach in solving the combined task allocation and path planning problem in a cooperative multi-robot system. keywords: {Schedules;Robot kinematics;Reinforcement learning;Path planning;Resource management;Multi-robot systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342073&isnumber=10341342

J. Blumenkamp, Q. Li, B. Wang, Z. Liu and A. Prorok, "See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7333-7340, doi: 10.1109/IROS55552.2023.10342535.Abstract: We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person- view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate to the target along the shortest path. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to 2.0 × improvement in SPL (Success weighted by Path Length) when compared to a communication-free baseline. This is done without requiring a global map, positioning data, nor pre-calibration of the sensor network. Second, we perform a zero-shot transfer of our model from simulation to the real world. Laboratory experiments demonstrate the feasibility of our approach in various cluttered environments. Finally, we showcase examples of successful navigation to the target while both the sensor network layout as well as obstacles are dynamically reconfigured as the robot navigates. We provide a video demo11https://www.youtube.com/watch?v=kcrnr6RUgucw, the dataset, trained models, and source code22https://github.com/proroklab/sensor-guided-visual-nav. keywords: {Visualization;Navigation;Layout;Robot sensing systems;Graph neural networks;Sensor systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342535&isnumber=10341342

P. Surynek, "Counterexample Guided Abstraction Refinement with Non-Refined Abstractions for Multi-Goal Multi-Robot Path Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7341-7347, doi: 10.1109/IROS55552.2023.10341952.Abstract: We address the problem of multi-goal multi robot path planning (MG-MRPP) via counterexample guided abstraction refinement (CEGAR) framework. MG-MRPP generalizes the standard discrete multi-robot path planning (MRPP) problem. While the task in MRPP is to navigate robots in an undirected graph from their starting vertices to one individual goal vertex per robot, MG-MRPP assigns each robot multiple goal vertices and the task is to visit each of them at least once. Solving MG-MRPP not only requires finding collision free paths for individual robots but also determining the order of visiting robot's goal vertices so that common objectives like the sum-of-costs are optimized. We use the Boolean satisfiability (SAT) techniques as the underlying paradigm. A specifically novel in this work is the use of non-refined abstractions when formulating the MG-MRPP problem as SAT. While the standard CEGAR approach for MG-MRPP does not encode collision elimination constraints in the initial abstraction and leave them to subsequent refinements. The novel CEGAR approach leaves some abstractions deliberately non-refined. This adds the necessity to post-process the answers obtained from the underlying SAT solver as these answers slightly differ from the correct MG-MRPP solutions. Non-refining however yields order-of-magnitude smaller SAT encodings than those of the previous CEGAR approach and speeds up the overall solving process. keywords: {Navigation;Path planning;Encoding;Collision avoidance;Task analysis;Robots;Standards},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341952&isnumber=10341342

X. Lyu, A. Banitalebi-Dehkordi, M. Chen and Y. Zhang, "Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7348-7353, doi: 10.1109/IROS55552.2023.10342281.Abstract: Cooperative multi-agent problems often require coordination between agents, which can be achieved through a centralized policy that considers the global state. Multi-agent policy gradient (MAPG) methods are commonly used to learn such policies, but they are often limited to problems with low-level action spaces. In complex problems with large state and action spaces, it is advantageous to extend MAPG methods to use higher-level actions, also known as options, to improve the policy search efficiency. However, multi-robot option executions are often asynchronous, that is, agents may select and complete their options at different time steps. This makes it difficult for MAPG methods to derive a centralized policy and evaluate its gradient, as centralized policy always select new options at the same time. In this work, we propose a novel, conditional reasoning approach to address this problem and demonstrate its effectiveness on representative option-based multi-agent cooperative tasks through empirical validation. Find code and videos at: https://sites.google.com/view/mahrlsupp/ keywords: {Codes;Search problems;Cognition;Multi-robot systems;Task analysis;Intelligent robots;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342281&isnumber=10341342

S. Bone, L. Bartolomei, F. Kennel-Maushart and M. Chli, "Decentralised Multi-Robot Exploration Using Monte Carlo Tree Search," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7354-7361, doi: 10.1109/IROS55552.2023.10341485.Abstract: Autonomous robotic systems are useful in automating tasks such as inspection and surveying of unknown areas, where speed is often an important factor. In order to effectively reduce the time required to complete missions, an efficient exploration and coordination strategy is needed. In this spirit, this work proposes an approach based on the Monte Carlo Tree Search (MCTS) algorithm to guide robots during exploration missions. Our method first expands a search tree of possible actions from the robot's position towards unknown regions, and then selects the sequence of movements that best drive the exploration process forward with respect to a given reward function. The proposed approach, which is able to balance short- and long-term decision-making, is then extended to accommodate the presence of multiple robots, in a bid to push the efficiency of exploration further. Our method allows for the coordination of the robots' movements in a decentralized manner, relying on point-to-point communication. This results in an efficient strategy, which we refer to as Decentralized Monte Carlo Exploration (DMCE). The experimental results demonstrate that our pipeline outperforms a greedy exploration approach, as well as state-of-the-art planners, with up to 30% reduction in exploration times in a series of real-world maps. keywords: {Monte Carlo methods;Robot kinematics;Pipelines;Decision making;Inspection;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341485&isnumber=10341342

C. Leet, C. Oh, M. Lora, S. Koenig and P. Nuzzo, "Task Assignment, Scheduling, and Motion Planning for Automated Warehouses for Million Product Workloads," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7362-7369, doi: 10.1109/IROS55552.2023.10341755.Abstract: We address the Warehouse Servicing Problem (WSP) in automated warehouses, which use teams of mobile robots to move products from shelves to packaging stations. Given a list of products, the WSP amounts to finding a motion plan which brings every product on the list from a shelf to a packaging station within a given time limit. The WSP consists of four subproblems, namely, deciding where to source and deposit a product (task formulation), who should transport each product (task assignment) and when (scheduling) and how (motion planning). These problems are NP-Hard individually and made more challenging by their interdependence. The difficulty of the WSP is compounded by the scale of automated warehouses, which use teams of hundreds of agents to transport thousands of products. In this paper, we present Contract-based Cyclic Motion Planning (CCMP), a novel contract-based methodology for solving the WSP at scale. CCMP decomposes a warehouse into a set of traffic system components. By assigning each component a contract which describes the traffic flows it can support, CCMP can generate a traffic flow which satisfies a given WSP instance. CCMP then uses a novel motion planner to transform this traffic flow into a motion plan for a team of robots. Evaluation shows that CCMP can solve WSP instances taken from real industrial scenarios with up to 1 million products while outperforming other methodologies for solving the WSP by up to 2.9×. keywords: {Job shop scheduling;Service robots;Transforms;Packaging;Planning;Mobile robots;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341755&isnumber=10341342

C. Zhang, Z. Huang, M. H. Ang and D. Rus, "LiDAR Missing Measurement Detection for Autonomous Driving in Rain," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7393-7399, doi: 10.1109/IROS55552.2023.10341932.Abstract: Autonomous driving in rain remains challenging. Rain causes sensor performance degradation that can affect sensor measurement quality. During the rain, lasers may suffer from energy loss due to raindrop absorption. As a result, some laser measurements reflected from obstacles may not be recognized by the LiDAR sensor, thus raising potential risks for autonomous vehicles. This work investigates a novel task that aims to detect those missing measurements. Our solution uses a two-stage learning method to generate an anomaly score for each missing measurement, representing the likelihood of being caused by rain. We evaluate our method with real-world data and demonstrate its effectiveness in identifying anomalous missing measurements through qualitative and quantitative experiments. keywords: {Degradation;Rain;Laser radar;Snow;Measurement by laser beam;Robot sensing systems;Loss measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341932&isnumber=10341342

C. Zhang, Z. Huang, B. X. L. Tung, M. H. Ang and D. Rus, "SMART-Degradation: A Dataset for LiDAR Degradation Evaluation in Rain," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7400-7406, doi: 10.1109/IROS55552.2023.10342323.Abstract: Sensor degradation is one of the major challenges for autonomous driving. During the rain, the interference from raindrops can negatively influence LiDAR measurements. For example, valid measurements could be reduced during the rain, and some measurements may become noisy. Unreliable measurements can lead to potential safety issues if autonomous driving systems are unaware of these changes. In this work, we will release a naturalistic driving dataset to advance the research in studying LiDAR degradation. Our dataset consists of 3D LiDAR scans collected by a data collection vehicle under various rainy conditions. Besides these raw scans, we also release LiDAR scan pairs (each pair consists of one scan from rainy weather and one scan from clear weather at the same location). These LiDAR pairs are developed to help researchers identify LiDAR degradation. Finally, we will release a toolbox integrated with mapping, localization, and scan synthesis functions used to create this dataset. This toolbox can facilitate dataset creation for studying degradation in other harsh weather conditions. More information can be found at https://smart-rain-dataset.github.io/. keywords: {Degradation;Location awareness;Laser radar;Rain;Three-dimensional displays;Safety;Noise measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342323&isnumber=10341342

Z. Pang, D. Ramanan, M. Li and Y. -X. Wang, "Streaming Motion Forecasting for Autonomous Driving," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7407-7414, doi: 10.1109/IROS55552.2023.10341894.Abstract: Trajectory forecasting is a widely-studied problem for autonomous navigation. However, existing benchmarks evaluate forecasting based on independent snapshots of trajectories, which are not representative of real-world applications that operate on a continuous stream of data. To bridge this gap, we introduce a benchmark that continuously queries future trajectories on streaming data and we refer to it as “streaming forecasting.” Our benchmark inherently captures the disappearance and re-appearance of agents, presenting the emergent challenge of forecasting for occluded agents, which is a safetycritical problem yet overlooked by snapshot-based benchmarks. Moreover, forecasting in the context of continuous timestamps naturally asks for temporal coherence between predictions from adjacent timestamps. Based on this benchmark, we further provide solutions and analysis for streaming forecasting. We propose a plug-and-play meta-algorithm called “Predictive Streamer” that can adapt any snapshot-based forecaster into a streaming forecaster. Our algorithm estimates the states of occluded agents by propagating their positions with multi-modal trajectories, and leverages differentiable filters to ensure temporal consistency. Both occlusion reasoning and temporal coherence strategies significantly improve forecasting quality, resulting in 25% smaller endpoint errors for occluded agents and 10-20% smaller fluctuations of trajectories. Our work is intended to generate interest within the community by highlighting the importance of addressing motion forecasting in its intrinsic streaming setting. Code is available at https://github.com/ziqipang/StreamingForecasting. keywords: {Fluctuations;Codes;Coherence;Benchmark testing;Filtering algorithms;Prediction algorithms;Cognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341894&isnumber=10341342

K. Tsiakas, D. Alexiou, D. Giakoumis, A. Gasteratos and D. Tzovaras, "Leveraging Multimodal Sensing and Topometric Mapping for Human-Like Autonomous Navigation in Complex Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7415-7421, doi: 10.1109/IROS55552.2023.10341358.Abstract: Autonomous vehicle navigation in complex and unpredictable outdoor environments requires extensive and detailed understanding of the surrounding area and compliance with the traffic rules. In this paper, we attempt to imitate human driver behavior towards autonomous navigation that is suitable for diverse, challenging environments, whether urban, semi-structured or rural-like. Our approach starts with a novel method that we propose for extracting free space area using RGB and LiDAR data, in combination with a rough topometric map for route planning. Local goals are extracted in the final drivable region, and the vehicle draws a local path in the free space that is approximately in line with the overall path via a lattice planner. Our method is evaluated both in the publicly available KITTI urban dataset and a custom-made dataset of a semi-structured environment. In both cases, the results highlight the potential of our approach for further advancements in autonomous navigation and the development of safer and more human-like behaviors in driverless vehicles compared to the existing trajectory prediction state-of-the-art methods that make use of a topometric map. keywords: {Space vehicles;Visualization;Navigation;Multimodal sensors;MIMICs;Behavioral sciences;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341358&isnumber=10341342

E. Fiasché, P. Martinet and E. Malis, "Towards Autonomous Robot Navigation in Human Populated Environments Using an Universal SFM and Parametrized MPC," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7422-7427, doi: 10.1109/IROS55552.2023.10341814.Abstract: Autonomous mobile robot navigation in a human populated and encumbered environment is recognized as a hard problem to be solved in real-time. Most of the time, robots face the so-called ‘Freezing Robot Problem’, that occurs when the robot stops because no feasible and safe motion can be found. In order to provide to the robot the capability of proactive navigation, in this work we generalize the classical Social Force Model into a Universal Social Force Model (USFM) that attributes to any object surrounding the robot (humans, robots, obstacles) a social behavior. Nonlinear Model Predictive Control (MPC) can be used to solve the autonomous navigation problem since it can take into account all the possible constraints coming from the interaction model between the robot and the different surrounding objects. However, to be effective, MPC requires a sufficiently large prediction horizon, which generally implies a high computational cost. In order to considerably reduce the computational cost, we propose a new control parametrisation based on Thin Plate Spline Radial Basis Functions that allow us to have a large prediction horizon with fewer parameters. The global control framework is validated in simulation with virtual pedestrians, and in real world environments. keywords: {Pedestrians;Navigation;Force;Real-time systems;Computational efficiency;Behavioral sciences;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341814&isnumber=10341342

A. Elhagry, H. Dai and A. El Saddik, "Pseudo-Stereo++: Cycled Generative Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7428-7435, doi: 10.1109/IROS55552.2023.10341610.Abstract: Recently, the feature-level generation has demonstrated the effectiveness of pseudo-stereo synthesis in Monocular 3D Detection (M3D). In this paper, we aim to further bridge the gap between the stereo and the monocular 3D object detectors in autonomous driving through direct image-level pseudo-stereo generation. We propose a novel Cycled Generative Pseudo-Stereo (CGPS) architecture to generate the right-view image from the left-view for constructing a pseudo-stereo pair to stereo 3D object detectors while maintaining the natural of M3D with the left-view image as the only input. Moreover, we use a triplet consistency loss to focus on the detected objects in the pseudo-stereo generation. Besides, we demonstrate that the proposed CGPS is an ad-hoc module to adapt top stereo 3D object detectors into monocular 3D object detectors. The proposed framework with CGPS achieves 74.80%, 55.28%, and 46.70% 3DAP for easy, moderate, and hard difficulty levels in monocular 3D detection on the KITTI benchmark with comparable performance to the stereo 3D object detectors but using a monocular image as the only input. Till the submission, the proposed M3D framework ranks 1st with dramatic improvements against the existing monocular 3D detectors on the KITTI benchmark. keywords: {Three-dimensional displays;Detectors;Object detection;Benchmark testing;Feature extraction;Autonomous vehicles;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341610&isnumber=10341342

Y. Shen et al., "Aggressive Trajectory Generation for a Swarm of Autonomous Racing Drones," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7436-7441, doi: 10.1109/IROS55552.2023.10341844.Abstract: Autonomous drone racing is becoming an excellent platform to challenge quadrotors' autonomy techniques including planning, navigation and control technologies. However, most research on this topic mainly focuses on single drone scenarios. In this paper, we describe a novel time-optimal trajectory generation method for generating time-optimal trajectories for a swarm of quadrotors to fly through pre-defined waypoints with their maximum maneuverability without collision. We verify the method in the Gazebo simulations where a swarm of 5 quadrotors can fly through a complex 6-waypoint racing track in a $35m\times 35m$ space with a top speed of 14m/s. Flight tests are performed on two quadrotors passing through 3 waypoints in a $4m\times 2m$ flight arena to demonstrate the feasibility of the proposed method in the real world. Both simulations and real-world flight tests show that the proposed method can generate the optimal aggressive trajectories for a swarm of autonomous racing drones. The method can also be easily transferred to other types of robot swarms. keywords: {Trajectory tracking;Navigation;Robot sensing systems;Trajectory;Sensors;Planning;Security},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341844&isnumber=10341342

D. M. Nguyen, M. Nazeri, A. Payandeh, A. Datar and X. Xiao, "Toward Human-Like Social Robot Navigation: A Large-Scale, Multi-Modal, Social Human Navigation Dataset," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7442-7447, doi: 10.1109/IROS55552.2023.10342447.Abstract: Humans are well-adept at navigating public spaces shared with others, where current autonomous mobile robots still struggle: while safely and efficiently reaching their goals, humans communicate their intentions and conform to unwritten social norms on a daily basis; conversely, robots become clumsy in those daily social scenarios, getting stuck in dense crowds, surprising nearby pedestrians, or even causing collisions. While recent research on robot learning has shown promises in data-driven social robot navigation, good-quality training data is still difficult to acquire through either trial and error or expert demonstrations. In this work, we propose to utilize the body of rich, widely available, social human navigation data in many natural human-inhabited public spaces for robots to learn similar, human-like, socially compliant navigation behaviors. To be specific, we design an open-source egocentric data collection sensor suite wearable by walking humans to provide multimodal robot perception data; we collect a large-scale (~100 km, 20 hours, 300 trials, 13 humans) dataset in a variety of public spaces which contain numerous natural social navigation interactions; we analyze our dataset, demonstrate its usability, and point out future research directions and use cases.11Website: https://cs.gmu.edu/-xiao/Research/MuSoHu/ keywords: {Legged locomotion;Pedestrians;Navigation;Social robots;Training data;Robot sensing systems;Robot learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342447&isnumber=10341342

B. Sharma, A. Sharma, K. M. Krishna and A. K. Singh, "Hilbert Space Embedding-Based Trajectory Optimization for Multi-Modal Uncertain Obstacle Trajectory Prediction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7448-7455, doi: 10.1109/IROS55552.2023.10342490.Abstract: Safe autonomous driving critically depends on how well the ego-vehicle can predict the trajectories of neighboring vehicles. To this end, several trajectory prediction algorithms have been presented in the existing literature. Many of these approaches output a multimodal distribution of obstacle trajectories instead of a single deterministic prediction to account for the underlying uncertainty. However, existing planners cannot handle the multimodality based on just sample-level information of the predictions. With this motivation, this paper proposes a trajectory optimizer that can leverage the distributional aspects of the prediction in a computationally tractable and sample-efficient manner. Our optimizer can work with arbitrarily complex distributions and thus can be used with output distribution represented as a deep neural network. The core of our approach is built on embedding distribution in Reproducing Kernel Hilbert Space (RKHS), which we leverage in two ways. First, we propose an RKHS embedding approach to select probable samples from the obstacle trajectory distribution. Second, we rephrase chance-constrained optimization as distribution matching in RKHS and propose a novel sampling-based optimizer for its solution. We validate our approach with handcrafted and neural network-based predictors trained on real-world datasets and show improvement over the existing stochastic optimization approaches in safety metrics. keywords: {Measurement;Uncertainty;Prediction algorithms;Hilbert space;Trajectory;Safety;Kernel},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342490&isnumber=10341342

J. Wu, Y. Wang, H. Asama, Q. An and A. Yamashita, "Risk-Sensitive Mobile Robot Navigation in Crowded Environment via Offline Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7456-7462, doi: 10.1109/IROS55552.2023.10341948.Abstract: Mobile robot navigation in a human-populated environment has been of great interest to the research community in recent years, referred to as crowd navigation. Currently, offline reinforcement learning (RL)-based method has been introduced to this domain, for its ability to alleviate the sim2real gap brought by online RL which relies on simulators to execute training, and its scalability to use the same dataset to train for differently customized rewards. However, the performance of the navigation policy suffered from the distributional shift between the training data and the input during deployment, since when it gets an input out of the training data distribution, the learned policy has the risk of choosing an erroneous action that leads to catastrophic failure such as colliding with a human. To realize risk sensitivity and improve the safety of the offline RL agent during deployment, this work proposes a multipolicy control framework that combines offline RL navigation policy with a risk detector and a force-based risk-avoiding policy. In particular, a Lyapunov density model is learned using the latent feature of the offline RL policy and works as a risk detector to switch the control to the risk-avoiding policy when the robot has a tendency to go out of the area supported by the training data. Experimental results showed that the proposed method was able to learn navigation in a crowded scene from the offline trajectory dataset and the risk detector substantially reduces the collision rate of the vanilla offline RL agent while maintaining the navigation efficiency outperforming the state-of-the-art methods. keywords: {Training;Navigation;Training data;Reinforcement learning;Detectors;Data models;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341948&isnumber=10341342

I. S. Mohamed, M. Ali and L. Liu, "GP-Guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7463-7470, doi: 10.1109/IROS55552.2023.10341382.Abstract: Robotic navigation in unknown, cluttered environ-ments with limited sensing capabilities poses significant chal-lenges in robotics. Local trajectory optimization methods, such as Model Predictive Path Intergal (MPPI), are a promising solution to this challenge. However, global guidance is required to ensure effective navigation, especially when encountering challenging environmental conditions or navigating beyond the planning horizon. This study presents the GP-MPPI, an online learning-based control strategy that integrates MPPI with a local perception model based on Sparse Gaussian Process (SGP). The key idea is to leverage the learning capability of SGP to construct a variance (uncertainty) surface, which enables the robot to learn about the navigable space surrounding it, identify a set of suggested subgoals, and ultimately recommend the optimal subgoal that minimizes a predefined cost function to the local MPPI planner. Afterward, MPPI computes the optimal control sequence that satisfies the robot and collision avoidance constraints. Such an approach eliminates the necessity of a global map of the environment or an offline training process. We validate the efficiency and robustness of our proposed control strategy through both simulated and real-world experiments of 2D autonomous navigation tasks in complex unknown en-vironments, demonstrating its superiority in guiding the robot safely towards its desired goal while avoiding obstacles and escaping entrapment in local minima. The GPU implementation of GP-MPPI, including the supplementary video, is available at https://github.com/IhabMohamed/GP-MPPI. keywords: {Training;Uncertainty;Navigation;Optimal control;Aerospace electronics;Robot sensing systems;Cost function},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341382&isnumber=10341342

Z. Deng, Y. Shi and W. Shen, "V2X-Lead: LiDAR-Based End-to-End Autonomous Driving with Vehicle-to-Everything Communication Integration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7471-7478, doi: 10.1109/IROS55552.2023.10342375.Abstract: This paper presents a LiDAR-based end-to-end autonomous driving method with Vehicle-to-Everything (V2X) communication integration, termed V2X-Lead, to address the challenges of navigating unregulated urban scenarios under mixed-autonomy traffic conditions. The proposed method aims to handle imperfect partial observations by fusing the onboard LiDAR sensor and V2X communication data. A model-free and off-policy deep reinforcement learning (DRL) algorithm is employed to train the driving agent, which incorporates a carefully designed reward function and multi-task learning technique to enhance generalization across diverse driving tasks and scenarios. Experimental results demonstrate the effectiveness of the proposed approach in improving safety and efficiency in the task of traversing unsignalized intersections in mixed-autonomy traffic, and its generalizability to previously unseen scenarios, such as roundabouts. The integration of V2X communication offers a significant data source for autonomous vehicles (AVs) to perceive their surroundings beyond onboard sensors, resulting in a more accurate and comprehensive perception of the driving environment and more safe and robust driving behavior. keywords: {Laser radar;Protocols;Soft sensors;Reinforcement learning;Multitasking;Robot sensing systems;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342375&isnumber=10341342

C. Chen, P. Xiang, H. Lu, Y. Wang and R. Xiong, "C2: Co-design of Robots via Concurrent-Network Coupling Online and Offline Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7487-7494, doi: 10.1109/IROS55552.2023.10341983.Abstract: With the increasing computing power, using data-driven approaches to co-design a robot's morphology and controller has become a promising way. However, most existing data-driven methods require training the controller for each morphology to calculate fitness, which is time-consuming. In contrast, the dual-network framework utilizes data collected by individual networks under a specific morphology to train a population network that provides a surrogate function for morphology optimization. This approach replaces the traditional evaluation of a diverse set of candidates, thereby speeding up the training. Despite considerable results, the online training of both networks impedes their performance. To address this issue, we propose a concurrent network framework that combines online and offline reinforcement learning (RL) methods. By leveraging the behavior cloning term in a flexible manner, we achieve an effective combination of both networks. We conducted multiple sets of comparative experiments in the simulator and found that the proposed method effectively addresses issues present in the dual-network framework, leading to overall algorithmic performance improvement. Furthermore, we validated the algorithm on a real robot, demonstrating its feasibility in a practical application. keywords: {Training;Sociology;Morphology;Reinforcement learning;Robot sensing systems;Control systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341983&isnumber=10341342

Z. Li, J. Xin and N. Li, "Autonomous Exploration and Mapping for Mobile Robots via Cumulative Curriculum Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7495-7500, doi: 10.1109/IROS55552.2023.10342066.Abstract: Deep reinforcement learning (DRL) has been widely applied in autonomous exploration and mapping tasks, but often struggles with the challenges of sampling efficiency, poor adaptability to unknown map sizes, and slow simulation speed. To speed up convergence, we combine curriculum learning (CL) with DRL, and first propose a Cumulative Curriculum Reinforcement Learning (CCRL) training framework to alleviate the issue of catastrophic forgetting faced by general CL. Besides, we present a novel state representation, which considers a local egocentric map and a global exploration map resized to the fixed dimension, so as to flexibly adapt to environments with various sizes and shapes. Additionally, for facilitating the fast training of DRL models, we develop a lightweight grid-based simulator, which can substantially accelerate simulation compared to popular robot simulation platforms such as Gazebo. Based on the customized simulator, comprehensive experiments have been conducted, and the results show that the CCRL framework not only mitigates the catastrophic forgetting problem, but also improves the sample efficiency and generalization of DRL models, compared to general CL as well as without a curriculum. Our code is available at https://github.com/BeamanLi/CCRL_Exploration. keywords: {Training;Deep learning;Adaptation models;Codes;Shape;Reinforcement learning;Mobile robots;Task analysis;Intelligent robots;Convergence},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342066&isnumber=10341342

R. Zhu, S. Li, T. Dai, C. Zhang and O. Celiktutan, "Learning to Solve Tasks with Exploring Prior Behaviours," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7501-7507, doi: 10.1109/IROS55552.2023.10342272.Abstract: Demonstrations are widely used in Deep Reinforcement Learning (DRL) for facilitating solving tasks with sparse rewards. However, the tasks in real-world scenarios can often have varied initial conditions from the demonstration, which would require additional prior behaviours. For example, consider we are given the demonstration for the task of picking up an object from an open drawer, but the drawer is closed in the training. Without acquiring the prior behaviours of opening the drawer, the robot is unlikely to solve the task. To address this, in this paper we propose an Intrinsic Reward Driven Example-based Control (IRDEC). Our method can endow agents with the ability to explore and acquire the required prior behaviours and then connect to the task-specific behaviours in the demonstration to solve sparse-reward tasks without requiring additional demonstration of the prior behaviours. The performance of our method outperforms other baselines on three navigation tasks and one robotic manipulation task with sparse rewards. Codes are available at https://github.com/Ricky-Zhu/IRDEC. keywords: {Training;Deep learning;Codes;Navigation;Reinforcement learning;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342272&isnumber=10341342

S. Gronauer, D. Stümke and K. Diepold, "Comparing Quadrotor Control Policies for Zero-Shot Reinforcement Learning under Uncertainty and Partial Observability," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7508-7514, doi: 10.1109/IROS55552.2023.10341941.Abstract: To alleviate the sample complexity of reinforcement learning algorithms, simulations are a common approach to train control policies before deploying the policy on a real-world robot. However, a gap between simulation and reality generally persists, which endorses the aim to train robust policies already in simulation such that those can be transferred to a real robot at a high success rate. In this paper, we investigate history-dependent policies for drone control in the context of zero-shot transfer learning, where the training is conducted exclusively in simulation. We compare policies represented by feed-forward neural networks with recurrent neural networks and assess both performance and robustness on a real-world quadrotor. Furthermore, we study if an end-to-end learned representation can control a quadrotor based on raw onboard-sensor information only, rendering accurate state estimation from a Kalman filter obsolete. Our results show that recurrent control policies achieve similar performance and robustness as feed-forward policies when acting on state estimates. With raw sensory data, however, recurrent networks offer higher success rates for sim-to-real transfer than feed-forward networks. We also find that recurrent architectures are advantageous when system parameters such as latency are uncertain. keywords: {Training;Uncertainty;Transfer learning;Reinforcement learning;Robot sensing systems;Rendering (computer graphics);Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341941&isnumber=10341342

J. Thumm, G. Pelat and M. Althoff, "Reducing Safety Interventions in Provably Safe Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7515-7522, doi: 10.1109/IROS55552.2023.10342464.Abstract: Deep Reinforcement Learning (RL) has shown promise in addressing complex robotic challenges. In real-world applications, RL is often accompanied by failsafe controllers as a last resort to avoid catastrophic events. While necessary for safety, these interventions can result in undesirable behaviors, such as abrupt braking or aggressive steering. This paper proposes two safety intervention reduction methods: proactive replacement and proactive projection, which change the action of the agent if it leads to a potential failsafe intervention. These approaches are compared to state-of-the-art constrained RL on the OpenAI safety gym benchmark and a human-robot collab-oration task. Our study demonstrates that the combination of our method with provably safe RL leads to high-performing policies with zero safety violations and a low number of failsafe interventions. Our versatile method can be applied to a wide range of real-world robotic tasks, while effectively improving safety without sacrificing task performance. keywords: {Deep learning;Reinforcement learning;Manipulators;Robustness;Production facilities;Safety;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342464&isnumber=10341342

J. Gao, S. Reddy, G. Berseth, A. D. Dragan and S. Levine, "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7523-7530, doi: 10.1109/IROS55552.2023.10341779.Abstract: Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated navigation task by using their eye gaze to modulate a 128-dimensional command signal from their webcam. The results show that our method enables successful goal navigation more often than a baseline directional interface, by learning to denoise user commands signals and provide shared autonomy assistance. We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated user commands, and find that our method improves over baseline interfaces in these domains as well. Extensive ablation experiments with simulated user commands empirically motivate each component of our method. keywords: {Training;Space vehicles;Machine learning algorithms;Navigation;Webcams;Moon;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341779&isnumber=10341342

R. Soni, D. Harnack, H. Isermann, S. Fushimi, S. Kumar and F. Kirchner, "End-to-End Reinforcement Learning for Torque Based Variable Height Hopping," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7531-7538, doi: 10.1109/IROS55552.2023.10342187.Abstract: Legged locomotion is arguably the most suited and versatile mode to deal with natural or unstructured terrains. Intensive research into dynamic walking and running controllers has recently yielded great advances, both in the optimal control and reinforcement learning (RL) literature. Hopping is a challenging dynamic task involving a flight phase and has the potential to increase the traversability of legged robots. Model based control for hopping typically relies on accurate detection of different jump phases, such as lift-off or touch down, and using different controllers for each phase. In this paper, we present a end-to-end RL based torque controller that learns to implicitly detect the relevant jump phases, removing the need to provide manual heuristics for state detection. We also extend a method for simulation to reality transfer of the learned controller to contact rich dynamic tasks, resulting in successful deployment on the robot after training without parameter tuning. keywords: {Training;Legged locomotion;Torque;Torque control;Neural networks;Propioception;Optimal control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342187&isnumber=10341342

L. Sacchetto, M. Korte, S. Gronauer, M. Kissel and K. Diepold, "Offline Reinforcement Learning for Quadrotor Control: Overcoming the Ground Effect," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7539-7544, doi: 10.1109/IROS55552.2023.10341599.Abstract: Applying Reinforcement Learning to solve real-world optimization problems presents significant challenges because of the large amount of data normally required. A popular solution is to train the algorithms in a simulation and transfer the weights to the real system. However, sim-to-real approaches are prone to fail when the Reality Gap is too big, e.g. in robotic systems with complex and non-linear dynamics. In this work, we propose the use of Offline Reinforcement Learning as a viable alternative to sim-to-real policy transfer to address such instances. On the example of a small quadrotor, we show that the ground effect causes problems in an otherwise functioning zero-shot sim-to-real framework. Our sim-to-real experiments show that, even with the explicit modelling of the ground effect and the employing of popular transfer techniques, the trained policies fail to capture the physical nuances necessary to perform a real-world take-off maneuver. Contrariwise, we show that state-of-the-art Offline Reinforcement Learning algorithms represent a feasible, reliable and sample efficient alternative in this use case. keywords: {Training;Heuristic algorithms;Reinforcement learning;Reliability;Optimization;Intelligent robots;Quadrotors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341599&isnumber=10341342

A. Karimi, J. Jin, J. Luo, A. R. Mahmood, M. Jagersand and S. Tosatto, "Dynamic Decision Frequency with Continuous Options," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7545-7552, doi: 10.1109/IROS55552.2023.10342408.Abstract: In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the problem's difficulty by requiring the agent to make numerous decisions to achieve its goal while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods on simulated continuous control tasks with various action-cycle times. We show that our algorithm's performance is not affected by the choice of environment interaction frequency. Furthermore, we demonstrate the efficacy of CTCO in facilitating exploration in a real-world visual reaching task for a 7 DOF robotic arm with sparse rewards. keywords: {Visualization;Time-frequency analysis;Heuristic algorithms;Reinforcement learning;Manipulators;Control systems;High frequency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342408&isnumber=10341342

Y. Lu et al., "Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7553-7560, doi: 10.1109/IROS55552.2023.10342038.Abstract: Imitation learning (IL) is a simple and powerful way to use high-quality human driving data, which can be collected at scale, to produce human-like behavior. However, policies based on imitation learning alone often fail to sufficiently account for safety and reliability concerns. In this paper, we show how imitation learning combined with reinforcement learning using simple rewards can substan-tially improve the safety and reliability of driving policies over those learned from imitation alone. In particular, we train a policy on over lOOk miles of urban driving data, and measure its effectiveness in test scenarios grouped by different levels of collision likelihood. Our analysis shows that while imitation can perform well in low-difficulty scenarios that are well-covered by the demonstration data, our proposed approach significantly improves robustness on the most challenging scenarios (over 38 % reduction in failures). To our knowledge, this is the first application of a combined imitation and reinforcement learning approach in autonomous driving that utilizes large amounts of real- world human driving data. keywords: {Training;Atmospheric measurements;Reinforcement learning;Particle measurements;Robustness;Safety;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342038&isnumber=10341342

S. Liu, C. Wu, Y. Li and L. Zhang, "Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7561-7567, doi: 10.1109/IROS55552.2023.10341990.Abstract: Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of a large amount of interactive feedback. This paper presents a new method that uses scores provided by humans instead of pairwise preferences to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by humans negatively impacting the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method for robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores while requiring less feedback compared to pairwise preference learning methods. The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL. keywords: {Training;Learning systems;Adaptive learning;Source coding;Reinforcement learning;Boosting;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341990&isnumber=10341342

J. Lee, J. Heo, D. Kim, G. Lee and S. Oh, "Dual Variable Actor-Critic for Adaptive Safe Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7568-7573, doi: 10.1109/IROS55552.2023.10341973.Abstract: Satisfying safety constraints in reinforcement learning (RL) is an important issue, especially in real-world applications. Many studies have approached safe RL with the Lagrangian method, which introduces dual variables. However, applying a trained policy with the optimal dual variable to a new environment can be hazardous since the optimal value of the dual variable, which represents a level of safety, depends on the environmental setting. To this end, we propose a new framework, dual variable actor-critic (DVAC), that solves the safe RL problem by simultaneously training a single policy over different safety levels. We introduce a universal policy and universal Q-function, which have a dual variable as an argument. Then, we extend the soft actor-critic so that the universal policy is guaranteed to converge to the Pareto optimal policy sets. We evaluate the proposed method in simulation and real-world environments. The universal policy learned with the proposed method ranges from extremely safe to high performance according to the dual variables, and is nearly Pareto optimal compared to policies learned with the baseline methods. In addition, the agent is able to adapt to environments with unseen state distributions without additional training by identifying a suitable dual variable using the proposed method. keywords: {Training;Reinforcement learning;Pareto optimization;Safety;Trajectory;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341973&isnumber=10341342

Y. Iioka, Y. Yoshida, Y. Wada, S. Hatanaka and K. Sugiura, "Multimodal Diffusion Segmentation Model for Object Segmentation from Manipulation Instructions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7590-7597, doi: 10.1109/IROS55552.2023.10341402.Abstract: In this study, we aim to develop a model that comprehends a natural language instruction (e.g., “Go to the living room and get the nearest pillow to the radio art on the wall”) and generates a segmentation mask for the target everyday object. The task is challenging because it requires (1) the understanding of the referring expressions for multiple objects in the instruction, (2) the prediction of the target phrase of the sentence among the multiple phrases, and (3) the generation of pixel-wise segmentation masks rather than bounding boxes. Studies have been conducted on language-based segmentation methods; however, they sometimes mask irrelevant regions for complex sentences. In this paper, we propose the Multimodal Diffusion Segmentation Model (MDSM), which generates a mask in the first stage and refines it in the second stage. We introduce a crossmodal parallel feature extraction mechanism and extend diffusion probabilistic models to handle crossmodal features. To validate our model, we built a new dataset based on the well-known Matterport3D and REVERIE datasets. This dataset consists of instructions with complex referring expressions accompanied by real indoor environmental images that feature various target objects, in addition to pixel-wise segmentation masks. The performance of MDSM surpassed that of the baseline method by a large margin of +10.13 mean IoU. keywords: {Image segmentation;Art;Natural languages;Object segmentation;Feature extraction;Transformers;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341402&isnumber=10341342

N. Wang, C. Shi, R. Guo, H. Lu, Z. Zheng and X. Chen, "InsMOS: Instance-Aware Moving Object Segmentation in LiDAR Data," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7598-7605, doi: 10.1109/IROS55552.2023.10342277.Abstract: Identifying moving objects is a crucial capability for autonomous navigation, consistent map generation, and future trajectory prediction of objects. In this paper, we propose a novel network that addresses the challenge of segmenting moving objects in 3D LiDAR scans. Our approach not only predicts point-wise moving labels but also detects instance information of main traffic participants. Such a design helps determine which instances are actually moving and which ones are temporarily static in the current scene. Our method exploits a sequence of point clouds as input and quantifies them into 4D voxels. We use 4D sparse convolutions to extract motion features from the 4D voxels and inject them into the current scan. Then, we extract spatio-temporal features from the current scan for instance detection and feature fusion. Finally, we design an upsample fusion module to output point-wise labels by fusing the spatio-temporal features and predicted instance information. We evaluated our approach on the LiDAR-MOS benchmark based on SemanticKITTI and achieved better moving object segmentation performance compared to state-of-the-art methods, demonstrating the effectiveness of our approach in integrating instance information for moving object segmentation. Furthermore, our method shows superior performance on the Apollo dataset with a pre-trained model on SemanticKITTI, indicating that our method generalizes well in different scenes. The code and pre-trained models of our method will be released at https://github.com/nubot-nudt/InsMOS. keywords: {Point cloud compression;Laser radar;Three-dimensional displays;Motion segmentation;Object segmentation;Feature extraction;Prediction algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342277&isnumber=10341342

W. Deng, K. Huang, Q. Yu, H. Lu, Z. Zheng and X. Chen, "ElC-OIS: Ellipsoidal Clustering for Open-World Instance Segmentation on LiDAR Data," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7606-7613, doi: 10.1109/IROS55552.2023.10342356.Abstract: Open-world Instance Segmentation (OIS) is a challenging task that aims to accurately segment every object instance appearing in the current observation, regardless of whether these instances have been labeled in the training set. This is important for safety-critical applications such as robust autonomous navigation. In this paper, we present a flexible and effective OIS framework for LiDAR point cloud that can accurately segment both known and unknown instances (i.e., seen and unseen instance categories during training). It first identifies points belonging to known classes and removes the back-ground by leveraging close-set panoptic segmentation networks. Then, we propose a novel ellipsoidal clustering method that is more adapted to the characteristic of LiDAR scans and allows precise segmentation of unknown instances. Furthermore, a diffuse searching method is proposed to handle the common over-segmentation problem presented in the known instances. With the combination of these techniques, we are able to achieve accurate segmentation for both known and unknown instances. We evaluated our method on the SemanticKITTI open-world LiDAR instance segmentation dataset. The experimental results suggest that it outperforms current state-of-the-art methods, especially with a 10.0% improvement in association quality. The source code of our method will be publicly available at https://github.com/nubot-nudt/ElC-OIS. keywords: {Instance segmentation;Training;Point cloud compression;Laser radar;Clustering methods;Source coding;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342356&isnumber=10341342

