A. Nuñez, F. H. Kong, A. González-Cantos and R. Fitch, "Risk-Aware Stochastic Ship Routing Using Conditional Value-at-Risk," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10543-10550, doi: 10.1109/IROS55552.2023.10341431.Abstract: Improving the safety and efficiency of maritime shipping has the potential to reduce carbon emissions and improve profitability. Stochastic ship routing, the problem of finding a safe, efficient, and timely route for a ship is difficult in large part because of uncertainty in weather forecasts, which come as an ensemble, a collection of many possible future weather conditions. Previous safety-aware ship routing methods have used either conservative interpretations of the ensemble by assuming the worst-case weather conditions, leading to excessive fuel consumption, or by using the average weather conditions, leading to potentially unsafe routes. In this paper, we investigate the use of the well-known Conditional Value-at-risk (CVaR) in the objective and constraint functions for ship routing problems, which allows a range of risk tolerances between the average and the worst case. We illustrate the advantages of using CVaR for the problem of ship routing in several simulation examples using real weather forecasts. keywords: {Uncertainty;Sensitivity analysis;Profitability;Weather forecasting;Tail;Routing;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341431&isnumber=10341342

V. Vasilopoulos, S. Garg, P. Piacenza, J. Huh and V. Isler, "RAMP: Hierarchical Reactive Motion Planning for Manipulation Tasks Using Implicit Signed Distance Functions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10551-10558, doi: 10.1109/IROS55552.2023.10342397.Abstract: We introduce Reactive Action and Motion Planner (RAMP), which combines the strengths of sampling-based and reactive approaches for motion planning. In essence, RAMP is a hierarchical approach where a novel variant of a Model Predictive Path Integral (MPPI) controller is used to generate trajectories which are then followed asynchronously by a local vector field controller. We demonstrate, in the context of a table clearing application, that RAMP can rapidly find paths in the robot's configuration space, satisfy task and robot-specific constraints, and provide safety by reacting to static or dynamically moving obstacles. RAMP achieves superior performance through a number of key innovations: we use Signed Distance Function (SDF) representations directly from the robot configuration space, both for collision checking and reactive control. The use of SDFs allows for a smoother definition of collision cost when planning for a trajectory, and is critical in ensuring safety while following trajectories. In addition, we introduce a novel variant of MPPI which, combined with the safety guarantees of the vector field trajectory follower, performs incremental real-time global trajectory planning. Simulation results establish that our method can generate paths that are comparable to traditional and state-of-the-art approaches in terms of total trajectory length while being up to 30 times faster. Real-world experiments demonstrate the safety and effectiveness of our approach in challenging table clearing scenarios. Videos and code are available at: https://samsunglabs.github.io/RAMP-project-page/ keywords: {Technological innovation;Trajectory planning;Aerospace electronics;Real-time systems;Trajectory;Safety;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342397&isnumber=10341342

J. Shao et al., "Towards Safe and Aggressive Motion Generation for Dynamic Targets Pick-and-Place," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10588-10595, doi: 10.1109/IROS55552.2023.10341580.Abstract: In this paper, we present a framework to generate time-optimal trajectories for dynamic target pick-and-place tasks. We develop an optimization-based trajectory generation method for manipulators, which can conduct spatial-temporal deformation under user-defined requirements. We formulate the problem of dynamic target pick-and-place, in which the trajectory duration and jerk are optimized and terminal states are adjusted instead of being fixed. The motions are constrained within the mechanical limits and to avoid collisions. Constraints transcription is adopted to convert constraints to weighted penalties. Then the problem can be solved based on the trajectory generation method with a high-level optimizer. We integrate the proposed method with online perception into a robot arm platform, in which a conveyor belt is used to transport the objects. Simulations and real-world experiments are conducted under a range of object speeds. Results show that the proposed method achieves online grasping under the object velocity up to 0.5m/s with an average computing time of 190ms. keywords: {Computational modeling;Dynamics;Pose estimation;Minimization;Robustness;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341580&isnumber=10341342

T. N. Le, F. J. Abu-Dakka and V. Kyrki, "SPONGE: Sequence Planning with Deformable-ON-Rigid Contact Prediction from Geometric Features," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10596-10603, doi: 10.1109/IROS55552.2023.10341704.Abstract: Planning robotic manipulation tasks, especially those that involve interaction between deformable and rigid objects, is challenging due to the complexity in predicting such interactions. We introduce SPONGE, a sequence planning pipeline powered by a deep learning-based contact prediction model for contacts between deformable and rigid bodies under interactions. The contact prediction model is trained on synthetic data generated by a developed simulation environ-ment to learn the mapping from point-cloud observation of a rigid target object and the pose of a deformable tool, to 3D representation of the contact points between the two bodies. We experimentally evaluated the proposed approach for a dish cleaning task both in simulation and on a real Franka Emika Panda with real-world objects. The experimental results demonstrate that in both scenarios the proposed planning pipeline is capable of generating high-quality trajectories that can accomplish the task by achieving more than 90% area coverage on different objects of varying sizes and curvatures while minimizing travel distance. Code and video are available at: https://irobotics.aalto.fi/sponge/. keywords: {Deformable models;Pipelines;Predictive models;Real-time systems;Data models;Planning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341704&isnumber=10341342

Y. Jiang, Y. Jia and X. Li, "Contact-Aware Non-Prehensile Manipulation for Object Retrieval in Cluttered Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10604-10611, doi: 10.1109/IROS55552.2023.10341476.Abstract: Non-prehensile manipulation methods usually use a simple end effector, e.g., a single rod, to manipulate the object. Compared to the grasping method, such an end effector is compact and flexible, and hence it can perform tasks in a constrained workspace; As a trade-off, it has relatively few degrees of freedom (DoFs), resulting in an under-actuation problem with complex constraints for planning and control. This paper proposes a new non-prehensile manipulation method for the task of object retrieval in cluttered environments, using a rod-like pusher. Specifically, a candidate trajectory in a cluttered environment is first generated with an improved Rapidly-Exploring Random Tree (RRT) planner; Then, a Model Predictive Control (MPC) scheme is applied to stabilize the slider's poses through necessary contact with obstacles. Different from existing methods, the proposed approach is with the contact-aware feature, which enables the synthesized effect of active removal of obstacles, avoidance behavior, and switching contact face for improved dexterity. Hence both the feasibility and efficiency of the task are greatly promoted. The performance of the proposed method is validated in a planar object retrieval task, where the target object, surrounded by many fixed or movable obstacles, is manipulated and isolated. Both simulation and experimental results are presented. keywords: {Robust control;Contacts;Dynamics;Switches;End effectors;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341476&isnumber=10341342

J. -H. Pan et al., "SDF-Pack: Towards Compact Bin Packing with Signed-Distance-Field Minimization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10612-10619, doi: 10.1109/IROS55552.2023.10341940.Abstract: Robotic bin packing is very challenging, especially when considering practical needs such as object variety and packing compactness. This paper presents SDF-Pack, a new approach based on signed distance field (SDF) to model the geometric condition of objects in a container and compute the object placement locations and packing orders for achieving a more compact bin packing. Our method adopts a truncated SDF representation to localize the computation, and based on it, we formulate the SDF -minimization heuristic to find optimized placements to compactly pack objects with the existing ones. To further improve space utilization, if the packing sequence is controllable, our method can suggest which object to be packed next. Experimental results on a large variety of everyday objects show that our method can consistently achieve higher packing compactness over 1,000 packing cases, enabling us to pack more objects into the container, compared with the existing heuristics under various packing settings. The code is publicly available at: https://github.com/kwpoon/SDF-Pack. keywords: {Codes;Computational modeling;Reinforcement learning;Containers;Aerospace electronics;Minimization;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341940&isnumber=10341342

J. Hu, Z. Tang and H. I. Christensen, "Multi-Modal Planning on Regrasping for Stable Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10620-10627, doi: 10.1109/IROS55552.2023.10341842.Abstract: Nowadays, a number of grasping algorithms [1], [2] have been proposed, that can predict a candidate of grasp poses, even for unseen objects. This enables a robotic manipulator to pick-and-place such objects. However, some of the predicted grasp poses to stably lift a target object may not be directly approachable due to workspace limitations. In such cases, the robot will need to re-grasp the desired object to enable successful grasping on it. This involves planning a sequence of continuous actions such as sliding, re-grasping, and transferring. To address this multi-modal problem, we propose a Markov-Decision Process-based multi-modal planner that can rearrange the object into a position suitable for stable manipulation. We demonstrate improved performance in both simulation and the real world for pick-and-place tasks. keywords: {Manifolds;Sequential analysis;Trajectory planning;Grasping;Manipulators;Stability analysis;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341842&isnumber=10341342

H. Zhu, A. Meduri and L. Righetti, "Efficient Object Manipulation Planning with Monte Carlo Tree Search," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10628-10635, doi: 10.1109/IROS55552.2023.10341813.Abstract: This paper presents an efficient approach to object manipulation planning using Monte Carlo Tree Search (MCTS) to find contact sequences and an efficient ADMM-based trajectory optimization algorithm to evaluate the dynamic feasibility of candidate contact sequences. To accelerate MCTS, we propose a methodology to learn a goal-conditioned policy-value network and a feasibility classifier to direct the search towards promising nodes. Further, manipulation-specific heuristics enable to drastically reduce the search space. Systematic object manipulation experiments in a physics simulator and on real hardware demonstrate the efficiency of our approach. In particular, our approach scales favorably for long manipulation sequences thanks to the learned policy-value network, significantly improving planning success rate. All source code including the baseline can be found at https://hzhu.io/contact-mcts. keywords: {Monte Carlo methods;Systematics;Heuristic algorithms;Source coding;Scalability;Search problems;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341813&isnumber=10341342

J. Zhang, C. Bai and J. Guo, "A New Method Combining Single-Point Push and Double-Point Complete Push for Partially Observable Scenarios," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10636-10643, doi: 10.1109/IROS55552.2023.10342422.Abstract: Pushing objects into a target configuration is an important skill for robots. When there are obstacles in the scenario, the movement range of the objects will be limited, and objects are easily lost due to obscured by obstacles, which complicates the pushing task. This paper proposes a push path planning algorithm SDP-RRT (Single and Double Push - Rapidly-exploring Random Tree) to combine single-point push and double-point complete push in a partially observable environment with obstacles. The single-double push combines the flexibility of a single-point push and the stability of a double-point complete push. To find the reliable pushing point and pushing direction of the double-point complete push, we improve the judgment conditions of the double-point complete push and estimate the object's center of mass based on the LSTM (Long Short-Term Memory) neural network. Finally, the success rate and push steps of single-double and single-point push are compared in both the simulation and the natural environments, and the feasibility of the SDP-RRT algorithm is verified. keywords: {Neural networks;Stability analysis;Path planning;Reliability;Task analysis;Manipulator dynamics;Long short term memory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342422&isnumber=10341342

M. Kim, J. Han, J. Kim and B. Kim, "Pre-and Post-Contact Policy Decomposition for Non-Prehensile Manipulation with Zero-Shot Sim-To-Real Transfer," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10644-10651, doi: 10.1109/IROS55552.2023.10341657.Abstract: We present a system for non-prehensile manipulation that require a significant number of contact mode transitions and the use of environmental contacts to successfully manipulate an object to a target location. Our method is based on deep reinforcement learning which, unlike state-of-the-art planning algorithms, does not require apriori knowledge of the physical parameters of the object or environment such as friction coefficients or centers of mass. The planning time is reduced to the simple feed-forward prediction time on a neural network. We propose a computational structure, action space design, and curriculum learning scheme that facilitates efficient exploration and sim-to-real transfer. In challenging real-world non-prehensile manipulation tasks, we show that our method can generalize over different objects, and succeed even for novel objects not seen during training. Project website: https://sites.google.com/view/nonprenehsile-decomposition keywords: {Training;Deep learning;Friction;Neural networks;Reinforcement learning;Prediction algorithms;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341657&isnumber=10341342

N. Alatur, O. Andersson, R. Siegwart and L. Ott, "Material-Agnostic Shaping of Granular Materials with Optimal Transport," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10652-10659, doi: 10.1109/IROS55552.2023.10342505.Abstract: From construction materials, such as sand or asphalt, to kitchen ingredients, like rice, sugar, or salt; the world is full of granular materials. Despite impressive progress in robotic manipulation of single objects, granular materials remain a challenge due to difficulties in modelling these highly deformable and inhomogeneous materials, which are governed by dynamics that are hard to capture analytically. We argue that despite the high degrees of freedom and the complex underlying dynamics of granular materials, many practical problems that require manipulating them can be solved by leveraging simple models, informative motion priors, and a fast feedback loop. In this work, we show that computational Optimal Transport (OT) can be leveraged to derive informative, robot-agnostic motion priors for transforming a pile of granular materials from a source into a target distribution and generate robot motion plans with a next-best sweep planner that uses a simple material-agnostic sweep model. We plan sweeps directly on a height map representation of the material distribution and hence avoid a costly particle-level treatment of the problem. We validate our approach with a large set of simulation and hardware experiments that demonstrate several complex shaping tasks, including gathering, separating, and writing letters with different types of granular materials. keywords: {Robot motion;Adaptation models;Shape;Computational modeling;Dynamics;Writing;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342505&isnumber=10341342

W. C. Agboh et al., "Learning to Efficiently Plan Robust Frictional Multi-Object Grasps," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10660-10667, doi: 10.1109/IROS55552.2023.10341895.Abstract: We consider a decluttering problem where multiple rigid convex polygonal objects rest in randomly placed positions and orientations on a planar surface and must be efficiently transported to a packing box using both single and multi-object grasps. Prior work considered frictionless multi-object grasping. In this paper, we introduce friction to increase the number of potential grasps for a given group of objects, and thus increase picks per hour. We train a neural network using real examples to plan robust multi-object grasps. In physical experiments, we find a 13.7% increase in success rate, a 1.6x increase in picks per hour, and a 6.3x decrease in grasp planning time compared to prior work on multi-object grasping. Compared to single-object grasping, we find a 3.1x increase in picks per hour. keywords: {Friction;Neural networks;Grasping;Planning;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341895&isnumber=10341342

C. Wang, H. Luo, K. Zhang, H. Chen, J. Pan and W. Zhang, "POMDP-Guided Active Force-Based Search for Robotic Insertion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10668-10675, doi: 10.1109/IROS55552.2023.10342421.Abstract: In robotic insertion tasks where the uncertainty exceeds the allowable tolerance, a good search strategy is essential for successful insertion and significantly influences efficiency. The commonly used blind search method is time-consuming and does not exploit the rich contact information. In this paper, we propose a novel search strategy that actively utilizes the information contained in the contact configuration and shows high efficiency. In particular, we formulate this problem as a Partially Observable Markov Decision Process (POMDP) with carefully designed primitives based on an in-depth analysis of the contact configuration's static stability. From the formulated POMDP, we can derive a novel search strategy. Thanks to its simplicity, this search strategy can be incorporated into a Finite-State-Machine (FSM) controller. The behaviors of the FSM controller are realized through a low-level Cartesian Impedance Controller. Our method is based purely on the robot's proprioceptive sensing and does not need visual or tactile sensors. To evaluate the effectiveness of our proposed strategy and control framework, we conduct extensive comparison experiments in simulation, where we compare our method with the baseline approach. The results demonstrate that our proposed method achieves a higher success rate with a shorter search time and search trajectory length compared to the baseline method. Additionally, we show that our method is robust to various initial displacement errors. keywords: {Visualization;Propioception;Search problems;Robot sensing systems;Stability analysis;Trajectory;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342421&isnumber=10341342

R. I. C. Muchacho, S. Bien, R. Laha, A. Naceri, L. F. C. Figueredo and S. Haddadin, "Shared Autonomy Control for Slosh-Free Teleoperation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10676-10683, doi: 10.1109/IROS55552.2023.10342234.Abstract: Shared-autonomy control strategies in teleoperation combine human decision-making and robot precision to solve complex tasks. In other words, advanced autonomous control algorithms can compensate for imprecise human commands, reduce the mental workload of the user, and enable the execution of tasks that otherwise wouldn't be feasible. This paper addresses one of these previously challenging scenarios. Herein, we present a novel control framework and motion generator that allows for real-time non-prehensile slosh-free teleoperation of liquids. The proposed approach is able to generate robust trajectories on the follower side which ensures task-space, joint-space, and manipulability constraint satisfaction. Our findings were evaluated through user studies and real-world scenarios. Participants were even explicitly challenged to try to spill liquid through teleoperation, reaching speeds up to 0.6 m/s. keywords: {Liquids;Robot kinematics;Transportation;Kinematics;Robot sensing systems;Real-time systems;Mathematical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342234&isnumber=10341342

S. R. Nekoo, J. Yao, A. Suarez, R. Tapia and A. Ollero, "Leader-Follower Formation Control of a Large-Scale Swarm of Satellite System Using the State-Dependent Riccati Equation: Orbit-to-Orbit and In-Same-Orbit Regulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10700-10707, doi: 10.1109/IROS55552.2023.10342383.Abstract: The state-dependent Riccati equation (SDRE) is a nonlinear optimal controller with a flexible structure which is one of the main advantages of this method. Here in this work, this flexibility is used to present a novel design for handling a soft constraint for state variables (trajectories). The concept is applied to a large-scale swarm control system, with more than 1000 agents. The control of the swarm satellite system is devoted to two modes of orbit-to-orbit and in-same-orbit cases. Keeping the satellites in one orbit in regulation (point-to-point motion) requires additional constraints while they are moving in Cartesian coordinates. For a small number of agents trajectory design could be done for each satellite individually, though, for a swarm with many agents, that is not practical. The constraint has been incorporated into the cost function of optimal control and resulted in a modified SDRE control law. The proposed method successfully controlled a swarm case of 1024 agents in leader-follower mode for orbit-to-orbit and in-same-orbit simulations. The soft constraint presented a percentage of 0.05 in the error of the satellites with respect to travel distance, in in-same-orbit regulation. The presented approach is systematic and could be performed for larger swarm systems with different agents and dynamics. keywords: {Visualization;Satellites;Systematics;Shape;Riccati equations;Orbits;Regulation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342383&isnumber=10341342

J. M. Cloud, M. Q. Tram, W. J. Beksi and M. A. DuPuis, "Lunar Excavator Mission Operations Using Dynamic Movement Primitives," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10708-10715, doi: 10.1109/IROS55552.2023.10342005.Abstract: To support sustainable infrastructure on the Moon, NASA must leverage robots to extract lunar resources for in-situ processing and construction. As part of this effort, NASA is launching the in-situ resource utilization (ISRU) Pilot Excavator later this decade to validate a robotic regolith excavator based on the Regolith Advanced Surface Systems Operations Robot (RASSOR). RASSOR is designed to extract and transport regolith to meet the needs of ISRU architectures. During its mission, Pilot Excavator will be tasked with driving in test patterns to demonstrate the operational concept. One possible test pattern is a circular trajectory around the lander while avoiding surface hazards such as lunar rocks. To this end, we utilize dynamic movement primitives to represent navigation sequences as primitive trajectories. We introduce a novel obstacle avoidance parameter, which is configured to avoid rocks throughout testing exercises. We demonstrate the effectiveness our method in a newly developed simulation tool called the Simulated Excavation Environment for Lunar Operations (SEELO) using models based on the NASA RASSOR 2.0 excavator. Our results show that the robot is able to safety and robustly navigate the lunar surface with densely populated rock obstacles while retaining the desired circle pattern behavior. keywords: {Navigation;Moon;NASA;Excavation;Rocks;Trajectory;Behavioral sciences;Space Robotics and Automation;Learning from Demonstration;Mining Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342005&isnumber=10341342

C. Debeunne, J. Vallvé, A. Torres and D. Vivet, "Fast Bi-Monocular Visual Odometry Using Factor Graph Sparsification," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10716-10722, doi: 10.1109/IROS55552.2023.10341644.Abstract: Visual navigation has become a standard in robotic applications with the emergence of robust and versatile algorithms. In particular, Visual Odometry (VO) has proven to be the most reliable navigation solution for space missions to estimate an unmanned vehicle's motion and state. Lava Tubes exploration is one of the recent challenges in this field of applied robotics. VO in this scenario requires more robustness to poor lighting conditions while keeping a low computational cost. We propose investigating an indirect bi-monocular VO based on sliding-window optimization in such a context. It focuses on maintaining the sparsity of the problem while keeping the information of the marginalized frames to reduce the computational burden. Different sparse graph topologies are studied to encode information from the past and are evaluated on accuracy and computation load. The best method retained is then compared to state-of-the-art systems on real data under extreme illumination conditions and reaches similar accuracy results at a lower computational cost. keywords: {Visualization;Three-dimensional displays;Navigation;Lava;Lighting;Topology;Electron tubes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341644&isnumber=10341342

S. Vijayarangan and D. Wettergreen, "Assisting Spectral Mapping Using Cameras," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10723-10728, doi: 10.1109/IROS55552.2023.10342072.Abstract: Spectral mapping, typically performed at the orbital scale, is hard at the rover scale as it necessitates larger coverage and field operable spectrometers. In this work, we propose using RGB cameras to assist spectral mapping. RGB cameras placed on a wide range of robotic platforms, including aerial vehicles, which can explore large regions compared to ground rovers. Our method uses a spectral model which learns the spatial relationship of spectra in a compressed feature space. We show that RGB data can contribute to this feature space and thereby enhance spectral reconstruction accuracy. keywords: {Space vehicles;Robot vision systems;NASA;Cameras;Solar system;Planetary orbits;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342072&isnumber=10341342

L. Gao, G. Cordova, C. Danielson and R. Fierro, "Autonomous Multi-Robot Servicing for Spacecraft Operation Extension," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10729-10735, doi: 10.1109/IROS55552.2023.10341875.Abstract: This paper considers the robotic on-orbit servicing of a client satellite. We present an adaptive robotic system to perform two on-orbit tasks: client manipulation and jammed component dislodging (e.g., a solar panel). We use adaptive control due to the uncertainty of the client dynamics and the stiffness of the jammed component. We present two methods for dislodging: a decentralized approach with multiple free-fiying collaborating agents and then transferring the same decentralized approach to a multi-robot arms system. We present simulation studies for these on-orbit servicing tasks. These studies validate the effectiveness of the presented controller despite uncertainty and unmodeled dynamics, such as nonlinear friction. keywords: {Space vehicles;Uncertainty;Satellites;Simulation;Manipulators;Nonlinear dynamical systems;Solar panels},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341875&isnumber=10341342

I. -W. Park et al., "SOLL-E: A Module Transport and Placement Robot for Autonomous Assembly of Discrete Lattice Structures," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10736-10741, doi: 10.1109/IROS55552.2023.10341479.Abstract: This paper presents the design and development of a transport and placement robot that demonstrates autonomous assembly of structural building blocks. The robots are intended to serve as a critical component of automated structural assembly and maintenance systems. The Scaling Omni-directional Lattice Locomoting Explorer (SOLL-E) uses a 5-DoF bipedal inchworm locomotion architecture with locking foot and cargo grippers. The locomotion system employs large magnet gap diameter BLDC motors with moderate timing belt gearing for primary joints, and DC planetary gearmotors for turning. Foot and cargo grippers are identical, with servo-actuated locking mechanisms. Three modular controller boards are used to control these actuators in real-time, with command and telemetry data transferred between the server and each controller board via WiFi. Functionality and performance were evaluated in a ground demonstration. keywords: {Scalability;Lattices;Turning;Timing;Telemetry;Servers;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341479&isnumber=10341342

E. Wittemyer and I. Abraham, "Bi-Level Image-Guided Ergodic Exploration with Applications to Planetary Rovers," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10742-10748, doi: 10.1109/IROS55552.2023.10341437.Abstract: We present a method for image-guided exploration for mobile robotic systems. Our approach extends ergodic exploration methods, a recent exploration approach that prioritizes complete coverage of a space, with the use of a learned image classifier that automatically detects objects and updates an information map to guide further exploration and localization of objects. Additionally, to improve outcomes of the information collected by our robot's visual sensor, we present a decomposition of the ergodic optimization problem as bi-level coarse and fine solvers, which act respectively on the robot's body and the robot's visual sensor. Our approach is applied to geological survey and localization of rock formations for Mars rovers, with real images from Mars rovers used to train the image classifier. Results demonstrate 1) improved localization of rock formations compared to naive approaches while 2) minimizing the path length of the exploration through the bi-level exploration. keywords: {Location awareness;Space vehicles;Surveys;Visualization;Mars;Navigation;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341437&isnumber=10341342

M. Ulmer, M. Durner, M. Sundermeyer, M. Stoiber and R. Triebel, "6D Object Pose Estimation from Approximate 3D Models for Orbital Robotics," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10749-10756, doi: 10.1109/IROS55552.2023.10341511.Abstract: We present a novel technique to estimate the 6D pose of objects from single images where the 3D geometry of the object is only given approximately and not as a precise 3D model. To achieve this, we employ a dense 2D-to-3D correspondence predictor that regresses 3D model coordinates for every pixel. In addition to the 3D coordinates, our model also estimates the pixel-wise coordinate error to discard correspondences that are likely wrong. This allows us to generate multiple 6D pose hypotheses of the object, which we then refine iteratively using a highly efficient region-based approach. We also introduce a novel pixel-wise posterior formulation by which we can estimate the probability for each hypothesis and select the most likely one. As we show in experiments, our approach is capable of dealing with extreme visual conditions including overexposure, high contrast, or low signal-to-noise ratio. This makes it a powerful technique for the particularly challenging task of estimating the pose of tumbling satellites for in-orbit robotic applications. Our method achieves state-of-the-art performance on the SPEED+ dataset and has won the SPEC2021 post-mortem competition. Code, trained models, and the used satellite model will be made publicly available. keywords: {Solid modeling;Visualization;Three-dimensional displays;Satellites;Robot kinematics;Pose estimation;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341511&isnumber=10341342

Y. Tanaka, A. A. Anibha, L. Jung, R. Gul, J. Sun and R. Dai, "An Origami-Inspired Deployable Space Debris Collector," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10757-10762, doi: 10.1109/IROS55552.2023.10341589.Abstract: This paper develops a novel approach to design, actuate, and manufacture a space debris collector based on the conical Kresling origami pattern. The deployable nature of origami structures and the radial closability of the conical Kresling pattern are leveraged to form an enclosure volume for collecting space debris at different sizes. We first introduce the geometric, volume, and energy models of the conical Kresling pattern. Based on these models, the debris collector design problem is formulated as a parameter optimization problem to minimize the actuation energy for the folding process while satisfying the minimum volume constraint and geometric/functional constraints. To automatically capture debris in space, an actuation system is designed, which is compatible with space environments. Moreover, the multi-material three-dimensional printing technology is applied to build the designed debris collector, which makes it feasible for manufacturing the product in orbit. The proposed design, actuation, and manufacturing approaches are verified with experimental tests using a designed prototype. keywords: {Space vehicles;Solid modeling;Three-dimensional displays;Space debris;Prototypes;Process control;Three-dimensional printing;Space Robots;Origami-Inspired Deployable System;Debris Connector;Optimal Design},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341589&isnumber=10341342

W. Talbot et al., "Principled ICP Covariance Modelling in Perceptually Degraded Environments for the EELS Mission Concept," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10763-10770, doi: 10.1109/IROS55552.2023.10341455.Abstract: The Exobiology Extant Life Surveyor (EELS) is a snake-like mobile instruments platform under development at Jet Propulsion Laboratory (JPL) for a mission concept to find evidence of life on Saturn's sixth largest moon, Enceladus. To conduct a life surveying mission there, the EELS platform must first traverse an unknown icy surface terrain before undertaking a controlled descent into a cryovolcanic vent. The remoteness of Enceladus and the icy nature of its terrain demands a level of autonomy in navigation significantly higher than previous rover missions. The perception system onboard EELS must be highly resilient to perceptually-degraded environments such as flat, open ice fields, icy plumes, and repeating geometries in vents. EELS' perception system is implemented as a multi-sensor Simultaneous Localisation And Mapping (SLAM) solution called SERPENT. State Estimation through Robust Perception in Extreme and Novel Terrains (SERPENT) estimates the robot trajectory and maintains a map database, from which dense global or local maps can be obtained on demand for downstream planning algorithms. This system opts to incorporate measurements from many sensor modalities (laser scans, images, IMU, altimeter, etc.), solving the SLAM problem through joint optimisation, and thus requires that the contribution of each sensor be balanced through careful modelling of their uncertainties. With a specific focus on Light Detection And Ranging (LiDAR) in this context, this paper proposes a principled approach to model the covariances of point-to-plane Iterative Closest Point (ICP). It performs a rigorous comparative analysis of new and existing covariance models, and is the first time some of these have been tested within a complete SLAM pipeline. These models are evaluated on perceptually challenging datasets collected in glacial environments by the EELS sensor suite (see Figures 1, 2). SERPENT is open-sourced at https://github.com/jpl-eels/serpent. keywords: {Simultaneous localization and mapping;Uncertainty;Saturn;Vents;Pipelines;Optimized production technology;Propulsion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341455&isnumber=10341342

K. Nazari et al., "Deep Functional Predictive Control (deep-FPC): Robot Pushing 3-D Cluster Using Tactile Prediction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10771-10776, doi: 10.1109/IROS55552.2023.10342410.Abstract: This paper introduces a novel approach to address the problem of Physical Robot Interaction (PRI) during robot pushing tasks. The approach uses a data-driven forward model based on tactile predictions to inform the controller about potential future movements of the object being pushed, such as a strawberry stem, using a robot tactile finger. The model is integrated into a Deep Functional Predictive Control (d-FPC) system to control the displacement of the stem on the tactile finger during pushes. Pushing an object with a robot finger along a desired trajectory in 3D is a highly nonlinear and complex physical robot interaction, especially when the object is not stably grasped. The proposed approach controls the stem movements on the tactile finger in a prediction horizon. The effectiveness of the proposed FPC is demonstrated in a series of tests involving a real robot pushing a strawberry in a cluster. The results indicate that the d-FPC controller can successfully control PRI in robotic manipulation tasks beyond the handling of strawberries. The proposed approach offers a promising direction for addressing the challenging PRI problem in robotic manipulation tasks. keywords: {Three-dimensional displays;Predictive models;Robot sensing systems;Trajectory;Sensors;Convolutional neural networks;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342410&isnumber=10341342

S. Ergun et al., "Wireless Capacitive Tactile Sensor Arrays for Sensitive/Delicate Robot Grasping," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10777-10784, doi: 10.1109/IROS55552.2023.10342163.Abstract: Uncertainties in grasp prediction for unknown, arbitrarily shaped objects in cluttered environments and un- certainty of the kinematics (e.g., series elastic robots or soft robots) can lead to poor grasps. For delicate objects, such poor grasps may damage the objects when they are dropped or when high local pressure is introduced in the grasping process. We propose a tactile sensor concept that allows predicting the quality of a grasp such that the object can be safely moved without being dropped. This prediction is done using an initial low force grasp and the force is only increased when the contact area is sufficiently large. The proposed customizable wireless Capacitive Tactile Sensor Array (CTSA) uses the deformation of a polymer to assess the contact area and the force distribution. A common homogeneous deformable electrode is embedded in the polymer. This electrode does not require any patterning nor any electrical connection but to ground. We present the manufacturing process which allows for robust yet cost effective realizations with a variety of electrode materials including conductive inks, conductive textiles, metal meshes and metal sheets. With the different approaches, parameters such as sensitivity and recovery time can be adjusted. Furthermore, the robustness of the sensor towards strong forces and objects with sharp edges and corners is shown. Finally, we demonstrate the benefits of the proposed sensor for grasping in a series of scenarios with rigid and soft 3D printed objects of various shapes. Allowing a reasonable false positive rate, 100 % of unsuccessful grasps in our evaluation experiments could be detected from the initial low force grasp. keywords: {Electrodes;Wireless communication;Wireless sensor networks;Contacts;Force;Tactile sensors;Metals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342163&isnumber=10341342

J. Yuan, C. Choi, E. B. Tadmor and V. Isler, "Active Planar Mass Distribution Estimation with Robotic Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10785-10791, doi: 10.1109/IROS55552.2023.10342506.Abstract: In this work, we present a method to estimate the planar mass distribution of a rigid object through robotic interactions and force/torque feedback. This is a challenging problem because of the complexity of modeling physical dynamics and the action dependencies across the model parameters. We propose a sequential estimation strategy combined with a set of robot action selection rules based on the analytical formulation of a discrete-time dynamics model. To evaluate the performance of our approach, we also manufactured re-configurable block objects that allow us to modify the object mass distribution while having access to the ground truth values. We compare our approach against multiple baselines and show that it can estimate the mass distribution with around 10% error, while the baselines have errors ranging from 18% to 68%. keywords: {Analytical models;Shape;Dynamics;Estimation;Distance measurement;Complexity theory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342506&isnumber=10341342

P. K. Murali, B. Porr and M. Kaboli, "Touch if it's Transparent! ACTOR: Active Tactile-Based Category-Level Transparent Object Reconstruction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10792-10799, doi: 10.1109/IROS55552.2023.10341680.Abstract: Accurate shape reconstruction of transparent ob-jects is a challenging task due to their non-Lambertian surfaces and yet necessary for robots for accurate pose perception and safe manipulation. As vision-based sensing can produce erroneous measurements for transparent objects, the tactile modality is not sensitive to object transparency and can be used for reconstructing the object's shape. We propose AC-TOR, a novel framework for ACtive tactile-based category-level Transparent Object Reconstruction. ACTOR leverages large datasets of synthetic object with our proposed self-supervised learning approach for object shape reconstruction as the collection of real-world tactile data is prohibitively expensive. ACTOR can be used during inference with tactile data from category-level unknown transparent objects for reconstruction. Furthermore, we propose an active-tactile object exploration strategy as probing every part of the object surface can be sample inefficient. We also demonstrate tactile-based category-level object pose estimation task using ACTOR. We perform an extensive evaluation of our proposed methodology with real-world robotic experiments with comprehensive comparison studies with state-of-the-art approaches. Our proposed method outperforms these approaches in terms of tactile-based object reconstruction and object pose estimation. keywords: {Surface reconstruction;Shape;Shape measurement;Pose estimation;Self-supervised learning;Robot sensing systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341680&isnumber=10341342

G. Cao, J. Jiang, D. Bollegala and S. Luo, "Learn from Incomplete Tactile Data: Tactile Representation Learning with Masked Autoencoders," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10800-10805, doi: 10.1109/IROS55552.2023.10341788.Abstract: The missing signal caused by the objects being occluded or an unstable sensor is a common challenge during data collection. Such missing signals will adversely affect the results obtained from the data, and this issue is observed more frequently in robotic tactile perception. In tactile perception, due to the limited working space and the dynamic environment, the contact between the tactile sensor and the object is frequently insufficient and unstable, which causes the partial loss of signals, thus leading to incomplete tactile data. The tactile data will therefore contain fewer tactile cues with low information density. In this paper, we propose a tactile representation learning method, named TacMAE, based on Masked Autoencoder to address the problem of incomplete tactile data in tactile perception. In our framework, a portion of the tactile image is masked out to simulate the missing contact regions. By reconstructing the missing signals in the tactile image, the trained model can achieve a high-level understanding of surface geometry and tactile properties from limited tactile cues. The experimental results of tactile texture recognition show that TacMAE can achieve a high recognition accuracy of 71.4% in the zero-shot transfer and 85.8% after fine-tuning, which are 15.2% and 8.2% higher than the results without using masked modeling. The extensive experiments on YCB objects demonstrate the knowledge transferability of our proposed method and the potential to improve efficiency in tactile exploration. keywords: {Representation learning;Geometry;Training;Surface reconstruction;Zero-shot learning;Shape;Tactile sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341788&isnumber=10341342

Y. Lin, M. Comi, A. Church, D. Zhang and N. F. Lepora, "Attention for Robot Touch: Tactile Saliency Prediction for Robust Sim-to-Real Tactile Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10806-10812, doi: 10.1109/IROS55552.2023.10341888.Abstract: High-resolution tactile sensing can provide accurate information about local contact in contact-rich robotic tasks. However, the deployment of such tasks in unstructured environments remains under-investigated. To improve the robustness of tactile robot control in unstructured environments, we propose and study a new concept: tactile saliency for robot touch, inspired by the human touch attention mechanism from neuroscience and the visual saliency prediction problem from computer vision. In analogy to visual saliency, this concept involves identifying key information in tactile images captured by a tactile sensor. While visual saliency datasets are commonly annotated by humans, manually labelling tactile images is challenging due to their counterintuitive patterns. To address this challenge, we propose a novel approach comprised of three interrelated networks: 1) a Contact Depth Network (ConDepNet), which generates a contact depth map to localize deformation in a real tactile image that contains target and noise features; 2) a Tactile Saliency Network (TacSalNet), which predicts a tactile saliency map to describe the target areas for an input contact depth map; 3) and a Tactile Noise Generator (TacNGen), which generates noise features to train the TacSalNet. Experimental results in contact pose estimation and edge-following in the presence of distractors showcase the accurate prediction of target features from real tactile images. Overall, our tactile saliency prediction approach gives robust sim-to-real tactile control in environments with unknown distractors. Project page: https://sites.google.com/view/tactile-saliency/. keywords: {Visualization;Neuroscience;Robot control;Pose estimation;Tactile sensors;Robustness;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341888&isnumber=10341342

P. Sathe, A. Schmitz, T. P. Tomo, S. Somlor, S. Funabashi and S. Shigeki, "FingerTac - An Interchangeable and Wearable Tactile Sensor for the Fingertips of Human and Robot Hands," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10813-10820, doi: 10.1109/IROS55552.2023.10342285.Abstract: Skill transfer from humans to robots is challenging. Presently, many researchers focus on capturing only position or joint angle data from humans to teach the robots. Even though this approach has yielded impressive results for grasping applications, reconstructing motion for object handling or fine manipulation from a human hand to a robot hand has been sparsely explored. Humans use tactile feedback to adjust their motion to various objects, but capturing and reproducing the applied forces is an open research question. In this paper we introduce a wearable fingertip tactile sensor, which captures the distributed 3-axis force vectors on the fingertip. The fingertip tactile sensor is interchangeable between the human hand and the robot hand, meaning that it can also be assembled to fit on a robot hand such as the Allegro hand. This paper presents the structural aspects of the sensor as well as the methodology and approach used to design, manufacture, and calibrate the sensor. The sensor is able to measure forces accurately with a mean absolute error of 0.21, 0.16, and 0.44 Newtons in X, Y, and Z directions, respectively. keywords: {Force measurement;Measurement uncertainty;Force;Tactile sensors;Grasping;Robots;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342285&isnumber=10341342

A. K. Kosta et al., "AcouSkin: Full Surface Contact localization Using Acoustic Waves," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10821-10828, doi: 10.1109/IROS55552.2023.10342359.Abstract: Contact sensing and localization capabilities that mimic human skin are highly desirable for robots. In this paper, we introduce AcouSkin, an acoustic wave based full surface contact localization system. Acoustic waves produced by piezoelectric transceivers using a monotone are coupled to surfaces turning them into an active sensor. Our system leverages information from four piezoelectric transceivers mounted on the surface of an acrylic sheet and vacuum cleaner robot bumper to localize contacts to 18 unique segments. We first characterize acoustic wave propagation based on signal and material properties and then propose hardware and software methods to realize full surface contact localization. Our results show that AcouSkin can reliably localize contact on a flat acrylic sheet with 18 uniformly spaced locations across a 54cm length with mean absolute error (MAE) of ≤ 1 locations using maximum likelihood estimator (MLE) and multilayer perceptron (MLP) models. On the vacuum cleaner robot bumper AcouSkin shows a zero MAE. Further, the system is also able to localize contacts made using forces as low as 2N (Newtons) and as high as 20N. Overall, AcouSkin provides full surface contact localization while requiring minimal instrumentation with easy deployment on real-world robots. keywords: {Location awareness;Maximum likelihood estimation;Vacuum systems;Surface acoustic waves;Robot sensing systems;Turning;Transceivers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342359&isnumber=10341342

Z. Chen, C. Lin, L. Nie, K. Liao and Y. Zhao, "Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10873-10879, doi: 10.1109/IROS55552.2023.10342332.Abstract: Omnidirectional multi-view stereo (MVS) vision is attractive for its ultra-wide field-of-view (FoV), enabling machines to perceive 360°3D surroundings. However, the existing solutions require expensive dense depth labels for supervision, making them impractical in real-world applications. In this paper, we propose the first unsupervised omnidirectional MVS framework based on multiple fisheye images. To this end, we project all images to a virtual view center and composite two panoramic images with spherical geometry from two pairs of back-to-back fisheye images. The two 360° images formulate a stereo pair with a special pose, and the photometric consistency is leveraged to establish the unsupervised constraint, which we term “Pseudo-Stereo Supervision”. In addition, we propose Un-OmniMVS, an efficient unsupervised omnidirectional MVS network, to facilitate the inference speed with two efficient components. First, a novel feature extractor with frequency attention is proposed to simultaneously capture the non-local Fourier features and local spatial features, explicitly facilitating the feature representation. Then, a variance-based light cost volume is put forward to reduce the computational complexity. Experiments exhibit that the performance of our unsupervised solution is competitive to that of the state-of-the-art (SoTA) supervised methods with better generalization in real-world data. The code will be available at https://github.com/Chen-z-s/Un-OmniMVS. keywords: {Geometry;Costs;Codes;Feature extraction;Cameras;Computational complexity;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342332&isnumber=10341342

F. E. Xavier, G. Burger, M. Pétriaux, J. -E. Deschaud and F. Goulette, "Multi-IMU Proprioceptive State Estimator for Humanoid Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10880-10887, doi: 10.1109/IROS55552.2023.10341849.Abstract: Algorithms for state estimation of humanoid robots usually assume that the feet remain flat and in a constant position while in contact with the ground. However, this hypothesis is easily violated while walking, especially for human-like gaits with heel-toe motion. This reduces the time during which the contact assumption can be used, or requires higher variances to account for errors. In this paper, we present a novel state estimator based on the extended Kalman filter that can properly handle any contact configuration. We consider multiple inertial measurement units (IMUs) distributed throughout the robot's structure, including on both feet, which are used to track multiple bodies of the robot. This multi-IMU instrumentation setup also has the advantage of allowing the deformations in the robot's structure to be estimated, improving the kinematic model used in the filter. The proposed approach is validated experimentally on the exoskeleton Atalante and is shown to present low drift, performing better than similar single-IMU filters. The obtained trajectory estimates are accurate enough to construct elevation maps that have little distortion with respect to the ground truth. keywords: {Legged locomotion;Deformation;Exoskeletons;Humanoid robots;Propioception;Kinematics;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341849&isnumber=10341342

I. Fürst-Walter, A. Nappi, T. Harbaum and J. Becker, "Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10888-10893, doi: 10.1109/IROS55552.2023.10341256.Abstract: Human Pose Estimation (HPE) to assess human motion in sports, rehabilitation or work safety requires accurate sensing without compromising the sensitive underlying personal data. Therefore, local processing is necessary and the limited energy budget in such systems can be addressed by Inertial Measurement Units (IMU) instead of common camera sensing. The central trade-off between accuracy and efficient use of hardware resources is rarely discussed in research. We address this trade-off by a simulative Design Space Exploration (DSE) of a varying quantity and positioning of IMU -sensors. First, we generate IMU-data from a publicly available body model dataset for different sensor configurations and train a deep learning model with this data. Additionally, we propose a combined metric to assess the accuracy-resource trade-off. We used the DSE as a tool to evaluate sensor configurations and identify beneficial ones for a specific use case. Exemplary, for a system with equal importance of accuracy and resources, we identify an optimal sensor configuration of 4 sensors with a mesh error of 6.03 cm, increasing the accuracy by 32.7 % and reducing the hardware effort by two sensors compared to state of the art. Our work can be used to design health applications with well-suited sensor positioning and attention to data privacy and resource-awareness. keywords: {Deep learning;Data privacy;Tracking;Sternum;Pose estimation;Robot sensing systems;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341256&isnumber=10341342

T. Nguyen, V. Hrosinkov, E. Rosen and S. Tellex, "Language-Conditioned Observation Models for Visual Object Search," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10894-10901, doi: 10.1109/IROS55552.2023.10341492.Abstract: Object search is a challenging task because when given complex language descriptions (e.g., “find the white cup on the table”), the robot must move its camera through the environment and recognize the described object. Previous works map language descriptions to a set of fixed object detectors with predetermined noise models, but these approaches are challenging to scale because new detectors need to be made for each object. In this work, we bridge the gap in realistic object search by posing the search problem as a partially observable Markov decision process (POMDP) where the object detector and visual sensor noise in the observation model is determined by a single Deep Neural Network conditioned on complex language descriptions. We incorporate the neural network's outputs into our language-conditioned observation model (LCOM) to represent dynamically changing sensor noise. With an LCOM, any language description of an object can be used to generate an appropriate object detector and noise model, and training an LCOM only requires readily available supervised image-caption datasets. We empirically evaluate our method by comparing against a state-of-the-art object search algorithm in simulation, and demonstrate that planning with our observation model yields a significantly higher average task completion rate (from 0.46 to 0.66) and more efficient and quicker object search than with a fixed-noise model. We demonstrate our method on a Boston Dynamics Spot robot, enabling it to handle complex natural language object descriptions and efficiently find objects in a room-scale environment. keywords: {Adaptation models;Solid modeling;Visualization;Three-dimensional displays;Natural languages;Detectors;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341492&isnumber=10341342

S. Pang, D. Morris and H. Radha, "TransCAR: Transformer-Based Camera-and-Radar Fusion for 3D Object Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10902-10909, doi: 10.1109/IROS55552.2023.10341793.Abstract: Despite radar's popularity in the automotive industry, for fusion-based 3D object detection, most existing works focus on LiDAR and camera fusion. In this paper, we propose TransCAR, a Transformer-based Camera-And-Radar fusion solution for 3D object detection. Our TransCAR consists of two modules. The first module learns 2D features from surround-view camera images and then uses a sparse set of 3D object queries to index into these 2D features. The vision-updated queries then interact with each other via transformer self-attention layer. The second module learns radar features from multiple radar scans and then applies transformer decoder to learn the interactions between radar features and vision-updated queries. The cross-attention layer within the transformer decoder can adaptively learn the soft-association between the radar features and vision-updated queries instead of hard-association based on sensor calibration only. Finally, our model estimates a bounding box per query using set-to-set Hungarian loss, which enables the method to avoid non-maximum suppression. TransCAR improves the velocity estimation using the radar scans without temporal information. The superior experimental results of our TransCAR on the challenging nuScenes datasets illustrate that our TransCAR outperforms state-of-the-art Camera-Radar fusion-based 3D object detection approaches. keywords: {Three-dimensional displays;Estimation;Radar;Object detection;Radar imaging;Transformers;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341793&isnumber=10341342

Y. Man, L. Gui and Y. -X. Wang, "DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10910-10917, doi: 10.1109/IROS55552.2023.10341473.Abstract: Closing the domain gap between training and deployment and incorporating multiple sensor modalities are two challenging yet critical topics for self-driving. Existing work only focuses on single one of the above topics, overlooking the simultaneous domain and modality shift which pervasively exists in real-world scenarios. A model trained with multi-sensor data collected in Europe may need to run in Asia with a subset of input sensors available. In this work, we propose DualCross, a cross-modality cross-domain adaptation framework to facilitate the learning of a more robust monocular bird's-eye-view (BEV) perception model, which transfers the point cloud knowledge from a LiDAR sensor in one domain during the training phase to the camera-only testing scenario in a different domain. This work results in the first open analysis of cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild. We benchmark our approach on large-scale datasets under a wide range of domain shifts and show state-of-the-art results against various baselines. Our project webpage is at https://yunzeman.github.io/DualCross. keywords: {Training;Point cloud compression;Adaptation models;Three-dimensional displays;Laser radar;Europe;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341473&isnumber=10341342

S. Zhou, S. Xie, R. Ishikawa, K. Sakurada, M. Onishi and T. Oishi, "INF: Implicit Neural Fusion for LiDAR and Camera," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10918-10925, doi: 10.1109/IROS55552.2023.10341648.Abstract: Sensor fusion has become a popular topic in robotics. However, conventional fusion methods encounter many difficulties, such as data representation differences, sensor variations, and extrinsic calibration. For example, the calibration methods used for LiDAR-camera fusion often require manual operation and auxiliary calibration targets. Implicit neural representations (INRs) have been developed for 3D scenes, and the volume density distribution involved in an INR unifies the scene information obtained by different types of sensors. Therefore, we propose implicit neural fusion (INF) for LiDAR and camera. INF first trains a neural density field of the target scene using LiDAR frames. Then, a separate neural color field is trained using camera images and the trained neural density field. Along with the training process, INF both estimates LiDAR poses and optimizes extrinsic parameters. Our experiments demonstrate the high accuracy and stable performance of the proposed method. keywords: {Training;Temperature sensors;Laser radar;Three-dimensional displays;Robot vision systems;Sensor fusion;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341648&isnumber=10341342

M. Adjel et al., "Multi-Modal Upper Limbs Human Motion Estimation from a Reduced Set of Affordable Sensors," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10926-10932, doi: 10.1109/IROS55552.2023.10342040.Abstract: This study aims at developing a new affordable motion capture system for human upper limbs' joint kinematics estimation based on a reduced set of visual inertial measurement units coupled with a markerless skeleton tracking algorithm. The markerless skeleton tracking algorithm allows to alleviate the kinematic redundancy that is observed if only a single visual inertial measurement unit is used at the hand level but it introduces undesired outliers. A Sliding Window Inverse Kinematics Algoritm based on a biomechanical model is proposed to filter out outliers. It has the advantage to constrain the evolution of joint kinematics while being able to handle multi- modalities. The proposed system was validated with five healthy volunteers performing a popular rehabilitation pick and place task. Joint angles estimated using our method were compared with the ones obtained using a reference stereophotogrammetric system. The results showed an average root mean square error of 9.7deg along with an average correlation of 0.8. These results compare favorably with literature results obtained with more numerous and relatively costly sensors or more elaborated and expensive markerless systems. keywords: {Wrist;Visualization;Measurement units;Tracking;Shoulder;Inertial navigation;Calibration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342040&isnumber=10341342

Y. Li, Z. Zhang, J. Wang, H. Zhang, E. Zhou and F. Zhang, "Cognition Difference-Based Dynamic Trust Network for Distributed Bayesian Data Fusion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10933-10938, doi: 10.1109/IROS55552.2023.10341770.Abstract: Distributed Data Fusion (DDF), as a prevalent technique that empowers scalable, flexible, and robust information fusing, has been employed in various multi-sensor networks operating in uncertain and dynamic environments. This paper proposes a cognition difference-based mechanism to construct a dynamic trust network for real-time DDF, where the cognition difference is defined as the statistical difference between the sensors' estimated probability distributions. Distinguished by the mutual correlation between trust and cognition difference, two principles of determining trust are investigated, and their performances are analyzed by conducting simulations in the scenarios of source seeking. Our simulation and experiment results show that the proposed approach is effective in providing comprehensive and robust performance in general and unstructured environments. keywords: {Analytical models;Correlation;Data integration;Cognition;Real-time systems;Probability distribution;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341770&isnumber=10341342

Y. Gao, C. Sima, S. Shi, S. Di, S. Liu and H. Li, "Sparse Dense Fusion for 3D Object Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10939-10946, doi: 10.1109/IROS55552.2023.10341620.Abstract: With the prevalence of multimodal learning, camera-LiDAR fusion has gained popularity in 3D object detection. Many fusion approaches have been proposed, falling into two main categories: sparse-only or dense-only, differentiated by their feature representation within the fusion module. We analyze these approaches within a shared taxonomy, identifying two key challenges: (1) Sparse-only methodologies maintain 3D geometric prior but fail to capture the semantic richness from camera data, and (2) Dense-only strategies preserve semantic continuity at the expense of precise geometric information derived from LiDAR. Upon analysis, we deduce that due to their respective architectural designs, some degree of information loss is inevitable. To counteract this loss, we introduce Sparse Dense Fusion (SD-Fusion), an innovative framework combining both sparse and dense fusion modules via the Transformer architecture. The simple yet effective fusion strategy enhances semantic texture and simultaneously leverages spatial structure data. Employing our SD-Fusion strategy, we assemble two popular methods with moderate performance, achieving a 4.3% increase in mAP and a 2.5% rise in NDS, thus ranking first in the nuScenes benchmark. Comprehensive ablation studies validate the effectiveness of our approach and empirically support our findings. keywords: {Three-dimensional displays;Laser radar;Semantics;Taxonomy;Object detection;Benchmark testing;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341620&isnumber=10341342

J. A. Collins, C. Houff, P. Grady and C. C. Kemp, "Visual Contact Pressure Estimation for Grippers in the Wild," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10947-10954, doi: 10.1109/IROS55552.2023.10342124.Abstract: Sensing contact pressure applied by a gripper can benefit autonomous and teleoperated robotic manipulation, but adding tactile sensors to a gripper's surface can be difficult or impractical. If a gripper visibly deforms, contact pressure can be visually estimated using images from an external camera that observes the gripper. While researchers have demonstrated this capability in controlled laboratory settings, prior work has not addressed challenges associated with visual pressure estimation in the wild, where lighting, surfaces, and other factors vary widely. We present a deep learning model and associated methods that enable visual pressure estimation under widely varying conditions. Our model, Visual Pressure Estimation for Robots (ViPER), takes an image from an eye-in-hand camera as input and outputs an image representing the pressure applied by a soft gripper. Our key insight is that force/torque sensing can be used as a weak label to efficiently collect training data in settings where pressure measurements would be difficult to ob-tain. When trained on this weakly labeled data combined with fully labeled data that includes pressure measurements, ViPER outperforms prior methods, enables precision manipulation in cluttered settings, and provides accurate estimates for unseen conditions relevant to in-home use. keywords: {Visualization;Force measurement;Robot vision systems;Estimation;Training data;Tactile sensors;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342124&isnumber=10341342

Y. Chang, N. Hughes, A. Ray and L. Carlone, "Hydra-Multi: Collaborative Online Construction of 3D Scene Graphs with Multi-Robot Teams," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10995-11002, doi: 10.1109/IROS55552.2023.10341838.Abstract: 3D scene graphs have recently emerged as an expressive high-level map representation that describes a 3D environment as a layered graph where nodes represent spatial concepts at multiple levels of abstraction (e.g., objects, rooms, buildings) and edges represent relations between concepts (e.g., inclusion, adjacency). This paper describes Hydra-Multi, the first multi-robot spatial perception system capable of constructing a multi-robot 3D scene graph online from sensor data collected by robots in a team. In particular, we develop a centralized system capable of constructing a joint 3D scene graph by taking incremental inputs from multiple robots, effectively finding the relative transforms between the robots' frames, and incorporating loop closure detections to correctly reconcile the scene graph nodes from different robots. We evaluate Hydra-Multi on simulated and real scenarios and show it is able to reconstruct accurate 3D scene graphs online. We also demonstrate Hydra-Multi's capability of supporting heterogeneous teams by fusing different map representations built by robots with different sensor suites. keywords: {Three-dimensional displays;Buildings;Collaboration;Transforms;Robot sensing systems;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341838&isnumber=10341342

Z. Zhang, Z. Yao and M. Lu, "TWO: A Simple Method of Directly Closing the Loop for LiDAR Odometry," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11003-11010, doi: 10.1109/IROS55552.2023.10341569.Abstract: In this paper, we propose a simple method, termed TWO, of directly closing the loop for LiDAR odometry. TWO suggests assigning high weights to the LIDAR observations corresponding to the old parts of the map; since these parts are built with the low-drift poses from the early odometry and can help drag the drifted odometry back to the correct global position when the LiDAR scans the points of these parts again. Also, we present the method of checking the consistency of the plane normal to address the two-side problem that may cause damage when using TWO. Moreover, we show that the proposed method is lightweight and needs little extra computation and storage space compared to the original odometry. The proposed TWO is integrated into the state-of-the-art LiDAR odometry A-LOAM and LiDAR-inertial odometry FAST-LIO2, and it is tested thoroughly on five public datasets and our private handheld dataset. The experiments show that the TWO can effectively help these two methods directly close most loops and produce localization results with apparently lower drifts. keywords: {Location awareness;Laser radar;Pose estimation;Odometry;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341569&isnumber=10341342

G. Erni, J. Frey, T. Miki, M. Mattamala and M. Hutter, "MEM: Multi-Modal Elevation Mapping for Robotics and Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11011-11018, doi: 10.1109/IROS55552.2023.10342108.Abstract: Elevation maps are commonly used to represent the environment of mobile robots and are instrumental for locomotion and navigation tasks. However, pure geometric information is insufficient for many field applications that require appearance or semantic information, which limits their applicability to other platforms or domains. In this work, we extend a 2.5D robot-centric elevation mapping framework by fusing multi-modal information from multiple sources into a popular map representation. The framework allows inputting data contained in point clouds or images in a unified manner. To manage the different nature of the data, we also present a set of fusion algorithms that can be selected based on the information type and user requirements. Our system is designed to run on the GPU, making it real-time capable for various robotic and learning tasks. We demonstrate the capabilities of our framework by deploying it on multiple robots with varying sensor configurations and showcasing a range of applications that utilize multi-modal layers, including line detection, human detection, and colorization. keywords: {Micromechanical devices;Point cloud compression;Navigation;Semantic segmentation;Semantics;Graphics processing units;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342108&isnumber=10341342

B. Cao, C. -N. Ritter, K. Alomari and D. Goehring, "Cooperative LiDAR Localization and Mapping for V2X Connected Autonomous Vehicles," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11019-11026, doi: 10.1109/IROS55552.2023.10341513.Abstract: Cooperative Simultaneous Localization and Mapping (C-SLAM) is an active research topic in mobile robotics. However, its application in the field of autonomous driving is rare. While the advent of Vehicle-to-Everything (V2X) communication has empowered Connected Autonomous Vehicles (CAV) to exchange data with each other, recent research on CAV cooperation tasks has primarily focused on cooperative perception and global positioning improvement. Techniques for organizing multiple CAV to work together to achieve localization and mapping in unknown environments have not been actively explored. We propose a C-SLAM system for CAVs that employs sparse LiDAR feature representations to enable vehicles to exchange data using standard V2X messages. The system was tested in real environments using two connected vehicles. The results show that the proposed V2X-based C-SLAM system can operate in both centralized and decentralized manners and output accurate pose estimates and global maps, showing promising application possibilities. keywords: {Location awareness;Simultaneous localization and mapping;Laser radar;Connected vehicles;Trajectory;Task analysis;Autonomous vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341513&isnumber=10341342

Y. Tian et al., "Resilient and Distributed Multi-Robot Visual SLAM: Datasets, Experiments, and Lessons Learned," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11027-11034, doi: 10.1109/IROS55552.2023.10342377.Abstract: This paper revisits Kimera-Multi, a distributed multi-robot Simultaneous Localization and Mapping (SLAM) system, towards the goal of deployment in the real world. In particular, this paper has three main contributions. First, we describe improvements to Kimera-Multi to make it resilient to large-scale real-world deployments, with particular emphasis on handling intermittent and unreliable communication. Second, we collect and release challenging multi-robot benchmarking datasets obtained during live experiments conducted on the MIT campus, with accurate reference trajectories and maps for evaluation. The datasets include up to 8 robots traversing long distances (up to 8 km) and feature many challenging elements such as severe visual ambiguities (e.g., in underground tunnels and hallways), mixed indoor and outdoor trajectories with different lighting conditions, and dynamic entities (e.g., pedestrians and cars). Lastly, we evaluate the resilience of Kimera-Multi under different communication scenarios, and provide a quantitative comparison with a centralized baseline system. Based on the results from both live experiments and subsequent analysis, we discuss the strengths and weaknesses of Kimera-Multi, and suggest future directions for both algorithm and system design. We release the source code of Kimera-Multi and all datasets to facilitate further research towards the reliable real-world deployment of multi-robot SLAM systems. keywords: {Visualization;Simultaneous localization and mapping;Pedestrians;Statistical analysis;Source coding;Urban areas;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342377&isnumber=10341342

P. Li and L. Zhou, "Assignment Algorithms for Multi-Robot Multi-Target Tracking with Sufficient and Limited Sensing Capability," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11035-11041, doi: 10.1109/IROS55552.2023.10341514.Abstract: We study the problem of assigning robots with actions to track targets. The objective is to optimize the robot team's tracking quality which can be defined as the reduction in the uncertainty of the targets' states. Specifically, we consider two assignment problems given the different sensing capabilities of the robots. In the first assignment problem, a single robot is sufficient to track a target. To this end, we present a greedy algorithm (Algorithm 1) that assigns a robot with its action to each target. We prove that the greedy algorithm has a 1/2-approximation bound and runs in polynomial time. Then, we study the second assignment problem where two robots are necessary to track a target. We design another greedy algorithm (Algorithm 2) that assigns a pair of robots with their actions to each target. We prove that the greedy algorithm achieves a 1/3-approximation bound and has a polynomial running time. Moreover, we illustrate the performance of the two greedy algorithms in the ROS-Gazebo environment where the tracking patterns of one robot following one target using Algorithm 1 and two robots following one target using Algorithm 2 are clearly observed. Further, we conduct extensive comparisons to demonstrate that the two greedy algorithms perform close to their optimal counterparts and much better than their respective (1/2 and 1/3) approximation bounds. keywords: {Greedy algorithms;Target tracking;Uncertainty;Computational modeling;Robot sensing systems;Approximation algorithms;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341514&isnumber=10341342

Y. Tang, M. Wang, Y. Deng, Y. Yang, Z. Lan and Y. Yue, "Multi-View Robust Collaborative Localization in High Outlier Ratio Scenes Based on Semantic Features," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11042-11047, doi: 10.1109/IROS55552.2023.10342524.Abstract: Filtering out outlier data associations between local maps can improve the robustness and accuracy of multi-robot localization. When the overlap is low and the field of view difference is large, it is likely to produce outlier data associations between local maps, which will reduce the matching accuracy and even lead to the failure of collaborative localization. To solve this problem, this paper proposes a novel outdoor robust collaborative localization algorithm (HORCL) capable for high outlier ratio scenes. The Mixture Probability Model (MPM) and the Hierarchical EM (Expectation Maximization) algorithm in HORCL are applied to screen two levels of outliers (loop closure constraints and point pairs) and improve localization performance. Specifically, the inlier probabilities of data associations are calculated in MPM to identify outliers by considering geometric distances, semantic consistency, and spatial consistency. Then, outlier loop closures and outlier point pairs in inlier constraints are filtered by applying the Hierarchical EM algorithm, thereby relieving the adverse effect of outliers on localization accuracy. The proposed algorithm is validated on public datasets and compared with the latest methods, demonstrating the improvement in localization accuracy and robustness. The code is available at https://github.com/BIT-TYJ/HORCL. keywords: {Location awareness;Semantics;Collaboration;Probability;Filtering algorithms;Robustness;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342524&isnumber=10341342

N. Hao, F. He, C. Tian, Y. Yao and W. Xia, "KD-EKF: A Consistent Cooperative Localization Estimator Based on Kalman Decomposition," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11064-11070, doi: 10.1109/IROS55552.2023.10341604.Abstract: In this paper, we revisit the inconsistency problem of EKF-based cooperative localization (CL) from the perspective of system decomposition. By transforming the linearized system used by the standard EKF into its Kalman observable canonical form, the observable and unobservable components of the system are separated. Consequently, the factors causing the dimension reduction of the unobservable subspace are explicitly isolated in the state propagation and measurement Jacobians of the Kalman observable canonical form. Motivated by these insights, we propose a new CL algorithm called KD-EKF which aims to enhance consistency. The key idea behind the KD-EKF algorithm involves perform state estimation in the transformed coordinates so as to eliminate the influencing factors of observability in the Kalman observable canonical form. As a result, the KD-EKF algorithm ensures correct observability properties and consistency. We extensively verify the effectiveness of the KD-EKF algorithm through both Monte Carlo simulations and real-world experiments. The results demonstrate that the KD-EKF outperforms state-of-the-art algorithms in terms of accuracy and consistency. keywords: {Location awareness;Jacobian matrices;Simultaneous localization and mapping;Monte Carlo methods;Filtering algorithms;Kalman filters;Observability;Cooperative localization;Kalman decomposition;nonlinear estimation;extended Kalman filter;consistency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341604&isnumber=10341342

R. Chen, J. Li, Y. Chen and Y. Huang, "A Distributed Scheduling Method for Networked UAV Swarm based on Computing for Communication," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11071-11078, doi: 10.1109/IROS55552.2023.10342228.Abstract: UAV swarms have attracted much attention for post-disaster search and rescue, pollution monitoring and trace-ability, etc., where distributed scheduling is required to arrange careful tasks and time quickly. The market-based methods are widely favored but they rely on the environmentally influenced communication network to complete negotiation, while the on-board computing of UAV is robust and redundant. This paper proposes a distributed scheduling method for networked UAV swarm based on computing for communication, which trades a modest increase in computing for a significant decrease in communication. First, by analyzing the task removal strategies of two representative methods, the consensus-based bundle algorithm (CBBA) and performance impact (PI) algorithm, a new removal strategy is proposed, which expands the explo-ration of the bundle and can potentially reduce communication rounds. Second, the proposed task-related optimization method can extract task conflict nodes from the native communication protocol, and use the sampling and estimation strategies to resolve task conflicts in advance. Third, historical bids are cleverly used to infer others' locations, which is necessary for task-related optimization. Fourth, to verify the algorithm in real communication, a hardware-in-the-loop (HIL) ad-hoc network simulation system is constructed, which uses real network protocols and simulated channel transmissions. Finally, the HIL Monte Carlo simulation results show that, compared with CBBA and PI, the proposed method can significantly reduce the number of communication rounds and the total scheduling time, without increasing the communication protocol overhead and loss of optimization. keywords: {Protocols;Pollution;Monte Carlo methods;Processor scheduling;Optimization methods;Autonomous aerial vehicles;Prediction algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342228&isnumber=10341342

P. Gao, J. Liang, Y. Shen, S. Son and M. C. Lin, "Visual, Spatial, Geometric-Preserved Place Recognition for Cross-View and Cross-Modal Collaborative Perception," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11079-11086, doi: 10.1109/IROS55552.2023.10341898.Abstract: Place recognition plays an important role in multi-robot collaborative perception, such as aerial-ground search and rescue, in order to identify the same place they have visited. Recently, approaches based on semantics showed the promising performance to address cross-view and cross-modal challenges in place recognition, which can be further categorized as graph-based and geometric-based methods. However, both methods have shortcomings, including ignoring geometric cues and affecting by large non-overlapped regions between observations. In this paper, we introduce a novel approach that integrates semantic graph matching and distance fields (DF) matching for cross-view and cross-modal place recognition. Our method uses a graph representation to encode visual-spatial cues of semantics and uses a set of class-wise DFs to encode geometric cues of a scene. Then, we formulate place recognition as a two-step matching problem. We first perform semantic graph matching to identify the correspondence of semantic objects. Then, we estimate the overlapped regions based on the identified correspondences and further align these regions to compute their geometric-based DF similarity. Finally, we integrate graph-based similarity and geometry-based DF similarity to match places. We evaluate our approach over two public benchmark datasets, including KITTI and AirSim. Compared with the previous methods, our approach achieves around 10% improvement in ground-ground place recognition in KITTI and 35% improvement in aerial-ground place recognition in AirSim. keywords: {Point cloud compression;Visualization;Laser radar;Runtime;Semantics;Robot vision systems;Collaboration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341898&isnumber=10341342

S. Acharya, M. Bharatheesha, Y. Simmhan and B. Amrutur, "A Co-Simulation Framework for Communication and Control in Autonomous Multi-Robot Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11087-11094, doi: 10.1109/IROS55552.2023.10342407.Abstract: Multi-Robot Systems (MRS) are transforming diverse domains like logistics, cargo management, and agriculture. However, ensuring that the behavior is correct under various network conditions within such complex environments is challenging, and meeting the desired automation goals is difficult. We propose the CORNET 2.0 co-simulation framework to jointly and accurately simulate multi-agent robotic systems within physical environments and the communication network models within such environments. Our modular framework allows diverse robot and network models to seamlessly integrate to simulate the robot's autonomy, physical space, and network features, such as latency, throughput, and loss intrinsic to the network topology and communication technology. A key novelty of CORNET 2.0 is its accurate synchronizing of mobility and time, which ensures that the physical location of a robot at a point in time, and the network properties and packets that flow from that location, are aligned. This is vital to model and validate MRS coordination algorithms that rely on network interactions. We provide a detailed evaluation of CORNET 2.0 in modeling real-world MRS use cases, such as leader-follower and warehouse environments, that help highlights the benefits. keywords: {Automation;Network topology;Robot kinematics;Throughput;Safety;Multi-robot systems;Communication networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342407&isnumber=10341342

R. Ou, G. Liang and T. L. Lam, "FPECMV: Learning-Based Fault-Tolerant Collaborative Localization Under Limited Connectivity," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11095-11102, doi: 10.1109/IROS55552.2023.10342426.Abstract: Collaborative localization (CL) has garnered substantial attention in the field of robotics in recent years. Nonetheless, conventional CL algorithms have faced challenges when dealing with practical issues such as spurious sensor data and limited or discontinued observation and communication in real-world settings. This paper proposes a fault-tolerant practical estimated cross-covariance minimum variance update method (FPECMV) designed to tackle these challenges under limited connectivity. The proposed algorithm uses a CNN-based method to evaluate confidence, along with a fault isolation module to identify faults and manage spurious data in real time. The proposed fault isolation module utilizes relative measurement information that randomly occurs, without requiring high observation and communication prerequisites. Notably, the algorithm takes into account correlations among agents to maintain consistency in localization filters and attain accurate localization despite constraints posed by limited connectivity. To evaluate the performance of the proposed algorithm, experiments were conducted in a collaborative multi-robot environment with spurious sensor data and limited connectivity, using both the BULLET simulation and physical mobile robots. The experimental results indicate that the overall localization performance of the proposed algorithm is improved by 21.0% compared to the state of the art. The experiment results demonstrate the effectiveness of our algorithm in localizing group agents in challenging and intricate scenarios with limited connectivity and spurious sensor data. keywords: {Location awareness;Fault tolerance;Visualization;Correlation;Fault tolerant systems;Collaboration;Distributed databases},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342426&isnumber=10341342

R. Lin and M. Egerstedt, "Dynamic Multi-Target Tracking Using Heterogeneous Coverage Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11103-11110, doi: 10.1109/IROS55552.2023.10342237.Abstract: A coverage-based collaborative control strategy is developed in this paper for a multi-robot system with heterogeneous effective sensing ranges and safe operation zones to simultaneously estimate the states of and follow multiple targets governed by stochastic dynamics. Multiplicatively weighted Voronoi diagrams are exploited to define each robot's dominant region considering its limited sensing radius. The asymptotically stable system dynamics enabling the heterogeneous multi-robot system to (locally) optimally cover the time-varying probability density distributions that characterize the uncertainties of the targets' positions is derived, and minimally perturbed by control barrier functions designed to ensure that each robot moves within its safe operation zone in a collision-free manner. Specific target dynamics and measurement models are chosen in the experiment, whose results demonstrate the effectiveness of the proposed dynamic multi-target tracking approach. keywords: {Target tracking;Uncertainty;System dynamics;Robot sensing systems;Sensors;Noise measurement;Multi-robot systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342237&isnumber=10341342

M. Bernard, C. Pacchierotti and P. R. Giordano, "Decentralized Connectivity Maintenance for Quadrotor UAVs with Field of View Constraints," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11111-11118, doi: 10.1109/IROS55552.2023.10342003.Abstract: We present a decentralized connectivity-maintenance algorithm for controlling a group of quadrotor UAVs with limited field of view (FOV) and not sharing a common reference frame for collectively expressing measurements and commands. This is in contrast to the vast majority of previous works on this topic which, instead, make the (simplifying) assumptions of omnidirectional sensing and presence of a common shared frame. For achieving this goal, we design a gradient-based connectivity-maintenance controller able to take into account the presence of a limited FOV. We also propose a novel (to our knowledge) decentralized estimator of the relative orientation among neighboring robots, which is a necessary quantity for correctly implementing the connectivity-maintenance action. We validate the framework in realistic simulations that show the effectiveness of our approach. keywords: {Maintenance engineering;Robot sensing systems;Sensors;Intelligent robots;Quadrotors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342003&isnumber=10341342

K. Chen et al., "FogROS2-SGC: A ROS2 Cloud Robotics Platform for Secure Global Connectivity," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341719.Abstract: The Robot Operating System (ROS2) is the most widely used software platform for building robotics applications. FogROS2 extends ROS2 to allow robots to access cloud computing on demand. We introduce FogROS2-SGC, an extension of FogROS2 that can effectively connect robot systems across different physical locations, networks, and Data Distribution Services (DDS). With globally unique and location-independent identifiers, FogROS2-SGC can securely and efficiently route data between robotics components around the globe. FogROS2-SGC is agnostic to the ROS2 distribution and configuration, is compatible with non-ROS2 software, and seamlessly extends existing ROS2 applications without any code modification. We evaluate FogROS2-SGC with 4 robots and compute nodes that are 3600 km apart. Experiments suggest FogROS2-SGC is 19x faster than rosbridge (a ROS2 package with comparable features, but lacking security). Videos and code are available on the website https://sites.google.com/view/fogros2-sgc. keywords: {Cloud computing;Codes;Operating systems;Buildings;Software;Security;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341719&isnumber=10341342

M. Coffey and A. Pierson, "Covering Dynamic Demand with Multi-Resource Heterogeneous Teams," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11127-11134, doi: 10.1109/IROS55552.2023.10342119.Abstract: In this work, we consider a team of heterogeneous robots equipped with various types and quantities of resources, and tasked with supplying these resources to multiple dynamic demand locations. We present an adaptive control policy that enables robots to serve a dynamic demand: we allow demand to deplete as robots supply resources, and we allow demand injection and movement of demand locations. We show that the demand is input-to-state stable (ISS) under our proposed resource dynamics, and thus the robots can drive the demand to a steady state. Finally, we present simulations and hardware experiments to demonstrate our approach, and demonstrate the benefits of coverage over a persistent monitoring approach. keywords: {Dynamic scheduling;Hardware;Steady-state;Task analysis;Adaptive control;Robots;Monitoring},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342119&isnumber=10341342

G. Shi and P. Tokekar, "Decision-Oriented Learning with Differentiable Submodular Maximization for Vehicle Routing Problem," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11135-11140, doi: 10.1109/IROS55552.2023.10342311.Abstract: We study the problem of learning a function that maps context observations (input) to parameters of a submodular function (output). Our motivating case study is a specific type of vehicle routing problem, in which a team of Unmanned Ground Vehicles (UGVs) can serve as mobile charging stations to recharge a team of Unmanned Ground Vehicles (UAVs) that execute persistent monitoring tasks. We want to learn the mapping from observations of UAV task routes and wind field to the parameters of a submodular objective function, which describes the distribution of landing positions of the UAVs. Traditionally, such a learning problem is solved independently as a prediction phase without considering the downstream task optimization phase. However, the loss function used in prediction may be misaligned with our final goal, i.e., a good routing decision. Good performance in the isolated prediction phase does not necessarily lead to good decisions in the downstream routing task. In this paper, we propose a framework that incorporates task optimization as a differentiable layer in the prediction phase. Our framework allows end-to-end training of the prediction model without using engineered intermediate loss that is targeted only at the prediction performance. In the proposed framework, task optimization (submodular maximization) is made differentiable by introducing stochastic perturbations into deterministic algorithms (i.e., stochastic smoothing). We demonstrate the efficacy of the proposed framework using synthetic data. Experimental results of the mobile charging station routing problem show that the proposed framework can result in better routing decisions, e.g. the average number of UAVs recharged increases, compared to the prediction-optimization separate approach. keywords: {Training;Smoothing methods;Vehicle routing;Stochastic processes;Charging stations;Routing;Land vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342311&isnumber=10341342

M. Peng, J. Wang, D. Song, F. Miao and L. Su, "Privacy-Preserving and Uncertainty-Aware Federated Trajectory Prediction for Connected Autonomous Vehicles," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11141-11147, doi: 10.1109/IROS55552.2023.10341638.Abstract: Deep learning is the method of choice for trajectory prediction for autonomous vehicles. Unfortunately, its data-hungry nature implicitly requires the availability of sufficiently rich and high-quality centralized datasets, which easily leads to privacy leakage. Besides, uncertainty-awareness becomes increasingly important for safety-crucial cyber physical systems whose prediction module heavily relies on machine learning tools. In this paper, we relax the data collection requirement and enhance uncertainty-awareness by using Federated Learning on Connected Autonomous Vehicles with an uncertainty-aware global objective. We name our algorithm as FLTP. We further introduce ALFLTP which boosts FLTP via using active learning techniques in adaptatively selecting participating clients. We consider two different metrics negative log-likelihood (NLL) and aleatoric uncertainty (AU) for client selection. Experiments on Argoverse dataset show that FLTP significantly outperforms the model trained on local data. In addition, ALFLTP-AU converges faster in training regression loss and performs better in terms of Miss Rate (MR) than FLTP in most rounds, and has more stable round-wise performance than ALFLTP-NLL. keywords: {Training;Measurement;Gold;Data privacy;Uncertainty;Federated learning;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341638&isnumber=10341342

V. Edwards, T. C. Silva, B. Mehta, J. Dhanoa and M. A. Hsieh, "On Collaborative Robot Teams for Environmental Monitoring: A Macroscopic Ensemble Approach," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11148-11153, doi: 10.1109/IROS55552.2023.10342495.Abstract: With the rapidly changing climate and an increase in extreme weather events, it is necessary to have better methods to monitor and study the impacts of these phenomena on urban river environments. Multi-robot environmental monitoring has long focused on strategies that assign individual robots to distinct regions or task objectives. While these methods have seen success for Autonomous Surface Vehicles (ASVs), the spatial expanse and temporal variability of rivers impose an increased burden on existing techniques, necessitating computationally intensive replanning. Alternative methods aim to model and control teams of robots by prescribing global constraints on the system, using the insight that robots' transitions between tasks are stochastic and time-based. These methods do not require replanning because robots will perform different tasks achieving the overall desired system state, focusing on temporal switching alone limits their overall descriptive power. In this paper, we present a method that considers collaborations between robots to inform task switching based on spatial proximity. Our results suggest that in unknown environments macroscopic models provide increased flexibility for individual robot task execution as compared to coverage control methods. keywords: {Collaboration;Stochastic processes;Switches;Predictive models;Rivers;Environmental monitoring;Task analysis;Macroscopic models;Robotic Teams;Environmental Monitoring},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342495&isnumber=10341342

Z. Zhang et al., "Part-level Scene Reconstruction Affords Robot Interaction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11178-11185, doi: 10.1109/IROS55552.2023.10342208.Abstract: Existing methods for reconstructing interactive scenes primarily focus on replacing reconstructed objects with CAD models retrieved from a limited database, resulting in significant discrepancies between the reconstructed and observed scenes. To address this issue, our work introduces a part-level reconstruction approach that reassembles objects using primitive shapes. This enables us to precisely replicate the observed physical scenes and simulate robot interactions with both rigid and articulated objects. By segmenting reconstructed objects into semantic parts and aligning primitive shapes to these parts, we assemble them as CAD models while estimating kinematic relations, including parent-child contact relations, joint types, and parameters. Specifically, we derive the optimal primitive alignment by solving a series of optimization problems, and estimate kinematic relations based on part semantics and geometry. Our experiments demonstrate that part-level scene reconstruction outperforms object-level reconstruction by accurately capturing finer details and improving precision. These reconstructed part-level interactive scenes provide valuable kinematic information for various robotic applications; we showcase the feasibility of certifying mobile manipulation planning in these interactive scenes before executing tasks in the physical world. keywords: {Geometry;Solid modeling;Shape;Databases;Semantics;Kinematics;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342208&isnumber=10341342

R. Qiao, H. Kawasaki and H. Zha, "Online Adaptive Disparity Estimation for Dynamic Scenes in Structured Light Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11186-11193, doi: 10.1109/IROS55552.2023.10342000.Abstract: In recent years, deep neural networks have shown remarkable progress in dense disparity estimation from dynamic scenes in monocular structured light systems. However, their performance significantly drops when applied in unseen environments. To address this issue, self-supervised online adaptation has been proposed as a solution to bridge this performance gap. Unlike traditional fine-tuning processes, online adaptation performs test-time optimization to adapt networks to new domains. Therefore, achieving fast convergence during the adaptation process is critical for attaining satisfactory accuracy. In this paper, we propose an unsupervised loss function based on long sequential inputs. It ensures better gradient directions and faster convergence. Our loss function is designed using a multi-frame pattern flow, which comprises a set of sparse trajectories of the projected pattern along the sequence. We estimate the sparse pseudo ground truth with a confidence mask using a filter-based method, which guides the online adaptation process. Our proposed framework significantly improves the online adaptation speed and achieves superior performance on unseen data. The code is available on https://github.com/CodePointer/TIDENet. keywords: {Bridges;Codes;Estimation;Computer architecture;Artificial neural networks;Trajectory;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342000&isnumber=10341342

H. Jang, M. Jung and A. Kim, "RaPlace: Place Recognition for Imaging Radar using Radon Transform and Mutable Threshold," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11194-11201, doi: 10.1109/IROS55552.2023.10341883.Abstract: Due to the robustness in sensing, radar has been highlighted, overcoming harsh weather conditions such as fog and heavy snow. In this paper, we present a novel radar-only place recognition that measures the similarity score by utilizing Radon-transformed sinogram images and cross-correlation in frequency domain. Doing so achieves rigid transform invariance during place recognition, while ignoring the effects of radar multipath and ring noises. In addition, we compute the radar similarity distance using mutable threshold to mitigate variability of the similarity score, and reduce the time complexity of processing a copious radar data with hierarchical retrieval. We demonstrate the matching performance for both intra-session loop-closure detection and global place recognition using a publicly available imaging radar datasets. We verify reliable performance compared to existing stable radar place recognition method. Furthermore, codes for the proposed imaging radar place recognition is released for community https://github.com/hyesu-jang/RaPlace. keywords: {Image recognition;Simultaneous localization and mapping;Snow;Imaging;Radar;Transforms;Radar imaging},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341883&isnumber=10341342

Z. Qiao, Z. Yu, H. Yin and S. Shen, "Pyramid Semantic Graph-Based Global Point Cloud Registration with Low Overlap," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11202-11209, doi: 10.1109/IROS55552.2023.10341394.Abstract: Global point cloud registration is essential in many robotics tasks like loop closing and relocalization. Unfortunately, the registration often suffers from the low overlap between point clouds, a frequent occurrence in practical applications due to occlusion and viewpoint change. In this paper, we propose a graph-theoretic framework to address the problem of global point cloud registration with low overlap. To this end, we construct a consistency graph to facilitate robust data association and employ graduated non-convexity (GNC) for reliable pose estimation, following the state-of-the-art (SoTA) methods. Unlike previous approaches, we use semantic cues to scale down the dense point clouds, thus reducing the problem size. Moreover, we address the ambiguity arising from the consistency threshold by constructing a pyramid graph with multi-level consistency thresholds. Then we propose a cascaded gradient ascend method to solve the resulting densest clique problem and obtain multiple pose candidates for every consistency threshold. Finally, fast geometric verification is employed to select the optimal estimation from multiple pose candidates. Our experiments, conducted on a self-collected indoor dataset and the public KITTI dataset, demonstrate that our method achieves the highest success rate despite the low overlap of point clouds and low semantic quality. We have open-sourced our code1 for this project. keywords: {Point cloud compression;Location awareness;Monte Carlo methods;Semantics;Buildings;Pose estimation;Merging},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341394&isnumber=10341342

C. Brewitt, M. Tamborski, C. Wang and S. V. Albrecht, "Verifiable Goal Recognition for Autonomous Driving with Occlusions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11210-11217, doi: 10.1109/IROS55552.2023.10342386.Abstract: Goal recognition (GR) involves inferring the goals of other vehicles, such as a certain junction exit, which can enable more accurate prediction of their future behaviour. In autonomous driving, vehicles can encounter many different scenarios and the environment may be partially observable due to occlusions. We present a novel GR method named Goal Recognition with Interpretable Trees under Occlusion (OGRIT). OGRIT uses decision trees learned from vehicle trajectory data to infer the probabilities of a set of generated goals. We demonstrate that OGRIT can handle missing data due to occlusions and make inferences across multiple scenarios using the same learned decision trees, while being computationally fast, accurate, interpretable and verifiable. We also release the inDO, rounDO and OpenDDO datasets of occluded regions used to evaluate OGRIT. keywords: {Data models;Trajectory;Decision trees;Junctions;Autonomous vehicles;Intelligent robots;Anomaly detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342386&isnumber=10341342

Y. Goel, N. Vaskevicius, L. Palmieri, N. Chebrolu, K. O. Arras and C. Stachniss, "Semantically Informed MPC for Context-Aware Robot Exploration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11218-11225, doi: 10.1109/IROS55552.2023.10341564.Abstract: We investigate the task of object goal navigation in unknown environments where a target object is given as a semantic label (e.g. find a couch). This task is challenging as it requires the robot to consider the semantic context in diverse settings (e.g. TVs are often nearby couches). Most of the prior work tackles this problem under the assumption of a discrete action policy whereas we present an approach with continuous control which brings it closer to real world applications. In this paper, we use information-theoretic model predictive control on dense cost maps to bring object goal navigation closer to real robots with kinodynamic constraints. We propose a deep neural network framework to learn cost maps that encode semantic context and guide the robot towards the target object. We also present a novel way of fusing mid-level visual representations in our architecture to provide additional semantic cues for cost map prediction. The experiments show that our method leads to more efficient and accurate goal navigation with higher quality paths than the reported baselines. The results also indicate the importance of mid-level representations for navigation by improving the success rate by 8 percentage points. keywords: {Visualization;Costs;Uncertainty;TV;Navigation;Semantics;Network architecture},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341564&isnumber=10341342

C. -Y. Hou, C. -C. Wang and W. -C. Lin, "Automotive Radar Missing Dimension Reconstruction from Motion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11226-11232, doi: 10.1109/IROS55552.2023.10342167.Abstract: Automotive radars have been reliably used in most assisted and autonomous driving systems due to their robustness to extreme weather conditions. With radial velocity measurements from automotive radars, moving targets such as cars, trucks, and buses can be tracked robustly. However, due to the lack of elevation angles in measurements from automotive radars, stationary targets at different heights, such as maintenance holes and bridges, cannot be distinguished. Most autonomous systems rely on sensor fusion or ignore stationary targets to tackle the problem of missing the elevation angle dimension, which derives safety issues. We propose a simple yet effective approach to estimate the elevation angles of stationary targets from relative velocity and radial velocity measurements from an automotive radar. In contrast to structure from motion in computer vision, we utilize the instantaneous velocity generated from the motion of the ego vehicle. The radial velocity of each target is the projection of relative velocity onto the radial direction from radar to target. The radial velocity of each target can be inferred given the target's azimuth, elevation angle, and relative velocity. Accordingly, the elevation angle of each stationary target can be uniquely calculated given the velocity of radar and the target's azimuth and radial velocity measurements. The radar's velocity is estimated with the existing radar odometry algorithm and IMU. The proposed method is verified with real-world data. We evaluate the system's performance with a pre-built point cloud map and a good localization module in a real-world scenario. The proposed elevation angle reconstruction can reach a 1.41-degree mean error and standard deviation of 0.6 degrees in elevation angle. keywords: {Target tracking;Structure from motion;Radar measurements;Azimuth;System performance;Radar;Radar tracking;Automotive radar;Autonomous driving;3D radar perception},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342167&isnumber=10341342

A. J. Sathyamoorthy et al., "VERN: Vegetation-Aware Robot Navigation in Dense Unstructured Outdoor Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11233-11240, doi: 10.1109/IROS55552.2023.10342393.Abstract: We propose a novel method for autonomous legged robot navigation in densely vegetated environments with a variety of pliable/traversable and non-pliable/untraversable vegetation. We present a novel few-shot learning classifier that can be trained on a few hundred RGB images to differentiate flora that can be navigated through, from the ones that must be circumvented. Using the vegetation classification and 2D lidar scans, our method constructs a vegetation-aware traversability cost map that accurately represents the pliable and non-pliable obstacles with lower, and higher traversability costs, respectively. Our cost map construction accounts for misclassifications of the vegetation and further lowers the risk of collisions, freezing and entrapment in vegetation during navigation. Furthermore, we propose holonomic recovery behaviors for the robot for scenarios where it freezes, or gets physically entrapped in dense, pliable vegetation. We demonstrate our method on a Boston Dynamics Spot robot in real-world unstructured environments with sparse and dense tall grass, bushes, trees, etc. We observe an increase of 25-90% in success rates, 10-90% decrease in freezing rate, and up to 65% decrease in the false positive rate compared to existing methods. keywords: {Legged locomotion;Costs;Laser radar;Navigation;Vegetation mapping;Vegetation;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342393&isnumber=10341342

H. Yu, C. Hirayama, C. Yu, S. Herbert and S. Gao, "Sequential Neural Barriers for Scalable Dynamic Obstacle Avoidance," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11241-11248, doi: 10.1109/IROS55552.2023.10341605.Abstract: There are two major challenges for scaling up robot navigation around dynamic obstacles: the complex interaction dynamics of the obstacles can be hard to model analytically, and the complexity of planning and control grows exponentially in the number of obstacles. Data-driven and learning-based methods are thus particularly valuable in this context. However, data-driven methods are sensitive to distribution drift, making it hard to train and generalize learned models across different obstacle densities. We propose a novel method for compositional learning of Sequential Neural Control Barrier models (SN-CBFs) to achieve scalability. Our approach exploits an important observation: the spatial interaction patterns of multiple dynamic obstacles can be decomposed and predicted through temporal sequences of states for each obstacle. Through decomposition, we can generalize control policies trained only with a small number of obstacles, to environments where the obstacle density can be 100x higher. We demonstrate the benefits of the proposed methods in improving dynamic collision avoidance in comparison with existing methods including potential fields, end-to-end reinforcement learning, and model-predictive control. We also perform hardware experiments and show the practical effectiveness of the approach in the supplementary video. keywords: {Training;Scalability;Robot control;Reinforcement learning;Predictive models;Robot sensing systems;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341605&isnumber=10341342

Y. Koh, E. Chang and M. Choi, "Collision Prevention Strategy Using Sparse 2D Spatial Information for Indoor Mobile Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11249-11256, doi: 10.1109/IROS55552.2023.10341443.Abstract: This paper proposes a collision prevention strategy for indoor mobile robots that utilizes a low-cost two-dimensional (2D) range scanner. The use of indoor mobile robots for versatile tasks, such as serving, cleaning, and delivery, has been increasing. As these robots navigate in environments with dynamic obstacles of arbitrary sizes and shapes that move at irregular speeds and directions, sophisticated motion planning is essential to ensure safe and successful task completion. Previous works have relied on object-wise detection and tracking methods that demand significant computing resources and rich information. However, these approaches are challenging to apply for commercial robots that require light computation and guaranteed safety. In this paper, we propose a novel and straightforward strategy that predicts the behavior of dynamic obstacles using agglomerated dynamic points-based trajectory envelope estimation (ADTE), instead of the conventional object-wise approach. We also introduce the stop trigger as a practical and safe strategy that uses the predicted information. The stop trigger operates independently of the primary motion planning module to isolate prediction uncertainty. The proposed strategy was verified through robot testing and simulation with standardized test scenarios defined in ISO 18646–2. The robot testing was officially conducted by an authorized organization. The results demonstrate the effectiveness of the proposed strategy in preventing collisions with moving obstacles. keywords: {Uncertainty;Tracking;Dynamics;Trajectory;Planning;Mobile robots;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341443&isnumber=10341342

L. Kästner et al., "Arena-Rosnav 2.0: A Development and Benchmarking Platform for Robot Navigation in Highly Dynamic Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11257-11264, doi: 10.1109/IROS55552.2023.10342152.Abstract: Following up on our previous works, in this paper, we present Arena-Rosnav 2.0 an extension to our previous works Arena-Bench [1] and Arena-Rosnav [2], which adds a variety of additional modules for developing and benchmarking robotic navigation approaches. The platform is fundamentally restructured and provides unified APIs to add additional functionalities such as planning algorithms, simulators, or evaluation functionalities. We have included more realistic simulation and pedestrian behavior and provide a profound documentation to lower the entry barrier. We evaluated our system by first, conducting a user study in which we asked experienced researchers as well as new practitioners and students to test our system. The feedback was mostly positive and a high number of participants are utilizing our system for other research endeavors. Finally, we demonstrate the feasibility of our system by integrating two new simulators and a variety of state of the art navigation approaches and benchmark them against one another. The platform is openly available at https://github.com/Arena-Rosnav. keywords: {Training;Navigation;Documentation;Tutorials;Benchmark testing;User interfaces;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342152&isnumber=10341342

A. Paudel and G. J. Stein, "Data-Efficient Policy Selection for Navigation in Partial Maps via Subgoal-Based Abstraction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11281-11288, doi: 10.1109/IROS55552.2023.10342047.Abstract: We present a novel approach for fast and reliable policy selection for navigation in partial maps. Leveraging the recent learning-augmented model-based Learning over Subgoals Planning (LSP) abstraction to plan, our robot reuses data collected during navigation to evaluate how well other alternative policies could have performed via a procedure we call offline all-policy replay. Costs from offline alt-policy replay constrain policy selection among the LSP-based policies during deployment, allowing for improvements in convergence speed, cumulative regret and average navigation cost. With only lim-ited prior knowledge about the nature of unseen environments, we achieve at least 67% and as much as 96% improvements on cumulative regret over the baseline bandit approach in our experiments in simulated maze and office-like environments. keywords: {Costs;Navigation;Data models;Planning;Reliability;Intelligent robots;Convergence},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342047&isnumber=10341342

T. Zhou, L. Wang, R. Chen, W. Wang and Y. Liu, "Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego-Centric Motion Skills," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11289-11296, doi: 10.1109/IROS55552.2023.10341449.Abstract: Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/suboptimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with task-specific skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-specific and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efficiently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts significantly in learning efficiency and task performance. keywords: {Visualization;Roads;Reinforcement learning;Color;Libraries;Space exploration;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341449&isnumber=10341342

X. Cai, M. Everett, L. Sharma, P. R. Osteen and J. P. How, "Probabilistic Traversability Model for Risk-Aware Motion Planning in Off-Road Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11297-11304, doi: 10.1109/IROS55552.2023.10341350.Abstract: A key challenge in off-road navigation is that even visually similar terrains or ones from the same semantic class may have substantially different traction properties. Existing work typically assumes no wheel slip or uses the expected traction for motion planning, where the predicted trajectories provide a poor indication of the actual performance if the terrain traction has high uncertainty. In contrast, this work proposes to analyze terrain traversability with the empirical distribution of traction parameters in unicycle dynamics, which can be learned by a neural network in a self-supervised fashion. The probabilistic traction model leads to two risk-aware cost formulations that account for the worst-case expected cost and traction. To help the learned model generalize to unseen environment, terrains with features that lead to unreliable predictions are detected via a density estimator fit to the trained network's latent space and avoided via auxiliary penalties during planning. Simulation results demonstrate that the proposed approach outperforms existing work that assumes no slip or uses the expected traction in both navigation success rate and completion time. Furthermore, avoiding terrains with low density-based confidence score achieves up to 30% improvement in success rate when the learned traction model is used in a novel environment. keywords: {Costs;Uncertainty;Navigation;Simulation;Semantics;Wheels;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341350&isnumber=10341342

L. Jin, X. Chen, J. Rückin and M. Popović, "NeU-NBV: Next Best View Planning Using Uncertainty Estimation in Image-Based Neural Rendering," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11305-11312, doi: 10.1109/IROS55552.2023.10342226.Abstract: Autonomous robotic tasks require actively perceiving the environment to achieve application-specific goals. In this paper, we address the problem of positioning an RGB camera to collect the most informative images to represent an unknown scene, given a limited measurement budget. We propose a novel mapless planning framework to iteratively plan the next best camera view based on collected image measurements. A key aspect of our approach is a new technique for uncertainty estimation in image-based neural rendering, which guides measurement acquisition at the most uncertain view among view candidates, thus maximising the information value during data collection. By incrementally adding new measurements into our image collection, our approach efficiently explores an unknown scene in a mapless manner. We show that our uncertainty estimation is generalisable and valuable for view planning in unknown scenes. Our planning experiments using synthetic and real-world data verify that our uncertainty-guided approach finds informative images leading to more accurate scene representations when compared against baselines. keywords: {Uncertainty;Robot vision systems;Measurement uncertainty;Estimation;Position measurement;Rendering (computer graphics);Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342226&isnumber=10341342

T. Barbie and S. Mukai, "ITIRRT: A Decoupled Framework for the Integration of Machine Learning Into Path Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11313-11320, doi: 10.1109/IROS55552.2023.10341747.Abstract: Machine learning models have been successfully applied in many separate domains in the recent years. Yet, there is still a large disconnect with the path planning community mainly due to implementation difficulty. In this paper, we identify the issues and present a path planning framework that aims to ease the integration of machine learning models. The framework is divided into two phases: prediction and planning. During the prediction phase machine learning models are used to predict a path solution which, while not perfect, is enough to jump-start our proposed planner. The planning phase begins with the initialization of multiple independent tree data structures and, during execution, our planner merges the different trees until a path is found. Due to the architecture of our framework, the prediction and the planning phases are totally decoupled, resulting in a planner that treats the information given by the prediction model as a guess and not as a path to be corrected. This also allows our planner to maintain its probabilistic completeness. Our planner was evaluated on simulated environments and on a small robotic arm. We show that our planner can successfully leverage predicted paths to significantly improve the planning performances. We also demonstrate the flexibility of our framework by integrating multiple machine learning models of varying complexity. keywords: {Tree data structures;Computational modeling;Machine learning;Computer architecture;Predictive models;Probabilistic logic;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341747&isnumber=10341342

K. Zhang, E. Lucet, J. A. dit Sandretto and D. Filliat, "Navigation Among Movable Obstacles Using Machine Learning Based Total Time Cost Optimization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11321-11327, doi: 10.1109/IROS55552.2023.10341355.Abstract: Most navigation approaches treat obstacles as static objects and choose to bypass them. However, the detour could be costly or could lead to failures in indoor environments. The recently developed navigation among movable obstacles (NAMO) methods prefer to remove all the movable obstacles blocking the way, which might be not the best choice when planning and moving obstacles takes a long time. We propose a pipeline where the robot solves the NAMO problems by optimizing the total time to reach the goal. This is achieved by a supervised learning approach that can predict the time of planning and performing obstacle motion before actually doing it if this leads to faster goal reaching. Besides, a pose generator based on reinforcement learning is proposed to decide where the robot can move the obstacle. The method is evaluated in two kinds of simulation environments and the results demonstrate its advantages compared to the classical bypass and obstacle removal strategies. keywords: {Costs;Navigation;Supervised learning;Pipelines;Reinforcement learning;Generators;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341355&isnumber=10341342

H. Zhou, I. Schubert, M. Toussaint and O. S. Oguz, "Spatial Reasoning via Deep Vision Models for Robotic Sequential Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11328-11335, doi: 10.1109/IROS55552.2023.10342010.Abstract: In this paper, we propose using deep neural architectures (i.e., vision transformers and ResNet) as heuristics for sequential decision-making in robotic manipulation problems. This formulation enables predicting the subset of objects that are relevant for completing a task. Such problems are often addressed by task and motion planning (TAMP) formulations combining symbolic reasoning and continuous motion planning. In essence, the action-object relationships are resolved for discrete, symbolic decisions that are used to solve manipulation motions (e.g., via nonlinear trajectory optimization). However, solving long-horizon tasks requires consideration of all possible action-object combinations which limits the scalability of TAMP approaches. To overcome this combinatorial complexity, we introduce a visual perception module integrated with a TAMP-solver. Given a task and an initial image of the scene, the learned model outputs the relevancy of objects to accomplish the task. By incorporating the predictions of the model into a TAMP formulation as a heuristic, the size of the search space is significantly reduced. Results show that our framework finds feasible solutions more efficiently when compared to a state-of-the-art TAMP solver. keywords: {Visualization;Image color analysis;Scalability;Vision sensors;Transformers;Cognition;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342010&isnumber=10341342

K. Yeon, H. Kim and S. -G. Jeong, "SpeedFormer: Learning Speed Profiles with Upper and Lower Boundary Constraints Based on Transformer," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11336-11341, doi: 10.1109/IROS55552.2023.10341689.Abstract: This paper presents a new method for generating speed profiles for autonomous vehicles using a Transformer-based network that predicts the coefficients of quintic polynomials. To train and validate the network, we curate a dataset of 500K simulated urban driving scenarios, where the ground truths are obtained by running offline model predictive control (MPC) optimization. We also present tailored loss functions to emulate MPC behavior and constrain upper-and-lower boundary conditions to provide feasible speed profiles. Extensive experimental results demonstrate the efficacy of the proposed method in providing high-quality speed profiles for a large number of path candidates and long planning horizons. The proposed method is capable of generating efficient speed profiles for 1024 path candidates within 30ms. keywords: {Scalability;Network architecture;Transformers;Boundary conditions;Linear programming;Real-time systems;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341689&isnumber=10341342

Y. Zhao and Q. Zhu, "Stackelberg Meta-Learning for Strategic Guidance in Multi-Robot Trajectory Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11342-11347, doi: 10.1109/IROS55552.2023.10342202.Abstract: Trajectory guidance requires a leader robotic agent to assist a follower robotic agent to cooperatively reach the target destination. However, planning cooperation becomes difficult when the leader serves a family of different followers and has incomplete information about the followers. There is a need for learning and fast adaptation of different cooperation plans. We develop a Stackelberg meta-learning approach to address this challenge. We first formulate the guided trajectory planning problem as a dynamic Stackelberg game to capture the leader-follower interactions. Then, we leverage meta-learning to develop cooperative strategies for different followers. The leader learns a meta-best-response model from a prescribed set of followers. When a specific follower initiates a guidance query, the leader quickly adapts to the follower-specific model with a small amount of learning data and uses it to perform trajectory guidance. We use simulations to elaborate that our method provides a better generalization and adaptation per-formance on learning followers' behavior than other learning approaches. The value and the effectiveness of guidance are also demonstrated by the comparison with zero guidance scenarios11The simulation codes are available at https://github.com/yuhan16/Stackelberg-Meta-Learning.. keywords: {Metalearning;Adaptation models;Trajectory planning;Human-robot interaction;Games;Trajectory;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342202&isnumber=10341342

W. Wang, R. Wang, L. Mao and B. -C. Min, "NaviSTAR: Socially Aware Robot Navigation with Hybrid Spatio-Temporal Graph Transformer and Preference Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11348-11355, doi: 10.1109/IROS55552.2023.10341395.Abstract: Developing robotic technologies for use in human society requires ensuring the safety of robots' navigation behaviors while adhering to pedestrians' expectations and social norms. However, understanding complex human-robot interactions (HRI) to infer potential cooperation and response among robots and pedestrians for cooperative collision avoid-ance is challenging. To address these challenges, we propose a novel socially-aware navigation benchmark called NaviS Tar, which utilizes a hybrid Spatio- Temporal grAph tRansformer to understand interactions in human-rich environments fusing crowd multi-modal dynamic features. We leverage an off-policy reinforcement learning algorithm with preference learning to train a policy and a reward function network with supervi-sor guidance. Additionally, we design a social score function to evaluate the overall performance of social navigation. To compare, we train and test our algorithm with other state-of-the-art methods in both simulator and real-world scenarios independently. Our results show that NaviSTAR outperforms previous methods with outstanding performance11The source code and experiment videos of this work are available at: https://sites.google.com/view/san-navistar keywords: {Training;Pedestrians;Navigation;Heuristic algorithms;Source coding;Human-robot interaction;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341395&isnumber=10341342

E. Goldsztejn, T. Feiner and R. Brafman, "PTDRL: Parameter Tuning Using Deep Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11356-11362, doi: 10.1109/IROS55552.2023.10342140.Abstract: A variety of autonomous navigation algorithms exist that allow robots to move around in a safe and fast manner. Many of these algorithms require parameter re-tuning when facing new environments. In this paper, we propose PTDRL, a parameter-tuning strategy that adaptively selects from a fixed set of parameters those that maximize the expected reward for a given navigation system. Our learning strategy can be used for different environments, different platforms, and different user preferences. Specifically, we attend to the problem of social navigation in indoor spaces, using a classical motion planning algorithm as our navigation system and training its parameters to optimize its behavior. Experimental results show that PTDRL can outperform other online parameter-tuning strategies. keywords: {Training;Deep learning;Navigation;Reinforcement learning;Planning;Behavioral sciences;Tuning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342140&isnumber=10341342

S. A. A. Shah and Z. Khalifa, "Hierarchical Transformer for Visual Affordance Understanding using a Large-scale Dataset," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11371-11376, doi: 10.1109/IROS55552.2023.10341976.Abstract: Recognition, detection, and segmentation tasks in machine vision have focused on studying the physical and textural attributes of objects. However, robots and intelligent machines require the ability to understand visual cues, such as the visual affordances that objects offer, to interact intelligently with novel objects. In this paper, we present a large-scale multi-view RGBD visual affordance learning dataset a benchmark of 47,210 RGBD images from 37 object categories, annotated with 15 visual affordance categories and 35 cluttered/complex scenes. We deploy a Vision Transformer (ViT), called Visual Affordance Transformer (VAT), for the affordance segmentation task. Due to its hierarchical architecture, VAT can learn multiple affordances at various scales, making it suitable for objects of varying sizes. Our experimental results show the superior performance of VAT compared to state-of-the-art deep learning networks. In addition, the challenging nature of the proposed dataset highlights the potential for new and robust affordance learning algorithms. Our dataset is publicly available at https://sites.google.com/view/afaqshah/dataset. keywords: {Deep learning;Training;Visualization;Image segmentation;Affordances;Object segmentation;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341976&isnumber=10341342

S. Nishiyama, T. Saito, R. Nakamura, G. Ohtani, H. Kataoka and K. Hara, "Traffic Incident Database with Multiple Labels Including Various Perspective Environmental Information," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11377-11383, doi: 10.1109/IROS55552.2023.10342176.Abstract: Traffic accident recognition is essential in developing automated driving and Advanced Driving Assistant System technologies. A large dataset of annotated traffic accidents is necessary to improve the accuracy of traffic accident recognition using deep learning models. Conventional traffic accident datasets provide annotations on the presence or absence of traffic accidents and other teacher labels, improving traffic accident recognition performance. However, the labels annotated in conventional datasets need to be more comprehensive to de-scribe traffic accidents in detail. Therefore, we propose V-TIDB, a large-scale traffic accident recognition dataset annotated with various environmental information as multi-labels. Our proposed dataset aims to improve the performance of traffic accident recognition by annotating ten types of environmental information as teacher labels in addition to the presence or absence of traffic accidents. V-TIDB is constructed by collecting many videos from the Internet and annotating them with appropriate environmental information. In our experiments, we compare the performance of traffic accident recognition when only labels related to the presence or absence of traffic accidents are trained and when environmental information is added as a multi-label. In the second experiment, we compare the performance of the training with only “contact level,” which represents the severity of the traffic accident, and the performance with environmental information added as a multi-label. The results showed that 6 out of 10 environmental information labels improved the performance of recognizing the presence or absence of traffic accidents. In the experiment on the degree of recognition of traffic accidents, the performance of recognition of car wrecks and contacts was improved for all environmental information. These experiments show that V-TIDB can be used to learn traffic accident recognition models that take environmental information into account in detail and can be used for appropriate traffic accident analysis. keywords: {Training;Deep learning;Analytical models;Databases;Annotations;Internet;Automobiles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342176&isnumber=10341342

H. Hu et al., "SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark Under Multiple Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11384-11389, doi: 10.1109/IROS55552.2023.10341917.Abstract: Different environments pose a great challenge to the outdoor robust visual perception for long-term autonomous driving, and the generalization of learning-based algorithms on different environments is still an open problem. Although monocular depth prediction has been well studied recently, few works focus on the robustness of learning-based depth prediction across different environments, e.g. changing illumination and seasons, owing to the lack of such a multi-environment real-world dataset and benchmark. To this end, the cross-season monocular depth prediction dataset and benchmark, SeasonDepth, is introduced to benchmark the depth estimation performance under different environments. We investigate several state-of-the-art representative open-source supervised and self-supervised depth prediction methods using newly-formulated metrics. Through extensive experimental evaluation on the proposed dataset and cross-dataset evaluation with current autonomous driving datasets, the performance and robustness against the influence of multiple environments are analyzed qualitatively and quantitatively. We show that long-term monocular depth prediction is still challenging and believe our work can boost further research on the long-term robustness and generalization for outdoor visual perception. The dataset is available on https://seasondepth.github.io. keywords: {Measurement;Lighting;Estimation;Prediction methods;Benchmark testing;Prediction algorithms;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341917&isnumber=10341342

Y. Jing et al., "Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11390-11395, doi: 10.1109/IROS55552.2023.10342201.Abstract: Visual pre-training with large-scale real-world data has made great progress in recent years, showing great potential in robot learning with pixel observations. However, the recipes of visual pre-training for robot manipulation tasks are yet to be built. In this paper, we thoroughly investigate the effects of visual pre-training strategies on robot manipulation tasks from three fundamental perspectives: pre-training datasets, model architectures and training methods. Several significant experimental findings are provided that are beneficial for robot learning. Further, we propose a visual pre-training scheme for robot manipulation termed Vi-PRoM, which combines self-supervised learning and supervised learning. Concretely, the former employs contrastive learning to acquire underlying patterns from large-scale unlabeled data, while the latter aims learning visual semantics and temporal dynamics. Extensive experiments on robot manipulations in various simulation environments and the real robot demonstrate the superiority of the proposed scheme. Videos and more details can be found on https://explore-pretrain-robot.github.io. keywords: {Training;Visualization;Supervised learning;Semantics;Self-supervised learning;Robot learning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342201&isnumber=10341342

F. Goldau, Y. Shivashankar, A. Baumeister, L. Drescher, P. Tolle and U. Frese, "DORMADL - Dataset of Human-Operated Robot Arm Motion in Activities of Daily Living," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11396-11403, doi: 10.1109/IROS55552.2023.10341459.Abstract: This work presents a dataset of human-operated robot motion to be used within the context of assistive robotics and assorted fields, such as learning from demonstrations, machine-learning based robot control, and activity recognition. The data consists of individual sequences of intentional robot motion performing a task in an environment of daily living. There are 2973 sequences generated in a high-resolution simulation and 986 sequences performed in reality, totaling to 1.16 M datapoints. The data includes labels for the robot's pose, motion and activity. This paper also provides data augmentation methods and a detailed dataset analysis as well as simple models trained on the dataset as a baseline for future research. The dataset can be downloaded free-of-charge at https://www.kaggle.com/f371xx/dormadl. keywords: {Robot motion;Measurement;Machine learning;Activity recognition;User interfaces;Predictive models;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341459&isnumber=10341342

J. Lindermayr et al., "IPA-3D1K: A Large Retail 3D Model Dataset for Robot Picking," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11404-11411, doi: 10.1109/IROS55552.2023.10342260.Abstract: Robotic applications like automated order picking in warehouses or retail stores, or fetch and carry tasks in hospitals, care homes, or households rely on the capability of service robots to find and handle a specific type of object. These applications are challenging as the set of objects is very large and varies over time. Despite its significance, there is no suitable universal large-scale dataset available from the retail domain, which allows for a principled analysis of all relevant robotics research aspects in that field. Hence, this paper introduces a novel dataset of more than 1,000 retail objects, including color images, 3D scans, and high-resolution textured 3D models of individual objects, synthetic scenes and real settings, which covers the specifics of the retail domain. The dataset was designed to serve researchers in all relevant robotics tasks in retail like 3D reconstruction and object modeling, large-scale object classification and instance detection including incremental learning and fine-grained detection, text reading, logo detection, semantic grounding and affordance detection, grasp analysis and manipulation planning, as well as digital twinning and virtual environments. Based on synthetic RGB images of scenes created from the 3D models, two exemplary use cases are examined in this paper to demonstrate the benefits of the dataset: we evaluate the state-of-the-art incremental object detection method InstanceNet and a few-shot fine-grained object classification method. The results prove the suitability of InstanceNet for incremental object detection on large datasets and are promising for the few-shot object classification system. keywords: {Solid modeling;Analytical models;Three-dimensional displays;Service robots;Semantics;Virtual environments;Object detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342260&isnumber=10341342

K. Ivanov, G. Ferrer and A. Kornilova, "EVOLIN Benchmark: Evaluation of Line Detection and Association," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11412-11419, doi: 10.1109/IROS55552.2023.10342185.Abstract: Lines are interesting geometrical features commonly seen in indoor and urban environments. There is missing a complete benchmark where one can evaluate lines from a sequential stream of images in all its stages: Line detection, Line Association and Pose error. To do so, we present a complete and exhaustive benchmark for visual lines in a SLAM front-end, both for RGB and RGBD, by providing a plethora of complementary metrics. We have also labeled data from well-known SLAM datasets in order to have all in one poses and accurately annotated lines. In particular, we have evaluated 17 line detection algorithms, 5 line associations methods and the resultant pose error for aligning a pair of frames with several combinations of detector-association. We have packaged all methods and evaluations metrics and made them publicly available on web-page33https://prime-slam.github.io/evolin/. keywords: {Measurement;Visualization;Simultaneous localization and mapping;Urban areas;Benchmark testing;Streaming media;Solids},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342185&isnumber=10341342

S. van Waveren, C. Pek, I. Leite, J. Tumova and D. Kragic, "Generating Scenarios from High-Level Specifications for Object Rearrangement Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11420-11427, doi: 10.1109/IROS55552.2023.10341369.Abstract: Rearranging objects is an essential skill for robots. To quickly teach robots new rearrangements tasks, we would like to generate training scenarios from high-level specifications that define the relative placement of objects for the task at hand. Ideally, to guide the robot's learning we also want to be able to rank these scenarios according to their difficulty. Prior work has shown how generating diverse scenario from specifications and providing the robot with easy-to-difficult samples can improve the learning. Yet, existing scenario generation methods typically cannot generate diverse scenarios while controlling their difficulty. We address this challenge by conditioning generative models on spatial logic specifications to generate spatially-structured scenarios that meet the specification and desired difficulty level. Our experiments showed that generative models are more effective and data-efficient than rejection sam-pling and that the spatially-structured scenarios can drastically improve training of downstream tasks by orders of magnitude. keywords: {Training;Computational modeling;Semantics;Reinforcement learning;Robot learning;Scenario generation;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341369&isnumber=10341342

A. Guo et al., "HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations, Affordances, and Reconstructions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11428-11435, doi: 10.1109/IROS55552.2023.10341672.Abstract: We present the HANDAL dataset for category-level object pose estimation and affordance prediction. Unlike previous datasets, ours is focused on robotics-ready manipulable objects that are of the proper size and shape for functional grasping by robot manipulators, such as pliers, utensils, and screwdrivers. Our annotation process is streamlined, requiring only a single off-the-shelf camera and semi-automated processing, allowing us to produce high-quality 3D annotations without crowd-sourcing. The dataset consists of 308k annotated image frames from 2.2k videos of 212 real-world objects in 17 categories. We focus on hardware and kitchen tool objects to facilitate research in practical scenarios in which a robot manipulator needs to interact with the environment beyond simple pushing or indiscriminate grasping. We outline the usefulness of our dataset for 6-DoF category-level pose+scale estimation and related tasks. We also provide 3D reconstructed meshes of all objects, and we outline some of the bottlenecks to be addressed for democratizing the collection of datasets like this one. Project website: https://nvlabs.github.io/HANDAL/ keywords: {Three-dimensional displays;Annotations;Shape;Affordances;Grasping;Streaming media;6-DOF},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341672&isnumber=10341342

Y. Hao et al., "Understanding the Impact of Image Quality and Distance of Objects to Object Detection Performance," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11436-11442, doi: 10.1109/IROS55552.2023.10342139.Abstract: Object detection is a fundamental task for autonomous driving, which aim to identify and localize objects within an image. Deep learning has made great strides for object detection, with popular models including Faster R-CNN, YOLO, and SSD. The detection accuracy and computational cost of object detection depend on the spatial resolution of an image, which may be constrained by both the camera and storage considerations. Furthermore, original images are often compressed and uploaded to a remote server for object detection. Compression is often achieved by reducing either spatial or amplitude resolution or, at times, both, both of which have well-known effects on performance. Detection accuracy also depends on the distance of the object of interest from the camera. Our work examines the impact of spatial and amplitude resolution, as well as object distance, on object detection accuracy and computational cost. As existing models are optimized for uncompressed (or lightly compressed) images over a narrow range of spatial resolution, we develop a resolution-adaptive variant of YOLOv5 (RA-YOLO), which varies the number of scales in the feature pyramid and detection head based on the spatial resolution of the input image. To train and evaluate this new method, we created a dataset of images with diverse spatial and amplitude resolutions by combining images from the TJU and Eurocity datasets and generating different resolutions by applying spatial resizing and compression. We first show that RA-YOLO achieves a good trade-off between detection accuracy and inference time over a large range of spatial resolutions. We then evaluate the impact of spatial and amplitude resolutions on object detection accuracy using the proposed RA-YOLO model. We demonstrate that the optimal spatial resolution that leads to the highest detection accuracy depends on the ‘tolerated’ image size (constrained by the available bandwidth or storage). We further assess the impact of the distance of an object to the camera on the detection accuracy and show that higher spatial resolution enables a greater detection range. These results provide important guidelines for choosing the image spatial resolution and compression settings predicated on available bandwidth, storage, desired inference time, and/or desired detection range, in practical applications. keywords: {YOLO;Image quality;Adaptation models;Image coding;Computational modeling;Bandwidth;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342139&isnumber=10341342

A. Bremers et al., "The Bystander Affect Detection (BAD) Dataset for Failure Detection in HRI," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11443-11450, doi: 10.1109/IROS55552.2023.10342442.Abstract: For a robot to repair its own error, it must first know it has made a mistake. One way that people detect errors is from the implicit reactions from bystanders - their confusion, smirks, or giggles clue us in that something unexpected occurred. To enable robots to detect and act on bystander responses to task failures, we developed a novel method to elicit bystander responses to human and robot errors. Using 46 different stimulus videos featuring a variety of human and machine task failures, we collected a total of 2,452 webcam videos of human reactions from 54 participants. To test the viability of the collected data, we used the bystander reaction dataset as input to a deep-learning model, BADNet, to predict failure occurrence. We tested different data labeling methods and learned how they affect model performance, achieving precisions above 90%. We discuss strategies (manual labelling, failure-vs-control, and failure-time) used to model bystander reactions and predict failure, and how this approach can be used in real-world robotic deployments to detect errors and improve robot performance. As part of this work, we also contribute with the “Bystander Affect Detection” (BAD) dataset of bystander reactions, supporting the development of better prediction models. keywords: {Webcams;Human-robot interaction;Manuals;Predictive models;Maintenance engineering;Data models;Labeling;affective computing;robot error;human response dataset},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342442&isnumber=10341342

I. Kabir et al., "Few-Shot Segmentation and Semantic Segmentation for Underwater Imagery," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11451-11457, doi: 10.1109/IROS55552.2023.10342227.Abstract: This paper tackles image segmentation problems for underwater environments. First, we introduce a novel under-water animal-centric dataset with dense pixel-level annotations containing diverse fine-grained animal categories to mitigate the lack of diverse categories in the existing benchmarks. Then, we solve two image segmentation tasks using underwater images in this dataset: (i) few-shot segmentation, and (ii) semantic segmentation. For the segmentation task in a few-shot learning framework, we propose a novel attention-guided deep neural network architecture by infusing attention modules in various stages of our proposed network. We systematically explore how the learned attention maps can improve few-shot segmentation performance for underwater imagery. Finally, we assess the semantic segmentation problem on our proposed dataset by benchmarking it with two state-of-the-art semantic segmentation methods. We believe our new problem setup, i.e., few-shot segmentation for underwater environments, will be a valuable addition to the existing underwater semantic segmentation task. We believe our novel dataset will pave the way for developing better algorithms and exploring new research directions for marine robotics and underwater image understanding. We publicly release our dataset and the code to advance image understanding research in underwater environments: https://github.com/Imran220S/uwsnet. keywords: {Codes;Annotations;Animals;Semantic segmentation;Artificial neural networks;Benchmark testing;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342227&isnumber=10341342

A. Kulkarni et al., "RACECAR - The Dataset for High-Speed Autonomous Racing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11458-11463, doi: 10.1109/IROS55552.2023.10342053.Abstract: This paper describes RACECAR, the first open dataset for full-scale and high-speed autonomous racing. Multi-modal sensor data was collected from fully autonomous Indy race cars operating at speeds of up to 170 mph (273 kph). Six teams who raced in the Indy Autonomous Challenge (2021–2022) have contributed to this dataset. The dataset spans 11 racing scenarios across two race tracks which include solo laps, multi-agent laps, overtaking situations, high-accelerations, banked tracks, obstacle avoidance, pit entry and exit at different speeds. The dataset contains 27 racing sessions across 11 scenarios with over 6.5 hours of autonomous racing. The data has been released in both ROS 2 and nuScenes format. We have also developed the ROSbag2nuScenes conversion library to achieve this. The RACECAR data is unique because of the high-speed environment of autonomous racing. We present several benchmark problems on localization, object detection and tracking (LiDAR, Radar, and Camera), and mapping to explore issues that arise at the limits of operation of the vehicle. RACECAR data can be accessed at https://github.com/linklab-uva/RACECAR_DATA. keywords: {Location awareness;Laser radar;Multimodal sensors;Radar detection;Object detection;Benchmark testing;Radar tracking},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342053&isnumber=10341342

A. Jong et al., "WIT-UAS: A Wildland-Fire Infrared Thermal Dataset to Detect Crew Assets from Aerial Views," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 11464-11471, doi: 10.1109/IROS55552.2023.10341683.Abstract: We present the Wildland-fire Infrared Thermal (WIT-UAS) dataset for long-wave infrared sensing of crew and vehicle assets amidst prescribed wildland fire environments. While such a dataset is crucial for safety monitoring in wildland fire applications, to the authors' awareness, no such dataset focusing on assets near fire is publicly available. Presumably, this is due to the barrier to entry of collaborating with fire management personnel. We present two related data subsets: WIT-UAS-ROS consists of full ROS bag files containing sensor and robot data of UAS flight over the fire, and WIT-UAS-Image contains hand-labeled long-wave infrared (LWIR) images extracted from WIT-UAS-ROS. Our dataset is the first to focus on asset detection in a wildland fire environment. We show that thermal detection models trained without fire data frequently detect false positives by classifying fire as people. By adding our dataset to training, we show that the false positive rate is reduced significantly. Yet asset detection in wildland fire environments is still significantly more challenging than detection in urban environments, due to dense obscuring trees, greater heat variation, and overbearing thermal signal of the fire. We publicize this dataset to encourage the community to study more advanced models to tackle this challenging environment. The dataset, code and pretrained models are available at https://github.com/castacks/WIT-UAS-Dataset. keywords: {Training;Analytical models;Wildfires;Urban areas;Training data;Robot sensing systems;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341683&isnumber=10341342

G. McFassel and D. A. Shell, "Infrastructure to support robots: a practical, scalable model for comparative evaluation of design choices," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10421774.Abstract: It is easier to program effective robots when they inhabit highly structured environments. The growing literature on methods to aid robot design has given comparatively little consideration to elements external to the robot itself, yet such elements can encode or enhance information (to improve perception), can alter the effects or costs of actions (to help control), and can provide regularity by imposing constraints. External elements have the potential to be shared, to scale elastically, and to spread both benefits and installation/operating costs. These are traits of infrastructure in support of robots. We introduce a basic but flexible mathematical model –via the MDP framework– for rational evaluation of proposed additions and changes to environments, including where infrastructure may improve precision or performance of either perception or actuation. Through it, one can assess the numbers of agents needed for infrastructure investment to be economical, determine when installation costs would be recouped, and evaluate the effect of behavior changes as responses to environmental modifications. To demonstrate how the model can be instantiated, four simple but practical case studies are presented. keywords: {Costs;Mathematical models;Behavioral sciences;Intelligent robots;Investment},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10421774&isnumber=10341342

