X. Zhang, C. Min, Y. Jia, L. Chen, J. Zhang and H. Sun, "Boosting Lidar 3D Object Detection with Point Cloud Semantic Segmentation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7614-7621, doi: 10.1109/IROS55552.2023.10341613.Abstract: The integration of semantic information can effectively enhance the performance of 3D object detection based on lidar point cloud. Most of previous researches utilize camera-lidar fusion to improve detection accuracy for distant or small objects. However, this approach is typically unsuitable for real-time applications due to the large amount of input data. Recently, a multi-task framework using only Iidar has emerged as an alternative that employs the same feature extraction backbone with different heads to simultaneously output detection and semantic segmentation results for lidar point clouds. Nonetheless, some previous works have failed to achieve an optimal balance between accuracy and speed. To address this issue, we propose a multi-task framework which leverages the Cartesian pillar and a multi-scale semantic segmentation head to overcome the shortcomings of existing works and improve the detection accuracy. We evaluate the proposed method using typical pillar-based and voxel-based detection models on the nuScenes dataset. The experimental results demonstrate that the proposed design achieves better performance especially on small objects, compared to single-task models. Moreover, the proposed network increases mAP and NDS by 3.1 % and 2.5 % respectively on the nuScenes test set, compared to the representative multi-task network. keywords: {Point cloud compression;Training;Three-dimensional displays;Laser radar;Semantic segmentation;Semantics;Object detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341613&isnumber=10341342

P. Wu, J. Mei, X. Zhao and Y. Hu, "Generalized Few-shot Semantic Segmentation for LiDAR Point Clouds," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7622-7628, doi: 10.1109/IROS55552.2023.10341978.Abstract: Semantic segmentation of LiDAR point clouds can provide assistance for precise perception in autonomous driving, but traditional segmentation methods face challenges such as unbalanced class distribution and insufficient labeling. Generalized few-shot learning has been researched on image data, but these methods are difficult to apply directly to LiDAR point clouds. To tackle these challenges, we propose a generalized few-shot semantic segmentation method based on LiDAR point cloud data, enabling us to predict base and novel classes simultaneously. To improve the performance with limited novel class samples, we integrate semantic vectors and leverage the intrinsic relationship between base and novel class vectors to facilitate learning. We conduct comprehensive comparisons with other methods on the SemanticKITTI and constantly surpass them with higher mIoU, demonstrating the effectiveness of our method. keywords: {Point cloud compression;Laser radar;Semantic segmentation;Semantics;Labeling;Intelligent robots;Faces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341978&isnumber=10341342

K. Blomqvist, L. Ott, J. J. Chung and R. Siegwart, "Baking in the Feature: Accelerating Volumetric Segmentation by Rendering Feature Maps," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7629-7634, doi: 10.1109/IROS55552.2023.10342071.Abstract: Methods have recently been proposed that densely segment 3D volumes into classes using only color images and expert supervision in the form of sparse semantically annotated pixels. While impressive, these methods still require a relatively large amount of supervision and segmenting an object can take several minutes in practice. Such systems typically only optimize the representation on the scene they are fitting, without leveraging prior information from previously seen images. In this paper, we propose to use features extracted with models pre-trained on large existing datasets to improve segmentation performance on novel scenes. We bake this feature representation into a Neural Radiance Field (NeRF) by volu-metrically rendering feature maps and supervising on features extracted from each input image. We show that by baking this representation into the NeRF, we make the subsequent classification task much easier. Our experiments show that our method achieves higher segmentation accuracy with fewer semantic annotations than existing methods over a wide range of scenes. keywords: {Image segmentation;Three-dimensional displays;Annotations;Shape;Semantics;Feature extraction;Rendering (computer graphics)},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342071&isnumber=10341342

X. Zhang and A. Boularias, "Optical Flow Boosts Unsupervised Localization and Segmentation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7635-7642, doi: 10.1109/IROS55552.2023.10342195.Abstract: Unsupervised localization and segmentation are long-standing robot vision challenges that describe the critical ability for an autonomous robot to learn to decompose images into individual objects without labeled data. These tasks are important because of the limited availability of dense image manual annotation and the promising vision of adapting to an evolving set of object categories in lifelong learning. Most recent methods focus on using visual appearance continuity as object cues by spatially clustering features obtained from self-supervised vision transformers (ViT). In this work, we leverage motion cues, inspired by the common fate principle that pixels that share similar movements tend to belong to the same object. We propose a new loss term formulation that uses optical flow in unlabeled videos to encourage self-supervised ViT features to become closer to each other if their corresponding spatial locations share similar movements, and vice versa. We use the proposed loss function to finetune vision transformers that were originally trained on static images. Our fine-tuning procedure outperforms state-of-the-art techniques for unsupervised semantic segmentation through linear probing, without the use of any labeled data. This procedure also demonstrates increased performance over original ViT networks across unsupervised object localization and semantic segmentation benchmarks. Our code is available at https://github.com/mlzxy/flowdino. keywords: {Location awareness;Optical losses;Visualization;Semantic segmentation;Robot vision systems;Object segmentation;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342195&isnumber=10341342

A. H. Gebrehiwot, D. Hurych, K. Zimmermann, P. Pérez and T. Svoboda, "T-UDA: Temporal Unsupervised Domain Adaptation in Sequential Point Clouds," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7643-7650, doi: 10.1109/IROS55552.2023.10341446.Abstract: Deep perception models have to reliably cope with an open-world setting of domain shifts induced by different geographic regions, sensor properties, mounting positions, and several other reasons. Since covering all domains with annotated data is technically intractable due to the endless possible variations, researchers focus on unsupervised domain adaptation (UDA) methods that adapt models trained on one (source) domain with annotations available to another (target) domain for which only unannotated data are available. Current predominant methods either leverage semi-supervised approaches, e.g., teacher-student setup, or exploit privileged data, such as other sensor modalities or temporal data consistency. We introduce a novel domain adaptation method that leverages the best of both approaches. Our approach combines input data's temporal and cross-sensor geometric consistency with the mean teacher method. Dubbed T-UDA for “temporal UDA”, such a combination yields massive performance gains for the task of 3D semantic segmentation of driving scenes. Experiments are conducted on Waymo Open Dataset, nuScenes, and SemanticKITTI, for two popular 3D point cloud architectures, Cylinder3D and MinkowskiNet. Our codes are publicly available on https://github.com/ctu-vras/T-UDA. keywords: {Point cloud compression;Measurement;Adaptation models;Three-dimensional displays;Semantic segmentation;Performance gain;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341446&isnumber=10341342

A. Z. Zhu et al., "Superpixel Transformers for Efficient Semantic Segmentation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7651-7658, doi: 10.1109/IROS55552.2023.10341519.Abstract: Semantic segmentation, which aims to classify every pixel in an image, is a key task in machine perception, with many applications across robotics and autonomous driving. Due to the high dimensionality of this task, most existing approaches use local operations, such as convolutions, to generate per-pixel features. However, these methods are typically unable to effectively leverage global context information due to the high computational costs of operating on a dense image. In this work, we propose a solution to this issue by leveraging the idea of superpixels, an over-segmentation of the image, and applying them with a modern transformer framework. In particular, our model learns to decompose the pixel space into a spatially low dimensional superpixel space via a series of local cross-attentions. We then apply multi-head self-attention to the superpixels to enrich the superpixel features with global context and then directly produce a class prediction for each superpixel. Finally, we directly project the superpixel class predictions back into the pixel space using the associations between the superpixels and the image pixel features. Reasoning in the superpixel space allows our method to be substantially more computationally efficient compared to convolution-based decoder methods. Yet, our method achieves state-of-the-art performance in semantic segmentation due to the rich superpixel features generated by the global self-attention mechanism. Our experiments on Cityscapes and ADE20K demonstrate that our method matches the state of the art in terms of accuracy, while outperforming in terms of model parameters and latency. keywords: {Semantic segmentation;Computational modeling;Network architecture;Transformers;Cognition;Computational efficiency;Decoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341519&isnumber=10341342

Z. Yuan, J. Lin and D. Zhang, "Hierarchical Semi-Supervised Learning Framework for Surgical Gesture Segmentation and Recognition Based on Multi-Modality Data," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7659-7666, doi: 10.1109/IROS55552.2023.10341673.Abstract: Segmenting and recognizing surgical operation trajectories into distinct, meaningful gestures is a critical preliminary step in surgical workflow analysis for robot-assisted surgery. This step is necessary for facilitating learning from demonstrations for autonomous robotic surgery, evaluating surgical skills, and so on. In this work, we develop a hierarchical semi-supervised learning framework for surgical gesture segmentation using multi-modality data (i.e. kinematics and vision data). More specifically, surgical tasks are initially segmented based on distance characteristics-based profiles and variance characteristics-based profiles constructed using kinematics data. Subsequently, a Transformer-based network with a pre-trained ‘ResNet-18’ backbone is used to extract visual features from the surgical operation videos. By combining the potential segmentation points obtained from both modalities, we can determine the final segmentation points. Furthermore, gesture recognition can be implemented based on supervised learning. The proposed approach has been evaluated using data from the publicly available JIGSAWS database, including Suturing, Needle Passing, and Knot Tying tasks. The results reveal an average F1 score of 0.623 for segmentation and an accuracy of 0.856 for recognition. For more details about this paper, please visit our website: https://sites.google.com/view/surseg/home. keywords: {Visualization;Surgery;Kinematics;Semisupervised learning;Feature extraction;Transformers;Needles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341673&isnumber=10341342

A. Agarwalla et al., "Lidar Panoptic Segmentation and Tracking without Bells and Whistles," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7667-7674, doi: 10.1109/IROS55552.2023.10341415.Abstract: State-of-the-art lidar panoptic segmentation (LPS) methods follow “bottom-up” segmentation-centric fashion wherein they build upon semantic segmentation networks by utilizing clustering to obtain object instances. In this paper, we re-think this approach and propose a surprisingly simple yet effective detection-centric network for both LPS and tracking. Our network is modular by design and optimized for all aspects of both the panoptic segmentation and tracking task. One of the core components of our network is the object instance detection branch, which we train using point-level (modal) annotations, as available in segmentation-centric datasets. In the absence of amodal (cuboid) annotations, we regress modal centroids and object extent using trajectory-level supervision that provides information about object size, which cannot be inferred from single scans due to occlusions and the sparse nature of the lidar data. We obtain fine-grained instance segments by learning to associate lidar points with detected centroids. We evaluate our method on several 3D/4D LPS benchmarks and observe that our model establishes a new state-of-the-art among open-sourced models, outperforming recent query-based models. keywords: {Laser radar;Annotations;Semantic segmentation;Benchmark testing;Spatiotemporal phenomena;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341415&isnumber=10341342

Z. Zhengl, Y. Chen, B. -S. Hua and S. -K. Yeung, "CompUDA: Compositional Unsupervised Domain Adaptation for Semantic Segmentation Under Adverse Conditions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7675-7681, doi: 10.1109/IROS55552.2023.10342102.Abstract: In autonomous driving, performing robust semantic segmentation under adverse weather conditions is a long-standing challenge. Imperfect camera observations under adverse conditions result in images with reduced visibility, which hinders label annotation and semantic scene understanding based on these images. A common solution is to adopt semantic segmentation models trained in a source domain with ground truth labels and perform unsupervised domain adaptation (UDA) from the source domain to an unlabeled target domain that has adverse conditions. Due to imperfect visual observations in the target domain, such adaptation needs special treatment to achieve good performance. In this paper, we propose a new compositional unsupervised domain adaptation (CompUDA) method that disentangles the domain gap based on multiple factors including style, visibility, and image quality. The domain gaps caused by these individual factors can then be addressed separately by introducing the intermediate domains. Specifically, 1) to address the style gap, we perform source-to-intermediate domain adaptation and generate pseudo-labels for self-training in the target domain; 2) to address the visibility gap, we perform a geometry-aligned normal-to-adverse image translation and introduce a synthetic domain; 3) finally, to address the image quality gap between the synthetic and target domain, we perform a synthetic-to-real adaptation based on the generated pseudo-labels. Our compositional unsupervised domain adaptation can be used in conjunction with a wide variety of semantic segmentation methods and result in significant performance improvement across datasets. The codes are available at https://github.com/zhengziqiang/CompUDA. keywords: {Image quality;Visualization;Codes;Image synthesis;Semantic segmentation;Semantics;Performance gain},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342102&isnumber=10341342

K. Garigapati, E. Blasch, J. Wei and H. Ling, "Transparent Object Tracking with Enhanced Fusion Module," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7696-7703, doi: 10.1109/IROS55552.2023.10341597.Abstract: Accurate tracking of transparent objects, such as glasses, plays a critical role in many robotic tasks such as robot-assisted living. Due to the adaptive and often reflective texture of such objects, traditional tracking algorithms that rely on general-purpose learned features suffer from reduced performance. Recent research has proposed to instill trans-parency awareness into existing general object trackers by fusing purpose-built features. However, with the existing fusion techniques, the addition of new features causes a change in the latent space making it impossible to incorporate transparency awareness on trackers with fixed latent spaces. For example, many of the current days' transformer-based trackers are fully pre-trained and are sensitive to any latent space perturbations. In this paper, we present a new feature fusion technique that integrates transparency information into a fixed feature space, enabling its use in a broader range of trackers. Our proposed fusion module, composed of a transformer encoder and an MLP module, leverages key query-based transformations to embed the transparency information into the tracking pipeline. We also present a new two-step training strategy for our fusion module to effectively merge transparency features. We propose a new tracker architecture that uses our fusion techniques to achieve superior results for transparent object tracking. Our proposed method achieves competitive results with state-of-the-art trackers on TOTB, which is the largest transparent object tracking benchmark recently released. Our results and the implementation of code will be made publicly available at https://github.com/kalyan0510/TOTEM. keywords: {Training;Perturbation methods;Pipelines;Object segmentation;Glass;Transformers;Object tracking},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341597&isnumber=10341342

J. Zhu et al., "Self-Supervised Event-Based Monocular Depth Estimation Using Cross-Modal Consistency," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7704-7710, doi: 10.1109/IROS55552.2023.10342434.Abstract: An event camera is a novel vision sensor that can capture per-pixel brightness changes and output a stream of asynchronous “events”. It has advantages over conventional cameras in those scenes with high-speed motions and challenging lighting conditions because of the high temporal resolution, high dynamic range, low bandwidth, low power consumption, and no motion blur. Therefore, several supervised monocular depth estimation from events is proposed to address scenes difficult for conventional cameras. However, depth annotation is costly and time-consuming. In this paper, to lower the annotation cost, we propose a self-supervised event-based monocular depth estimation framework named EMoDepth. EMoDepth constrains the training process using the cross-modal consistency from intensity frames that are aligned with events in the pixel coordinate. Moreover, in inference, only events are used for monocular depth prediction. Additionally, we design a multi-scale skip-connection architecture to effectively fuse features for depth estimation while maintaining high inference speed. Experiments on MVSEC and DSEC datasets demonstrate that our contributions are effective and that the accuracy can outperform existing supervised event-based and unsupervised frame-based methods. keywords: {Training;Power demand;Fuses;Annotations;Estimation;Lighting;Vision sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342434&isnumber=10341342

L. Crupi, E. Cereda, A. Giusti and D. Palossi, "Sim-to-Real Vision-Depth Fusion CNNs for Robust Pose Estimation Aboard Autonomous Nano-quadcopters," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7711-7717, doi: 10.1109/IROS55552.2023.10342162.Abstract: Nano-quadcopters are versatile platforms attracting the interest of both academia and industry. Their tiny form factor, i.e., ~ 10 cm diameter, makes them particularly useful in narrow scenarios and harmless in human proximity. However, these advantages come at the price of ultra-constrained onboard computational and sensorial resources for autonomous operations. This work addresses the task of estimating human pose aboard nano-drones by fusing depth and images in a novel CNN exclusively trained in simulation yet capable of robust predictions in the real world. We extend a commercial off-the-shelf (COTS) Crazyflie nano-drone - equipped with a 320x240 px camera and an ultra-low-power System-on-Chip - with a novel multi-zone (8 x 8) depth sensor. We design and compare different deep-learning models that fuse depth and image inputs. Our models are trained exclusively on simulated data for both inputs, and transfer well to the real world: field testing shows an improvement of 58% and 51 % of our depth+camera system w.r.t. a camera-only State-of-the-Art baseline on the horizontal and angular mean pose errors, respectively. Our prototype is based on COTS components, which facilitates reproducibility and adoption of this novel class of systems. keywords: {Fuses;Pose estimation;Prototypes;Training data;Cameras;Robot sensing systems;Reproducibility of results},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342162&isnumber=10341342

J. Mei, Y. Yang, M. Wang, T. Huang, X. Yang and Y. Liu, "SSC-RS: Elevate LiDAR Semantic Scene Completion with Representation Separation and BEV Fusion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341742.Abstract: Semantic scene completion (SSC) jointly predicts the semantics and geometry of the entire 3D scene, which plays an essential role in 3D scene understanding for autonomous driving systems. SSC has achieved rapid progress with the help of semantic context in segmentation. However, how to effectively exploit the relationships between the semantic context in semantic segmentation and geometric structure in scene completion remains under exploration. In this paper, we propose to solve outdoor SSC from the perspective of representation separation and BEV fusion. Specifically, we present the network, named SSC-RS, which uses separate branches with deep supervision to explicitly disentangle the learning procedure of the semantic and geometric representations. And a BEV fusion network equipped with the proposed Adaptive Representation Fusion (ARF) module is presented to aggregate the multi-scale features effectively and efficiently. Due to the low computational burden and powerful representation ability, our model has good generality while running in real-time. Extensive experiments on SemanticKITTI demonstrate our SSC-RS achieves state-of-the-art performance. Code is available at https://github.com/Jieqianyu/SSC-RS.git. keywords: {Geometry;Visualization;Three-dimensional displays;Adaptive systems;Laser radar;Fuses;Semantic segmentation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341742&isnumber=10341342

J. Mei, Y. Yang, M. Wang, X. Hou, L. Li and Y. Liu, "PANet: LiDAR Panoptic Segmentation with Sparse Instance Proposal and Aggregation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7726-7733, doi: 10.1109/IROS55552.2023.10342468.Abstract: Reliable LiDAR panoptic segmentation (LPS), including both semantic and instance segmentation, is vital for many robotic applications, such as autonomous driving. This work proposes a new LPS framework named PANet to eliminate the dependency on the offset branch and improve the performance on large objects, which are always over-segmented by clustering algorithms. Firstly, we propose a non-learning Sparse Instance Proposal (SIP) module with the “sampling-shifting-grouping” scheme to directly group thing points into instances from the raw point cloud efficiently. More specifically, balanced point sampling is introduced to generate sparse seed points with more uniform point distribution over the distance range. And a shift module, termed bubble shifting, is proposed to shrink the seed points to the clustered centers. Then we utilize the connected component label algorithm to generate instance proposals. Furthermore, an instance aggregation module is devised to integrate potentially fragmented instances, improving the performance of the SIP module on large objects. Extensive experiments show that PANet achieves state-of-the-art performance among published works on the SemanticKITII validation and nuScenes validation for the panoptic segmentation task. Code is available at https://github.com/Jieqianyu/PANet.git. keywords: {Point cloud compression;Training;Instance segmentation;Laser radar;Semantics;Clustering algorithms;Proposals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342468&isnumber=10341342

C. Lee, J. G. Frennert, L. Gan, M. Anderson and S. -J. Chung, "Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7734-7741, doi: 10.1109/IROS55552.2023.10342016.Abstract: We present a new method to adapt an RGB-trained water segmentation network to target-domain aerial thermal imagery using online self-supervision by leveraging texture and motion cues as supervisory signals. This new thermal capability enables current autonomous aerial robots operating in near-shore environments to perform tasks such as visual navigation, bathymetry, and flow tracking at night. Our method overcomes the problem of scarce and difficult-to-obtain near-shore thermal data that prevents the application of conventional supervised and unsupervised methods. In this work, we curate the first aerial thermal near-shore dataset, show that our approach outperforms fully-supervised segmentation models trained on limited target-domain thermal data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded computing platform. Code and datasets used in this work will be available at: https://github.com/connorlee77/uav-thermal-water-segmentation. keywords: {Training;Visualization;Target tracking;Navigation;Motion segmentation;Autonomous aerial vehicles;Bathymetry},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342016&isnumber=10341342

A. C. Stutts, D. Erricolo, T. Tulabandhula and A. R. Trivedi, "Lightweight, Uncertainty-Aware Conformalized Visual Odometry," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7742-7749, doi: 10.1109/IROS55552.2023.10341924.Abstract: Data-driven visual odometry (VO) is a critical subroutine for autonomous edge robotics, and recent progress in the field has produced highly accurate point predictions in complex environments. However, emerging autonomous edge robotics devices like insect-scale drones and surgical robots lack a computationally efficient framework to estimate VO's predictive uncertainties. Meanwhile, as edge robotics continue to proliferate into mission-critical application spaces, awareness of the model's predictive uncertainties has become crucial for risk-aware decision-making. This paper addresses this challenge by presenting a novel, lightweight, and statistically robust framework that leverages conformal inference (CI) to extract VO's uncertainty bands. Our approach represents the uncertainties using flexible, adaptable, and adjustable prediction intervals that, on average, guarantee the inclusion of the ground truth across all degrees of freedom (DOF) of pose estimation. We discuss the architectures of generative deep neural networks for estimating multivariate uncertainty bands along with point (mean) prediction. We also present techniques to improve the uncertainty estimation accuracy, such as leveraging Monte Carlo dropout (MC-dropout) for data augmentation. Finally, we propose a novel training loss function that combines interval scoring and calibration loss with traditional training metrics-mean-squared error and KL-divergence-to improve uncertainty-aware learning. Our simulation results demonstrate that the presented framework consistently captures true uncertainty in pose estimations across different datasets, estimation models, and applied noise types, indicating its wide applicability. keywords: {Training;Measurement;Uncertainty;Computational modeling;Pose estimation;Computer architecture;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341924&isnumber=10341342

W. Boettcher, L. Hoyer, O. Unal, K. Li and D. Dai, "LiDAR Meta Depth Completion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7750-7756, doi: 10.1109/IROS55552.2023.10341349.Abstract: Depth estimation is one of the essential tasks to be addressed when creating mobile autonomous systems. While monocular depth estimation methods have improved in recent times, depth completion provides more accurate and reliable depth maps by additionally using sparse depth information from other sensors such as LiDAR. However, current methods are specifically trained for a single LiDAR sensor. As the scanning pattern differs between sensors, every new sensor would require re- training a specialized depth completion model, which is computationally inefficient and not flexible. Therefore, we propose to dynamically adapt the depth completion model to the used sensor type enabling LiDAR adaptive depth completion. Specifically, we propose a meta depth completion network that uses data patterns derived from the data to learn a task network to alter weights of the main depth completion network to solve a given depth completion task effectively. The method demonstrates a strong capability to work on multiple LiDAR scanning patterns and can also generalize to scanning patterns that are unseen during training. While using a single model, our method yields significantly better results than a non-adaptive baseline trained on different LiDAR patterns. It outperforms LiDAR-specific expert models for very sparse cases. These advantages allow flexible deployment of a single depth completion model on different sensors, which could also prove valuable to process the input of nascent LiDAR technology with adaptive instead of fixed scanning patterns. The source code is available at github.com/wbkit/ResLAN keywords: {Training;Adaptation models;Laser radar;Computational modeling;Source coding;Estimation;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341349&isnumber=10341342

A. Hiranaka et al., "Primitive Skill-Based Robot Learning from Human Evaluative Feedback," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7817-7824, doi: 10.1109/IROS55552.2023.10341912.Abstract: Reinforcement learning (RL) algorithms face significant challenges when dealing with long-horizon robot manipulation tasks in real-world environments due to sample inefficiency and safety issues. To overcome these challenges, we propose a novel framework, SEED, which leverages two approaches: reinforcement learning from human feedback (RLHF) and primitive skill-based reinforcement learning. Both approaches are particularly effective in addressing sparse reward issues and the complexities involved in long-horizon tasks. By combining them, SEED reduces the human effort required in RLHF and increases safety in training robot manipulation with RL in real-world settings. Additionally, parameterized skills provide a clear view of the agent's high-level intentions, allowing humans to evaluate skill choices before they are executed. This feature makes the training process even safer and more efficient. To evaluate the performance of SEED, we conducted extensive experiments on five manipulation tasks with varying levels of complexity. Our results show that SEED significantly outperforms state-of-the-art RL algorithms in sample efficiency and safety. In addition, SEED also exhibits a substantial reduction of human effort compared to other RLHF methods. Further details and video results can be found at https://seediros23.github.io/. keywords: {Training;Costs;Reinforcement learning;Robot learning;Safety;Complexity theory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341912&isnumber=10341342

B. Ibrahim, M. H. Hussein, I. H. Elhajj and D. Asmar, "Autocomplete of 3D Motions for UAV Teleoperation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7825-7831, doi: 10.1109/IROS55552.2023.10342390.Abstract: Tele-operating aerial vehicles without any automated assistance is challenging due to various limitations, especially for inexperienced users. Autocomplete addresses this problem by automatically identifying and completing the user's intended motion. Such a framework uses machine learning to recognize and classify human inputs as one of a set of motion primitives, and then, if the human operator accepts, synthesizes the motion in order to complete the desired motion. This has been shown to improve the performance of the system and reduce operator workload. Previous Autocomplete systems focused on different 2D motions (line, arc, sine,..). However, since most UAVs tasks are in a 3D world, this paper introduces 3D Autocomplete for 3D motions. Moreover, the proposed framework presents just-in-time prediction of the 3D motions by proposing a change point detection technique, which allows the framework to autonomously identify when to conduct a prediction. Also, it deals with variable motion sizes. Real time simulation results show that the proposed framework is capable of predicting the user intentions after change point detection. keywords: {Deep learning;Three-dimensional displays;Simulation;Autonomous aerial vehicles;Real-time systems;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342390&isnumber=10341342

D. Lagamtzis, F. Schmidt, J. Seyler, T. Dang and S. Schober, "Exploiting Spatio-Temporal Human-Object Relations Using Graph Neural Networks for Human Action Recognition and 3D Motion Forecasting," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7832-7838, doi: 10.1109/IROS55552.2023.10342491.Abstract: Human action recognition and motion forecasting is becoming increasingly successful, in particular with utilizing graphs. We aim to transfer this success into the context of industrial Human-Robot Collaboration (HRC), where humans work closely with robots and interact with workpieces in defined workspaces. For this purpose, it is necessary to use all the available information extractable in such a workspace and represent it with a natural structure, such as graphs, that can be used for learning. Since humans are the center of HRC, it is mandatory to construct the graph in a human-centered way and use real-world 3D information as well as object labels to represent their environment. Therefore, we present a novel Graph Neural Network (GNN) architecture which combines, human action recognition and motion forecasting for industrial HRC environments. We evaluate our method with two different and publicly available human action datasets, including one that is a particularly realistic representation of the industrial HRC, and compare the results with baseline methods for classifying the current human action and predicting the human motion. Our experiments show that our combined GNN approach improves the accuracy of action recognition compared to previous work, and significantly on the CoAx dataset by up to 20%. Further, our motion forecasting approach performs better than existing baselines, predicting human trajectories with a Final Displacement Error (FDE) of less than 10cm for a prediction horizon of 1s. keywords: {Three-dimensional displays;Service robots;Collaboration;Graph neural networks;Trajectory;Human activity recognition;Data mining},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342491&isnumber=10341342

J. E. Domínguez-Vidal and A. Sanfeliu, "Improving Human-Robot Interaction Effectiveness in Human-Robot Collaborative Object Transportation Using Force Prediction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7839-7845, doi: 10.1109/IROS55552.2023.10342517.Abstract: In this work, we analyse the use of a prediction of the human's force in a Human-Robot collaborative object transportation task at a middle distance. We check that this force prediction can improve multiple parameters associated with effective Human-Robot Interaction (HRI) such as perception of the robot's contribution to the task, comfort or trust in the robot in a physical Human Robot Interaction (pHRI). We present a Deep Learning model that allows to predict the force that a human will exert in the next 1 $s$ using as inputs the force previously exerted by the human, the robot's velocity and environment information obtained from the robot's LiDAR. Its success rate is up to 92.3% in testset and up to 89.1 % in real experiments. We demonstrate that this force prediction, in addition to being able to be used directly to detect changes in the human's intention, can be processed to obtain an estimate of the human's desired trajectory. We have validated this approach with a user study involving 18 volunteers. keywords: {Deep learning;Force;Human-robot interaction;Collaboration;Transportation;Predictive models;Transformers;Physical Human-Robot Interaction;Object Transportation;Human-in-the-Loop;Force Prediction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342517&isnumber=10341342

D. Weber, V. Bolz, A. Zell and E. Kasneci, "Leveraging Saliency-Aware Gaze Heatmaps for Multiperspective Teaching of Unknown Objects," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7846-7853, doi: 10.1109/IROS55552.2023.10342312.Abstract: As robots become increasingly prevalent amidst diverse environments, their ability to adapt to novel scenarios and objects is essential. Advances in modern object detection have also paved the way for robots to identify interaction entities within their immediate vicinity. One drawback is that the robot's operational domain must be known at the time of training, which hinders the robot's ability to adapt to unexpected environments outside the preselected classes. However, when encountering such challenges a human can provide support to a robot by teaching it about the new, yet unknown objects on an ad hoc basis. In this work, we merge augmented reality and human gaze in the context of multimodal human-robot interaction to compose saliency-aware gaze heatmaps leveraged by a robot to learn emerging objects of interest. Our results show that our proposed method exceeds the capabilities of the current state of the art and outperforms it in terms of commonly used object detection metrics. keywords: {Heating systems;Measurement;Training;Human-robot interaction;Object detection;Object recognition;Usability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342312&isnumber=10341342

S. Patki, J. Arkin, N. Raicevic and T. M. Howard, "Language Guided Temporally Adaptive Perception for Efficient Natural Language Grounding in Cluttered Dynamic Worlds," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7854-7861, doi: 10.1109/IROS55552.2023.10341527.Abstract: As robots operate alongside humans in shared spaces, such as homes and offices, it is essential to have an effective mechanism for interacting with them. Natural language offers an intuitive interface for communicating with robots, but most of the recent approaches to grounded language understanding reason only in the context of an instantaneous state of the world. Though this allows for interpreting a variety of utterances in the current context of the world, these models fail to interpret utterances which require the knowledge of past dynamics of the world, thereby hindering effective human-robot collaboration in dynamic environments. Constructing a comprehensive model of the world that tracks the dynamics of all objects in the robot's workspace is computationally expensive and difficult to scale with increasingly complex environments. To address this challenge, we propose a learned model of language and perception that facilitates the construction of temporally compact models of dynamic worlds through closed-loop grounding and perception. Our experimental results on the task of grounding referring expressions demonstrate more accurate interpretation of robot instructions in cluttered and dynamic table-top environments without a significant increase in runtime as compared to an open-loop baseline. keywords: {Adaptation models;Runtime;Grounding;Computational modeling;Natural languages;Collaboration;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341527&isnumber=10341342

M. -A. Maheux, A. M. Panchea, P. Warren, D. Létourneau and F. Michaud, "T-Top, an Open Source Tabletop Robot with Advanced Onboard Audio, Vision and Deep Learning Capabilities," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7862-7869, doi: 10.1109/IROS55552.2023.10342252.Abstract: In recent years, studies on Socially Assistive Robots (SARs) examine how to improve the quality of life of people living with dementia and older adults (OAs) in general. However, most SARs have somewhat limited perception capabilities or interact using simple pre-programmed responses, providing limited or repetitive interaction modalities. Integrating more advanced perceptual capabilities with deep learning processing would help move beyond such limitations. This paper presents T-Top, a tabletop robot designed with advanced audio and vision processing using deep learning neural networks. T-Top is made available as an open source platform with the goal of providing an experimental SAR platform that can implement richer interaction modalities with OAs. keywords: {Deep learning;Neural networks;Human-robot interaction;Assistive robots;Older adults;Intelligent robots;Dementia},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342252&isnumber=10341342

P. Franceschi, F. Bertini, F. Braghin, L. Roveda, N. Pedrocchi and M. Beschi, "Learning Human Motion Intention for pHRI Assistive Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7870-7877, doi: 10.1109/IROS55552.2023.10342014.Abstract: This work addresses human intention identification during physical Human-Robot Interaction (pHRI) tasks to include this information in an assistive controller. To this purpose, human intention is defined as the desired trajectory that the human wants to follow over a finite rolling prediction horizon so that the robot can assist in pursuing it. This work investigates a Recurrent Neural Network (RNN), specifically, Long-Short Term Memory (LSTM) cascaded with a Fully Connected layer. In particular, we propose an iterative training procedure to adapt the model. Such an iterative procedure is powerful in reducing the prediction error. Still, it has the drawback that it is time-consuming and does not generalize to different users or different co-manipulated objects. To overcome this issue, Transfer Learning (TL) adapts the pre-trained model to new trajectories, users, and co-manipulated objects by freezing the LSTM layer and fine-tuning the last FC layer, which makes the procedure faster. Experiments show that the iterative procedure adapts the model and reduces prediction error. Experiments also show that TL adapts to different users and to the co-manipulation of a large object. Finally, to check the utility of adopting the proposed method, we compare the proposed controller enhanced by the intention prediction with the other two standard controllers of pHRI. keywords: {Training;Adaptation models;Recurrent neural networks;Computational modeling;Transfer learning;Predictive models;Trajectory;physical human-robot interaction;human intention prediction;LSTM;transfer learning;differential game theory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342014&isnumber=10341342

D. Marta, S. Holk, C. Pek, J. Tumova and I. Leite, "VARIQuery: VAE Segment-Based Active Learning for Query Selection in Preference-Based Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7878-7885, doi: 10.1109/IROS55552.2023.10341795.Abstract: Human-in-the-loop reinforcement learning (RL) methods actively integrate human knowledge to create reward functions for various robotic tasks. Learning from preferences shows promise as alleviates the requirement of demonstrations by querying humans on state-action sequences. However, the limited granularity of sequence-based approaches complicates temporal credit assignment. The amount of human querying is contingent on query quality, as redundant queries result in excessive human involvement. This paper addresses the often-overlooked aspect of query selection, which is closely related to active learning (AL). We propose a novel query selection approach that leverages variational autoencoder (VAE) representations of state sequences. In this manner, we formulate queries that are diverse in nature while simultaneously taking into account reward model estimations. We compare our approach to the current state-of-the-art query selection methods in preference-based RL, and find ours to be either on-par or more sample efficient through extensive benchmarking on simulated environments relevant to robotics. Lastly, we conduct an online study to verify the effectiveness of our query selection approach with real human feedback and examine several metrics related to human effort. keywords: {Measurement;Visualization;Estimation;Clustering algorithms;Reinforcement learning;Market research;Human in the loop},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341795&isnumber=10341342

Y. Wen, Z. Tang, Y. Pang, B. Ding and M. Liu, "Interactive Spatiotemporal Token Attention Network for Skeleton-Based General Interactive Action Recognition," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7886-7892, doi: 10.1109/IROS55552.2023.10342472.Abstract: Recognizing interactive action plays an important role in human-robot interaction and collaboration. Previous methods use late fusion and co-attention mechanism to capture interactive relations, which have limited learning capability or inefficiency to adapt to more interacting entities. With assumption that priors of each entity are already known, they also lack evaluations on a more general setting addressing the diversity of subjects. To address these problems, we propose an Interactive Spatiotemporal Token Attention Network (ISTA-Net), which simultaneously model spatial, temporal, and interactive relations. Specifically, our network contains a tokenizer to partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to represent motions of multiple diverse entities. By extending the entity dimension, ISTs provide better interactive representations. To jointly learn along three dimensions in ISTs, multi-head self-attention blocks integrated with 3D convolutions are designed to capture inter-token correlations. When modeling correlations, a strict entity ordering is usually irrelevant for recognizing interactive actions. To this end, Entity Rearrangement is proposed to eliminate the orderliness in ISTs for interchangeable entities. Extensive experiments on four datasets verify the effectiveness of ISTA-Net by outperforming state-of-the-art methods. Our code is publicly available at https://github.com/Necolizer/ISTA-Net. keywords: {Knowledge engineering;Convolutional codes;Correlation;Three-dimensional displays;Human-robot interaction;Collaboration;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342472&isnumber=10341342

Y. Hayamizu, Z. Yu and S. Zhang, "Learning Joint Policies for Human-Robot Dialog and Co-Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7893-7898, doi: 10.1109/IROS55552.2023.10341663.Abstract: Service robots need language capabilities for communicating with people, and navigation skills for beyond-proximity interaction in the real world. When the robot explores the real world with people side by side, there is the compound problem of human-robot dialog and co-navigation. The human-robot team uses dialog to decide where to go, and their shared spatial awareness affects the dialog state. In this paper, we develop a framework that learns a joint policy for human-robot dialog and co-navigation toward efficiently and accurately completing tour guide and information delivery tasks. We show that our approach outperforms baselines from the literature in task completion rate and execution time, and demonstrate our approach in the real world. keywords: {Service robots;Navigation;Compounds;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341663&isnumber=10341342

R. Wang, D. Zhao and B. -C. Min, "Initial Task Allocation for Multi-Human Multi-Robot Teams with Attention-Based Deep Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7915-7922, doi: 10.1109/IROS55552.2023.10341410.Abstract: Multi-human multi-robot teams have great potential for complex and large-scale tasks through the collaboration of humans and robots with diverse capabilities and expertise. To efficiently operate such highly heterogeneous teams and maximize team performance timely, sophisticated initial task allocation strategies that consider individual differences across team members and tasks are required. While existing works have shown promising results in reallocating tasks based on agent state and performance, the neglect of the inherent heterogeneity of the team hinders their effectiveness in realistic scenarios. In this paper, we present a novel formulation of the initial task allocation problem in multi-human multi-robot teams as a contextual multi-attribute decision-make process and propose an attention-based deep reinforcement learning approach. We introduce a cross-attribute attention module to encode the latent and complex dependencies of multiple attributes in the state representation. We conduct a case study in a massive threat surveillance scenario and demonstrate the strengths of our model. keywords: {Deep learning;Representation learning;Adaptation models;Surveillance;Decision making;Collaboration;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341410&isnumber=10341342

J. Xia et al., "Human-Robot Collaboration for Unknown Flexible Surface Exploration and Treatment Based on Mesh Iterative Learning Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7923-7930, doi: 10.1109/IROS55552.2023.10341612.Abstract: Contact tooling operations like sanding and polishing have been high in demand for robotics and automation, as manual operations are labour-intensive with inconsistent quality. However, automating these operations remains a challenge since they are highly dependent on prior knowledge about the geometry of the workpiece. While several methods have been developed in existing research to automate the geometry learning process and adjust the contact force, human supervision is heavily required in the calibration of workpieces and the path planning of robot motion in such methods. Furthermore, the stiffness identification of the workpiece is not considered in most of these methods. This paper presents a human-robot collaboration (HRC) framework, which is able to perform surface exploration on an unknown object combining the operator's flexibility with the control precision of the robot. The operator moves the robot along the surface of the target object, and the robot recognizes the surface geometry and surface stiffness while exerting a desired contact force through control. For this purpose, a mesh iterative learning control (MILC) is developed to learn the surface stiffness, plan the exploration path, and adjust contact force through repetitive online correction based on HRC. The proof of learning convergence and the results of the simulation and experiments performed using a 7-DOF Sawyer robot demonstrate the validity of the proposed controller. keywords: {Geometry;Uncertainty;Target recognition;Force;Collaboration;Stability analysis;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341612&isnumber=10341342

S. Sonawani, Y. Zhou and H. B. Amor, "Projecting Robot Intentions Through Visual Cues: Static vs. Dynamic Signaling," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7931-7938, doi: 10.1109/IROS55552.2023.10342222.Abstract: Augmented and mixed-reality techniques harbor a great potential for improving human-robot collaboration. Visual signals and cues may be projected to a human partner in order to explicitly communicate robot intentions and goals. However, it is unclear what type of signals support such a process and whether signals can be combined without adding additional cognitive stress to the partner. This paper focuses on identifying the effective types of visual signals and quantify their impact through empirical evaluations. In particular, the study compares static and dynamic visual signals within a collaborative object sorting task and assesses their ability to shape human behavior. Furthermore, an information-theoretic analysis is performed to numerically quantify the degree of information transfer between visual signals and human behavior. The results of a human subject experiment show that there are significant advantages to combining multiple visual signals within a single task, i.e., increased task efficiency and reduced cognitive load. keywords: {Visualization;Shape;Collaboration;Virtual reality;Behavioral sciences;Task analysis;Stress},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342222&isnumber=10341342

R. M. Mayoral, A. Helmi, S. -T. Warren, S. W. Logan and N. T. Fitter, "Robottheory Fitness: GoBot's Engagement Edge for Spurring Physical Activity in Young Children," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7939-7944, doi: 10.1109/IROS55552.2023.10341442.Abstract: Children around the world are growing more sedentary over time, which leads to considerable accompanying wellness challenges. Pilot results from our research group have shown that robots may offer something different or better than other developmentally appropriate toys when it comes to motivating physical activity. However, the foundations of this work involved larger-group interactions in which it was difficult to tease apart potential causes of motion, or one-time sessions during which the impact of the robot may have been due to novelty. Accordingly, the work in this paper covers more controlled interactions focused on one robot and one child participant, in addition to considering interactions over longitudinal observation. We discuss the results of a deployment during which $N=8$ participants interacted with our custom GoBot robot over two months of weekly sessions. Within each session, the child users experienced a teleoperated robot mode, a semi-autonomous robot mode, and a control condition during which the robot was present but inactive. Results showed that children tended to be more active when the robot was active and the teleoperated mode did not yield significantly different results than the semi-autonomous mode. These insights can guide future application of assistive robots in child motor interventions, in addition to informing how these robots can be equipped to assist busy human clinicians. keywords: {Toy manufacturing industry;Assistive robots;Robots;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341442&isnumber=10341342

A. Boateng, W. Zhang and Y. Zhang, "Implicit Projection: Improving Team Situation Awareness for Tacit Human-Robot Interaction via Virtual Shadows," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7945-7952, doi: 10.1109/IROS55552.2023.10341641.Abstract: Fluent teaming is characterized by tacit interaction without explicit communication. Such interaction requires team situation awareness (TSA) to facilitate. However, existing approaches often rely on explicit communication (such as visual projection) to support TSA, resulting in a paradox. In this paper, we consider implicit projection (IP) to improve TSA for tacit human-robot interaction. IP minimizes interruption and can thus reduce the cognitive demand to maintain TSA in teaming. We introduce a novel process for achieving IP via virtual shadows (referred to as IPS). We compare our method with two baselines that use explicit projection to maintain TSA. Results via human factors studies demonstrate that IPS supports better TSA and significantly improves unsolicited human responsiveness to robots, a key feature of fluent teaming. Participants acknowledged robots implementing IPS more favorable as a teammate. Simultaneously, our results also demonstrate that IPS is comparable to, and sometimes better than, the best-performing baselines on information accuracy. keywords: {Visualization;Human-robot interaction;Human factors;IP networks;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341641&isnumber=10341342

A. Suárez-Hernández, A. Andriella, C. Torras and G. Alenyà, "User Interactions and Negative Examples to Improve the Learning of Semantic Rules in a Cognitive Exercise Scenario," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7953-7960, doi: 10.1109/IROS55552.2023.10341942.Abstract: Enabling a robot to perform new tasks is a complex endeavor, usually beyond the reach of non-technical users. For this reason, research efforts that aim at empowering end-users to teach robots new abilities using intuitive modes of interaction are valuable. In this article, we present INtuitive PROgramming 2 (INPRO2), a learning framework that allows inferring planning actions from demonstrations given by a human teacher. INPRO2 operates in an assistive scenario, in which the robot may learn from a healthcare professional (a therapist or caregiver) new cognitive exercises that can be later administered to patients with cognitive impairment. INPRO2 features significant improvements over previous work, namely: (1) exploitation of negative examples; (2) proactive interaction with the teacher to ask questions about the legality of certain movements; and (3) learning goals in addition to legal actions. Through simulations, we show the performance of different proactive strategies for gathering negative examples. Real-world experiments with human teachers and a TIAGo robot are also presented to qualitatively illustrate INPRO2. keywords: {Industries;Technological innovation;Law;Semantics;Europe;Medical services;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341942&isnumber=10341342

B. Zhang and H. Soh, "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7961-7968, doi: 10.1109/IROS55552.2023.10341488.Abstract: Human models play a crucial role in human-robot interaction (HRI), enabling robots to consider the impact of their actions on people and plan their behavior accordingly. However, crafting good human models is challenging; capturing context-dependent human behavior requires significant prior knowledge and/or large amounts of interaction data, both of which are difficult to obtain. In this work, we explore the potential of large language models (LLMs) — which have consumed vast amounts of human-generated text data — to act as zero-shot human models for HRI. Our experiments on three social datasets yield promising results; the LLMs are able to achieve performance comparable to purpose-built models. That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our findings, we demonstrate how LLM-based human models can be integrated into a social robot's planning process and applied in HRI scenarios focused on the important element of trust. Specifically, we present one case study on a simulated trust-based table-clearing task and replicate past results that relied on custom models. Next, we conduct a new robot utensil-passing experiment ($n=65$) where preliminary results show that planning with an LLM-based human model can achieve gains over a basic myopic plan. In summary, our results show that LLMs offer a promising (but incomplete) approach to human modeling for HRI. keywords: {Sensitivity;Human-robot interaction;Robot sensing systems;Data models;Cognition;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341488&isnumber=10341342

J. Peng, Z. Liao, H. Yao, Z. Su, Y. Zeng and H. Dai, "MPC-Based Human-Accompanying Control Strategy for Improving the Motion Coordination Between the Target Person and the Robot," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7969-7975, doi: 10.1109/IROS55552.2023.10342246.Abstract: Social robots have gained widespread attention for their potential to assist people in diverse domains, such as living assistance and logistics transportation. Human-accompanying, i.e., walking side-by-side with a person, is an expected and essential capability for social robots. However, due to the complexity of motion coordination between the target person and the mobile robot, the accompanying action is still unstable. In this study, we propose a human-accompanying control strategy to improve the motion coordination for better practicability of the human-accompanying robot. Our approach allows the robot to adapt to the motion variations of the target person and avoid obstacles while accompanying them. First, a human-robot interaction model based on the separation-bearing-orientation scheme is developed to ascertain the relative position and orientation between the robot and the target person. Then, a human-accompanying controller based on behavioral dynamics and model predictive control (MPC) is designed to avoid obstacles and simultaneously track the direction and velocity of the target person. Experimental results indicate that the proposed method can effectively achieve side-by-side accompanying by simultaneously controlling the relative position, direction, and velocity between the target person and robot. keywords: {Adaptation models;Target tracking;Robot kinematics;Social robots;Transportation;Behavioral sciences;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342246&isnumber=10341342

I. Idrees et al., "Improved Inference of Human Intent by Combining Plan Recognition and Language Feedback," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7976-7983, doi: 10.1109/IROS55552.2023.10342380.Abstract: Conversational assistive robots can aid people, especially those with cognitive impairments, to accomplish various tasks such as cooking meals, performing exercises, or operating machines. However, to interact with people effectively, robots must recognize human plans and goals from noisy observations of human actions, even when the user acts sub-optimally. Previous works on Plan and Goal Recognition (PGR) as planning have used hierarchical task networks (HTN) to model the actor/human. However, these techniques are insufficient as they do not have user engagement via natural modes of interaction such as language. Moreover, they have no mechanisms to let users, especially those with cognitive impairments, know of a deviation from their original plan or about any sub-optimal actions taken towards their goal. We propose a novel framework for plan and goal recognition in partially observable domains—Dialogue for Goal Recognition (D4GR) enabling a robot to rectify its belief in human progress by asking clarification questions about noisy sensor data and sub-optimal human actions. We evaluate the performance of D4GR over two simulated domains—kitchen and blocks domain. With language feedback and the world state information in a hierarchical task model, we show that D4GR framework for the highest sensor noise performs 1% better than HTN in goal accuracy in both domains. For plan accuracy, D4GR outperforms by 4% in the kitchen domain and 2% in the blocks domain in comparison to HTN. The ALWAYS-ASK oracle outperforms our policy by 3% in goal recognition and 7% in plan recognition. D4GR does so by asking 68% fewer questions than an oracle baseline. We also demonstrate a real-world robot scenario in the kitchen domain, validating the improved plan and goal recognition of D4GR in a realistic setting. keywords: {Uncertainty;Target tracking;Sociology;Social robots;Robot sensing systems;Planning;Noise measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342380&isnumber=10341342

C. Sun, A. G. Cohn and M. Leonetti, "Online Human Capability Estimation Through Reinforcement Learning and Interaction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7984-7991, doi: 10.1109/IROS55552.2023.10341868.Abstract: Service robots are expected to assist users in a constantly growing range of environments and tasks. People may be unique in many ways, and online adaptation of robots is central to personalized assistance. We focus on collaborative tasks in which the human collaborator may not be fully ablebodied, with the aim for the robot to automatically determine the best level of support. We propose a methodology for online adaptation based on Reinforcement Learning and Bayesian inference. As the Reinforcement Learning process continuously adjusts the robot's behavior, the actions that become part of the improved policy are used by the Bayesian inference module as local evidence of human capability, which can be generalized across the state space. The estimated capabilities are then used as pre-conditions to collaborative actions, so that the robot can quickly disable actions that the person seems unable to perform. We demonstrate and validate our approach on two simulated tasks and one real-world collaborative task across a range of motion and sensing capabilities. keywords: {Q-learning;Service robots;Estimation;Collaboration;Robot sensing systems;Bayes methods;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341868&isnumber=10341342

S. T. Shah Bukhari, B. A. Anima, D. Feil-Seifer and W. M. Qazi, "Cognitive Approach to Hierarchical Task Selection for Human-Robot Interaction in Dynamic Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7992-7998, doi: 10.1109/IROS55552.2023.10341768.Abstract: In an efficient and flexible human-robot collaborative work environment, a robot team member must be able to recognize both explicit requests and implied actions from human users. Identifying “what to do” in such cases requires an agent to have the ability to construct associations between objects, their actions, and the effect of actions on the environment. In this regard, semantic memory is being introduced to understand the explicit cues and their relationships with available objects and required skills to make “tea” and “sandwich”. We have extended our previous hierarchical robot control architecture to add the capability to execute the most appropriate task based on both feedback from the user and the environmental context. To validate this system, two types of skills were implemented in the hierarchical task tree: 1) Tea making skills and 2) Sandwich making skills. During the conversation between the robot and the human, the robot was able to determine the hidden context using ontology and began to act accordingly. For instance, if the person says “I am thirsty” or “It is cold outside” the robot will start to perform the tea-making skill. In contrast, if the person says, “I am hungry” or “I need something to eat”, the robot will make the sandwich. A humanoid robot Baxter was used for this experiment. We tested three scenarios with objects at different positions on the table for each skill. We observed that in all cases, the robot used only objects that were relevant to the skill. keywords: {Semantics;Robot control;Memory management;Humanoid robots;Oral communication;Ontologies;Object recognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341768&isnumber=10341342

Y. Guo, X. J. Yang and C. Shi, "Reward Shaping for Building Trustworthy Robots in Sequential Human-Robot Interaction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 7999-8005, doi: 10.1109/IROS55552.2023.10341904.Abstract: Trust-aware human-robot interaction (HRI) has received increasing research attention, as trust has been shown to be a crucial factor for effective HRI. Research in trust-aware HRI discovered a dilemma - maximizing task rewards often leads to decreased human trust, while maximizing human trust would compromise task performance. In this work, we address this dilemma by formulating the HRI process as a two-player Markov game and utilizing the reward-shaping technique to improve human trust while limiting performance loss. Specifically, we show that when the shaping reward is potential-based, the performance loss can be bounded by the potential functions evaluated at the final states of the Markov game. We apply the proposed framework to the experience-based trust model, resulting in a linear program that can be efficiently solved and deployed in real-world applications. We evaluate the proposed framework in a simulation scenario where a human-robot team performs a search-and-rescue mission. The results demonstrate that the proposed framework successfully modifies the robot's optimal policy, enabling it to increase human trust at a minimal task performance cost. keywords: {Sufficient conditions;Costs;Limiting;Buildings;Human-robot interaction;Games;Markov processes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341904&isnumber=10341342

K. Chen, J. Y. Lim, K. Kuan and H. Soh, "Latent Emission-Augmented Perspective-Taking (LEAPT) for Human-Robot Interaction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8006-8013, doi: 10.1109/IROS55552.2023.10342126.Abstract: Perspective-taking is the ability to perceive or understand a situation or concept from another individual's point of view, and is crucial in daily human interactions. Enabling robots to perform perspective-taking remains an unsolved problem; existing approaches that use deterministic or handcrafted methods are unable to accurately account for uncertainty in partially-observable settings. This work proposes to address this limitation via a deep world model that enables a robot to perform both perception and conceptual perspective taking, i.e., the robot is able to infer what a human sees and believes. The key innovation is a decomposed multi-modal latent state space model able to generate and augment fictitious observations/emissions. Optimizing the ELBO that arises from this probabilistic graphical model enables the learning of uncertainty in latent space, which facilitates uncertainty estimation from high-dimensional observations. We tasked our model to predict human observations and beliefs on three partially-observable HRI tasks. Experiments show that our method significantly outperforms existing baselines and is able to infer visual observations available to other agent and their internal beliefs. keywords: {Visualization;Technological innovation;Uncertainty;Human-robot interaction;Training data;State-space methods;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342126&isnumber=10341342

A. Schreiber, T. Ji, D. L. McPherson and K. Driggs-Campbell, "An Attentional Recurrent Neural Network for Occlusion-Aware Proactive Anomaly Detection in Field Robot Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8038-8045, doi: 10.1109/IROS55552.2023.10341852.Abstract: The use of mobile robots in unstructured environments like the agricultural field is becoming increasingly common. The ability for such field robots to proactively identify and avoid failures is thus crucial for ensuring efficiency and avoiding damage. However, the cluttered field environment introduces various sources of noise (such as sensor occlusions) that make proactive anomaly detection difficult. Existing approaches can show poor performance in sensor occlusion scenarios as they typically do not explicitly model occlusions and only leverage current sensory inputs. In this work, we present an attention-based recurrent neural network architecture for proactive anomaly detection that fuses current sensory inputs and planned control actions with a latent representation of prior robot state. We enhance our model with an explicitly-learned model of sensor occlusion that is used to modulate the use of our latent representation of prior robot state. Our method shows improved anomaly detection performance and enables mobile field robots to display increased resilience to predicting false positives regarding navigation failure during periods of sensor occlusion, particularly in cases where all sensors are briefly occluded. Our code is available at: https://github.com/andreschreiber/roar. keywords: {Recurrent neural networks;Navigation;Fuses;Supervised learning;Robot sensing systems;Robustness;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341852&isnumber=10341342

B. Xia et al., "Collaborative Trolley Transportation System with Autonomous Nonholonomic Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8046-8053, doi: 10.1109/IROS55552.2023.10341508.Abstract: Cooperative object transportation using multiple robots has been intensively studied in the control and robotics literature, but most approaches are either only applicable to omnidirectional robots or lack a complete navigation and decision-making framework that operates in real time. This paper presents an autonomous nonholonomic multi-robot system and an end-to-end hierarchical autonomy framework for collaborative luggage trolley transportation. This framework finds kinematic-feasible paths, computes online motion plans, and provides feedback that enables the multi-robot system to handle long lines of luggage trolleys and navigate obstacles and pedestrians while dealing with multiple inherently complex and coupled constraints. We demonstrate the designed collaborative trolley transportation system through practical transportation tasks, and the experiment results reveal their effectiveness and reliability in complex and dynamic environments. (Video11Video demonstration: https://youtu.be/efnPERm0Rco.) keywords: {Pedestrians;Navigation;Decision making;Transportation;Collaboration;Reliability engineering;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341508&isnumber=10341342

S. Wang, Y. Pan, Z. Pu, B. Liu and J. Yi, "Deconfounded Opponent Intention Inference for Football Multi-Player Policy Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8054-8061, doi: 10.1109/IROS55552.2023.10341469.Abstract: Due to the high complexity of a football match, the opponents' strategies are variable and unknown. Thus predicting the opponents' future intentions accurately based on current situation is crucial for football players' decision-making. To better anticipate the opponents and learn more effective strategies, a deconfounded opponent intention inference (DOII) method for football multi-player policy learning is proposed in this paper. Specifically, opponents' intentions are inferred by an opponent intention supervising module. Furthermore, for some confounders which affect the causal relationship among the players and the opponents, a decon-founded trajectory graph module is designed to mitigate the influence of these confounders and increase the accuracy of the inferences about opponents' intentions. Besides, an opponent-based incentive module is designed to improve the players' sensitivity to the opponents' intentions and further to train reasonable players' strategies. Representative results indicate that DOII can effectively improve the performance of players' strategies in the Google Research Football environment, which validates the superiority of the proposed method. keywords: {Sensitivity;Simulation;Decision making;Trajectory;Internet;Complexity theory;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341469&isnumber=10341342

I. Ilinkin, D. Song and Y. J. Kim, "Stroke-Based Rendering and Planning for Robotic Performance of Artistic Drawing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8062-8068, doi: 10.1109/IROS55552.2023.10341808.Abstract: We present a new robotic drawing system based on stroke-based rendering (SBR). Our motivation is the artistic quality of the whole performance. Not only should the generated strokes in the final drawing resemble the input image, but the stroke sequence should also exhibit a human artist's planning process. Thus, when a robot executes the drawing task, both the drawing results and the way the robot executes would look artistic. Our SBR system is based on image segmentation and depth estimation. It generates the drawing strokes in an order that allows for the intended shape to be perceived quickly and for its detailed features to be filled in and emerge gradually when observed by the human. This ordering represents a stroke plan that the drawing robot should follow to create an artistic rendering of images. We experimentally demonstrate that our SBR-based drawing makes visually pleasing artistic images, and our robotic system can replicate the result with proper sequences of stroke drawing. keywords: {Image segmentation;Shape;Estimation;Rendering (computer graphics);Planning;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341808&isnumber=10341342

Y. Kwon et al., "Heterogeneous Robot-Assisted Services in Isolation Wards: A System Development and Usability Study," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8069-8076, doi: 10.1109/IROS55552.2023.10341857.Abstract: Isolation wards operate in quarantine rooms to prevent cross-contamination caused by infectious diseases. Behind the benefits, medical personnel can have the infection risk from patients and the heavy workload due to the isolation. This work proposes a robot-assisted system to alleviate these problems in isolation wards. We conducted a survey about the medical staff's difficulties and envisioning robots. Using the investigation result, we devised three valuable services using two kinds of heterogeneous robots: telemedicine, emergency alert, and delivery services by care robots and delivery robots. Our system also provides user-interactive components such as a dashboard for medical staff and a patient app for inpatients. To manage the services efficiently, we suggest the robotic system based on a central control server and a hierarchical management architecture. Through a user study, we reviewed the usability of the developed system and its future directions. keywords: {Surveys;Infectious diseases;Telemedicine;Personnel;Servers;Usability;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341857&isnumber=10341342

N. Stathoulopoulos, A. Koval and G. Nikolakopoulos, "Irregular Change Detection in Sparse Bi-Temporal Point Clouds Using Learned Place Recognition Descriptors and Point-to-Voxel Comparison," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8077-8083, doi: 10.1109/IROS55552.2023.10342248.Abstract: Change detection and irregular object extraction in 3D point clouds is a challenging task that is of high importance not only for autonomous navigation but also for updating existing digital twin models of various industrial environments. This article proposes an innovative approach for change detection in 3D point clouds using deep learned place recognition descriptors and irregular object extraction based on voxel-to-point comparison. The proposed method first aligns the bi-temporal point clouds using a map-merging algorithm in order to establish a common coordinate frame. Then, it utilizes deep learning techniques to extract robust and discriminative features from the 3D point cloud scans, which are used to detect changes between consecutive point cloud frames and therefore find the changed areas. Finally, the altered areas are sampled and compared between the two time instances to extract any obstructions that caused the area to change. The proposed method was successfully evaluated in real-world field experiments, where it was able to detect different types of changes in 3D point clouds, such as object or muck-pile addition and displacement, showcasing the effectiveness of the approach. The results of this study demonstrate important implications for various applications, including safety and security monitoring in construction sites, mapping and exploration and suggests potential future research directions in this field. keywords: {Point cloud compression;Deep learning;Solid modeling;Three-dimensional displays;Feature extraction;Real-time systems;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342248&isnumber=10341342

H. Sun et al., "Magnetically Controlled Cell Robots with Immune-Enhancing Potential," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8084-8089, doi: 10.1109/IROS55552.2023.10341753.Abstract: Magnetic microrobots exhibit enormous potential in targeted drug delivery owing to the remote wireless manipulation and minimum invasion for medical treatment. High degree of freedom offers the magnetic propelled robots extraordinary application prospect since they can be controlled precisely when different magnetic fields sources working cooperatively. However, the biocompatibility of microrobots have attracted sustained and general concern. Therefore, it is highly necessary to develop a promising carrier with high biocompatibility and investigate the mechanism of drug loading-release triggered by special microenvironment in the targeted region. In this paper, we proposed a magnetically controlled cell robots (MCRs) based on macrophages propelled by a rotating magnetic field. The innovative MCRs exhibit good biocompatibility and low toxicity by optimizing the concentration of polylysine-coated Fe nanoparticles (PLL@FeNPs) to 40 µg/mL. These MCRs loaded with murine interleukin-12 (IL-12), murine chemokine (C-C motif) ligand 5 (CCL-5), and murine C-X-C motif chemokine ligand 10 (CXCL-10) which can stimulate T cell differentiation and recruitment of monocytes, respectively. The macrophages showed an obvious M1-polarization tendency of macrophages to phagocytose intracellular pathogens and resist the growth of tumor cells. Under the control of a magnetic propelling system composed of 3 pairs of Helmholtz coil, the cell robot can be propelled wirelessly and moved along a predefined path with high accuracy. Moreover, the MCRs could approach to cancer cells and stop at places of interest in vitro. In conclusion, we have accomplished the preliminary construction of a targeted drug delivery system which displays great immune-enhancing potential for targeted drug delivery. keywords: {Drugs;Wireless communication;Targeted drug delivery;Toxicology;Propulsion;Magnetic fields;In vitro},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341753&isnumber=10341342

Y. Huang et al., "Tightly-Coupled Visual-DVL Fusion For Accurate Localization of Underwater Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8090-8095, doi: 10.1109/IROS55552.2023.10342197.Abstract: This paper proposes a tightly-coupled visual-Doppler-Velocity-Log (visual-DVL) fusion method for underwater robot localization through integrating the velocity measurements from a DVL into a visual odometry (VO). Considering that employing the DVL measurements in dead-reckoning systems easily leads to error accumulation and suboptimal results in previous works, we directly integrate them into the visual tracking process. Specifically, the velocity measurements are utilized to improve the initial estimation of camera pose during visual tracking, aiming to provide a better initial value for pose optimization. Thereafter, these velocity measurements are also directly employed to constrain the position change of the camera between two adjacent frames by constructing a novel DVL error term, which is optimized jointly with the visual constrains to obtain a more accurate camera pose. Various experiments are carried out in the datasets collected from several scenarios of the underwater simulation environment HoloOcean, and the results illustrate that the proposed fusion method can effectively improve the localization accuracy for underwater robots by about 20% compared to pure visual odometry. The proposed method provides valuable guidance for the accurate localization of underwater robots. keywords: {Location awareness;Autonomous underwater vehicles;Visualization;Robot vision systems;Measurement uncertainty;Cameras;Velocity measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342197&isnumber=10341342

X. Yu et al., "Fully Proprioceptive Slip-Velocity-Aware State Estimation for Mobile Robots via Invariant Kalman Filtering and Disturbance Observer," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8096-8103, doi: 10.1109/IROS55552.2023.10342519.Abstract: This paper develops a novel slip estimator using the invariant observer design theory and Disturbance Observer (DOB). The proposed state estimator for mobile robots is fully proprioceptive and combines data from an inertial measurement unit and body velocity within a Right Invariant Extended Kalman Filter (RI-EKF). By embedding the slip velocity into SE3 (3) matrix Lie group, the developed DOB-based RI-EKF provides real-time velocity and slip velocity estimates on different terrains. Experimental results using a Husky wheeled robot confirm the mathematical derivations and effectiveness of the proposed method in estimating the observable state variables. Open-source software is available for download and reproducing the presented results. keywords: {Measurement units;Propioception;Real-time systems;Disturbance observers;Mobile robots;Kalman filters;Standards},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342519&isnumber=10341342

M. Eder and G. Steinbauer-Wagner, "Predicting Energy Consumption and Traversal Time of Ground Robots for Outdoor Navigation on Multiple Types of Terrain," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341716.Abstract: The outdoor navigation capabilities of ground robots have improved significantly in recent years, opening up new potential applications in a variety of settings. Cost-based representations of the environment are frequently used in the path planning domain to obtain an optimized path based on various objectives, such as traversal time or energy consumption. However, obtaining such cost representations is still cumbersome, particularly in outdoor settings with diverse terrain types and slope angles. In this paper, we address this problem by using a data-driven approach to develop a cost representation for various outdoor terrain types that supports two optimization objectives, namely energy consumption and traversal time. We train a supervised machine learning model whose inputs consists of extracted environment data along a path and whose outputs are the predicted energy consumption and traversal time. The model is based on a ResNet neural network architecture and trained using field-recorded data. The error of the proposed method on different types of terrain is within 11% of the ground truth data. To show that it performs and generalizes better than currently existing approaches on various types of terrain, a comparison to a baseline method is made. keywords: {Energy consumption;Adaptation models;Visualization;Costs;Navigation;Machine learning;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341716&isnumber=10341342

L. Booth and S. Carpin, "Informative Path Planning for Scalar Dynamic Reconstruction Using Coregionalized Gaussian Processes and a Spatiotemporal Kernel," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8112-8119, doi: 10.1109/IROS55552.2023.10341858.Abstract: The proliferation of unmanned vehicles offers many opportunities for solving environmental sampling tasks with applications in resource monitoring and precision agriculture. Informative path planning (IPP) includes a family of methods which offer improvements over traditional surveying techniques for suggesting locations for observation collection. In this work, we present a novel solution to the IPP problem by using a coregionalized Gaussian processes to estimate a dynamic scalar field that varies in space and time. Our method improves previous approaches by using a composite kernel accounting for spatiotemporal correlations and at the same time, can be readily incorporated in existing IPP algorithms. Through extensive simulations, we show that our novel modeling approach leads to more accurate estimations when compared with formerly proposed methods that do not account for the temporal dimension. keywords: {Gaussian processes;Robot sensing systems;Path planning;Spatiotemporal phenomena;Planning;Multi-robot systems;Kernel},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341858&isnumber=10341342

P. Akella, W. Ubellacker and A. D. Ames, "Probabilistic Guarantees for Nonlinear Safety-Critical Optimal Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8120-8126, doi: 10.1109/IROS55552.2023.10342474.Abstract: Leveraging recent developments in black-box risk-aware verification, we provide three algorithms that generate probabilistic guarantees on (1) optimality of solutions, (2) recursive feasibility, and (3) maximum controller runtimes for general nonlinear safety-critical finite-time optimal controllers. These methods forego the usual (perhaps) restrictive assumptions required for typical theoretical guarantees, e.g. terminal set calculation for recursive feasibility in Nonlinear Model Predictive Control, or convexification of optimal controllers to ensure optimality. Furthermore, we show that these methods can directly be applied to hardware systems to generate controller guarantees on their respective systems. keywords: {Runtime;Optimal control;Closed box;Probabilistic logic;Prediction algorithms;Hardware;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342474&isnumber=10341342

W. Han, A. Jasour and B. Williams, "Non-Gaussian Uncertainty Minimization Based Control of Stochastic Nonlinear Robotic Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8147-8154, doi: 10.1109/IROS55552.2023.10342065.Abstract: In this paper, we consider the closed-loop control problem of nonlinear robotic systems in the presence of probabilistic uncertainties and disturbances. More precisely, we design a state feedback controller that minimizes deviations of the states of the system from the nominal state trajectories due to uncertainties and disturbances. Existing approaches to address the control problem of probabilistic systems are limited to particular classes of uncertainties and systems such as Gaussian uncertainties and processes and linearized systems. We present an approach that deals with nonlinear dynamics models and arbitrary known probabilistic uncertainties. We formulate the controller design problem as an optimization problem in terms of statistics of the probability distributions including moments and characteristic functions. In particular, in the provided optimization problem, we use moments and characteristic functions to propagate uncertainties throughout the nonlinear motion model of robotic systems. In order to reduce the tracking deviations, we minimize the uncertainty of the probabilistic states around the nominal trajectory by minimizing the trace and the determinant of the covariance matrix of the probabilistic states. To obtain the state feedback gains, we solve deterministic optimization problems in terms of moments, characteristic functions, and state feedback gains using off-the-shelf interior-point optimization solvers. To illustrate the performance of the proposed method, we compare our method with existing probabilistic control methods. keywords: {State feedback;Uncertainty;Tracking;Stochastic processes;Process control;Probabilistic logic;Probability distribution},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342065&isnumber=10341342

M. Okada, M. Komatsu, R. Okumura and T. Taniguchi, "Learning Compliant Stiffness by Impedance Control-Aware Task Segmentation and Multi-Objective Bayesian Optimization with Priors," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8155-8162, doi: 10.1109/IROS55552.2023.10342096.Abstract: Rather than traditional position control, impedance control is preferred to ensure the safe operation of industrial robots programmed from demonstrations. However, variable stiffness learning studies have focused on task performance rather than safety (or compliance). Thus, this paper proposes a novel stiffness learning method to satisfy both task performance and compliance requirements. The proposed method optimizes the task and compliance objectives ($T/C$ objectives) simultaneously via multi-objective Bayesian optimization. We define the stiffness search space by segmenting a demonstration into task phases, each with constant responsible stiffness. The segmentation is performed by identifying impedance control-aware switching linear dynamics (IC-SLD) from the demonstration. We also utilize the stiffness obtained by proposed IC-SLD as priors for efficient optimization. Experiments on simulated tasks and a real robot demonstrate that IC-SLD-based segmentation and the use of priors improve the optimization efficiency compared to existing baseline methods. keywords: {Learning systems;Position control;Switches;Aerospace electronics;Control systems;Bayes methods;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342096&isnumber=10341342

H. Xu et al., "Efficient Exploration Using Extra Safety Budget in Constrained Policy Optimization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8163-8169, doi: 10.1109/IROS55552.2023.10342149.Abstract: Reinforcement learning (RL) has achieved promising results on most robotic control tasks. Safety of learning-based controllers is an essential notion of ensuring the effectiveness of the controllers. Current methods adopt whole consistency constraints during the training, thus resulting in inefficient exploration in the early stage. In this paper, we propose an algorithm named Constrained Policy Optimization with Extra Safety Budget (ESB-CPO) to strike a balance between the exploration efficiency and the constraints satis-faction. In the early stage, our method loosens the practical constraints of unsafe transitions (adding extra safety bud-get) with the aid of a new metric we propose. With the training process, the constraints in our optimization problem become tighter. Meanwhile, theoretical analysis and practical experiments demonstrate that our method gradually meets the cost limit's demand in the final training stage. When evaluated on Safety-Gym and Bullet-Safety-Gym benchmarks, our method has shown its advantages over baseline algorithms in terms of safety and optimality. Remarkably, our method gains remarkable performance improvement under the same cost limit compared with baselines. keywords: {Training;Measurement;Costs;Estimation;Reinforcement learning;Stability analysis;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342149&isnumber=10341342

E. Caldarelli, A. Colomé, C. Ocampo-Martinez and C. Torras, "Quadratic Dynamic Matrix Control for Fast Cloth Manipulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8178-8185, doi: 10.1109/IROS55552.2023.10341850.Abstract: Robotic cloth manipulation is an increasingly relevant area of research, challenging classic control algorithms due to the deformable nature of cloth. While it is possible to apply linear model predictive control to make the robot move the cloth according to a given reference, this approach suffers from a large dimensionality of the state-space representation of the cloth models. To address this issue, in this work we study the application of an input-output model predictive control strategy, based on quadratic dynamic matrix control, to robotic cloth manipulation. To account for uncertain disturbances on the cloth's motion, we further extend the algorithm with suitable chance constraints. In extensive simulated experiments, involving disturbances and obstacle avoidance, we show that quadratic dynamic matrix control can be successfully applied in different cloth manipulation scenarios, with significant gains in optimization speed compared to standard model predictive control strategies. The experiments further demonstrate that the closed-loop model used by quadratic dynamic matrix control can be beneficial to the tracking accuracy, leading to improvements over the standard predictive control strategy. Moreover, a preliminary experiment on a real robot shows that quadratic dynamic matrix control can indeed be employed in real settings. keywords: {Heuristic algorithms;Dynamics;Stochastic processes;Predictive models;Prediction algorithms;Solids;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341850&isnumber=10341342

E. L. Zhu, F. L. Busch, J. Johnson and F. Borrelli, "A Gaussian Process Model for Opponent Prediction in Autonomous Racing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8186-8191, doi: 10.1109/IROS55552.2023.10341566.Abstract: In head-to-head racing, performing tightly con-strained, but highly rewarding maneuvers, such as overtaking, require an accurate model of interactive behavior of the opposing target vehicle (TV). We propose to construct a prediction model given data of the TV from previous races. In particular, a one-step Gaussian process (GP) model is trained on closed-loop interaction data to learn the behavior of a TV driven by an unknown policy. Predictions of the nominal trajectory and associated uncertainty are rolled out via a sampling-based approach and are used in a model predictive control (MPC) policy for the ego vehicle in order to intelligently trade-off between safety and performance when racing against a TV. In a Monte Carlo study, we compare the GP-based predictor in closed-loop with the MPC policy against several predictors from literature and observe that the GP-based predictor achieves similar win rates while maintaining safety in up to 3x more races. Through experiments, we demonstrate the approach in real-time on a 1/10th scale racecar platform operating at speeds of around 2.8 m/s, and show a significant level of improvement when using the GP-based predictor over a baseline MPC predictor. Videos of the experiments can be found at https://voutu.be/KMSs4ofDfIs. keywords: {TV;Uncertainty;Gaussian processes;Predictive models;Data models;Real-time systems;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341566&isnumber=10341342

A. Pupa, P. R. Giordano and C. Secchi, "Optimal Energy Tank Initialization for Minimum Sensitivity to Model Uncertainties," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8192-8199, doi: 10.1109/IROS55552.2023.10341568.Abstract: Energy tanks have gained popularity inside the robotics and control communities over the last years, since they represent a formidable tool to enforce passivity (and, thus, input/output stability) of a controlled robot, possibly interacting with uncertain environments. One weak point of passification strategies based on energy tanks concerns, however, their initialization. Indeed, a too large initial energy can cause practical unstable behaviors, while a too low initial energy level can prevent the correct execution of the task. This shortcoming becomes even more relevant in presence of uncertainties in the robot model and/or environment, since it may be hard to predict in advance the correct (safe) amount of initial tank energy for a successful task execution. In this paper we then propose a new strategy for addressing this issue. The recent notion of closed-loop state sensitivity is exploited to derive precise bounds (tubes) on the tank energy behavior by assuming parametric uncertainty in the robot model. These tubes are then exploited in a novel nonlinear optimization problem aiming at finding both the best trajectory and the minimal initial tank energy that allow executing a positioning task for any value of the uncertain parameters in a given range. The approach is finally validated via a statistical analysis in simulation and experiments on real robot hardware. keywords: {Uncertainty;Sensitivity;Statistical analysis;Predictive models;Robot sensing systems;Stability analysis;Electron tubes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341568&isnumber=10341342

A. Srikanthan, F. Yang, I. Spasojevic, D. Thakur, V. Kumar and N. Matni, "A Data-Driven Approach to Synthesizing Dynamics-Aware Trajectories for Underactuated Robotic Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8215-8222, doi: 10.1109/IROS55552.2023.10341651.Abstract: We consider joint trajectory generation and tracking control for under-actuated robotic systems. A common solution is to use a layered control architecture, where the top layer uses a simplified model of system dynamics for trajectory generation, and the low layer ensures approximate tracking of this trajectory via feedback control. While such layered control architectures are standard and work well in practice, selecting the simplified model used for trajectory generation typically relies on engineering intuition and experience. In this paper, we propose an alternative data-driven approach to dynamicsaware trajectory generation. We show that a suitable augmented Lagrangian reformulation of a global nonlinear optimal control problem results in a layered decomposition of the overall problem into trajectory planning and feedback control layers. Crucially, the resulting trajectory optimization is dynamicsaware, in that, it is modified with a tracking penalty regularizer encoding the dynamic feasibility of the generated trajectory. We show that this tracking penalty regularizer can be learned from system rollouts for independently-designed low layer feedback control policies, and instantiate our framework in the context of a unicycle and a quadrotor control problem in simulation. Further, we show that our approach handles the sim-to-real gap through experiments on the quadrotor hardware platform without any additional training. For both the synthetic unicycle example and the quadrotor system, our framework shows significant improvements in both computation time and dynamic feasibility in simulation and hardware experiments. keywords: {Training;Costs;Systematics;Trajectory planning;Computer architecture;Hardware;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341651&isnumber=10341342

K. Pfeiffer and Q. -C. Pham, "Time-Optimal Control via Heaviside Step-Function Approximation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8223-8230, doi: 10.1109/IROS55552.2023.10342255.Abstract: Least-squares programming is a popular tool in robotics due to its simplicity and availability of open-source solvers. However, certain problems like sparse programming in the $\ell_{0}$- or $\ell_{0}-\mathbf{norm}$ for time-optimal control are not equivalently solvable. In this work, we propose a non-linear hierarchical least-squares programming (NL-HLSP) for time-optimal control of non-linear discrete dynamic systems. We use a continuous approximation of the heaviside step function with an additional term that avoids vanishing gradients. We use a simple discretization method by keeping states and controls piece-wise constant between discretization steps. This way, we obtain a comparatively easily implementable NL-HLSP in contrast to direct transcription approaches of optimal control. We show that the NL-HLSP indeed recovers the discrete time-optimal control in the limit for resting goal points. We confirm the results in simulation for linear and non-linear control scenarios. keywords: {Bang-bang control;Humanoid robots;Programming;Behavioral sciences;Motion control;Dynamical systems;Computational complexity},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342255&isnumber=10341342

B. Parilusyan, A. -E. Nicolae, T. Batigne, C. Duhart and M. Serrano, "Bi-Component Silicone 3D Printing with Dynamic Mix Ratio Modification for Soft Robotic Actuators," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8237-8242, doi: 10.1109/IROS55552.2023.10342125.Abstract: Pneumatically operated soft actuators are increasingly researched due to their fabrication simplicity, actuation capabilities, and low production cost. Depending on the Soft Pneumatic Actuator (SPA) objective, its design can be modified to reach new bending angles or increase its actuation strength. However, increasing the abilities of Soft Pneumatic Actuators (SPAs) requires increasing the complexity of their air cavities or using multiple materials with different mechanical stiffness. Both solutions complexify the fabrication of SPAs, reducing their primary benefits of manufacturing simplicity and low production cost. This paper presents a novel additive manufacturing fabrication process incorporating multiple mechanical stiffnesses using a single bicomponent soft material. This process aims to integrate multiple bending angles with multi-channel SPAs without increasing their manufacturing complexity. Our process uses a dynamic modification of the bi-component silicone mix ratio to generate the desired mechanical properties of the material. Modifying the mix ratio allows us to control the material's cure time and mechanical properties, such as its final stiffness. We found that using a single 30 shore-A bi-component silicone, we could achieve several stiffness values with different reticulation times and levels of stickiness. Using these shore ranges and our fabrication process, we built several SPAs. We explored how the printing orientation of the SPAs modifies its bending actuation using our fabrication process to illustrate the capabilities of our approach. keywords: {Fabrication;Pneumatic actuators;Costs;Deformation;Production;Soft robotics;Bending},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342125&isnumber=10341342

G. Peng, Z. Li, S. Wan and Z. Deng, "Two-stage Train Components Defect Detection Based on Prior Knowledge," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8243-8248, doi: 10.1109/IROS55552.2023.10341915.Abstract: The existing method of detecting defects in train components, which relies on visual identification, requires extensive involvement from inspectors and presents certain limitations. In this study, a two-stage defect detection based on prior knowledge was developed, which first detects the types and positions of components, and then conducts targeted detection of possible existing defect types. The algorithm introduces the prior knowledge of the relative spatial position relationship of components and optimizes the detection of sub-components by cascaded convolutional neural networks and local scale-up. In this study, three methods were used, including deep learning, template matching, and quantitative evaluation based on prior knowledge, to perform targeted detection of defect types that may occur in components. Experiments have verified the adaptability and accuracy of the method, demonstrating its high value for engineering applications. keywords: {Knowledge engineering;Deep learning;Rails;Visualization;Costs;Inspection;Rail transportation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341915&isnumber=10341342

L. Yi, A. W. Y. Sang, A. A. Hayat, Q. Tang, A. V. Le and M. R. Elara, "Complete Coverage Path Planning for Omnidirectional Expand and Collapse Robot Panthera," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8249-8254, doi: 10.1109/IROS55552.2023.10342525.Abstract: Autonomous mobile robots (AMRs) face challenges in efficiently covering complex environments. To navigate narrow and expansive areas, AMRs must have two essential attributes: compact size for confined spaces and larger size with omnidirectional locomotion for broader spaces. This study utilizes omnidirectional expand and collapse robots (OECRs) to demonstrate efficient area coverage. OECRs can collapse to navigate through confined spaces and expand for efficient coverage in broad spaces. However, current complete coverage path planning (CCPP) methods do not account for the expanded and collapsed states of OECRs. To address this, a depth-first search (DFS) approach is proposed for OECRs' CCPP, which can adjust the robotic footprint along the CCPP path to reduce path length. The proposed DFS outperforms the state-of-the-art CCPP in terms of increased area coverage and reduced distance traveled on a selected map. keywords: {Navigation;Path planning;Mobile robots;Intelligent robots;Faces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342525&isnumber=10341342

K. Pfeiffer, L. Edgar and Q. -C. Pham, "Monte-Carlo Tree Search with Prioritized Node Expansion for Multi-Goal Task Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8255-8261, doi: 10.1109/IROS55552.2023.10342430.Abstract: Symbolic task planning for robots is computationally challenging due to the combinatorial complexity of the possible action space. This fact is amplified if there are several sub-goals to be achieved due to the increased length of the action sequences. In this work, we propose a multi-goal symbolic task planner for deterministic decision processes based on Monte Carlo Tree Search. We augment the algorithm by prioritized node expansion which prioritizes nodes that already have fulfilled some sub-goals. Due to its linear complexity in the number of sub-goals, our algorithm is able to identify symbolic action sequences of 145 elements to reach the desired goal state with up to 48 sub-goals while the search tree is limited to under 6500 nodes. We use action reduction based on a kinematic reachability criterion to further ease computational complexity. We combine our algorithm with object localization and motion planning and apply it to a real-robot demonstration with two manipulators in an industrial bearing inspection setting. keywords: {Location awareness;Monte Carlo methods;Machine learning;Kinematics;Inspection;Manipulators;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342430&isnumber=10341342

M. Atad, J. Feng, I. Rodríguez, M. Durner and R. Triebel, "Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8262-8269, doi: 10.1109/IROS55552.2023.10342352.Abstract: Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. With GRACE, we are able to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a dual-armed robotic system. We further demonstrate that our method is capable of detecting infeasible assemblies, substantially alleviating the undesirable impacts from false predictions, and hence facilitating real-world deployment soon. Code and training data are available at https://github.com/DLR-RM/GRACE. keywords: {Robotic assembly;Representation learning;Three-dimensional displays;Aluminum;Training data;Solids;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342352&isnumber=10341342

Y. M. Al-Rawashdeh, M. Heertjes and M. Al Janaideh, "Statistical Characterization of Position-Dependent Behavior Using Frequency-Aware B-Spline," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8270-8275, doi: 10.1109/IROS55552.2023.10341845.Abstract: Stretching the definition of the standard Sine profile allows building a generalized symmetric frequency-aware basis function that can be used to generate reference motion trajectories. Other profiles such as polynomials, sigmoid, and harmonic-based models can be equally used under the proposed technique. Despite being suitable at the level of any higher-order time derivative, in this study, the generic basis function is realized at the jerk level such that the generated signals adhere to the limitations of the driven motion system. Introducing suitable time shifts, replicas of basis functions can be obtained giving rise to B-spline like frequency-aware profiles that can be used to realize the actual motion under any desired kinematical constraints, which are neatly written to reduces the computation burden at the motion controller side. Utilizing mainly the frequency-aware B-spline profiles, frequency-dependent random walk motion is presented and used to collect information about the driven motion system to help in characterizing any position-dependent errors through the statistical means, i.e. Analysis of Variance, and Design of Experiments. This allows dividing the working space in which motion takes place into several spatial regions with preferred frequency contents. The effectiveness of these proposed profiles is shown through hardware experiments using a precision motion system. keywords: {Time-frequency analysis;Switches;Frequency conversion;Hardware;Behavioral sciences;Trajectory;Splines (mathematics)},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341845&isnumber=10341342

J. Ming, D. Bargmann, H. Cao and M. Caccamo, "Flexible Gear Assembly with Visual Servoing and Force Feedback," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8276-8282, doi: 10.1109/IROS55552.2023.10341833.Abstract: This paper presents a vision-guided two-stage approach with force feedback to achieve high-precision and flexible gear assembly. The proposed approach integrates YOLO to coarsely localize the target workpiece in a searching phase and deep reinforcement learning (DRL) to complete the insertion. Specifically, DRL addresses the challenge of partial visibility when the on-wrist camera is too close to the workpiece of a small size. Moreover, we use force feedback to improve the robustness of the vision-guided assembly process. To reduce the effort of collecting training data on real robots, we use synthetic RGB images for training YOLO and construct an offline interaction environment leveraging sampled real-world data for training DRL agents. The proposed approach was evaluated in an industrial gear assembly experiment, which requires an assembly clearance of 0.3 mm, demonstrating high robustness and efficiency in gear searching and insertion from arbitrary positions. keywords: {YOLO;Training;Deep learning;Three-dimensional displays;Gears;Force feedback;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341833&isnumber=10341342

Y. Nakajima et al., "Robotic Powder Grinding with Audio-Visual Feedback for Laboratory Automation in Materials Science," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8283-8290, doi: 10.1109/IROS55552.2023.10341526.Abstract: This study focuses on the powder grinding process, which is a necessary step for material synthesis in materials science experiments. In material science, powder grinding is a time-consuming process that is typically executed by hand, as commercial grinding machines are unsuitable for samples of small size. Robotic powder grinding would solve this problem, but it is a challenging task for robots, as it requires observing the powder state and generating appropriate motions. Our previous study proposed a robotic powder grinding system using visual feedback. Although visual feedback is helpful for observing the powder distribution, the particle size during the grinding process remains invisible, leading to suboptimal robot actions. In some cases, the robot chose to gather the powder even though continuing to grind instead would have produced finer powder. In this paper, we present a multi-modal robotic grinding system that utilizes both audio and visual feedback. It makes use of the grinding sound which carries information about the grinding progress, as the particle size strongly affects the audio intensity. The audio feedback enables the robot to grind until the powder is sufficiently fine. In our experiments, the robot ground 80.5% of the powder to a particle size smaller than $250\ \mu\mathrm{m}$ with audio and visual feedback and 68% without audio feedback, indicating that multi-modal feedback is an effective tool to produce finer powder. We conclude that the addition of audio feedback provides crucial information to the robot, allowing it to better understand the progress of the grinding process and make more optimal decisions. This robot system can be used to prepare samples in material science experiments and analyze the grinding process. keywords: {Visualization;Materials science and technology;Powders;Mortar;Robot vision systems;Timing;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341526&isnumber=10341342

A. Deb, K. Kim and D. J. Cappelleri, "Deep Learning-Based Leaf Detection for Robotic Physical Sampling with P-AgBot," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8291-8297, doi: 10.1109/IROS55552.2023.10341516.Abstract: Automating leaf detection and physical leaf sample collection using Internet of Things (IoT) technologies is a crucial task in precision agriculture. In this paper, we present a deep learning-based approach for detecting and segmenting crop leaves for robotic physical sampling. We discuss a method for generating a physical dataset of agricultural crops. Our proposed pipeline incorporates using an RGB-D camera for dataset collection, fusing the depth frame along with RGB images to train Mask R-CNN and YOLOv5 models. We also propose our novel leaf pose estimating algorithm for physical sampling and maximizing leaf sample area while using a robotic arm integrated to the P-AgBot platform. The proposed approach has been experimentally validated on corn and sorghum, in both indoor and outdoor environments. Our method has achieved a best-case detection rate of 90.6%, a 9% smaller error compared to our previous method, and approximately 80% smaller error compared to other state-of-the-art methods in estimating the leaf position. keywords: {YOLO;Deep learning;Three-dimensional displays;Robot kinematics;Robot vision systems;Crops;Grasping},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341516&isnumber=10341342

A. Shabani and U. Martinez-Hernandez, "In-Situ Measurement of Extrusion Width for Fused Filament Fabrication Process Using Vision and Machine Learning Models," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8298-8303, doi: 10.1109/IROS55552.2023.10341406.Abstract: Measuring geometry of the printing road is key for detection of anomalies in 3D printing processes. Although commercial 3D printers can measure the extrusion height using various distance sensors, measuring of the width in real-time remains a challenge. This paper presents a visual in-situ monitoring system to measure width of the printing filament road in 2D patterns. The proposed system is composed of a printable shroud with embedded camera setup and a visual detection approach based on a two-stage instance segmentation method. Each of the segmentation and localization stages can use multiple computational approaches including Gaussian mixture model, color filter, and deep neural network models. The visual monitoring system is mounted on a standard 3D printer and validated with the measurement of printed filament roads of sub-millimeter widths. The results on accuracy and robustness reveal that combinations of deep models for both segmentation and localization stages have better performance. Particularly, fully connected CNN segmentation model combined with YOLO object detector can measure sub-millimeter extrusion width with 90 μm accuracy at 125 ms speed. This visual monitoring system has potential to improve the control of printing processes by the real-time measurement of printed filament geometry. keywords: {Location awareness;Instance segmentation;Visualization;Three-dimensional displays;Atmospheric measurements;Roads;Particle measurements;Computer vision;Additive manufacturing;Instance segmentation;Machine learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341406&isnumber=10341342

Y. M. Al-Rawashdeh, M. Heertjes and M. Al Janaideh, "Motion Orchestration in Dual-Stage Wafer Scanners," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8304-8309, doi: 10.1109/IROS55552.2023.10342526.Abstract: In semiconductor manufacturing, lithography machines are becoming more and more sophisticated system of systems. As an example, a TWINSCAN wafer scanner machine is composed of a wafer, and reticle handlers, reticle, optics, and two wafer chains or systems. In previous studies, we covered the interactions between the reticle, optics, and wafer chains during the step-and-scan cycle. In this study, we focus on the interaction between the additional wafer chain responsible for aligning the wafer substrate and taking its height map during the measurement cycle, and the other chains that are active during the step-and-scan cycle. Working in parallel to increase machine throughput, the inertial forces associated motion of the two cycles induce vibration that may propagate throughout the chains in the machine if no appropriate measures are taken. In this investigation, we look at the reference trajectories responsible for steering the chains throughout the two cycles, and propose two reference trajectory orchestrations that factor in the machine design, geometry, mass distribution, and functions. Theoretically, these orchestrations lead to suppressing the induced vibration without sacrificing the machine throughput while keeping the involved control loops intact. keywords: {Vibrations;Semiconductor device measurement;Upper bound;Vibration measurement;Throughput;Optics;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342526&isnumber=10341342

M. Völk, K. Kleeberger, W. Kraus and R. Bormann, "Towards Packaging Unit Detection for Automated Palletizing Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8310-8317, doi: 10.1109/IROS55552.2023.10342076.Abstract: For various automated palletizing tasks, the detection of packaging units is a crucial step preceding the actual handling of the packaging units by an industrial robot. We propose an approach to this challenging problem that is fully trained on synthetically generated data and can be robustly applied to arbitrary real world packaging units without further training or setup effort. The proposed approach is able to handle sparse and low quality sensor data, can exploit prior knowledge if available and generalizes well to a wide range of products and application scenarios. To demonstrate the practical use of our approach, we conduct an extensive evaluation on real-world data with a wide range of different retail products. Further, we integrated our approach in a lab demonstrator and a commercial solution will be marketed through an industrial partner. keywords: {Training;Packaging;Robot sensing systems;Industrial robots;Task analysis;Pallets;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342076&isnumber=10341342

H. J. Hee Kim et al., "Robotic Barrier Construction through Weaved, Inflatable Tubes," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8318-8323, doi: 10.1109/IROS55552.2023.10342190.Abstract: In this article, we present a mechanism and related path planning algorithm to construct light-duty barriers out of extruded, inflated tubes weaved around existing environmental features. Our extruded tubes are based on everted vine-robots and in this context, we present a new method to steer their growth. We characterize the mechanism in terms of accuracy resilience, and, towards their use as barriers, the ability of the tubes to withstand distributed loads. We further explore an algorithm which, given a feature map and the size and direction of the external load, can determine where and how to extrude the barrier. Finally, we showcase the potential of this method in an autonomously extruded two-layer wall weaved around three pipes. While preliminary, our work indicates that this method has potential for barrier construction in cluttered environments, e.g. shelters against wind or snow. Future work may show how to achieve tighter weaves, how to leverage weave friction for improved strength, how to assess barrier performance for feedback control, and how to operate the extrusion mechanism off of a mobile robot. keywords: {Snow;Friction;Path planning;Electron tubes;Feedback control;Mobile robots;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342190&isnumber=10341342

K. Shimura, N. Iwamoto and T. Umedachi, "Bistable Tensegrity Robot with Jumping Repeatability Based on Rigid Plate-Shaped Compressors," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8324-8330, doi: 10.1109/IROS55552.2023.10342069.Abstract: This study presents a bistable tensegrity robot that can perform repetitive jumps using one motor. This robot is based on a tensegrity structure that uses rigid plate-shaped compressors. To achieve bistability in this structure, we optimized the position of additional springs using a physics simulator that considers geometric constraints attributed to the collision between compression materials. A prototype was constructed based on the simulation model. To achieve jumping repeatability, we used one motor to control three tendons, each used; to control the additional spring strain, trigger the snap-through motion, and reform the structure to its original form. The prototype could jump using snap-through motion and reform back to its original form based on motor rotation. Furthermore, the robot demonstrated its ability to jump over flights of stairs by attaching a stand with a slight angle and using jumping repeatability. keywords: {Prototypes;Stairs;Compressors;Springs;Collision avoidance;Physics;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342069&isnumber=10341342

Y. -S. Wu, K. Gilday and J. Hughes, "Accessible Soft Robotics Education with Re-Configurable Balloon Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8331-8336, doi: 10.1109/IROS55552.2023.10342299.Abstract: Soft robotics requires effective tools to educate the next generation of engineers and researchers. Stemming from a lack of universally accepted principles for education and with high barriers to entry in terms of fabrication and hardware, education to date has been highly ad hoc. We present a low-cost toolkit based on re-configurable balloon which allows rapid development of soft yet functional robots. This provides practical demonstrations of key soft robotic principles including: morphology, stiffness control, controller dependencies and modulation of environmental interactions, while grounding robot behaviours in fundamental mechani-cal models. We provide a framework for assembling balloon structures, incorporating actuation and exploring interactions. A diverse set of robots have been developed to show the potential to use this balloon-bots for educational activities for undergraduate teaching or below. In particular, different modes of locomotion are shown using robots each of which has an assembly time under 5 minutes. These robots can teach skills ranging from component integration and implementation, to key soft robotic design principles and embodied intelligence. keywords: {Uncertainty;Three-dimensional displays;Educational robots;Grounding;Education;Morphology;Soft robotics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342299&isnumber=10341342

J. M. Bern, Z. J. Patterson, L. Z. Yañez, K. K. Misquitta and D. Rus, "A Fabrication and Simulation Recipe for Untethering Soft-Rigid Robots with Cable-Driven Stiffness Modulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8337-8342, doi: 10.1109/IROS55552.2023.10341630.Abstract: We explore the idea of robotic mechanisms that can shift between soft and rigid states, with the long-term goal of creating robots that marry the flexibility and robustness of soft robots with the strength and precision of rigid robots. We present a simple yet effective method to achieve large and rapid stiffness variations by compressing and relaxing a flexure using cables. Next, we provide a differentiable modeling framework that can be used for motion planning, which simultaneously reasons about the modulated stiffness joints, tendons, rigid joints, and basic hydrodynamics. We apply this stiffness tuning and simulation recipe to create SoRiTu, an untethered soft-rigid robotic sea turtle capable of various swimming maneuvers. keywords: {Modulation;Mechanical cables;Soft robotics;Robustness;Planning;Robots;Tuning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341630&isnumber=10341342

T. Chevillon, S. Mbakop, G. Tagne and R. Merzouki, "Integrated Design of a Robotic Bio-Inspired Trunk," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8343-8348, doi: 10.1109/IROS55552.2023.10341706.Abstract: Soft-Continuum Manipulators are of increasing interest to researchers for various non-destructive applications (minimally invasive surgery, fibroscopy, oncology, pipe exploration and many others). They are made with soft material or special arrangement of actuators allowing them to exhibit resilience and dexterity. The concept of Proprioceptive Soft-Continuum Manipulators still remains a major challenge for soft roboticists due to the big issues related to manufacturing process which becomes very expensive including sophisticated or experimental tools, highly skilled technicians and time. However, the manipulator proprioception is very usefull for enhancing the dexterity during their manipulation. Henceforth, this paper investigates a quick and simple approach for the integrated design of a proprioceptive Soft Robotic Bio- Inspired Trunk made with dragon skin 30 Material. This soft manipulator is made up of two segments composed of three independent physical control inputs each. It has an embedded electronics mainly composed of IMUs. The latter have allowed controlling the shape kinematics using a control-oriented modeling approach inspired from the kinematics control of a puppet toy. The developed modeling approach is a Reduced Order Modeling (ROM) which uses Pythagorean Hodograph (PH) curves which lowers in real time, the control dimension of the robot to virtual control points of its representative PH curve. The proposed investigation presents also a comprehensive approach for the manufacturing process of Soft-Continuum Manipulators with complex geometry. keywords: {Geometry;Manufacturing processes;Shape;Toy manufacturing industry;Propioception;Kinematics;Integrated design},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341706&isnumber=10341342

R. Michikawa, K. Tadakuma and F. Matsuno, "A New Design of Multilayered String Jamming Mechanism with Three-Degree-of-Freedom," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8349-8355, doi: 10.1109/IROS55552.2023.10342146.Abstract: A robot must exhibit softness so as not to accidentally damage its environment. However, stiffness is also necessary, so that the robot can transmit forces and perform tasks. In soft robotics, it is desirable to be able to switch between two states, namely a flexible state for adapt ion to the environment and a rigid state for the transmission of forces. String jamming mechanisms, which comprise many units connected in a bead-like pattern, have received attention for their ability to switch between flexible and rigid states. In this study, we propose a new design of the string jamming mechanism that enhances the maximum stiffness in the rigid state while maintaining a high fitting performance for the environment in the flexible state. We evaluate the fitting of the mechanism to the environment in a qualitative geometric discussion and compare the performance of the mechanism with that of existing string jamming mechanisms. The results of experiments measuring the maximum stiffness show the usefulness of the proposed mechanism from a quantitative point of view. keywords: {Fitting;Switches;Soft robotics;Ions;Jamming;Task analysis;Intelligent robots;Soft Robot Materials and Design;Ten-don/Wire Mechanism;Mechanism Design},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342146&isnumber=10341342

K. Tadakuma, M. Watanabe and S. Tadokoro, "Protective Skin Mechanism with an Exhaustive Arrangement of Tiny Rigid Bodies for Soft Robots: Evaluation of Puncture Resistance, Elasticity, and Descaling Resistance of the Scale Mechanism," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8356-8363, doi: 10.1109/IROS55552.2023.10341363.Abstract: Soft actuators have several advantages, including large deformation, safety and adaptability to the environment, and shock absorbance. However, they are weak against sharp objects owing to their soft bodies. This paper proposes a novel protective skin mechanism with an exhaustive arrangement of tiny rigid bodies. Small pieces were sewed on an elastic sheet using Kevlar strings. We conducted some measurements of puncture resistance, elasticity, and descaling resistance of the scale mechanisms. The results indicate that the elasticity of the proposed scale mechanism was just 150% larger than a simple silicone sheet. In addition, approximately 15 N was required for descaling, which is seven times larger than that of the glued scales. There was no puncture even when pricked with a needle. keywords: {Resistance;Actuators;Electric shock;Soft robotics;Elasticity;Needles;Skin;Soft Robot Materials and Design;Mechanism Design;Biomimetics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341363&isnumber=10341342

T. D. Ta and Y. Kawahara, "Printable Bistable Structures for Programmable Frictional Skins of Soft-Bodied Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8364-8370, doi: 10.1109/IROS55552.2023.10341445.Abstract: Soft robots made of flexible materials are highly adaptive, easy to fabricate, and safer to interact with. One of the ways for soft robots to interact with the surrounding environment is through their deformable bodily characteristics including internal body stiffness and external body friction. Though the flexibility of soft-bodied robots has been rigorously studied, the frictional skin of such soft-bodied robots, acting as a mechanical interface between the robot and the environment, remains unexplored. Being able to design the frictional skin will make soft-bodied robots more versatile in environmental navigation, more dexterous in manipulation tasks, and more flexible in haptic feedback. In this paper, we propose a robotic skin that can be programmed dynamically to change the mode of friction. The robotic skin is based on bistable bellow structures that can be switched between two folding states to change the contact points between the robotic skin and the ground. Our robotic skin can dynamically change its anisotropic frictional behavior to add another dimension to the designing space of soft robotics. keywords: {Navigation;Friction;Switches;Grasping;Soft robotics;Skin;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341445&isnumber=10341342

H. Kabutz, A. Hedrick, W. P. McDonnell and K. Jayaram, "mCLARI: A Shape-Morphing Insect-Scale Robot Capable of Omnidirectional Terrain-Adaptive Locomotion in Laterally Confined Spaces," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8371-8376, doi: 10.1109/IROS55552.2023.10341588.Abstract: Soft compliant microrobots have the potential to deliver significant societal impact when deployed in applications such as search and rescue. In this research we present mCLARI, a body compliant quadrupedal microrobot of 20mm neutral body length and 0.97g, improving on its larger predecessor, CLARI. This robot has four independently actuated leg modules with 2 degrees of freedom, each driven by piezoelectric actuators. The legs are interconnected in a closed kinematic chain via passive body joints, enabling passive body compliance for shape adaptation to external constraints. Despite scaling its larger predecessor down to 60 % in length and 38% in mass, mCLARI maintains 80% of the actuation power to achieve high agility. Additionally, we demonstrate the new capability of passively shape-morphing mCLARI - omnidirectional laterally confined locomotion - and experimentally quantify its running performance achieving a new unconstrained top speed of ~3 bodylengths/s (60 mms-1). Leveraging passive body compliance, mCLARI can navigate through narrow spaces with a body compression ratio of up to 1.5 × the neutral body shape. keywords: {Legged locomotion;Shape;Navigation;Piezoelectric actuators;Kinematics;Quadrupedal robots;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341588&isnumber=10341342

F. Wang et al., "FABRIKv: A Fast, Iterative Inverse Kinematics Solver for Surgical Continuum Robot with Variable Curvature Model," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8417-8424, doi: 10.1109/IROS55552.2023.10342089.Abstract: Due to the advantages of high flexibility, large workspace, and good human-body compatibility, flexible tendon-driven surgical continuum robots have attracted a lot of attention in robot-assisted minimally invasive surgery. However, due to the coupling of the position and angle of the continuum robot, and the easy deformation of the external force, its inverse kinematics solution has always been a challenge. This paper proposes a fast inverse kinematics solver for surgical continuum robots with a variable curvature model. Firstly, the deformation of the continuum robot is analyzed, and a representation method of the variable curvature model is proposed. Next, to solve the inverse kinematics problem when the continuum robot deforms under load, FABRIKv is proposed by improving the Forward And Backward Reaching Inverse Kinematics (FABRIK). During the inverse kinematics solution, the algorithm preserves the real-time nature of FABRIK and corrects for deformation effects caused by the load. Finally, the experiment verifies the rationality and effectiveness of the variable curvature model representation method, as well as the fastness and accuracy of the FARIKv solver. keywords: {Deformable models;Minimally invasive surgery;Deformation;Force;Kinematics;Real-time systems;Iterative methods},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342089&isnumber=10341342

Z. Li and Q. Xu, "Design and Synchronous Control of a Magnetically-Actuated and Ultrasound-Guided Multi-Arm Robotic System," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8425-8430, doi: 10.1109/IROS55552.2023.10341399.Abstract: This paper presents the design of a new multi-arm robotic system with mobile magnetic actuation and extracorpo-real ultrasound guidance dedicated to magnetic catheterization. The kinematic model of the external mobile actuation arm (EMAA) and extracorporeal ultrasound-integrated tracking arm (EUTA) are derived based on Denavit-Hartenberg (DH) parameters, including specially designed end-effectors. The synchronous control scheme for the mobile magnet and mobile ultrasound probe is introduced with polar coordinate-based magnetic actuation and visual servo-based ultrasound tracking method. Meanwhile, a denoising algorithm based on Speckle Reduction Anisotropic Diffusion (SRAD) is implemented. The effectiveness of the proposed robotic system has been verified by conducting several experimental studies, e.g., ex-vivo tests of catheter steering in endovascular phantom and soft tissue-imitating phantom with the average error of 0.32 mm and signal-to-noise-ratio (SNR) of 12.2 for the ultrasound imaging. keywords: {Visualization;Ultrasonic imaging;Magnetic resonance imaging;Robot kinematics;Imaging phantoms;Robot sensing systems;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341399&isnumber=10341342

Y. Wang et al., "A Shared-Control Dexterous Robotic System for Assisting Transoral Mandibular Fracture Reduction: Development and Cadaver Study," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8431-8438, doi: 10.1109/IROS55552.2023.10342097.Abstract: The rigid and straight nature of conventional surgical drills and screwdrivers makes it difficult to access the posterior mandible for fracture reduction without the creation of facial incisions. To assist transoral mandibular fracture reduction in hard-to-reach areas, we propose a shared-control dexterous robotic system. The end effector of this system is an articulated drilling/screwing tool to provide distal dexterity. This system uses an admittance-control-based approach to provide precision and stability during shared-control hole-drilling processes. A cadaver study showed the efficacy of the proposed system to assist plate fixation in the reduction of mandibular fractures. The proposed articulated surgical tool was capable of drilling holes in and driving screws into the mandible of a cadaver head. In addition, the shared-control robotic system ensured that the drill moved along its axial direction, leading to stable and precise hole drilling. keywords: {Drilling;Torque;Instruments;Fasteners;End effectors;Robots;Cadaver},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342097&isnumber=10341342

Y. Sun et al., "Model-Based Bending Control of Magnetically-Actuated Robotic Endoscopes for Automatic Retroflexion in Confined Spaces," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8439-8445, doi: 10.1109/IROS55552.2023.10342414.Abstract: This paper is concerned with the issue of the kinematic model-based bending control for the magnetically actuated robotic endoscope and its application for automatic retroflexion. By the utilization of the Cosserat rod theory and the transformation in the magnetic tip of the endoscope, the comprehensive kinematic model of the magnetically-actuated robotic endoscope is established. Afterward, a magnetic control scheme for the bending motion is proposed by co-developing an error feedback PID control strategy and the model-based feedback approach. Moreover, as one unique kind of bending motion, retroflexion is taken into account, and the strategy aimed at the bid of compact space retroflexion is presented by virtue of the introduction of serial waypoints pursuing the position of the magnetic tip being close to the midline as possible. Eventually, the developed modeling and bending control scheme and the compact space retroflexion strategy are examined in a magnetically actuated robotic endoscope system to manifest the effectiveness and applicability of the theoretical approach. The experimental results indicate that the designed controller can drive the endoscope to bend to the desired pose and show a reduction of about 47.01% in the sweeping area and 79.25% in the last distance to midline achieved by conducting compact space retroflexion in comparison to “U” type one. keywords: {PI control;Endoscopes;Magnetic confinement;Kinematics;Bending;Aerospace electronics;Drives},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342414&isnumber=10341342

G. Ma, R. Prakash, B. Mann, W. Ross and P. Codd, "3D Laser-and-Tissue Agnostic Data-Driven Method for Robotic Laser Surgical Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8446-8453, doi: 10.1109/IROS55552.2023.10341343.Abstract: In robotic laser surgery, shape prediction of an one-shot ablation crater is an important problem for minimizing errant overcutting of healthy tissue during the course of pathological tissue resection and precise tumor removal. Since it is difficult to physically model the laser-tissue interaction due to the variety of optical tissue properties, complicated process of heat transfer, and uncertainty about the chemical reaction, we propose a 3D crater prediction model based on an entirely data-driven method without any assumptions of laser settings and tissue properties. Based on the crater prediction model, we formulate a novel robotic laser planning problem to determine the optimal laser incident configuration, which aims to create a crater that aligns with the surface target (e.g. tumor, pathological tissue). To solve the one-shot ablation crater prediction problem, we model the 3D geometric relation between the tissue surface and the laser energy profile as a non-linear regression problem that can be represented by a single-layer perceptron (SLP) network. The SLP network is encoded in a novel kinematic model to predict the shape of the post-ablation crater with an arbitrary laser input. To estimate the SLP network parameters, we formulate a dataset of one-shot laser-phantom craters reconstructed by the optical coherence tomography (OCT) B-scan images. To verify the method. The learned crater prediction model is applied to solve a simplified robotic laser planning problem modelled as a surface alignment error minimization problem. The initial results report about $(91.2\pm 3.0)\%$ 3D-crater-Intersection-over-Union (3D-crater-IoU) for the 3D crater prediction and an average of about 98.0% success rate for the simulated surface alignment experiments. keywords: {Solid modeling;Pathology;Three-dimensional displays;Surface emitting lasers;Predictive models;Laser modes;Chemical lasers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341343&isnumber=10341342

Y. Lu, J. Ramos, M. G. Ghosn, D. J. Shah, A. T. Becker and J. Leclerc, "Insertion, Retrieval and Performance Study of Miniature Magnetic Rotating Swimmers for the Treatment of Thrombi," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8454-8460, doi: 10.1109/IROS55552.2023.10342355.Abstract: Miniature Magnetic Rotating Swimmers (MMRSs) are untethered machines containing magnetic materials. An external rotating magnetic field produces a torque on the swimmers to make them rotate. MMRSs have propeller fins that convert the rotating motion into forward propulsion. This type of robot has been shown to have potential applications in the medical realm. This paper presents new MMRS designs with (1) an increased permanent magnet volume to increase the available torque and prevent the MMRS from becoming stuck inside a thrombus; (2) new helix designs that produce an increased force to compensate for the weight added by the larger permanent magnet volume; (3) different head drill shape designs that have different interactions with thrombi. The two best MMRS designs were tested experimentally by removing a partially dried 1-hour-old thrombus with flow in a bifurcating artery model. The first MMRS disrupted a large portion of the thrombus. The second MMRS retrieved a small remaining piece of the thrombus. In addition, a tool for inserting, retrieving, and switching MMRSs during an experiment is presented and demonstrated. Finally, this paper shows that the two selected MMRS designs can perform accurate 3D path-following. keywords: {Three-dimensional displays;Torque;Shape;Velocity control;Fluid flow;Switches;Propulsion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342355&isnumber=10341342

G. Pittiglio, M. Mencattelli, A. Donder, Y. Chitalia and P. E. Dupont, "Hybrid Tendon and Ball Chain Continuum Robots for Enhanced Dexterity in Medical Interventions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8461-8466, doi: 10.1109/IROS55552.2023.10341686.Abstract: A hybrid continuum robot design is introduced that combines a proximal tendon-actuated section with a distal telescoping section comprised of permanent-magnet spheres actuated using an external magnet. While, individually, each section can approach a point in its workspace from one or at most several orientations, the two-section combination possesses a dexterous workspace. The paper describes kinematic modeling of the hybrid design and provides a description of the dexterous workspace. We present experimental validation which shows that a simplified kinematic model produces tip position mean and maximum errors of 3% and 7% of total robot length, respectively. keywords: {Shape;Kinematics;Switches;Feedback control;Electron tubes;Task analysis;Feedforward systems;Medical Rebots and Systems;Steerable Catheters;Flexible Robotics;Magnetic Actuation;Continuum robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341686&isnumber=10341342

H. Ishida, A. Munawar, R. H. Taylor and P. Kazanzides, "Semi-Autonomous Assistance for Telesurgery Under Communication Loss," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8467-8473, doi: 10.1109/IROS55552.2023.10341450.Abstract: Telesurgery has a clear potential for providing high-quality surgery to medically underserved areas like rural areas, battlefields, and spacecraft; nevertheless, effective methods to overcome unreliable communication systems are still lacking. Furthermore, it is not well understood how users react at the moment of communication loss and also during the loss. In this paper, we aim to analyze human response by proposing a telesurgery simulation framework that models an environment incorporating local and remote sites. Furthermore, this framework generates structural data for human behavior analysis and can provide different forms of assistance during the communication failure and at the communication recovery. We investigated three different types of assistance: User-centered, Robot-centered and Hybrid. A 12-person user-study was carried out using the proposed telesurgery simulation where participants completed a peg transfer task with random communication loss. The collected data was used to analyze the human response to a communication failure. The proposed Hybrid method reduced temporal demand with no increase in completion time compared to the baseline control method where users were unable to move the input device during the communication loss. The Hybrid method also significantly reduced both the task completion time and workload compared to the other two proposed methods (User-centered and Robot-centered). keywords: {Space vehicles;Analytical models;Communication systems;Robot control;Surgery;Input devices;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341450&isnumber=10341342

H. Ishida et al., "Improving Surgical Situational Awareness with Signed Distance Field: A Pilot Study in Virtual Reality," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8474-8479, doi: 10.1109/IROS55552.2023.10342004.Abstract: The introduction of image-guided surgical navigation (IGSN) has greatly benefited technically demanding surgical procedures by providing real-time support and guidance to the surgeon during surgery. To develop effective IGSN, a careful selection of the surgical information and the medium to present this information to the surgeon is needed. However, this is not a trivial task due to the broad array of available options. To address this problem, we have developed an open-source library that facilitates the development of multimodal navigation systems in a wide range of surgical procedures relying on medical imaging data. To provide guidance, our system calculates the minimum distance between the surgical instrument and the anatomy and then presents this information to the user through different mechanisms. The real-time performance of our approach is achieved by calculating Signed Distance Fields at initialization from segmented anatomical volumes. Using this framework, we developed a multimodal surgical navigation system to help surgeons navigate anatomical variability in a skull base surgery simulation environment. Three different feedback modalities were explored: visual, auditory, and haptic. To evaluate the proposed system, a pilot user study was conducted in which four clinicians performed mastoidectomy procedures with and without guidance. Each condition was assessed using objective performance and subjective workload metrics. This pilot user study showed improvements in procedural safety without additional time or workload. These results demonstrate our pipeline's successful use case in the context of mastoidectomy. keywords: {Measurement;Visualization;Navigation;Surgery;Virtual reality;Skull;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342004&isnumber=10341342

J. Liu et al., "Development and Evaluation of a Single-arm Robotic System for Autonomous Suturing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8480-8486, doi: 10.1109/IROS55552.2023.10341999.Abstract: This article introduces a novel suture managing device (SMD) and new suture management controller to enable single-arm suture management during autonomous suturing with the Smart Tissue Autonomous Robot (STAR). The primary function of the SMD is to tension and manage the suture thread, a task that was previously carried out by a second manipulator or a human assistant. The SMD and its controller are integrated into STAR's autonomous suturing workflow. Experiments were conducted to quantify the tensioning force of SMD and to evaluate the suture quality of the new single-arm system. The prototype of SMD achieves 1.67N tensioning force with suturing time of 29.1±0.42 seconds per stitch. Our study results demonstrate that the single-arm STAR system with SMD achieves equivalent performance to our previous works in suturing efficiency where suture management was performed with either a dual-armed robotic system or by a human surgical assistant. The study's findings contribute to the field of medical robotics and to our knowledge represent the first known instance of single-arm suturing with suture management during autonomous anastomosis. keywords: {Laparoscopes;Medical robotics;Force;Stars;Surgery;Prototypes;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341999&isnumber=10341342

H. Lin, B. Li, X. Chu, Q. Dou, Y. Liu and K. W. Samuel Au, "End-to-End Learning of Deep Visuomotor Policy for Needle Picking," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8487-8494, doi: 10.1109/IROS55552.2023.10342194.Abstract: Needle picking is a challenging manipulation task in robot-assisted surgery due to the characteristics of small slender shapes of needles, needles' variations in shapes and sizes, and demands for millimeter-level control. Prior works, heavily relying on the prior of needles (e.g., geometric models), are hard to scale to unseen needles' variations. In this paper, we present the first end- to-end learning method to train deep visuomotor policy for needle picking. Concretely, we propose DreamerfD to maximally leverage demonstrations to improve the learning efficiency of a state-of-the-art model-based reinforcement learning method, DreamerV2; Since Variational Auto-Encoder (VAE) in DreamerV2 is difficult to scale to high-resolution images, we propose Dynamic Spotlight Adaptation to represent control-related visual signals in a low-resolution image space; Virtual Clutch is also proposed to reduce per-formance degradation due to significant error between prior and posterior encoded states at the beginning of a rollout. We conducted extensive experiments in simulation to evaluate the performance, robustness, in-domain variation adaptation, and effectiveness of individual components of our method. Our method, trained by 8k demonstration timesteps and 140k online policy timesteps, can achieve a remarkable success rate of 80%. Furthermore, our method effectively demonstrated its superiority in generalization to unseen in-domain variations including needle variations and image disturbance, highlighting its robustness and versatility. Codes and videos are available at https://sites.google.com/view/DreamerfD. keywords: {Learning systems;Visualization;Shape;Surgery;Reinforcement learning;Needles;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342194&isnumber=10341342

T. Huang, K. Chen, W. Wei, J. Li, Y. Long and Q. Dou, "Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8495-8501, doi: 10.1109/IROS55552.2023.10342180.Abstract: Reinforcement learning is still struggling with solving long-horizon surgical robot tasks which involve multiple steps over an extended duration of time due to the policy exploration challenge. Recent methods try to tackle this problem by skill chaining, in which the long-horizon task is decomposed into multiple subtasks for easing the exploration burden and subtask policies are temporally connected to complete the whole long-horizon task. However, smoothly connecting all subtask policies is difficult for surgical robot scenarios. Not all states are equally suitable for connecting two adjacent subtasks. An undesired terminate state of the previous subtask would make the current subtask policy unstable and result in a failed execution. In this work, we introduce value-informed skill chaining (ViSkill), a novel reinforcement learning framework for long-horizon surgical robot tasks. The core idea is to distinguish which terminal state is suitable for starting all the following subtask policies. To achieve this target, we introduce a state value function that estimates the expected success probability of the entire task given a state. Based on this value function, a chaining policy is learned to instruct subtask policies to terminate at the state with the highest value so that all subsequent policies are more likely to be connected for accomplishing the task. We demonstrate the effectiveness of our method on three complex surgical robot tasks from SurRoL, a comprehensive surgical simulation platform, achieving high task success rates and execution efficiency. Code is available at https: / /github. com/med-air/ViSkill. keywords: {Medical robotics;Codes;Reinforcement learning;Task analysis;Intelligent robots;Software development management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342180&isnumber=10341342

J. Zhang, J. Shen, Y. Liu and D. Hong, "Design of a Jumping Control Framework with Heuristic Landing for Bipedal Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8502-8509, doi: 10.1109/IROS55552.2023.10342265.Abstract: Generating dynamic jumping motions on legged robots remains a challenging control problem as the full flight phase and large landing impact are expected. Compared to quadrupedal robots or other multi-legged robots, bipedal robots place higher requirements for the control strategy given a much smaller support polygon. To solve this problem, a novel heuristic landing planner is proposed in this paper. With the momentum feedback during the flight phase, landing locations can be updated to minimize the influence of uncertainties from tracking errors or external disturbances when landing. To the best of our knowledge, this is the first approach to take advantage of the flight phase to reduce the impact of the jump landing which is implemented in the actual robot. By integrating it with a modified kino-dynamics motion planner with centroidal momentum and a low-level controller which explores the whole-body dynamics to hierarchically handle multiple tasks, a complete and versatile jumping control framework is designed in this paper. Extensive results of simulation and hardware jumping experiments on a miniature bipedal robot with proprioceptive actuation are provided to demonstrate that the proposed framework is able to achieve human-like efficient and robust jumping tasks, including directional jump, twisting jump, step jump, and somersaults. keywords: {Legged locomotion;Uncertainty;Tracking;Dynamics;Propioception;Hardware;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342265&isnumber=10341342

D. Lim, M. -J. Kim, J. Cha, D. Kim and J. Park, "Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8510-8517, doi: 10.1109/IROS55552.2023.10342530.Abstract: The estimation of external joint torque and contact wrench is essential for achieving stable locomotion of humanoids and safety-oriented robots. Although the contact wrench on the foot of humanoids can be measured using a force-torque sensor (FTS), FTS increases the cost, inertia, complexity, and failure possibility of the system. This paper introduces a method for learning external joint torque solely using proprioceptive sensors (encoders and IMUs) for a floating base robot. For learning, the GRU network is used and random walking data is collected. Real robot experiments demonstrate that the network can estimate the external torque and contact wrench with significantly smaller errors compared to the model-based method, momentum observer (MOB) with friction modeling. The study also validates that the estimated contact wrench can be utilized for zero moment point (ZMP) feedback control, enabling stable walking. Moreover, even when the robot's feet and the inertia of the upper body are changed, the trained network shows consistent performance with a model-based calibration. This result demonstrates the possibility of removing FTS on the robot, which reduces the disadvantages of hardware sensors. keywords: {Legged locomotion;Learning systems;Torque;Costs;Humanoid robots;Propioception;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342530&isnumber=10341342

M. Ciocca, P. -B. Wieber and T. Fraichard, "Time to Danger, an Alternative to Passive Safety for the Locomotion of a Biped Robot in a Crowd," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8518-8524, doi: 10.1109/IROS55552.2023.10342001.Abstract: A biped robot walking in a crowd must avoid falls and collisions at the same time. The latter is usually addressed through Passive Safety (PS), which guarantees that the robot is at rest when a collision is inevitable. Since PS may limit the robot's mobility, the purpose of this work is to introduce and explore the novel concept of Time To Danger (TTD) as an alternative. For a given robot motion, TTD is the time where the robot enters the region that a person can potentially occupy in the future. After having studied the properties of TTD, a novel locomotion strategy is proposed, which computes an optimal locomotion plan that guarantees balance preservation and TTD maximization, following a receding horizon Model Predictive Control scheme. Controlled experiments in a challenging simulated crowd scenario demonstrate how the novel locomotion strategy outperforms a Passive Safety-based locomotion strategy from a collision avoidance point of view. keywords: {Legged locomotion;Robot motion;Safety;Collision avoidance;Intelligent robots;Predictive control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342001&isnumber=10341342

M. Konishi, K. Kojima, K. Okada, M. Inaba and K. Kawasaki, "ZMP Feedback Balance Control of Humanoid in Response to Ground Acceleration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8525-8531, doi: 10.1109/IROS55552.2023.10341851.Abstract: In order for a humanoid robot to balance on the movable ground, balance feedback control in response to its unpredictable movement is required. However, feedback control in response to ground movement has the following two issues, (A) Interaction between the ground dynamics and the balance control may cause vibration. (B) The balance control may rather deteriorate the stability due to the response delay. To solve these problems, this study proposes the support foot acceleration term in the walking stabilizer and gives its gain by considering the following two conditions, (A) Avoiding steady-state vibration in a two-mass linear inverted pendulum model on an arbitrary ground, and (B) reducing the influence of inertial forces resulting from the delay of ZMP feedback. Experiments with a life-size humanoid JAXON verified the steady-state vibration phenomenon and improved the stability of acceleration and deceleration when boarding the Two-Wheeled Scooter. keywords: {Vibrations;Phase measurement;Humanoid robots;Motorcycles;Vibration measurement;Stability analysis;Steady-state},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341851&isnumber=10341342

T. Takuma, I. Hashimoto, R. Andachi, Y. Sugimoto and S. Aoi, "Manipulation of Center of Pressure for Bipedal Locomotion by Passive Twisting of Viscoelastic Trunk Joint and Asymmetrical Arm Swinging," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8532-8537, doi: 10.1109/IROS55552.2023.10342461.Abstract: To implement successful bipedal locomotion in a robot, its center of pressure (CoP) is placed on a supporting area. To achieve locomotion, many studies have focused on the lower body. Given that the human upper body has a large mass and its behavior influences locomotion even in the case of the robot, this study investigates the effect of the upper body, which contains moving arms and a twisting trunk, on CoP. The dynamics is analyzed using a simple model that has a passive viscoelastic trunk joint around the vertical axis and arms that oscillates back and forth. From the derivations of CoP and trunk joint trajectory, three important findings are made: (i) the CoP oscillates along the lateral direction only when the arm swings in an anterior-posterior asymmetric manner, (ii) the trajectories of CoP and trunk joint are in anti-phase, (iii) the phase of CoP along the lateral direction is influenced by the swinging cycle and viscoelasticity of the trunk joint, the mechanical elements of the upper body. An experiment using a robot verified the first and second findings, and simulation verified the last finding. The last finding will contribute to making a feedback controller that converges to the desired phase, which is an important factor for successful bipedal locomotion. keywords: {Legged locomotion;Torso;Phase measurement;Simulation;Manipulators;Stability analysis;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342461&isnumber=10341342

S. Thomas et al., "An Implantable Variable Length Actuator for Modulating in Vivo Musculo-Tendon Force in a Bipedal Animal Model," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 8538-8543, doi: 10.1109/IROS55552.2023.10341584.Abstract: Mobility, a critical factor in quality of life, is often rehabilitated using simplistic solutions, such as walkers. Exoskeletons (wearable robotics) offer a more sophisticated rehabilitation approach. However, non-adherence to externally worn mobility aids limits their efficacy. Here, we present the concept of a fully implantable assistive limb actuator that overcomes non-adherence constraints, and which can provide high-precision assistive force. In a bipedal animal model (fowl), we have developed a variable length isometric actuator (measuring ϕ9 x 30 mm) that is able to be directly implanted within the leg via a bone anchor and tendon fixation, replacing the lateral gastrocnemius muscle belly. The actuator is able to generate isometric force similar to the in vivo force of the native muscle, designed to generate assistive torque at the ankle and reduce muscular demand at no additional energy cost. The device has a stroke of 10 mm that operates up to 770 mm/s (77 stroke lengths/s), capable of acting as a clutch (disengaging when needed) and with a tunable slack length to modulate the timing and level of assistive force during gait. Surgical techniques to attach the actuator to the biological system, the Achilles tendon and tibia, have been established and validated using survival surgeries and cadaveric specimens. keywords: {Actuators;In vivo;Torque;Animals;Force;Surgery;Muscles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341584&isnumber=10341342

