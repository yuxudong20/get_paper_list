Y. Zhang, M. Poggi and S. Mattoccia, "TemporalStereo: Efficient Spatial-Temporal Stereo Matching Network," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9528-9535, doi: 10.1109/IROS55552.2023.10341598.Abstract: We present TemporalStereo, a coarse-to-fine stereo matching network that is highly efficient, and able to effectively exploit the past geometry and context information to boost matching accuracy. Our network leverages sparse cost volume and proves to be effective when a single stereo pair is given. However, its peculiar ability to use spatio-temporal information across stereo sequences allows TemporalStereo to alleviate problems such as occlusions and reflective regions while enjoying high efficiency also in this latter case. Notably, our model - trained once with stereo videos - can run in both single-pair and temporal modes seamlessly. Experiments show that our network relying on camera motion is robust even to dynamic objects when running on videos. We validate TemporalStereo through extensive experiments on synthetic (SceneFlow, TartanAir) and real (KITTI 2012, KITTI 2015) datasets. Our model achieves state-of-the-art performance on any of these datasets. keywords: {Geometry;Costs;Dynamics;Cameras;Intelligent robots;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341598&isnumber=10341342

N. Mishra, P. Abbeel, X. Chen and M. Sieb, "Convolutional Occupancy Models for Dense Packing of Complex, Novel Objects," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9536-9542, doi: 10.1109/IROS55552.2023.10341466.Abstract: Dense packing in pick-and-place systems is an important feature in many warehouse and logistics applications. Prior work in this space has largely focused on planning algorithms in simulation, but real-world packing performance is often bottlenecked by the difficulty of perceiving 3D object geometry in highly occluded, partially observed scenes. In this work, we present a fully-convolutional shape completion model, F-CON, which can be easily combined with off-the-shelf planning methods for dense packing in the real world. We also release a simulated dataset, COB-3D-v2, that can be used to train shape completion models for real-word robotics applications, and use it to demonstrate that F-CON outperforms other state-of-the-art shape completion methods. Finally, we equip a real-world pick-and-place system with F-CON, and demonstrate dense packing of complex, unseen objects in cluttered scenes. Across multiple planning methods, F-CON enables substantially better dense packing than other shape completion methods. keywords: {Training;Geometry;Three-dimensional displays;Shape;Planning;Intelligent robots;Logistics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341466&isnumber=10341342

L. Wang et al., "Real-Time Video Inpainting for RGB-D Pipeline Reconstruction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9543-9550, doi: 10.1109/IROS55552.2023.10341971.Abstract: This paper presents a Video Inpainting algorithm that enables monocular-camera-laser-based pipeline inspection robots to capture both color and 3D information using only one video stream. Conventional monocular-camera-laser inspection methods are limited to capture either 2D color images or 3D point clouds since the laser tends to overexpose the actual color of the scanning area. We propose a real-time Video Inpainting method to solve this problem with minimal hardware needs that can be easily integrated with conventional pipeline profiling robots. The algorithm is accelerated by two components: a lightweight network that directly predicts the complete optical flow and simplifies the algorithm pipeline, and the Polar coordinate transformation, which significantly reduces the image processing compexity. Real-world experiments demonstrate that our online algorithm has comparable or better color estimation accuracy against state-of-the-art offline algorithms, while is capable of running at 23 frames per second (FPS) on a laptop computer with a resolution of 1024 × 1024 pixels. In addition, we verify that this method can be used for video pre-processing for downstream tasks that require high-quality visual inputs, such as Simultaneously Localization and Mapping (SLAM). To the best of our knowledge, this is the first real-time Video Inpainting algorithm that can be used for in-pipe environments, serving as an important building block for highly compact RGB-D inspection sensors and robots for the pipeline industry. keywords: {Visualization;Three-dimensional displays;Image color analysis;Robot kinematics;Pipelines;Streaming media;Inspection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341971&isnumber=10341342

S. -H. Kuo, T. -H. Wu, Z. -Y. Chen and K. -W. Chen, "MUFeat: Multi-Level CNN and Unsupervised Learning for Local Feature Detection and Description," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9551-9557, doi: 10.1109/IROS55552.2023.10342482.Abstract: Local feature detection and description are two essential steps in many visual applications. Most learned local feature methods require high-quality labeled data to achieve superior performance, but such labels are often expensive. To address this problem, we propose MUFeat, an unsupervised learning framework of jointly learning local feature detector and descriptor without requirement of ground-truth correspondences. MUFeat trains the network based on the putative matches from the pretrained model and two proposed unsupervised loss functions. Furthermore, the MUFeat framework includes a pyramidal feature hierarchy network to obtain keypoints and descriptors from feature maps. Experiments indicate that MUFeat outperforms most state-of-the-art supervised learning methods on image matching, medical image registration and visual localization tasks. keywords: {Location awareness;Training;Visualization;Image matching;Feature detection;Supervised learning;Detectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342482&isnumber=10341342

G. Tziafas and H. Kasaei, "Early or Late Fusion Matters: Efficient RGB-D Fusion in Vision Transformers for 3D Object Recognition," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9558-9565, doi: 10.1109/IROS55552.2023.10341422.Abstract: The Vision Transformer (ViT) architecture has established its place in computer vision literature, however, training ViTs for RGB-D object recognition remains an understudied topic, viewed in recent literature only through the lens of multi-task pretraining in multiple vision modalities. Such approaches are often computationally intensive, relying on the scale of multiple pretraining datasets to align RGB with 3D information. In this work, we propose a simple yet strong recipe for transferring pretrained ViTs in RGB-D domains for 3D object recognition, focusing on fusing RGB and depth representations encoded jointly by the ViT. Compared to previous works in multimodal Transformers, the key challenge here is to use the attested flexibility of ViTs to capture cross-modal interactions at the downstream and not the pretraining stage. We explore which depth representation is better in terms of resulting accuracy and compare early and late fusion techniques for aligning the RGB and depth modalities within the ViT architecture. Experimental results in the Washington RGB-D Objects dataset (ROD) demonstrate that in such RGB → RGB-D scenarios, late fusion techniques work better than most popularly employed early fusion. With our transfer baseline, fusion ViTs score up to 95.4% top-1 accuracy in ROD, achieving new state-of-the-art results in this benchmark. We further show the benefits of using our multimodal fusion baseline over unimodal feature extractors in a synthetic-to-real visual adaptation as well as in an open-ended lifelong learning scenario in the ROD benchmark, where our model outperforms previous works by a margin of >8%. Finally, we integrate our method with a robot framework and demonstrate how it can serve as a perception utility in an interactive robot learning scenario, both in simulation and with a real robot. keywords: {Training;Visualization;Three-dimensional displays;Computer architecture;Benchmark testing;Transformers;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341422&isnumber=10341342

L. Bian, P. Shi, W. Chen, J. Xu, L. Yi and R. Chen, "TransTouch: Learning Transparent Objects Depth Sensing Through Sparse Touches," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9566-9573, doi: 10.1109/IROS55552.2023.10341417.Abstract: Transparent objects are common in daily life. However, depth sensing for transparent objects remains a challenging problem. While learning-based methods can leverage shape priors to improve the sensing quality, the labor-intensive data collection in real world and the sim-to-real domain gap restrict these methods' scalability. In this paper, we propose a method to finetune a stereo network with sparse depth labels automatically collected using a probing system with tactile feedback. We present a novel utility function to evaluate the benefit of touches. By approximating and optimizing the utility function, we can optimize the probing locations given a fixed touching budget to better improve the network's performance on real objects. We further combine tactile depth supervision with a confidence-based regularization to prevent over-fitting during finetuning. To evaluate the effectiveness of our method, we construct a real-world dataset including both diffuse and transparent objects. Experimental results on this dataset show that our method can significantly improve real-world depth sensing accuracy, especially for transparent objects. keywords: {Learning systems;Shape;Scalability;Tactile sensors;Data collection;Sensors;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341417&isnumber=10341342

J. A. Abbasi, N. M. Imran, L. C. Das and M. Won, "WatchPed: Pedestrian Crossing Intention Prediction Using Embedded Sensors of Smartwatch," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9574-9581, doi: 10.1109/IROS55552.2023.10341607.Abstract: The pedestrian crossing intention prediction problem is to estimate whether or not the target pedestrian will cross the street. State-of-the-art techniques heavily depend on visual data acquired through the front camera of the ego-vehicle to make a prediction of the pedestrian's crossing intention. Hence, the efficiency of current methodologies tends to decrease notably in situations where visual input is imprecise, for instance, when the distance between the pedestrian and ego-vehicle is considerable or the illumination levels are inadequate. To address the limitation, in this paper, we present the design, implementation, and evaluation of the first-of-its-kind pedestrian crossing intention prediction model based on integration of motion sensor data gathered through the smartwatch (or smartphone) of the pedestrian. We propose an innovative machine learning framework that effectively integrates motion sensor data with visual input to enhance the predictive accuracy significantly, particularly in scenarios where visual data may be unreliable. Moreover, we perform an extensive data collection process and introduce the first pedestrian intention prediction dataset that features synchronized motion sensor data. The dataset comprises 255 video clips that encompass diverse distances and lighting conditions. We trained our model using the widely-used JAAD and our own datasets and compare the performance with a state-of-the-art model. The results demonstrate that our model outperforms the current state-of-the-art method, particularly in cases where the distance between the pedestrian and the observer is considerable (more than 70 meters) and the lighting conditions are inadequate. keywords: {Visualization;Wearable Health Monitoring Systems;Pedestrians;Lighting;Predictive models;Data collection;Motion detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341607&isnumber=10341342

G. Lu, "Object Detection Based on Raw Bayer Images," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9582-9589, doi: 10.1109/IROS55552.2023.10342008.Abstract: Bayer pattern is a widely used Color Filter Array (CFA) for digital image sensors, efficiently capturing different light wavelengths on different pixels without the need for a costly ISP pipeline. The resulting single-channel raw Bayer images offer benefits such as spectral wavelength sensitivity and low time latency. However, object detection based on Bayer images has been underexplored due to challenges in human observation and algorithm design caused by the discontinuous color channels in adjacent pixels. To address this issue, we propose the BayerDetect network, an end-to-end deep object detection framework that aims to achieve fast, accurate, and memory-efficient object detection. Unlike RGB color images, where each pixel encodes spectral context from adjacent pixels during ISP color interpolation, raw Bayer images lack spectral context. To enhance the spectral context, the BayerDetect network introduces a spectral frequency attention block, transforming the raw Bayer image pattern to the frequency domain. In object detection, clear object boundaries are essential for accurate bounding box predictions. To handle the challenges posed by alternating spectral channels and mitigate the influence of discontinuous boundaries, the BayerDetect network incorporates a spatial attention scheme that utilizes deformable convolutional kernels in multiple scales to explore spatial context effectively. The extracted convolutional features are then passed through a sparse set of proposal boxes for detection and classification. We conducted experiments on both public and self-collected raw Bayer images, and the results demonstrate the superb performance of the BayerDetect network in object detection tasks. keywords: {Image sensors;Sensitivity;Image color analysis;Frequency-domain analysis;Object detection;Sensor phenomena and characterization;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342008&isnumber=10341342

S. Liu and J. Zhu, "SDFMAP: Neural Signed Distance Fields for Mapping and Positioning in Real-Time," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9590-9597, doi: 10.1109/IROS55552.2023.10341429.Abstract: Neural surface reconstruction has recently gained a bit attention due to the promising result on scene rendering. Nevertheless, most of existing approaches either treat the camera parameters as the prior during training or indirectly estimate them through structure-from-motion. To tap the potential of implicit neural networks, we present a novel end-to-end neural network, termed SDFMAP, without any prior knowledge of the scene, like pre-computed camera parameters and pretrained geometric priors. Specifically, our method adopts a single multilayer perceptron to achieve simultaneously pose estimation and indoor scene reconstruction in real-time through learning the truncated signed distance function. Comparing to the recent neural implicit vSLAM systems, our approach achieves higher tracking speed via a lightweight network. Experiments on several challenging benchmark datasets show that our SDFMAP method achieves the state-of-the-art results on camera tracking and scene reconstruction. keywords: {Training;Surface reconstruction;Image color analysis;Robot vision systems;Pose estimation;Neural networks;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341429&isnumber=10341342

Y. Li et al., "LocalViT: Analyzing Locality in Vision Transformers," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9598-9605, doi: 10.1109/IROS55552.2023.10342025.Abstract: The aim of this paper is to study the influence of locality mechanisms in vision transformers. Transformers originated from machine translation and are particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking is a locality mechanism for infor-mation exchange within a local region. In this paper, locality mechanism is systematically investigated by carefully designed controlled experiments. We add locality to vision transformers into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to vision transformers with different architecture designs, which shows the generalization of the locality concept. For ImageNet2012 classification, the locality-enhanced transformers outperform the baselines Swin-T [1], DeiT-T [2] and PVT-T [3] by 1.0%, 2.6 % and 3.1 % with a negligible increase in the number of parameters and computational effort. Code is available at https://github.com/ofsoundof/LocalViT. keywords: {Convolutional codes;Lattices;Computer architecture;Performance gain;Transformers;Machine translation;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342025&isnumber=10341342

C. Buerkle, F. Oboril and K. -U. Scholl, "HistoDepth - Novel Depth Perception for Safe Collaborative Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9677-9684, doi: 10.1109/IROS55552.2023.10341816.Abstract: The trend towards Industry 4.0 demands an increasing flexibility of system configurations including more agile collaborative human-robot systems that can quickly adapt to new tasks and missions. At the same time, state-of-the-art safety solution for many industrial robots is the use of barriers around the robot to limit the exposure to human co-workers. As these barriers cannot be reconfigured easily, it considerably limits the flexibility of many robotic solutions. Thus, new more agile safety solutions are required, and these require new, robust perception systems that can ensure detection of all safety relevant objects. In this paper, we propose a novel depth perception approach called HistoDepth, which addresses this need. It uses depth sensors that are mounted at fixed, static positions outside of the workspace and tracks each pixel measurement statistically over time. This enables our solution to differentiate static scene elements from dynamic (potentially hazardous) elements and to enforce a robot safety maneuver upon detection of a critical object. Moreover, it can adapt automatically to changing noise or scene configurations without user input or the need for setup changes. Using an UR5 robot arm, we demonstrate that this solution can enable new agile and flexible safety concepts for collaborative robots. keywords: {Service robots;Collaboration;Robot sensing systems;Manipulators;Time measurement;Safety;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341816&isnumber=10341342

M. Dio, O. Demir, A. Trachte and K. Graichen, "Safe Active Learning and Probabilistic Design of Experiment for Autonomous Hydraulic Excavators," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9685-9690, doi: 10.1109/IROS55552.2023.10342052.Abstract: Recently, data-driven and hybrid control of hydraulic cylinders for excavator assistance functions have been in the focus of many research papers. To ensure an accurate behavior, data-driven controllers and models need a large amount of data to cover all relevant operation regions, which requires a time-consuming data generation process. In this work, we introduce two learning-based methods to enhance the efficiency of this procedure: a static learning method and an active learning method. Both methods reduce the amount of required data to learn a hydraulic inverse actuation model. Compared to previous collection methods, the required data was reduced by factor 7.5, while the information content of the dataset remains nearly the same. keywords: {Learning systems;Tracking loops;Process control;Hydraulic systems;Data collection;Excavation;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342052&isnumber=10341342

C. Zhang, Z. Huang, H. Guo, L. Qin, M. H. Ang and D. Rus, "SMART-Rain: A Degradation Evaluation Dataset for Autonomous Driving in Rain," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9691-9698, doi: 10.1109/IROS55552.2023.10342015.Abstract: Autonomous driving in the rain remains a challenge. One main problem is performance degradation caused by rain. This work introduces a new dataset to study this problem. Our dataset is collected from a full-scale vehicle equipped with a 3D LiDAR sensor and multiple forward-facing cameras under various rainy conditions. In addition, rainfall intensity is recorded in real-time from a rain sensor. The combination of sensor and rainfall intensity measurement is designed for studying algorithm performance under different levels of rainfall. In this work, in addition to presenting dataset creation details, we also introduce three degradation evaluation tasks with baseline results, including rainfall intensity estimation, LiDAR degradation estimation, and 2D object detection evaluation. This dataset, development kit, and baseline codes will be made available at https://smart-rain-dataset.github.io/ keywords: {Degradation;Rain;Laser radar;Three-dimensional displays;Urban areas;Estimation;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342015&isnumber=10341342

R. Jiao et al., "Learning Representation for Anomaly Detection of Vehicle Trajectories," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9699-9706, doi: 10.1109/IROS55552.2023.10342070.Abstract: Predicting the future trajectories of surrounding vehicles based on their history trajectories is a critical task in autonomous driving. However, when small crafted perturbations are introduced to those history trajectories, the resulting anomalous (or adversarial) trajectories can significantly mislead the future trajectory prediction module of the ego vehicle, which may result in unsafe planning and even fatal accidents. Therefore, it is of great importance to detect such anomalous trajectories of the surrounding vehicles for system safety, but few works have addressed this issue. In this work, we propose two novel methods for learning effective and efficient representations for online anomaly detection of vehicle trajectories. Different from general time-series anomaly detection, anomalous vehicle trajectory detection deals with much richer contexts on the road and fewer observable patterns on the anomalous trajectories themselves. To address these challenges, our methods exploit contrastive learning techniques and trajectory semantics to capture the patterns underlying the driving scenarios for effective anomaly detection under supervised and unsupervised settings, respectively. We conduct extensive experiments to demonstrate that our supervised method based on contrastive learning and unsupervised method based on reconstruction with semantic latent space can significantly improve the performance of anomalous trajectory detection in their corresponding settings over various baseline methods. We also demonstrate our methods' generalization ability to detect unseen patterns of anomalies. keywords: {Roads;Perturbation methods;Semantics;Trajectory;Safety;Planning;History},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342070&isnumber=10341342

J. Shen, Y. Luo, Z. Wan and Q. A. Chen, "Lateral-Direction Localization Attack in High-Level Autonomous Driving: Domain-Specific Defense Opportunity via Lane Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9707-9713, doi: 10.1109/IROS55552.2023.10342017.Abstract: Localization in high-level Autonomous Driving (AD) systems is highly security critical. Recently, researchers found that state-of-the-art Multi-Sensor Fusion (MSF) based localization is vulnerable to GPS spoofing, which can cause road hazards such as driving off road or onto the wrong way. In this work, we perform the first exploration of using Lane Detection (LD) to detect and correct deviations caused by such attacks and design a novel LD-based system-level defense, LD3. We evaluate LD3 on real-world sensor traces and find that it can achieve effective and timely detection against the state-of-the-art attack with 100% true positive rates and 0% false positive rates. Results show that LD3 can be highly effective at steering the AD vehicle to safely stop within the current traffic lane. We implement LD3 on 2 open-source AD systems and validate its end-to-end defense capability using an industry-grade AD simulator and also in the physical world with a real vehicle-sized AD R&D vehicle. keywords: {Location awareness;Lane detection;Roads;Robot sensing systems;Hazards;Security;Autonomous vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342017&isnumber=10341342

X. Liu, R. Jiao, Y. Wang, Y. Han, B. Zheng and Q. Zhu, "Safety-Assured Speculative Planning with Adaptive Prediction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9714-9721, doi: 10.1109/IROS55552.2023.10341530.Abstract: Recently significant progress has been made in vehicle prediction and planning algorithms for autonomous driving. However, it remains quite challenging for an autonomous vehicle to plan its trajectory in complex scenarios when it is difficult to accurately predict its surrounding vehicles' behaviors and trajectories. In this work, to maximize performance while ensuring safety, we propose a novel speculative planning framework based on a prediction-planning interface that quantifies both the behavior-level and trajectory-level uncertainties of surrounding vehicles. Our framework leverages recent prediction algorithms that can provide one or more possible behaviors and trajectories of the surrounding vehicles with probability estimation. It adapts those predictions based on the latest system states and traffic environment, and conducts planning to maximize the expected reward of the ego vehicle by considering the probabilistic predictions of all scenarios and ensure system safety by ruling out actions that may be unsafe in worst case. We demonstrate the effectiveness of our approach in improving system performance and ensuring system safety over other baseline methods, via extensive simulations in SUMO on a challenging multi-lane highway lane-changing case study. keywords: {Road transportation;Uncertainty;System performance;Prediction algorithms;Probabilistic logic;Planning;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341530&isnumber=10341342

R. Peddi and N. Bezzo, "A Decision Tree-based Monitoring and Recovery Framework for Autonomous Robots with Decision Uncertainties," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9722-9728, doi: 10.1109/IROS55552.2023.10342207.Abstract: Autonomous mobile robots (AMR) operating in the real world often need to make critical decisions that directly impact their own safety and the safety of their surroundings. Learning-based approaches for decision making have gained popularity in recent years, since decisions can be made very quickly and with reasonable levels of accuracy for many applications. These approaches, however, typically return only one decision, and if the learner is poorly trained or observations are noisy, the decision may be incorrect. This problem is further exacerbated when the robot is making decisions about its own failures, such as faulty actuators or sensors and external disturbances, when a wrong decision can immediately cause damage to the robot. In this paper, we consider this very case study: a robot dealing with such failures must quickly assess uncertainties and make safe decisions. We propose an uncertainty aware learning-based failure detection and recovery approach, in which we leverage Decision Tree theory along with Model Predictive Control to detect and explain which failure is compromising the system, assess uncertainties associated with the failure, and lastly, find and validate corrective controls to recover the system. Our approach is validated with simulations and real experiments on a faulty unmanned ground vehicle (UGV) navigation case study, demonstrating recovery to safety under uncertainties. keywords: {Uncertainty;Navigation;Decision making;Robot sensing systems;Safety;Sensors;Decision trees},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342207&isnumber=10341342

K. Feng, Z. Lu, J. Xu, H. Chen and Y. Lou, "A Safety Filter for Realizing Safe Robot Navigation in Crowds," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9729-9736, doi: 10.1109/IROS55552.2023.10342054.Abstract: It is challenging to realize the safe navigation of mobile robots in crowds. Most of the previous studies may lead to unsafe robot navigation in crowds, as safety guarantee is lacked. To solve this problem, we devise a safety filter (SF) that enables realization of safe robot navigation in crowds, and provides safety guarantees by verifying whether the optimal action recommended by an unsafe method is safe and, if not, corrects the action. The three main processes performed by the SF applied to given robot are (1) construction of the safe state constraints of the robot using a safe set; (2) construction of the safe action constraints of the robot based on discrete-time generalized velocity obstacles (DGVOs); and (3) determination of a feasible solution of the SF design problem, or, if none can be found, replacement of the above hard constraints with heuristic soft constraints. We used the SF with a reaction-based method and three learning-based methods in simulation experiments of random and non-random crowds, and the results showed that the SF decreases the collision rates and danger rates and thereby increases the success rates of these methods. We also deployed the SF with three learning-based methods on an mr1000 robot in real-world experiments, and the results showed that the SF enabled the robot using learning-based methods to navigate to its goal without colliding with humans. keywords: {Learning systems;Navigation;Safety;Mobile robots;Collision avoidance;Robots;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342054&isnumber=10341342

Y. Lyu, W. Luo and J. M. Dolan, "Risk-Aware Safe Control for Decentralized Multi-Agent Systems via Dynamic Responsibility Allocation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341720.Abstract: Decentralized control schemes are increasingly favored in various domains that involve multi-agent systems due to the need for computational efficiency as well as general applicability to large-scale systems. However, in the absence of an explicit global coordinator, it is hard for distributed agents to determine how to efficiently interact with others. In this paper, we present a risk-aware decentralized control framework that provides guidance on how much relative responsibility share (a percentage) an individual agent should take to avoid collisions with others while moving efficiently without direct communications. We propose a novel Control Barrier Function (CBF)-inspired risk measurement to characterize the aggregate risk agents face from potential collisions under motion uncertainty. We use this measurement to allocate responsibility shares among agents dynamically and develop risk-aware decentralized safe controllers. In this way, we are able to leverage the flexibility of robots with lower risk to improve the motion flexibility for those with higher risk, thus achieving improved collective safety. We demonstrate the validity and efficiency of our proposed approach through two examples: ramp merging in autonomous driving and a multi-agent position-swapping game. keywords: {Visualization;Uncertainty;Robot kinematics;Measurement uncertainty;Dynamics;Decentralized control;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341720&isnumber=10341342

D. S. Sundarsingh, J. Bhagiya, J. Chatrola and P. Jagtap, "Autonomous Exploration Using Ground Robots with Safety Guarantees," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9745-9750, doi: 10.1109/IROS55552.2023.10341929.Abstract: Autonomous exploration in an unknown environment is widely studied, and many exploration strategies exist. However, in most of the works, safety is not usually given top priority. The reason behind the violation of safety by most of the exploration algorithms in real-world applications is the ignorance of some or all of the following factors (i) the mathematical model of the robot, (ii) practical constraints on states (like constrained steering angle, speed, etc.) and inputs (like actuator saturation), and (iii) hardware constraints such as sampling time, sensor noise, modelling uncertainties, etc. In this work, we propose an autonomous exploration framework for the 2-D exploration problem that considers the factors above to provide safety guarantees for the robot and the environment. The effectiveness of this method is shown in a high-fidelity simulation of different robots with onboard sensors in different simulation environments and real-world implementation. keywords: {Actuators;Uncertainty;Robot sensing systems;Mathematical models;Hardware;Safety;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341929&isnumber=10341342

A. Yiğit, T. -S. Nguyen and C. Gosselin, "Exploiting the Kinematic Redundancy of a Backdrivable Parallel Manipulator for Sensing During Physical Human-Robot Interaction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9788-9793, doi: 10.1109/IROS55552.2023.10341495.Abstract: Robots need to adapt their behaviour while physically interacting with an operator to guarantee safety and provide intuitiveness. Inferring the intentions of the operator is a challenging problem that can be addressed by introducing sensors, in addition to motor encoders. Also, kinematic redundancy can be used to avoid issues such as singularities or mechanical interference, and the redundant coordinates can be controlled freely. In this work, we propose to use the redundant degrees of freedom to infer the intentions of an operator interacting with a backdrivable kinematically redundant parallel robot, without introducing any additional sensors. The proposed approach is based on the fact that, in mechanically backdrivable robots, the operator can control the redundant degrees of freedom, and this can be sensed using solely motor encoders through the solution of the forward kinematics. This approach is implemented to switch between a position controller and a controller that allows the operator to guide the robot freely thanks to gravity compensation. Experiments are carried out to compare this approach with an existing one and show that it improves intuitiveness during interaction by reducing false mode change detections. keywords: {Mechanical sensors;Parallel robots;Robot kinematics;Redundancy;Kinematics;Switches;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341495&isnumber=10341342

R. Nagaya, S. H. Seo and T. Kanda, "Measuring People's Boredom and Indifference to the Robot's Explanation in a Museum Scenario," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9794-9799, doi: 10.1109/IROS55552.2023.10341996.Abstract: To personalize the robot guide experience, the robot needs to detect a person's indifference and adjust its explanation toward the person's interest in topics. However, detecting the person's indifference is challenging in a museum, as we cannot use a bulky wearable or facial expression recognition due to unexpected light condition or standing position. We propose to observe people's behaviors and movements on detecting people's indifference. To prove its feasibility, we invited 11 participants to our in-lab museum-like environment. Our robot explains exhibits while videorecording the interaction. Then, we asked participants to watch the recordings and report when they felt bored or indifferent to the explanation. We labelled their movement and matched them to their report so that we know which behaviors and movements hint the person's indifference. We used the decision tree and random forest methods to understand the common pattern when people are indifferent during the explanation in a museum scenario. From our observation experiment, we found that if the listener nods their heads many times or looks at the exhibit for a long time, they are likely interested in the topic, fewer overall movements or looking elsewhere hint that the listener may be indifferent, and if the explanation goes longer than three minutes, the listener would be likely bored. keywords: {Face recognition;Museums;Real-time systems;Behavioral sciences;Recording;Decision trees;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341996&isnumber=10341342

F. Yang, S. Odashima, S. Masui and S. Jiang, "Is Weakly-Supervised Action Segmentation Ready for Human-Robot Interaction? No, Let's Improve It with Action-Union Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9800-9807, doi: 10.1109/IROS55552.2023.10341931.Abstract: Action segmentation plays an important role in enabling robots to automatically understand human activities. To train the action recognition model, while obtaining action labels for all frames is costly, annotating timestamp labels for weak supervision is cost-effective. However, existing methods may not fully utilize timestamp labels, which leads to insufficient performance. To alleviate this issue, we proposed a novel learning pattern in our training stage, which maximizes the probability of action union of surrounding timestamps for unlabeled frames. In our inference stage, we provided a new refinement solution to generate better hard-assigned action classes from soft-assigned predictions. Importantly, our methods are model-agnostic and can be applied to existing frameworks. On three commonly used action-segmentation data, our method outperforms previous timestamp-supervision methods and achieves new state-of-the-art performance. More-over, our method uses less than 1% of fully-supervised labels to obtain comparable or even better results. keywords: {Training;Human-robot interaction;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341931&isnumber=10341342

S. Lu, Y. Yoon and A. Feng, "Co-Speech Gesture Synthesis using Discrete Gesture Token Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9808-9815, doi: 10.1109/IROS55552.2023.10342027.Abstract: Synthesizing realistic co-speech gestures is an important and yet unsolved problem for creating believable motions that can drive a humanoid robot to interact and communicate with human users. Such capability will improve the impressions of the robots by human users and will find applications in education, training, and medical services. One challenge in learning the co-speech gesture model is that there may be multiple viable gesture motions for the same speech utterance. The deterministic regression methods can not resolve the conflicting samples and may produce over-smoothed or damped motions. We proposed a two-stage model to address this uncertainty issue in gesture synthesis by modeling the gesture segments as discrete latent codes. Our method utilizes RQ- VAE in the first stage to learn a discrete codebook consisting of gesture tokens from training data. In the second stage, a two-level autoregressive transformer model is used to learn the prior distribution of residual codes conditioned on input speech context. Since the inference is formulated as token sampling, multiple gesture sequences could be generated given the same speech input using top-k sampling. The quantitative results and the user study showed the proposed method outperforms the previous methods and is able to generate realistic and diverse gesture motions. keywords: {Training;Codes;Uncertainty;Speech coding;Motion segmentation;Training data;Medical services},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342027&isnumber=10341342

W. Nie, B. Chen, W. Wu, X. Xu, W. Ren and H. Liu, "WSCFER: Improving Facial Expression Representations by Weak Supervised Contrastive Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9816-9823, doi: 10.1109/IROS55552.2023.10342450.Abstract: The major challenge of Facial Expression Recog-nition (FER) is to learn class discriminative representations, and the existing works mainly address it by designing various classification networks from class level. However, learning representations at class level is limited due to the inconspicuous class discrimination among different facial expressions. Thus, in this paper, we propose a Weak Supervised Contrastive learning FER (WSCFER) method to improve facial expression representations by simultaneously learning instance-level representations which are highly complementary to the general class-level representations. Specifically, our proposed WSCFER consists of three components: a major task for FER classification, an auxiliary task for Weak Supervised Contrastive (WSC) learning which pulls augmented samples of the same image together while pushing apart instance samples from different classes, and a Partial Consistency Loss (PCL) for optimizing the two embedding spaces from both the class level and the instance level. We compare WSC with some state-of-the-art contrastive methods and find that it can efficiently learn instance-level representations but avoid overemphasizing irrelevant parts, which is crucial for FER. WSCFER achieves superior performance on several in-the-wild databases, and it also shows the promising potential for learning representations under noisy annotations. keywords: {Learning systems;Databases;Annotations;Reliability;Noise measurement;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342450&isnumber=10341342

L. Lastrico et al., "Expressing and Inferring Action Carefulness in Human-to-Robot Handovers," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9824-9831, doi: 10.1109/IROS55552.2023.10342111.Abstract: Implicit communication plays such a crucial role during social exchanges that it must be considered for a good experience in human-robot interaction. This work addresses implicit communication associated with the detection of physical properties, transport, and manipulation of objects. We propose an ecological approach to infer object characteristics from subtle modulations of the natural kinematics occurring during human object manipulation. Similarly, we take inspiration from human strategies to shape robot movements to be communica-tive of the object properties while pursuing the action goals. In a realistic HRI scenario, participants handed over cups - filled with water or empty - to a robotic manipulator that sorted them. We implemented an online classifier to differentiate careful/not careful human movements, associated with the cups' content. We compared our proposed “expressive” controller, which modulates the movements according to the cup filling, against a neutral motion controller. Results show that human kinematics is adjusted during the task, as a function of the cup content, even in reach-to-grasp motion. Moreover, the carefulness during the handover of full cups can be reliably inferred online, well before action completion. Finally, although questionnaires did not reveal explicit preferences from partici-pants, the expressive robot condition improved task efficiency. keywords: {Robot motion;Shape;Human-robot interaction;Transportation;Kinematics;Handover;Filling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342111&isnumber=10341342

H. Granado, R. J. Alitappeh, A. John, A. J. Van Opstal and A. Bernardino, "Learning Open-Loop Saccadic Control of a 3D Biomimetic Eye Using the Actor-Critic Algorithm," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9832-9838, doi: 10.1109/IROS55552.2023.10341913.Abstract: The application of reinforcement learning algorithms to robotics has increased over the last decade, especially for the control of robots with non-linear dynamics and a redundant number of degrees of freedom using classic control techniques. Here we study the control of a biomimetic robotic eye with three extraocular muscle pairs as a prime example. Using an actor-critic algorithm, this paper aims to link reinforcement learning to this control problem, and create a framework that will learn the open-loop control of saccadic movements of the robotic eye. The basis for the implemented control is inspired by the primate physiological pulsed control signal, which is generated, integrated and sent to the appropriate muscles to perform the saccade. The metric that evaluates the saccadic output is also inspired by the primate oculomotor system and is used to shape the reward function. This methodology was applied to a simplified 3D physical model of the human eye as a proof of concept. The algorithm managed to learn a saccadic control strategy in 3D. The trajectories obtained, have similar non-linear dynamics as those recorded in humans and their 3D rotational kinematics are constrained by Listing's law. keywords: {Measurement;Weight measurement;Solid modeling;Three-dimensional displays;Heuristic algorithms;Biomimetics;Biological system modeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341913&isnumber=10341342

P. Vanc, J. K. Behrens, K. Stepanova and V. Hlavac, "Communicating human intent to a robotic companion by multi-type gesture sentences," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9839-9845, doi: 10.1109/IROS55552.2023.10341944.Abstract: Human-Robot collaboration in home and industrial workspaces is on the rise. However, the communication between robots and humans is a bottleneck. Although people use a combination of different types of gestures to complement speech, only a few robotic systems utilize gestures for communication. In this paper, we propose a gesture pseudo-language and show how multiple types of gestures can be combined to express human intent to a robot (i.e., expressing both the desired action and its parameters - e.g., pointing to an object and showing that the object should be emptied into a bowl). The demonstrated gestures and the perceived tabletop scene (object poses detected by CosyPose) are processed in real-time) to extract the human's intent. We utilize behavior trees to generate reactive robot behavior that handles various possible states of the world (e.g., a drawer has to be opened before an object is placed into it) and recovers from errors (e.g., when the scene changes). Furthermore, our system enables switching between direct teleoperation of the end-effector and high-level operation using the proposed gesture sentences. The system is evaluated on increasingly complex tasks using a real 7-DoF Franka Emika Panda manipulator. Controlling the robot via action gestures lowered the execution time by up to 60%, compared to direct teleoperation. keywords: {Service robots;Collaboration;Switches;Real-time systems;End effectors;Behavioral sciences;Task analysis;Human-robot collaboration;Intent recognition;Scene awareness;Gesture detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341944&isnumber=10341342

D. C. Jeong et al., "MoEmo Vision Transformer: Integrating Cross-Attention and Movement Vectors in 3D Pose Estimation for HRI Emotion Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9846-9852, doi: 10.1109/IROS55552.2023.10342417.Abstract: Emotion detection presents challenges to intelligent human-robot interaction (HRI). Foundational deep learning techniques used in emotion detection are limited by information-constrained datasets or models that lack the necessary complexity to learn interactions between input data elements, such as the the variance of human emotions across different contexts. In the current effort, we introduce 1) MoEmo (Motion to Emotion), a cross-attention vision transformer (ViT) for human emotion detection within robotics systems based on 3D human pose estimations across various contexts, and 2) a data set that offers full-body videos of human movement and corresponding emotion labels based on human gestures and environmental contexts. Compared to existing approaches, our method effectively leverages the subtle connections between movement vectors of gestures and environmental contexts through the use of cross-attention on the extracted movement vectors of full-body human gestures/poses and feature maps of environmental contexts. We implement a cross-attention fusion model to combine movement vectors and environment contexts into a joint representation to derive emotion estimation. Leveraging our Naturalistic Motion Database, we train the MoEmo system to jointly analyze motion and context, yielding emotion detection that outperforms the current state-of-the-art. keywords: {Emotion recognition;Three-dimensional displays;Databases;Pose estimation;Human-robot interaction;Feature extraction;Transformers;Data mining;Task analysis;Context modeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342417&isnumber=10341342

T. M. Higgins and A. M. Fey, "How Do Humans Provide Motion Assistance for a Robotic Shape-Tracing Task?," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9853-9858, doi: 10.1109/IROS55552.2023.10341499.Abstract: Often in the field of haptic guidance, an important question is how the robotic device should assist some imperfect human movement. While many control strategies have been suggested to help improve human performance in particular tasks, structuring guidance in a generalizable way remains elusive. Many assistive controllers rely on predicting a user's goal movement or knowing some idealized trajectory a-priori but may fail to assist during an arbitrary task. In this study, we propose a ‘flipped’ approach to studying human-robot collaborative behavior - we ask humans to assist a robotic device whose movements are in some way imperfect. We conducted an experiment during which subjects assisted a haptic device performing a shape-following task autonomously but with different types of error-prone controllers. For each shape, we evaluated a simple trajectory-following controller as well as one with human-like motion constraints. We also evaluated the role of visual feedback on a user's ability to help the robot accomplish the unknown task. We found that the human was generally able to improve the robotic error in all trajectories when the robotic motion did not include gravity compensation; however, error reduction was primarily in the vertical direction. When the robotic controller included gravity compensation, the human user was not able to improve errors significantly, except for vertical errors when provided visual feedback. In the no visual feedback conditions, the human user contributed to significantly greater error for most paths compared to a robot with gravity compensation, indicating an inability to provide assistance in that case. keywords: {Performance evaluation;Robot motion;Visualization;Shape;Trajectory;Haptic interfaces;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341499&isnumber=10341342

P. Howell, J. Kolb, Y. Liu and H. Ravichandar, "The Effects of Robot Motion on Comfort Dynamics of Novice Users in Close-Proximity Human-Robot Interaction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9859-9864, doi: 10.1109/IROS55552.2023.10341487.Abstract: Effective and fluent close-proximity human-robot interaction requires understanding how humans get habituated to robots and how robot motion affects human comfort. While prior work has identified humans' preferences over robot motion characteristics and studied their influence on comfort, we are yet to understand how novice first-time robot users get habituated to robots and how robot motion impacts the dynamics of comfort over repeated interactions. To take the first step towards such understanding, we carry out a user study to investigate the connections between robot motion and user comfort and habituation. Specifically, we study the influence of workspace overlap, end-effector speed, and robot motion legibility on overall comfort and its evolution over repeated interactions. Our analyses reveal that workspace overlap, in contrast to speed and legibility, has a significant impact on users' perceived comfort and habituation. In particular, lower workspace overlap leads to users reporting significantly higher overall comfort, lower variations in comfort, and fewer fluctuations in comfort levels during habituation. keywords: {Robot motion;Measurement;Fluctuations;Dynamics;Sociology;Human-robot interaction;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341487&isnumber=10341342

H. Su and Y. Jia, "Estimating Human Comfort Levels in Autonomous Vehicles Based on Vehicular Behaviors and Physiological Signals," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9865-9870, doi: 10.1109/IROS55552.2023.10342480.Abstract: In this study, we proposed a dynamic model that could quantify human comfort in autonomous vehicles (AVs) based on vehicular behaviors and a Kalman filter (KF) based approach to further refine comfort level estimation by leveraging physiological signals. The dynamic model could capture the dynamics in human comfort when the passenger was exposed to a continuous sequence of vehicular behaviors during an AV journey. The KF-based comfort estimation approach could fuse comfort level estimations based on physiological signals and the dynamic model. A simulator-based user study was conducted to evaluate the comfort estimation approaches in which the participants experienced a set of virtual AV journeys on a high-fidelity driving simulator with 6-degree-of-freedom motions. Experimental results show that the proposed approaches could quantify human comfort levels and the KF-based approach outperforms the others. keywords: {Fuses;Heuristic algorithms;Dynamics;Estimation;Particle measurements;Physiology;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342480&isnumber=10341342

R. Thakker et al., "EELS: Towards Autonomous Mobility in Extreme Terrain with a Versatile Snake Robot with Resilience to Exteroception Failures," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9886-9893, doi: 10.1109/IROS55552.2023.10341448.Abstract: The discovery of ocean worlds such as Enceladus, Titan, and Europa motivates the development of versatile autonomous mobility systems to enable the next era of space exploration where there is large uncertainty in terrain specifications due to a lack of prior surface reconnaissance missions. To explore these environments, we propose Exobiology Extant Life Surveyor (EELS): the first large-scale (4 lm long with 400 Nm peak torque) snake robot. The large scale is achieved by using a screw-based active skin mechanism to decouple motion and shape control. Autonomous mobility for such a system remains an open problem due to its many Degrees of Freedom (DoFs), complex terrain interactions, and intermittent localization failures in GPS-denied perceptually degraded environments due to the presence of fog, dust, featureless terrains, etc. We propose NEO, an autonomy architecture that scales to large DoFs to generate a versatile set of gaits to achieve mobility in unknown extreme environments. We also discuss the resilience capabilities of NEO that achieves closed-loop tracking performance by leveraging exteroception when available but can also operate with proprioception only, leading to resiliency against localization failures via graceful degradation in performance rather than unsafe behaviors. A quantitative hardware evaluation of exteroceptive leader-follower gait is performed indoors on synthetic ice along with qualitative results of field deployment of the proprioceptive leader-follower and sidewinding gaits in extreme environments of icy and sandy terrains with mobility-stressing elements such as trenches, undulations, and steep slopes (up to 35 degrees). We present a set of lessons learned from field deployments with a summary of challenges and open research problems. Video: www.rohanthakker.in/eels-neo-autonomy.html keywords: {Location awareness;Sea surface;Uncertainty;Torque;Shape control;Saturn;Snake robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341448&isnumber=10341342

C. D. Alvarenga and S. Carpin, "Track, Stop, and Eliminate: an Algorithm to Solve Stochastic Orienteering Problems Using MCTS," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9894-9901, doi: 10.1109/IROS55552.2023.10341892.Abstract: We present a novel algorithm to solve the stochastic orienteering problem with chance constraints that combines Monte Carlo Tree Search (MCTS) with a best arm identification (BAI) algorithm. This method extends our recently proposed solution that builds a search planning tree considering both an objective function to maximize, as well as a chance constraint on the failure probability, i.e., the probability of violating the assigned budget constraint. By combining these two approaches, we obtain a new planner that tunes the amount of tree search at run time. Extensive simulation results on our benchmark problems show that the new approach is significantly faster than the previous one, while incurring in just marginal decrements in terms of performance. keywords: {Monte Carlo methods;Simulation;Benchmark testing;Search problems;Linear programming;Planning;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341892&isnumber=10341342

G. Billings, A. Phung and R. Camilli, "DVL-Based Odometry for Autonomous Underwater Gliders," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9910-9917, doi: 10.1109/IROS55552.2023.10341554.Abstract: Autonomous underwater gliders (AUGs) are capable of traversing basin scale distances but lack sufficient localization accuracy to operate without periodic surfacing to obtain GPS fixes and constrain localization drift. Conventionally, AUGs use a dynamic flight model with depth averaged current correction (DACC) to dead-reckon their position while subsurface. However, these flight models become unstable at shallow pitch angles and DACC is inaccurate in dynamic and highly sheared water column currents. We present a method and preliminary results from field trials for improved real-time AUG localization using a Doppler Velocity Logger to estimate vehicle velocity and dynamically profile water column currents. This improved localization reduces the need for periodic surfacing, while independence from a dynamic flight model makes it particularly suited for shallow-yo profile missions. keywords: {Location awareness;Surveys;Uncertainty;Computational modeling;Sea measurements;Real-time systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341554&isnumber=10341342

E. R. Damm, J. M. Gregory, E. S. Lancaster, F. A. Sanchez, D. M. Sahu and T. M. Howard, "Terrain-Aware Kinodynamic Planning with Efficiently Adaptive State Lattices for Mobile Robot Navigation in Off-Road Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9918-9925, doi: 10.1109/IROS55552.2023.10341537.Abstract: To safely traverse non-flat terrain, robots must account for the influence of terrain shape in their planned motions. Terrain-aware motion planners use an estimate of the vehicle roll and pitch as a function of pose, vehicle suspension, and ground elevation map to weigh the cost of edges in the search space. Encoding such information in a traditional two-dimensional cost map is limiting because it is unable to capture the influence of orientation on the roll and pitch estimates from sloped terrain. The research presented herein addresses this problem by encoding kinodynamic information in the edges of a recombinant motion planning search space based on the Efficiently Adaptive State Lattice (EASL). This approach, which we describe as a Kinodynamic Efficiently Adaptive State Lattice (KEASL), differs from the prior representation in two ways. First, this method uses a novel encoding of velocity and acceleration constraints and vehicle direction at expanded nodes in the motion planning graph. Second, this approach describes additional steps for evaluating the roll, pitch, constraints, and velocities associated with poses along each edge during search in a manner that still enables the graph to remain recombinant. Velocities are computed using an iterative bidirectional method using Eulerian integration that more accurately estimates the duration of edges that are subject to terrain-dependent velocity limits. Real-world experiments on a Clearpath Robotics Warthog Unmanned Ground Vehicle were performed in a non-flat, unstructured environment. Results from 2093 planning queries from these experiments showed that KEASL provided a more efficient route than EASL in 83.72% of cases when EASL plans were adjusted to satisfy terrain-dependent velocity constraints. An analysis of relative runtimes and differences between planned routes is additionally presented. These results reinforce the importance of considering kinodynamic constraints for motion planning in non-flat environments and illustrate how such information can be encoded in an adaptive recombinant motion planning search space. keywords: {Space vehicles;Suspensions (mechanical systems);Costs;Navigation;Shape;Government;Lattices},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341537&isnumber=10341342

N. Bauschmann, D. A. Duecker, T. L. Alff and R. Seifried, "Evaluation of Underwater AprilTag Localization for Highly Agile Micro Underwater Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9926-9932, doi: 10.1109/IROS55552.2023.10341764.Abstract: Accurate localization systems are still a bottleneck for Unmanned Underwater Vehicles (UUVs). In recent years, fiducial markers have become a readily available, low-cost option. However, an in-depth analysis of marker detection accuracy in the underwater domain has yet to be performed. We propose a methodology to evaluate fiducial marker systems, namely the popular AprilTag system, in experiments. Our study especially focuses on aspects crucial for highly agile micro underwater robots, such as dynamic motions and the calibration medium. This class of robots is typically extensively studied in research tanks which motivates a first focus on clear-water settings. However, the proposed method and the findings can be transferred to similar scenarios. We demonstrate the importance of calibrating underwater and that the detection accuracy decreases linearly with camera distance and could therefore easily be compensated for. Moreover, we identify a suitable camera that maximizes the detection rate during highly dynamic motions. In sum, this work is an initial step towards application-relevant design strategies for designing low-cost, accessible localization systems for agile, mobile robots. keywords: {Location awareness;Autonomous underwater vehicles;Visualization;Robot vision systems;Dynamics;Cameras;Fiducial markers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341764&isnumber=10341342

B. Yu, R. Tibbetts, T. Barna, A. Morales, I. Rekleitis and M. J. Islam, "Weakly Supervised Caveline Detection for AUV Navigation Inside Underwater Caves," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9933-9940, doi: 10.1109/IROS55552.2023.10342435.Abstract: Underwater caves are challenging environments that are crucial for water resource management, and for our understanding of hydro-geology and history. Mapping underwater caves is a time-consuming, labor-intensive, and hazardous operation. For autonomous cave mapping by underwater robots, the major challenge lies in vision-based estimation in the complete absence of ambient light, which results in constantly moving shadows due to the motion of the camera-light setup. Thus, detecting and following the caveline as navigation guidance is paramount for robots in autonomous cave mapping missions. In this paper, we present a computationally light caveline detection model based on a novel Vision Transformer (ViT)-based learning pipeline. We address the problem of scarce annotated training data by a weakly supervised formulation where the learning is reinforced through a series of noisy predictions from intermediate sub-optimal models. We validate the utility and effectiveness of such weak supervision for caveline detection and tracking in three different cave locations: USA, Mexico, and Spain. Experimental results demonstrate that our proposed model, CL-ViT, balances the robustness-efficiency trade-off, ensuring good generalization performance while offering 10+ FPS on single-board (Jetson TX2) devices. keywords: {Navigation;Computational modeling;Pipelines;Training data;Predictive models;Transformers;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342435&isnumber=10341342

T. Manzini, R. Murphy, D. Merrick and J. Adams, "Wireless Network Demands of Data Products from Small Uncrewed Aerial Systems at Hurricane Ian," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9941-9946, doi: 10.1109/IROS55552.2023.10342413.Abstract: Data collected at Hurricane Ian (2022) quantifies the demands that small uncrewed aerial systems (UAS), or drones, place on the network communication infrastructure and identifies gaps in the field. Drones have been increasingly used since Hurricane Katrina (2005) for disaster response, however getting the data from the drone to the appropriate decision makers throughout incident command in a timely fashion has been problematic. These delays have persisted even as countries such as the USA have made significant investments in wireless infrastructure, rapidly deployable nodes, and an increase in commercial satellite solutions. Hurricane Ian serves as a case study of the mismatch between communications needs and capabilities. In the first four days of the response, nine drone teams flew 34 missions under the direction of the State of Florida FL-UAS1, generating 636GB of data. The teams had access to six different wireless communications networks but had to resort to physically transferring data to the nearest intact emergency operations center in order to make the data available to the relevant agencies. The analysis of the mismatch contributes a model of the drone data-to-decision workflow in a disaster and quantifies wireless network communication requirements throughout the workflow in five factors. Four of the factors-availability, bandwidth, burstiness, and spatial distribution-were previously identified from analyses of Hurricanes Harvey (2017) and Michael (2018). This work adds upload rate as a fifth attribute. The analysis is expected to improve drone design and edge computing schemes as well as inform wireless communication research and development. keywords: {Cloud computing;Satellites;Wireless networks;Bandwidth;Streaming media;Autonomous aerial vehicles;Hurricanes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342413&isnumber=10341342

S. Gaurav et al., "Robot Learning to Mop Like Humans Using Video Demonstrations," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9947-9954, doi: 10.1109/IROS55552.2023.10342231.Abstract: Though mopping the floor is a mundane and tedious daily task, enabling robots to perform it comparably to humans remains a challenge. Hand-coding desired mopping behaviors for variable surfaces and situations is particularly difficult. In this paper, we develop a robotic system for mopping the floor by mimicking the human behavior demonstrated in videos. Our baseline robotic system uses traditional computer vision techniques for tracking and inverse kinematics. Our proposed robot mop learning system comprises advanced computer vision techniques, Time Contrastive Network (TCN), and reinforcement learning. Using these, we devise a reward function for the mopping task. We use a Universal 10e robotic arm attached to a mop to perform the mopping task and a first-person camera attached on top of the robotic arm to provide feedback for robotic learning. We evaluate our proposed robot mop learning system's imitative similarity using optical flow, distance in mop location, and force applied to the floor, as well as cleaning efficiency using a white glove method. keywords: {Computer vision;Robot vision systems;Reinforcement learning;Manipulators;Robot learning;Behavioral sciences;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342231&isnumber=10341342

K. Norman, D. Butterfield and J. G. Mangelson, "AcTag: Opti-Acoustic Fiducial Markers for Underwater Localization and Mapping," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9955-9962, doi: 10.1109/IROS55552.2023.10341885.Abstract: Fiducial markers are important tools for robotic navigation and imaging, enabling accurate localization and tracking of objects in challenging environments. In this paper, we present AcTag, a new fiducial marker design for use underwater with imaging sonar and cameras, as well as a method for the detection of AcTags within acoustic images. High amounts of noise and a nonlinear projection model make it difficult to use imaging sonar in autonomous localization and mapping. In order to expand the use of imaging sonar in autonomous underwater vehicles, our markerb design and detection algorithm for sonar images facilitate the identification of four unique landmarks per tag, and provide relative range and azimuth values to each landmark. We evaluate our marker and detection algorithm with simulated and real-world sonar data, reporting on the false positive and true positive rates, as well as the estimated error for the range and azimuth estimates per landmark. We also release an open-source library for generating tag families and detecting the tags. keywords: {Location awareness;Azimuth;Aluminum;Sonar;Sonar navigation;Cameras;Fiducial markers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341885&isnumber=10341342

A. K. Srinivasan, S. Singh, G. Gutow, H. Choset and B. Vundurthy, "Multi-Agent Collective Construction Using 3D Decomposition," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9963-9969, doi: 10.1109/IROS55552.2023.10341964.Abstract: Consider a Multi-Agent Collective Construction (MACC) problem that aims to generate a plan for fictitious cubic robots to build a three-dimensional structure comprised of cubic blocks. These cubic robots can carry one cubic block at a time; robots may move left, right, forwards, backward, or climb up or down one block. To construct structures taller than one cube, the robots must build supporting scaffolding made of blocks and remove the scaffolding once the structure is built. Prior works sought to create a planner that considered the structure as one monolithic assembly, which becomes intractable for larger workspaces and complex structures. To this end, we present a decomposition algorithm that breaks the structure into substructures that can be planned for independently. We use Mixed Integer Linear Programming (MILP) to plan for each of these substructures and then aggregate the solutions to construct the entire structure. Extensive testing on 200 randomly generated structures shows an order of magnitude improvement in the solution computation time compared to an MILP approach without decomposition. Finally, we leverage the independence between substructures to detect which substructures can be built in parallel. keywords: {Measurement;Three-dimensional displays;Aggregates;Parallel processing;Mixed integer linear programming;Optimization;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341964&isnumber=10341342

K. Kawaharazuka, T. Makabe, K. Okada and M. Inaba, "Daily Assistive Modular Robot Design Based on Multi-Objective Black-Box Optimization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9970-9977, doi: 10.1109/IROS55552.2023.10342041.Abstract: The range of robot activities is expanding from industries with fixed environments to diverse and changing environments, such as nursing care support and daily life support. In particular, autonomous construction of robots that are personalized for each user and task is required. Therefore, we develop an actuator module that can be reconfigured to various link configurations, can carry heavy objects using a locking mechanism, and can be easily operated by human teaching using a releasing mechanism. Given multiple target coordinates, a modular robot configuration that satisfies these coordinates and minimizes the required torque is automatically generated by Tree-structured Parzen Estimator (TPE), a type of black-box optimization. Based on the obtained results, we show that the robot can be reconfigured to perform various functions such as moving monitors and lights, serving food, and so on. keywords: {Actuators;Torque;Service robots;Robot kinematics;Education;Closed box;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342041&isnumber=10341342

H. Li, W. Yu, T. Zhang and P. M. Wensing, "A Unified Perspective on Multiple Shooting In Differential Dynamic Programming," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9978-9985, doi: 10.1109/IROS55552.2023.10342217.Abstract: Differential Dynamic Programming (DDP) is an efficient computational tool for solving nonlinear optimal control problems. It was originally designed as a single shooting method and thus is sensitive to the initial guess supplied. This work considers the extension of DDP to multiple shooting (MS), improving its robustness to initial guesses. A novel derivation is proposed that accounts for the defect between shooting segments during the DDP backward pass, while still maintaining quadratic convergence locally. The derivation enables unifying multiple previous MS algorithms, and opens the door to many smaller algorithmic improvements. A penalty method is introduced to strategically control the step size, further improving the convergence performance. An adaptive merit function and a more reliable acceptance condition are employed for globalization. The effects of these improvements are benchmarked for trajectory optimization with a quadrotor, an acrobot, and a manipulator. MS-DDP is also demonstrated for use in Model Predictive Control (MPC) for dynamic jumping with a quadruped robot, showing its benefits over a single shooting approach. keywords: {Heuristic algorithms;Optimal control;Robustness;Dynamic programming;Quadrupedal robots;Trajectory optimization;Manipulator dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342217&isnumber=10341342

R. Liu, G. Shi and P. Tokekar, "Data-Driven Distributionally Robust Optimal Control with State-Dependent Noise," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 9986-9991, doi: 10.1109/IROS55552.2023.10342392.Abstract: Distributionally Robust Optimal Control (DROC) is a technique that enables robust control in a stochastic setting when the true distribution is not known. Traditional DROC approaches require given ambiguity sets or a KL divergence bound to represent the distributional uncertainty. These may not be known a priori and may require hand-crafting. In this paper, we lift this assumption by introducing a data-driven technique for estimating the uncertainty and a bound for the KL divergence. We call this technique D3ROC. To evaluate the effectiveness of our approach, we consider a navigation problem for a car-like robot with unknown noise distributions. The results demonstrate that D3ROC provides robust and efficient control policies that outperform the iterative Linear Quadratic Gaussian (iLQG) control. The results also show the effectiveness of our proposed approach in handling different noise distributions. keywords: {Robust control;Maximum likelihood estimation;Uncertainty;Navigation;Robot kinematics;Optimal control;Iterative methods},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342392&isnumber=10341342

W. Li, Y. Zou, Z. Yi, H. Wu and Y. Pan, "A Mangasarian-Soldov Function Based Neural Network for Constrained Control of Parallel and Serial Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-7, doi: 10.1109/IROS55552.2023.10341741.Abstract: Zeroing neural networks (ZNNs) are powerful alternatives to solving quadratic programming (QP) for constrained control of parallel and serial robots. A recent study showed that a ZNN solver designed based on a perturbed Fischer-Burmeister function (pFB-ZNN) achieves more satisfactory performance than other ZNN solvers. The pFB-ZNN solver suffers from manual tuning of an extra hyper-parameter and may encounter residual error peaks. To tackle the above issues, this paper proposes a new Mangasarian-Solodov function-based ZNN (MS-ZNN) solver. The MS-ZNN solver has no extra hyper-parameter to be tuned and it can eliminate residual error peaks appeared in the pFB-ZNN solver, ensuring a higher solution accuracy. Mathematically, this paper details the design and convergence analysis of the MS-ZNN solver, demonstrating its convergence in the sense of Lyapunov. Numerical studies are comparatively performed, verifying the effectiveness and superiority of the MS-ZNN solver. The MS-ZNN solver is then successfully applied to kinematic control of a parallel robot and a serial robot under joint constraints. Both simulative and experimental results demonstrate that the proposed MS-ZNN solver is applicable to constrained control of parallel and serial robots with joint-limit avoidance achieved. keywords: {Parallel robots;Neural networks;Robot control;Manuals;Kinematics;Robot sensing systems;Quadratic programming},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341741&isnumber=10341342

A. Srour, A. Franchi and P. R. Giordano, "Controller and Trajectory Optimization for a Quadrotor UAV with Parametric Uncertainty," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-7, doi: 10.1109/IROS55552.2023.10341739.Abstract: In this work, we exploit the recent notion of closed-loop state sensitivity to critically compare three typical controllers for a quadrotor UAV with the goal of evaluating the impact of controller choice, gain tuning and shape of the reference trajectory in minimizing the sensitivity of the closed-loop system against uncertainties in the model parameters. To this end, we propose a novel optimization problem that takes into account both the shape of the reference trajectory and the controller gains. We then run a large statistical campaign for comparing the performance of the three controllers which provides some interesting insight for the goal of increasing closed-loop robustness against parametric uncertainties. keywords: {Uncertainty;Sensitivity;Shape;Statistical analysis;Autonomous aerial vehicles;Robustness;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341739&isnumber=10341342

B. B. Carlos, M. Williams and B. Pelourdeau, "Real-Time NMPC for an Automated Valet Parking with Load-Based Safety Constraints and a Path-Parametric Model," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10006-10013, doi: 10.1109/IROS55552.2023.10342085.Abstract: As global living standards continue to rise and urbanization accelerates, cities worldwide face high demands for parking. To tackle this issue and foster the efficient use of limited parking capacity, automated valet parking (AVP) solutions have emerged as a resource optimization and hassle-free alternative. This paper addresses the problem of reducing retrieval time in AVP systems while ensuring the integrity of both the robot and the car through critical adherence conditions. The key novelty of our approach is that we dovetail these conditions as acceleration constraints into a multi-stage motion generation framework formulated as a real-time nonlinear model predictive control (NMPC) scheme that approximates time-optimal behavior by maximizing progress on a path. The NMPC generates trajectories based on the motion direction, allowing for convenient parameterization of controller instances and unlimited parking maneuvers. Thanks to high-performance software implementations, the resulting quadratic subprograms can be solved in the order of milliseconds. Comparative analysis against the currently implemented pure pursuit algorithm and an experimental validation on Stanley Robotics' robot have shown a 31% time improvement following a path and constraint satisfaction in the loaded scenario for the proposed controller. keywords: {Urban areas;Real-time systems;Software;Trajectory;Safety;Standards;Pursuit algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342085&isnumber=10341342

T. Marauli, D. H. Salunkhe, H. Gattringer, A. Müller, D. Chablat and P. Wenger, "Time-Optimal Point-To-Point Motion Planning and Assembly Mode Change of Cuspidal Manipulators: Application to 3R and 6R Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10014-10019, doi: 10.1109/IROS55552.2023.10341420.Abstract: The kinematics of cuspidal 3R regional robots was studied extensively in the past. Moreover, certain industrial 6R robots were found to be cuspidal (e.g. Fanuc CRX series, Kinova GEN2), which makes cuspidal robots finally interesting for practical applications. This necessitates optimal trajectory planning, respecting the dynamics and technical limits of the particular robot. In this paper, a method for singularity-free time-optimal point-to-point trajectory (PtP) trajectory planning is proposed. As a special case, this method is applicable to time-optimal singularity-free assembly mode changing. Results are shown for 3R robots and a 6R Fanuc CRX10iA/L. keywords: {Trajectory planning;Service robots;Welding;Kinematics;Manipulators;Trajectory;Planning;Singularities;Cuspidality;Optimal Control;Dynamics;Kinematics;Industrial Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341420&isnumber=10341342

J. Shrestha, S. Idoko, B. Sharma and A. K. Singh, "End-to-End Learning of Behavioural Inputs for Autonomous Driving in Dense Traffic," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10020-10027, doi: 10.1109/IROS55552.2023.10341439.Abstract: Trajectory sampling in the Frenet(road-aligned) frame, is one of the most popular methods for motion planning of autonomous vehicles. It operates by sampling a set of behavioral inputs, such as lane offset and forward speed, before solving a trajectory optimization problem conditioned on the sampled inputs. The sampling is handcrafted based on simple heuristics, does not adapt to driving scenarios, and is oblivious to the capabilities of downstream trajectory planners. In this paper, we propose an end-to-end learning of behavioral input distribution from expert demonstrations or in a self-supervised manner. We embed a novel differentiable trajectory optimizer as a layer in neural networks, allowing us to update behavioral inputs by considering the optimizer's feedback. Moreover, our end-to-end approach also ensures that the learned behavioral inputs aid the convergence of the optimizer. We improve the state-of-the-art in the following aspects. First, we show that learned behavioral inputs substantially decrease collision rate while improving driving efficiency over handcrafted approaches. Second, our approach outperforms model predictive control methods based on sampling-based optimization. keywords: {Neural networks;Behavioral sciences;Planning;Autonomous vehicles;Trajectory optimization;Intelligent robots;Predictive control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341439&isnumber=10341342

H. Ma, T. Zhang, Y. Wu, F. P. Calmon and N. Li, "Gaussian Max-Value Entropy Search for Multi-Agent Bayesian Optimization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10028-10035, doi: 10.1109/IROS55552.2023.10341675.Abstract: We study the multi-agent Bayesian optimization (BO) problem, where multiple agents maximize a black-box function via iterative queries. We focus on Entropy Search (ES), a sample-efficient BO algorithm that selects queries to maximize the mutual information about the maximum of the black-box function. One of the main challenges of ES is that calculating the mutual information requires computationallycostly approximation techniques. For multi-agent BO problems, the computational cost of ES is exponential in the number of agents. To address this challenge, we propose the Gaussian Max-value Entropy Search, a multi-agent BO algorithm with favorable sample and computational efficiency. The key to our idea is to use a normal distribution to approximate the function maximum and calculate its mutual information accordingly. The resulting approximation allows queries to be cast as the solution of a closed-form optimization problem which, in turn, can be solved via a modified gradient ascent algorithm and scaled to a large number of agents. We demonstrate the effectiveness of Gaussian max-value Entropy Search through numerical experiments on standard test functions and real-robot experiments on the source seeking problem. Results show that the proposed algorithm outperforms the multi-agent BO baselines in the numerical experiments and can stably seek the source with a limited number of noisy observations on real robots. keywords: {Closed box;Gaussian distribution;Approximation algorithms;Search problems;Entropy;Computational efficiency;Bayes methods},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341675&isnumber=10341342

L. Vanroye, A. Sathya, J. De Schutter and W. Decré, "FATROP: A Fast Constrained Optimal Control Problem Solver for Robot Trajectory Optimization and Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10036-10043, doi: 10.1109/IROS55552.2023.10342336.Abstract: Trajectory optimization is a powerful tool for robot motion planning and control. State-of-the-art general-purpose nonlinear programming solvers are versatile, handle constraints effectively and provide a high numerical robustness, but they are slow because they do not fully exploit the optimal control problem structure at hand. Existing structure-exploiting solvers are fast, but they often lack techniques to deal with nonlinearity or rely on penalty methods to enforce (equality or inequality) path constraints. This work presents FATROP: a trajectory optimization solver that is fast and benefits from the salient features of general-purpose nonlinear optimization solvers. The speed-up is mainly achieved through the integration of a specialized linear solver, based on a Riccati recursion that is generalized to also support stagewise equality constraints. To demonstrate the algorithm's potential, it is bench-marked on a set of robot problems that are challenging from a numerical perspective, including problems with a minimum-time objective and no-collision constraints. The solver is shown to solve problems for trajectory generation of a quadrotor, a robot manipulator and a truck-trailer problem in a few tens of milliseconds. The algorithm's C++-code implementation accompanies this work as open source software, released under the GNU Lesser General Public License (LGPL). This software framework may encourage and enable the robotics community to use trajectory optimization in more challenging applications. keywords: {Robot motion;Software algorithms;Optimal control;Programming;Manipulators;Robustness;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342336&isnumber=10341342

P. Pueyo, E. Montijano, A. C. Murillo and M. Schwager, "CineTransfer: Controlling a Robot to Imitate Cinematographic Style from a Single Example," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10044-10049, doi: 10.1109/IROS55552.2023.10342280.Abstract: This work presents CineTransfer, an algorithmic framework that drives a robot to record a video sequence that mimics the cinematographic style of an input video. We propose features that abstract the aesthetic style of the input video, so the robot can transfer this style to a scene with visual details that are significantly different from the input video. The framework builds upon CineMPC, a tool that allows users to control cinematographic features, like subjects' position on the image and the depth of field, by manipulating the intrinsics and extrinsics of a cinematographic camera. However, CineMPC requires a human expert to specify the desired style of the shot (composition, camera motion, zoom, focus, etc). CineTransfer bridges this gap, aiming a fully autonomous cinematographic platform. The user chooses a single input video as a style guide. CineTransfer extracts and optimizes two important style features, the composition of the subject in the image and the scene depth of field, and provides instructions for CineMPC to control the robot to record an output sequence that matches these features as closely as possible. In contrast with other style transfer methods, our approach is a lightweight and portable framework which does not require deep network training or extensive datasets. Experiments with real and simulated videos demonstrate the system's ability to analyze and transfer style between recordings, and are available in the supplementary video11https://youtu.be/_QzNz5WUtpk keywords: {Training;Visualization;Video sequences;Robot vision systems;Feature extraction;Cameras;Recording},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342280&isnumber=10341342

E. Zanolli and A. Del Prete, "Robust Satisfaction of Joint Position and Velocity Bounds in Discrete-Time Acceleration Control of Robot Manipulators," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10058-10065, doi: 10.1109/IROS55552.2023.10341667.Abstract: This paper deals with the robust control of fully-actuated robots subject to joint position, velocity and acceleration bounds. Robotic systems are subject to disturbances, which may arise from modeling errors, sensor noises or communication delays. This work presents mathematical and computational tools to ensure the robust satisfaction of joint bounds in the control of robot manipulators. We consider a system subject to bounded additive disturbances on the control inputs, with constant joint position, velocity and acceleration bounds. We compute the robust viability kernel, which is the set of states such that, starting from any such state, it is possible to avoid violating the constraints in the future, despite the presence of disturbances. Then we develop an efficient algorithm to compute the range of feasible accelerations that allow the state to remain inside the robust viability kernel. Our derivation ensures the continuous-time robust satisfaction of the joint bounds, while considering the discrete-time nature of the control inputs. Tests are performed in simulation with a single joint and a 6-DOF robot manipulator, demonstrating the effectiveness of the proposed approach compared to other state-of-the-art methods. keywords: {Robust control;Additives;Computational modeling;Robot sensing systems;Manipulators;Mathematical models;6-DOF},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341667&isnumber=10341342

M. Singh et al., "Toward Closed-Loop Additive Manufacturing: Paradigm Shift in Fabrication, Inspection, and Repair," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10066-10073, doi: 10.1109/IROS55552.2023.10342148.Abstract: Increased usage of additive manufacturing (AM) in various industries has solidified its role as an advanced manufacturing technique. However, there is an inherent lack of reliability in AM processes, particularly common in extrusion or deposition-based methods due to the stochastic nature of ma-terial deposition. This necessitates an intelligent manufacturing solution to address the drawbacks of AM. Thus, we propose a novel layer-wise approach toward closed-loop AM, which is capable of in-situ monitoring and repairing geometric defects. In this paper, we present a system that uses a robotic AM experimental platform that mimics a conventional open-loop fabrication setup, which we augment into a closed-loop system using two add-ons: in-situ inspection subsystem and online process correction subsystem. The in-situ inspection subsystem collects 3D point cloud scans and compares them against a reference CAD model, categorizing geometric deviations as positive or negative defects. Then the subsequent online process correction subsystem uses a re-plan and/or repair strategy to address the positive and/or negative defects, respectively. To evaluate this idea, we conducted three experiments on parts with manually induced defects to investigate the system's ability to repair those parts, thereby reducing defects, improving part accuracy, and enhancing mechanical properties. Comparing the defective and repaired parts, we observe a reduction in defect percent by volume from 10.7% to 1.3%, an improvement in geometric tolerance from 3.86% error to 0.08% error, and an increase in the part's breaking load from 4.77 kN to 6.31 kN. These experiments prove that our layer-wise closed-loop additive manufacturing approach improves the quality, tolerance, and reliability of plastic 3D printed parts, with the potential to extend to other extrusion/deposition-based AM processes, or even subtractive manufacturing and hybrid manufacturing methods. keywords: {Fabrication;Point cloud compression;Solid modeling;Three-dimensional displays;Stochastic processes;Maintenance engineering;Inspection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342148&isnumber=10341342

J. Watson and N. Correll, "Optimal Decision Making in Robotic Assembly and Other Trial-and-Error Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10074-10081, doi: 10.1109/IROS55552.2023.10341863.Abstract: Uncertainty in perception, actuation, and the environment often require multiple attempts for a robotic task to be successful. We study a class of problems providing (1) low-entropy indicators of terminal success / failure, and (2) unreliable (high-entropy) data to predict the final outcome of an ongoing task. Examples include a robot trying to connect with a charging station, parallel parking, or assembling a tightly-fitting part. The ability to restart after predicting a failure early, versus simply running to failure, can significantly decrease the makespan, that is, the total time to completion, with the drawback of potentially short-cutting an otherwise successful operation. Assuming task running times to be Poisson distributed, and using a Markov Jump Process to capture the dynamics of the underlying Markov Decision Process, we derive a closed-form solution that predicts makespan based on the confusion matrix of the failure predictor. This allows the robot to learn failure prediction in a production environment, and only adopt a preemptive policy when it actually saves time. We demonstrate this approach using a robotic peg-in-hole assembly problem. Failures are predicted by a dilated convolutional network based on force-torque data, showing an average makespan reduction from 101s to 81s ($\mathrm{N}=120,\ \mathrm{p} < 0.05$). We posit that the proposed algorithm generalizes to any robotic behavior with an unambiguous terminal reward, with wide ranging applications on how robots can learn and improve their behaviors in the wild. keywords: {Robotic assembly;Uncertainty;Production;Markov processes;Prediction algorithms;Distance measurement;Behavioral sciences;Failure Detection and Recovery;Assembly;Manipulation Planning;Industrial Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341863&isnumber=10341342

S. Müller, S. Ilić, V. Scamarcio and J. Hughes, "A Cartesian Platform for Cooperative Multi-Robot Manipulation Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10082-10087, doi: 10.1109/IROS55552.2023.10342469.Abstract: For many manipulation tasks in environments such as laboratory or a kitchen, the presence of two robot arms is important to enable collaborative tasks requiring two arms (e.g. lid removal or tool use) or to improve the efficiency of scheduling of tasks. Currently, the development of multi-arm manipulation solutions has largely focused on 6 degrees of freedom articulated robot arms. However, cartesian robots have many advantages, including their precision, reliability, efficiency, and simple path planning. By developing a cartesian platform such that the end effectors of two mirrored systems can interact freely without collisions in 5 degrees of freedom, we can leverage the advantages of cartesian robots (high precision, simple planning, and low-cost hardware) and show robot cooperation. We equip each robot with end-effectors with different skills to increase the range of tasks the robots can cooperatively complete. To exploit this robotic hardware, we have developed a task-allocation and path-planning algorithm that enables these two mirror robots to work together to solve tasks collaboratively, exploiting the different skills and workspace of the two robots. We show how this robot can be used for cooperative tasks in lab automation, including pick and place, unscrewing vial caps, liquid pouring, and weighing. These demonstrate the feasibility and capabilities of the proposed robotic system for cooperative automation using cartesian robots. keywords: {Automation;Software algorithms;Hardware;End effectors;Software;Planning;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342469&isnumber=10341342

Q. T. Barthelme and C. Lehnert, "Robotic Crop Handling in Cluttered and Unstructured Environments using Simulated L-System Dynamic Plant Models," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10088-10095, doi: 10.1109/IROS55552.2023.10341658.Abstract: This paper presents the development of a simulation for dynamic plant models, generated from L-system functional models. A key application of these dynamic plant models is to aid in developing new methods for robotic manipulation of plants that minimize damage due to physical interaction. We present a use case of the dynamic plant model by evaluating its performance against standard RRT and novel keyhole robot arm pruning algorithms in comparison with a physical plant. Through this paper, we show that the simulated plant model was able to predict the failure modes for each pruning algorithm. The dynamic plant model was also able to predict the performance difference between algorithms, simulated experiments predicting an increase in target point capture success rate from 57% RRT to 90% keyhole compared with 65% RRT to 86% keyhole when applied to a physical sample; thus validating it as a useful simulation tool for developing and testing novel robotic methods for plant handling within cluttered and unstructured environments. keywords: {Heuristic algorithms;Predictive models;Prediction algorithms;End effectors;Reliability;Robots;Standards},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341658&isnumber=10341342

S. Ilić, E. C. Montes, C. Sanders, C. Gehin-Delval, G. Marchesini and J. Hughes, "Understanding the Influence of Robot Motion on the Experimental Processes Present in Food Science Applications," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10096-10101, doi: 10.1109/IROS55552.2023.10341710.Abstract: Laboratory experiments in modern food labs are human-driven and tedious processes which can have limited throughput, reliability, repeatability or robustness. Through repeatable motions and precise control of process parameters, robotic automation can provide significant improvements to the existing experimental processes, and also improve manual assessment of the sensory data. By developing a robotic automation system which performs the make, measure, adjust and clean processes for a milk beverage made from water and powdered milk, we explore how variation in different process parameters impacts quality of the beverage in terms of the measured pH value. Using collected data we also identify optimal process parameters from robustness and time-cost standpoint. By comparing performance of the robotic system to a human we demonstrate varied performance in the pH adjustment process and 3x better precision in the pH probe cleaning. We identify that designed robotic system requires 45% more time to perform the experiment when compared to a human, yet provides significant advances in terms of repeatability and reproducibility. These findings demonstrate feasibility and benefits of the robotic automation in the food lab environments, thus paving the way for the broader implementation. keywords: {Viscosity;Automation;Dairy products;Robot sensing systems;Cleaning;Robustness;Space exploration},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341710&isnumber=10341342

A. Angelopoulos, M. Verber, C. McKinney, J. Cahoon and R. Alterovitz, "High-Accuracy Injection Using a Mobile Manipulation Robot for Chemistry Lab Automation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10102-10109, doi: 10.1109/IROS55552.2023.10341743.Abstract: Lab automation has the potential to accelerate scientific progress in the natural sciences, allowing tedious experiments that would require many hours of human time to be automated, enabling higher accuracy, efficiency, and repeatability. Mobile manipulation robots have the potential to work in chemistry labs designed for humans to complete tasks for which setting up customized factory-scale automation is premature or infeasible. We present a new method to enable a mobile manipulation robot to automate injections, a common task in chemistry labs when using equipment such as gas chromatographs (GCs) for analyzing the contents of a sample mixture. This task is challenging for a mobile manipulation robot due to the need to navigate to the equipment in the lab and then achieve millimeter-scale accuracy required for the syringe positioning. Our approach leverages deep learning to create a model capable of localizing the syringe with high accuracy using cameras mounted on the chemistry equipment, and then uses a visual servoing approach based on the syringe's needle localization to achieve the injection. We demonstrate that our approach is robust to uncertainty in navigation as well as uncertainty in the grasping position and orientation of the syringe, achieving errors sufficiently small to enable the mobile manipulation robot to automate injections in real chemistry equipment. keywords: {Chemistry;Automation;Uncertainty;Navigation;Grasping;Needles;Visual servoing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341743&isnumber=10341342

A. Agarwal et al., "Robotic Defect Inspection with Visual and Tactile Perception for Large-Scale Components," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10110-10116, doi: 10.1109/IROS55552.2023.10341590.Abstract: In manufacturing processes, surface inspection is a key requirement for quality assessment and damage localization. Due to this, automated surface anomaly detection has become a promising area of research in various industrial inspection systems. A particular challenge in industries with large-scale components, like aircraft and heavy machinery, is inspecting large parts with very small defect dimensions. Moreover, these parts can be of curved shapes. To address this challenge, we present a 2-stage multi-modal inspection pipeline with visual and tactile sensing. Our approach combines the best of both visual and tactile sensing by identifying and localizing defects using a global view (vision) and using the localized area for tactile scanning for identifying remaining defects. To benchmark our approach, we propose a novel real-world dataset with multiple metallic defect types per image, collected in the production environments on real aerospace manufacturing parts, as well as online robot experiments in two environments. Our approach is able to identify 85% defects using Stage I and identify 100% defects after Stage II. keywords: {Visualization;Pipelines;Transfer learning;Tactile sensors;Lighting;Production;Inspection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341590&isnumber=10341342

T. Makabe, K. Okada and M. Inaba, "Development of the Whole-Body Waterproof Shell Applying and Removing System Using Phase-Change Paraffin and Grease for the Multi-DOF Robot," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10125-10132, doi: 10.1109/IROS55552.2023.10341470.Abstract: We need to build robot systems that can operate in multiple environments, including underwater. For a robot to be waterproof, it needs to be covered all over its body and have a waterproof structure, but this is expensive to produce and consumes hardware resources such as weight. In this study, we propose a method of constructing a robot system in which paraffin, which can change its phase between solid and liquid at different temperatures, is cloaked on the robot's body surface, and grease is inserted into the robot's body to give it an acquired the waterproof shell for waterproofing purposes. We have studied an automated method of the waterproof shell application system by combining a small spider-shaped multi-legged robot that can measure the temperature of the environment as a robot to which we apply the waterproof shell and a life-size arm robot that performs the operation applying the waterproof shell to the small robot as a system. Through the addition of the waterproof shell to the spider-shaped robot and the removing the shell in unnecessary areas by controlling temperature, and the realization of operation in land and water environments, we used the proposed robot system to add acquired functions to operate underwater robots, thereby expanding the supported area of robots which have many DOFs and sensors from conventional ground to underwater. keywords: {Temperature measurement;Temperature sensors;Water;Temperature;Robot sensing systems;Manipulators;Sensor systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341470&isnumber=10341342

F. Ahmad, M. Mayr and V. Krueger, "Learning to Adapt the Parameters of Behavior Trees and Motion Generators (BTMGs) to Task Variations," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10133-10140, doi: 10.1109/IROS55552.2023.10341636.Abstract: The ability to learn new tasks and quickly adapt to different variations or dimensions is an important attribute in agile robotics. In our previous work, we have explored Behavior Trees and Motion Generators (BTMGs) as a robot arm policy representation to facilitate the learning and execution of assembly tasks. The current implementation of the BTMGs for a specific task may not be robust to the changes in the environment and may not generalize well to different variations of tasks. We propose to extend the BTMG policy representation with a module that predicts BTMG parameters for a new task variation. To achieve this, we propose a model that combines a Gaussian process and a weighted support vector machine classifier. This model predicts the performance measure and the feasibility of the predicted policy with BTMG parameters and task variations as inputs. Using the outputs of the model, we then construct a surrogate reward function that is utilized within an optimizer to maximize the performance of a task over BTMG parameters for a fixed task variation. To demonstrate the effectiveness of our proposed approach, we conducted experimental evaluations on push and obstacle avoidance tasks in simulation and with a real KUKA iiwa robot. Furthermore, we compared the performance of our approach with four baseline methods. keywords: {Support vector machines;Gaussian processes;Predictive models;Manipulators;Generators;Behavioral sciences;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341636&isnumber=10341342

U. Khalid, H. Iqbal, S. Vahidian, J. Hua and C. Chen, "CEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10141-10148, doi: 10.1109/IROS55552.2023.10341467.Abstract: Human-robot interaction (HRI) is a rapidly growing field that encompasses social and industrial applications. Machine learning plays a vital role in industrial HRI by enhancing the adaptability and autonomy of robots in complex environments. However, data privacy is a crucial concern in the interaction between humans and robots, as companies need to protect sensitive data while machine learning algorithms require access to large datasets. Federated Learning (FL) offers a solution by enabling the distributed training of models without sharing raw data. Despite extensive research on Federated learning (FL) for tasks such as natural language processing (NLP) and image classification, the question of how to use FL for HRI remains an open research problem. The traditional FL approach involves transmitting large neural network parameter matrices between the server and clients, which can lead to high communication costs and often becomes a bottleneck in FL. This paper proposes a communication-efficient FL framework for human-robot interaction (CEFHRI) to address the challenges of data heterogeneity and communication costs. The framework leverages pre-trained models and introduces a trainable spatiotemporal adapter for video understanding tasks in HRI. Experimental results on three human-robot interaction benchmark datasets: HRI30, InHARD, and COIN demonstrate the superiority of CEFHRI over full fine-tuning in terms of communication costs. The proposed methodology provides a secure and efficient approach to HRI federated learning, particularly in industrial environments with data privacy concerns and limited communication bandwidth. Our code is available at https://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning. keywords: {Training;Data privacy;Adaptation models;Costs;Federated learning;Service robots;Human-robot interaction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341467&isnumber=10341342

C. Xiao and J. Wachs, "COMPlacent: A Compliant Whisker Manipulator for Object Tactile Exploration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10184-10190, doi: 10.1109/IROS55552.2023.10341547.Abstract: Handling fragile objects requires minimally invasive interaction skills in order to avoid any permanent deformation, alternation or damages. Such need is often required in tactile exploration tasks. In this paper, we propose an innovative whisker manipulator (COMPlacent), which is designed to accomplish tactile exploration with minimum intrusiveness. The design is inspired by biological whiskers observed in animals, where whiskers are used as means of tactile exploration in an analogous way as fingers are. Artificial whiskers are compliant but robust, which mitigates contact forces by bending or conforming to the object surface. The intrusiveness is further reduced by reactive control, which is implemented based on tactile sensors and actuators installed on each whisker. This allows the whisker to be retracted from the object surface, so that the energy transferred by contacts is minimized. The tactile sensor is designed to be ultrasensitive, which allows it to gather contact information with high fidelity. By modeling contact pressure as a time-series signal, a machine learning framework is leveraged to discriminate object properties including shape and texture. Evaluation experiments were conducted on real objects, which successfully demonstrates object classification at an accuracy of 97.3%, and texture discrimination accuracy of 92.1%. keywords: {Pressure sensors;Minimally invasive surgery;Microorganisms;Shape;Time series analysis;Tactile sensors;Machine learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341547&isnumber=10341342

D. Fan, H. Liu, T. Wang, R. Zhu and H. Wang, "Monolithic Microchannels in Miniature Pneumatic Soft Robots for Sequential Motions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10191-10196, doi: 10.1109/IROS55552.2023.10341400.Abstract: Miniature soft robots present great potential in delicate manipulations due to their gentle force, complaint structures, and flexible motions. Easy control and fast response make pneumatic actuation a prevalent method for driving soft robotics. In addition, sequential motions are also crucial for enhancing the grasping and moving abilities of soft robots. How-ever, existing miniature pneumatic soft robots are limited to one-dimensional geometries and simple motions due to the difficulties in designing and fabricating intricate small airways in miniature pneumatic soft robots, which restricts them from more versatile deformations. Here, we employ intricate monolithic microchannels embedded into miniature soft robots' mon-olithic bodies for sequential motions. After verifying the effects of the channel diameter, strain-limiting layer, and elastic modulus of the robot's body on the bending behaviors of the ID soft robots, we fabricated a soft flower robot capable of sequential and simultaneous 3D-to-3D shape morphing through five individual microchannels and a soft carnivorous plant robot containing 2D interconnected microchannels capable of sequential enclosed grasping through a single inlet. keywords: {Statistical analysis;Shape;Grasping;Soft robotics;Pneumatic systems;Bending;Flowering plants},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341400&isnumber=10341342

W. Chen, Y. Yan, Z. Zhang, L. Yang and J. Pan, "Polymer-Based Self-Calibrated Optical Fiber Tactile Sensor," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10197-10203, doi: 10.1109/IROS55552.2023.10341656.Abstract: Human skin can accurately sense the self-decoupled normal and shear forces when in contact with objects of different sizes. Although there exist many soft and conformable tactile sensors on robotic applications able to decouple the normal force and shear forces, the impact of the size of object in contact on the force calibration model has been commonly ignored. Here, using the principle that contact force can be derived from the light power loss in the soft optical fiber core, we present a soft tactile sensor that decouples normal and shear forces and calibrates the measurement results based on the object size, by designing a two-layered weaved polymer-based optical fiber anisotropic structure embedded in a soft elastomer. Based on the anisotropic response of optical fibers, we developed a linear calibration algorithm to simultaneously measure the size of the contact object and the decoupled normal and shear forces calibrated the object size. By calibrating the sensor at the robotic arm tip, we show that robots can reconstruct the force vector at an average accuracy of 0.15N for normal forces, 0.17N for shear forces in X-axis, and 0.18N for shear forces in Y-axis, within the sensing range of 0-2N in all directions, and the average accuracy of object size measurement of 0.4mm, within the test indenter diameter range of 5-12mm. keywords: {Force measurement;Wearable computers;Force;Tactile sensors;Optical variables measurement;Size measurement;Loss measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341656&isnumber=10341342

M. Osada, G. Zhang, S. Yoshimoto and A. Yamamoto, "2-DOF Robot Arm with Variable Torque Limiters Realized by Electrostatic Film Motors," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10204-10209, doi: 10.1109/IROS55552.2023.10341997.Abstract: When a conventional robot arm, driven by magnetic motors and reduction gears, operates at high speed, it can produce an excessive force due to unexpected contact. Toward the development of a safe robot arm, the present study investigated a 2-degree-of-freedom robot arm driven by direct-drive electrostatic film motors. The motors operate synchronously, except when they are subjected to an excessive force. This loss of synchronicity is referred to as a “step-out” process, and in the present study it was used to produce a variable torque limiter. By setting appropriate limits, contact forces caused by unexpected collisions could be suppressed. The torque limiter was applied to a robot arm with a length of 50 cm. Experiments showed that the arm was impact-resistant; when it was struck by a hammer, the torque limiter was activated and suppressed the collision force, allowing the robot arm to successfully resume its motion. When the robot arm unexpectedly hit an obstacle while moving at 1.2 m/s, the collision force was maintained at 21 N by the torque limiter, which is sufficiently small for a high-speed collision. The experiments verified the effectiveness of the electrostatic film motor as a variable torque limiter to realize safe contact. keywords: {Torque;Force;Prototypes;2-DOF;Manipulators;Synchronous motors;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341997&isnumber=10341342

J. Lin, R. Xiao and Z. Guo, "Design and Stiffness Analysis of a Bio-Inspired Soft Actuator with Bi-Direction Tunable Stiffness Property," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-6, doi: 10.1109/IROS55552.2023.10341730.Abstract: Modulating the stiffness of soft actuators is crucial for improving the efficiency of interaction with the environment. However, current stiffness modulation mechanisms are hard to achieve high lateral stiffness and a wide range of bending stiffness simultaneously. Here, we draw inspiration from the anatomical structure of the finger and propose a bi-directional tunable stiffness actuator (BTSA). BTSA is a soft-rigid hybrid structure that combines air-tendon hybrid actuation (ATA) and bone-like structures (BLS). We develop a corresponding fabrication method and a stiffness analysis model to support the design of BLS. The results show that the influence of the BLS on bending deformation is negligible, with a distal point distance error of less than 1.5 mm. Moreover, the bi-directional tunable stiffness is proved to be functional. The bending stiffness can be tuned by ATA from 0.23 N/mm to 0.70 N/mm, with a magnification of 3 times. The addition of BLS improves lateral stiffness up to 4.2 times compared with the one without BLS, and the lateral stiffness can be tuned decoupling within 1.2 to 2.1 times (e.g. from 0.35 N/mm to 0.46 N/mm when the bending angle is 45 deg). Finally, a four-BTSA gripper is developed to conduct horizontal lifting and grasping tasks to demonstrate the advantages of BTSA. keywords: {Actuators;Deformation;Fingers;Modulation;Bidirectional control;Anatomical structure;Bending},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341730&isnumber=10341342

E. Papadakis, M. Sigalas, M. Vangos and P. Trahanias, "MIGHTY: Multi-Functional Suction Cup for Object Gripping and Surface Attachment," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341732.Abstract: The spectrum of applications of robotic systems is constantly being expanded in the research, industrial and even the defense sectors, ranging from manipulation and assembly to critical infrastructure monitoring and post-disaster response. Nevertheless, contemporary robotic capabilities are significantly hindered when traversing through or interacting with complex, unstructured and dynamic environments. To alleviate for that, we introduce in the current work MIGHTY (Multi-functional Intelligent Gripping with High Tolerance), a novel, lightweight, sensor-enhanced vacuum suction cup providing not only enhanced attachment capabilities on a plethora of surfaces of varying roughness, but also robust and accurate contact and force estimation. Its unique design facilitates multiple functionalities, from acting as a gripper for object manipulation to operating as a foot for stable walking and steep surface climbing. The proposed suction cup was extensively assessed under varying experimental setups, in order to validate its capacity to sense the applied force and torque and the torque's axis, as well as its ability to attach on a variety of surfaces. In all cases remarkable results were demonstrated, attesting for its effectiveness and robustness. keywords: {Legged locomotion;Torque;Service robots;Force;Robot sensing systems;Surface roughness;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341732&isnumber=10341342

T. -D. Job, M. Bensch and M. Schappler, "Multiple-Contact Estimation for Tendon-Driven Continuum Robots with Proprioceptive Sensor Information by Contact Particle Filter and Kinetostatic Models," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10224-10231, doi: 10.1109/IROS55552.2023.10341897.Abstract: This paper presents a new approach to determine single and multiple simultaneous contact forces on a tendon-driven continuum robot (CR). The estimation is based solely on the proprioceptive tendon force and length sensors that are already present. Unlike for rigid-body robots, only indirect measurements of the external forces' deflection is available. The required full kinetostatic model, which is prone to local minima due to the unknown contacts, is solved with a particle filter. The method is validated by simulative studies and experimental investigations on a new robot setup for visual inspection of aircraft engines. The algorithm allows the estimation of single contacts with an error up to 4.43 mm or 2.9 % of the robot's length. Multiple contacts can only be correctly determined at the two distal of the three segments. keywords: {Visualization;Robot vision systems;Estimation;Propioception;Robot sensing systems;Particle filters;Sensor systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341897&isnumber=10341342

A. Hassan, T. Abrar, F. Aljaber, I. Vitanov and K. Althoefer, "Eversion-Capable Fabric Robot Gripper with Novel Retraction Mechanism," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10232-10237, doi: 10.1109/IROS55552.2023.10342364.Abstract: Soft grippers have a number of advantages over their conventional stiff-bodied counterparts; not only do they surpass them in ease of fabrication and safety but also, in many cases, require less complex control strategies - due to natural compliance and form-fitting plasticity. Pneumatically actuated soft grippers made from non-extensible fabrics or polyethylene sheets have been shown to outperform soft silicone grippers capable of applying greater forces to the environment. Despite progress in the field, grasping a given object within a confined space proves challenging for both soft and conventional robotic grippers. A key issue is that most grippers use rotary or lateral translational motion when grasping an object, hence other objects in the scene may impede the closing motion as the gripper attempts to reach the target. In this study, we present a novel design for a soft robotic gripper equipped with a brace of fabric-based fingers capable, by way of eversion, of longitudinal extension, bending, and retraction, i.e. returning to a stowed state. Our experiments show that from the retracted to fully-extended state, the gripper fingers extend by up to 200% in length. To test the performance of the design, force characterisation experiments and grasping operations were carried out, demonstrating that each finger is capable of as much as 30 N of maximum tip force and can bend to an angle of 127°• At a bending pressure of 82.7 kPa (the maximum tested pressure in the bending chamber), a maximum (pullout) force of 16 N is needed to release an object that has been grasped by the finger. The experimental scenario detailed features all three mechanisms (eversion, bending and retraction) discussed. keywords: {Polyethylene;Fingers;Force;Grasping;Bending;Soft robotics;Fabrics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342364&isnumber=10341342

A. Sedal, M. Kohler, G. Agbofode, T. Y. Moore and S. Kota, "Emergent Sequential Motion Through Compliant Auxetic Shells," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10238-10244, doi: 10.1109/IROS55552.2023.10341387.Abstract: Though they are compliant, nimble and morpho-logically intelligent, fluidic soft robots often rely on bulky components for power and actuation. This work contributes a design methodology which enables development of soft fluidic robots that move in a sequenced fashion, enabling lightweight devices with embodied intelligence. Bezier-curved beams were introduced as a design building block whose antagonistic placement results in Representative Auxetic Element (RAE) that can be patterned on inflatable shells. Kinematics and loading behaviour of these design building blocks were studied through Finite Element Analysis (FEA). We give a methodology for patterning RAEs on cylindrical and conic shells to create soft fluidic components that move (motion components) and those that delay fluid flow (pinch components). We verify the physical concepts governing the design methodology through two prototype devices that produce sequenced motion under a single fluidic input. Devices using this framework have the potential to perform complicated sequenced motions with lightweight control components. keywords: {Performance evaluation;Sequential analysis;Three-dimensional displays;Auxetic materials;Loading;Prototypes;Kinematics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341387&isnumber=10341342

J. Ketchum, S. Schiffer, M. Sun, P. Kaarthik, R. L. Truby and T. D. Murphey, "Automated Gait Generation for Walking, Soft Robotic Quadrupeds," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10245-10251, doi: 10.1109/IROS55552.2023.10342059.Abstract: Gait generation for soft robots is challenging due to the nonlinear dynamics and high dimensional input spaces of soft actuators. Limitations in soft robotic control and perception force researchers to hand-craft open loop controllers for gait sequences, which is a non-trivial process. Moreover, short soft actuator lifespans and natural variations in actuator behavior limit machine learning techniques to settings that can be learned on the same time scales as robot deployment. Lastly, simulation is not always possible, due to heterogeneity and nonlinearity in soft robotic materials and their dynamics change due to wear. We present a sample-efficient, simulation free, method for self-generating soft robot gaits, using very minimal computation. This technique is demonstrated on a motorized soft robotic quadruped that walks using four legs constructed from 16 “handed shearing auxetic” (HSA) actuators. To manage the dimension of the search space, gaits are composed of two sequential sets of leg motions selected from 7 possible primitives. Pairs of primitives are executed on one leg at a time; we then select the best-performing pair to execute while moving on to subsequent legs. This method-which uses no simulation, sophisticated computation, or user input-consistently generates good translation and rotation gaits in as low as 4 minutes of hardware experimentation, outperforming hand-crafted gaits. This is the first demonstration of completely autonomous gait generation in a soft robot. keywords: {Legged locomotion;Actuators;Navigation;Transfer learning;Process control;Soft robotics;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342059&isnumber=10341342

K. K. Lin, Y. Qiu, K. Yan, Q. Ding and S. S. Cheng, "Towards MR-Safe Concentric Bellows-Based Hydrostatic Linear Actuator for a Needle Driver," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10252-10259, doi: 10.1109/IROS55552.2023.10342078.Abstract: Magnetic resonance imaging (MRI) is increasingly used for robotic needle-based clinical diagnosis and therapy due to its high resolution and high soft tissue contrast. A needle driver therefore becomes an essential part of these MRI -guided robotic systems to perform in-bore needle placement under continuous MR imaging. However, existing actuator designs for needle drivers are constrained by the high magnetic field and the bore size of the MR scanner, and most lack high- performance capability, especially in terms of large output force and long stroke. In this work, we introduce an MR-safe concentric bellows linear actuator (CBLA) with improved performance over the existing designs. This soft actuator achieves 45N output force, 77 % stroke-length ratio for the bellows, and around 250% stroke-diameter ratio. A mathematical model was built to characterize the behavior of the actuator to provide guidance for the actuator design. The fabrication and experimental evaluation of the actuator are also presented. The results demonstrate the effectiveness of the CBLA design and the strong potential of its application in MR-guided surgical procedures. keywords: {Actuators;Bellows;Magnetic resonance imaging;Force;Medical treatment;Soft robotics;Needles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342078&isnumber=10341342

Y. Itsarachaiyot, R. Hao and M. C. Çavuşoğlu, "Analytical Computation of the Contact Force Jacobian for MRI-Actuated Robotic Catheter," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10268-10274, doi: 10.1109/IROS55552.2023.10342418.Abstract: Contact force Jacobian relates the changes in the contact force to the changes in the actuation of a robotic catheter in contact with a surface. In this paper, we present an analytical method for calculating the contact force Jacobian for the Cosserat rod model of an MRI-actuated robotic catheter. First, the Cosserat rod model of the MRI-actuated robotic catheter under tip contact position constraint is introduced. For the analytical derivation of contact force Jacobian, the initial value problem parameter derivatives are defined and calculated analytically. Finally, simulation results show that the presented analytical method calculates the contact force Jacobian in significantly shorter computation time with comparable accuracy, compared to direct numerical computation. keywords: {Jacobian matrices;Analytical models;Computational modeling;Simulation;Force;Benchmark testing;Numerical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342418&isnumber=10341342

X. Gu, Q. Zhang, X. Sun, Z. Shen, B. W. Drinkwater and F. Ju, "Magnet Array-Actuated Steerable Flexible Robot with Beacon-TFM Ultrasonic Position Sensing for Robotic Neurosurgery," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10275-10280, doi: 10.1109/IROS55552.2023.10341991.Abstract: A magnetically controlled steerable robot with the capability of flexible navigation and intraoperative ultrasonic imaging and position sensing in neurosurgery is introduced in this paper. The robot system uses a piezoelectric transducer as the ultrasonic imaging beacon and applies a permanent magnet as the actuation unit, which can steer the flexible robot following a planned complex path to access a target in the brain. In order to enable the robot to navigate flexibly within tissues, a method is proposed to enhance magnetic actuation force by utilizing an array of small magnetic blocks instead of using a large magnet. The improvement of the array for magnetic field distribution was verified by simulation. Simulation studies also involved modeling the ultrasonic transmitting and receiving functions of the piezoelectric transducer to verify the feasibility of using them for intraoperative imaging. In addition, a prototype of the robot is fabricated and tested. The feasibility of the proposed magnetically controlled steerable robot for navigation in soft tissue is verified. keywords: {Ultrasonic imaging;Navigation;Magnetic resonance imaging;Piezoelectric transducers;Prototypes;Robot sensing systems;Acoustics;Permanent magnets;Sensors;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341991&isnumber=10341342

H. Ranjan et al., "Sunram 7: An MR Safe Robotic System for Breast Biopsy," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10281-10288, doi: 10.1109/IROS55552.2023.10342425.Abstract: In breast cancer patients, some nodules are only visible on MRI, thus, requiring MRI-guidance to perform the biopsy. MRI interventions are cumbersome due to the magnetic field and the constrained working space. An MR safe robotic system actuated by pneumatic stepper motors may enable these procedures, improving both accuracy and image-guided navigation. A compact multipurpose pneumatic stepper motor has been designed with outer dimensions $(45 \times 40\times 15)\mathbf{mm}^{\mathbf{3}}$. This is configurable as a linear, rotational or curved stepper motor with a customizable step size and radius of curvature. Five copies of these motors actuate the Sunram 7 biopsy robot, of which the moving part (without protruding racks and tubes) measures $(130 \times 65\times 55)\mathbf{mm}^{\mathbf{3}}$. After manually choosing the target location and angle of approach, the needle is robotically inserted into the breast and the integrated pneumatic biopsy gun is fired to sample tissue from the lesion. The maximum torque of the presented motor is 0.61 N m at 6 bar which can be achieved using 13-teeth polycarbonate gears. Using 17-teeth gears for higher accuracy and a more convenient working pressure of 2 bar the maximum torque is 0.28 N m. The accuracy in free air of the Sunram 7 robot is 1.69mm and 1.72mm in X and Z-direction respectively, with a resulting 2-D error of 2.54 mm. The workspace volume is 4.1 L. When targeting 10 mm-sized lesions in phantoms under MRI guidance, Sunram 7 achieved a success rate of 68%. The minimum interval between two successive biopsies was 5:47 minutes. The presented multipurpose stepper motor has distinct advantages over previous designs in terms of robustness, customizability, printability and ease of integration in MR safe robotics. The Sunram 7 is able to perform accurate MRI-guided biopsies in a large workspace volume while reducing the intervention time when compared to the gold standard (i.e., MRI-guided free-hand biopsy). keywords: {Torque;Gears;Robot kinematics;Magnetic resonance imaging;Biopsy;Phantoms;Breast biopsy},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342425&isnumber=10341342

D. Corsi et al., "Constrained Reinforcement Learning and Formal Verification for Safe Colonoscopy Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10289-10294, doi: 10.1109/IROS55552.2023.10341789.Abstract: The field of robotic Flexible Endoscopes (FEs) has progressed significantly, offering a promising solution to reduce patient discomfort. However, the limited autonomy of most robotic FEs results in non-intuitive and challenging manoeuvres, constraining their application in clinical settings. While previous studies have employed lumen tracking for autonomous navigation, they fail to adapt to the presence of obstructions and sharp turns when the endoscope faces the colon wall. In this work, we propose a Deep Reinforcement Learning (DRL)-based navigation strategy that eliminates the need for lumen tracking. However, the use of DRL methods poses safety risks as they do not account for potential hazards associated with the actions taken. To ensure safety, we exploit a Constrained Reinforcement Learning (CRL) method to restrict the policy in a predefined safety regime. Moreover, we present a model selection strategy that utilises Formal Verification (FV) to choose a policy that is entirely safe before deployment. We validate our approach in a virtual colonoscopy environment and report that out of the 300 trained policies, we could identify three policies that are entirely safe. Our work demonstrates that CRL, combined with model selection through FV, can improve the robustness and safety of robotic behaviour in surgical applications. keywords: {Virtual colonoscopy;Navigation;Endoscopes;Reinforcement learning;Lumen;Iron;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341789&isnumber=10341342

O. C. Kara et al., "Design and Development of a Novel Soft and Inflatable Tactile Sensing Balloon for Early Diagnosis of Colorectal Cancer Polyps," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10295-10300, doi: 10.1109/IROS55552.2023.10342343.Abstract: In this paper, with the goal of addressing the high early-detection miss rate of colorectal cancer (CRC) polyps during a colonoscopy procedure, we propose the design and fabrication of a unique inflatable vision-based tactile sensing balloon (VTSB). The proposed soft VTSB can readily be integrated with the existing colonoscopes and provide a radiation-free, safe, and high-resolution textural mapping and morphology characterization of CRC polyps. The performance of the proposed VTSB has been thoroughly characterized and evaluated on four different types of additively manufactured CRC polyp phantoms with three different stiffness levels. Additionally, we integrated the VTSB with a colonoscope and successfully performed a simulated colonoscopic procedure inside a tube with a few CRC polyp phantoms attached to its internal surface. keywords: {Fabrication;Phantoms;Surface morphology;Morphology;Colonoscopy;Robot sensing systems;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342343&isnumber=10341342

S. Rezaeian, B. Badie and J. Sheng, "A Telescopic Tendon-Driven Needle Robot for Minimally Invasive Neurosurgery," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10301-10307, doi: 10.1109/IROS55552.2023.10341660.Abstract: This paper presents the design, characterization, and testing of a steerable needle robot for minimally invasive neurosurgery. The robot consists of a rigid outer tube and two telescopic tendon-driven steerable tubes. Through the rotation, translation, and bending of individual tubes, this telescopic tendon-driven needle robot can perform dexterous motion and follow the path of the tip. We presented the design of the needle robot and its actuation system, modeling of the robotic kinematics, characterization of the robot motion, results of the open-loop kinematic control, and demonstration of the follow-the-leader motion. The position error of the robot tip is 0.92 mm, and follow-the-leader motion error is 1.1 mm. Due to its small footprint and unique motion ability, the robot has the potential to be manipulated inside human brain and used for minimally invasive neurosurgery. keywords: {Robot motion;Minimally invasive surgery;Kinematics;Systems modeling;Needles;Neurosurgery;Electron tubes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341660&isnumber=10341342

T. Zhang, J. Li, T. Cheng, C. S. H. Ng, P. W. Y. Chiu and Z. Li, "Characteristics of Permanent Magnet Coupling Based Wireless Manipulation via Simulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10308-10314, doi: 10.1109/IROS55552.2023.10341587.Abstract: Characteristics of wireless manipulation based on permanent magnet coupling, including anchoring distance, panning torque, and translational force, are assessed in this paper. The study focuses on a typical scenario where a slave robot embedded with a small permanent magnet can be remotely controlled within a constrained area by a master large permanent magnet placed outside the environment at a certain distance from it. The key parameters (force and torque) acting on the slave robot are quantified and evaluated. In this article, several combinations of permanent magnets with various dimensions and configurations are studied using finite element methods. Based on the obtained results, we create a lookup table for each parameter, serving as a guideline to help interested researchers choose suitable magnetic combinations for their applications. keywords: {Wireless communication;Couplings;Economics;Torque;Medical robotics;Force;Permanent magnets},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341587&isnumber=10341342

F. Tavakkolmoghaddam, C. Bales, Y. Wang, Z. Zhao and G. S. Fischer, "Design and Evaluation of Bidirectional Continuous Rotation and Variable Curvature Needle Steering Algorithm," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10315-10322, doi: 10.1109/IROS55552.2023.10342431.Abstract: The success rate of robotic-assisted needle-guided interventions for applications such as tissue biopsy and targeted drug delivery relies heavily on the accuracy of the needle placement. Tissue shift and needle tip deflection due to needle-tissue interaction are some factors that can adversely affect the outcome of these procedures. In this paper, we present a novel algorithm for robotically-steered bevel tip needles that provides variable needle curvatures by continuously controlling the rotation speed of the needle in a bidirectional manner. Our algorithm is an extension of the Continuous Rotation and Variable Curvature (CURV) algorithm and extends its use with wired sensorized needles. Additionally, we present algorithms for the implementation of our proposed method for closed-loop needle steering in robotic systems with image or sensor feedback. To validate our approach, we perform two benchtop needle insertion experiments in a gelatin phantom and ex vivo tissue. In the first experiment, we demonstrate the capability of our proposed algorithm in achieving variable curvatures and compare it with the CURV algorithm and our simulation results. The second experiment studies the effect of the unidirectional and bidirectional needle steering on the tissue wind-up using a novel force collection setup. Our results highlight the capability of the proposed algorithm in achieving variable curvature profiles and suggest a potential advantage compared to the original method in terms of reducing the imbalanced forces sensed at the load cell due to the needle-tissue friction buildup. keywords: {Targeted drug delivery;Sensitivity;Simulation;Force;Pipelines;Phantoms;Needles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342431&isnumber=10341342

Y. Matsuura, K. Kawaharazuka, N. Hiraoka, K. Kojima, K. Okada and M. Inaba, "Development of a Whole-Body Work Imitation Learning System by a Biped and Bi-Armed Humanoid," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10374-10381, doi: 10.1109/IROS55552.2023.10342502.Abstract: Imitation learning has been actively studied in recent years. In particular, skill acquisition by a robot with a fixed body, whose root link position and posture and camera angle of view do not change, has been realized in many cases. On the other hand, imitation of the behavior of robots with floating links, such as humanoid robots, is still a difficult task. In this study, we develop an imitation learning system using a biped robot with a floating link. There are two main problems in developing such a system. The first is a teleoperation device for humanoids, and the second is a control system that can withstand heavy workloads and long-term data collection. For the first point, we use the whole body control device TABLIS. It can control not only the arms but also the legs and can perform bilateral control with the robot. By connecting this TABLIS with the high-power humanoid robot JAXON, we construct a control system for imi-tation learning. For the second point, we will build a system that can collect long-term data based on posture optimization, and can simultaneously move the robot's limbs. We combine high-cycle posture generation with posture optimization methods, including whole-body joint torque minimization and contact force optimization. We designed an integrated system with the above two features to achieve various tasks through imitation learning. Finally, we demonstrate the effectiveness of this system by experiments of manipulating flexible fabrics such that not only the hands but also the head and waist move simultaneously, manipulating objects using legs characteristic of humanoids, and lifting heavy objects that require large forces. keywords: {Legged locomotion;Learning systems;Torque;Robot vision systems;Humanoid robots;Data collection;Control systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342502&isnumber=10341342

S. Sato, K. Kojima, N. Hiraoka, K. Okada and M. Inaba, "Humanoid Walking System with CNN-Based Uneven Terrain Recognition and Landing Control with Swing-Leg Velocity Constraints," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10382-10389, doi: 10.1109/IROS55552.2023.10342511.Abstract: In order for a humanoid robot to traverse uneven terrain without falling over, the robot must control its landing position appropriately. To determine the landing position, there are two difficulties in terrain recognition and leg motion control. In terrain recognition, it is difficult to recognize and avoid terrain such as steps and obstacles that cannot be landed on in real-time. In leg motion control, it is necessary to land at appropriate positions and times to control the CoG trajectory while limiting the velocity of the swing-leg to suppress the landing impact. For solving these problems, we propose a recognition and walking control system on uneven terrain. In terrain recognition, we improved the recognition accuracy while satisfying real-time performance by using a CNN that learns the relationship between the foot and the geometric information of the surrounding terrain. In the leg motion control, landing impact was reduced by modifying the landing position under not only (1) terrain constraint and (2) robot stability constraint, but also (3) leg velocity constraint. We verified the effectiveness of the proposed system through uneven terrain walking and push recovery experiments using the actual robot. keywords: {Legged locomotion;Limiting;Humanoid robots;Control systems;Real-time systems;Trajectory;Motion control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342511&isnumber=10341342

S. Sovukluk, J. Englsberger and C. Ott, "Whole Body Control Formulation for Humanoid Robots with Closed/Parallel Kinematic Chains: Kangaroo Case Study," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10390-10396, doi: 10.1109/IROS55552.2023.10341391.Abstract: This study extends the whole-body control (WBC) formulation for bipedal humanoid robots that include closed (parallel) kinematic chains in their structure. Along with general formulation, we also stress the implementation of this formulation on Kangaroo, which is a highly dynamic humanoid robot developed by PAL Robotics. This 76-DOF robot includes 24 independent closed-kinematic chains in its structure and constitutes a good case study for our approach. We discuss the WBC formulation for various control structures, including inverse dynamics control (IDC) and Modular Passive Tracking Control (MPTC). As a test scenario, we employ a 3D spring-loaded inverted pendulum (SLIP) jumping trajectory with disturbance rejection as the desired CoM trajectory. keywords: {Couplings;Three-dimensional displays;Force;Humanoid robots;Kinematics;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341391&isnumber=10341342

A. Adu-Bredu, G. Gibson and J. Grizzle, "Exploring Kinodynamic Fabrics for Reactive Whole-Body Control of Underactuated Humanoid Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10397-10404, doi: 10.1109/IROS55552.2023.10342091.Abstract: For bipedal humanoid robots to successfully operate in the real world, they must be competent at simultaneously executing multiple motion tasks while reacting to unforeseen external disturbances in real-time. We propose Kinodynamic Fabrics as an approach for the specification, solution and simultaneous execution of multiple motion tasks in real-time while being reactive to dynamism in the environment. Kinodynamic Fabrics allows for the specification of prioritized motion tasks as forced spectral semi-sprays and solves for desired robot joint accelerations at real-time frequencies. We evaluate the capabilities of Kinodynamic fabrics on diverse physically-challenging whole-body control tasks with a bipedal humanoid robot both in simulation and in the real-world. Kinodynamic Fabrics outperforms the state-of-the-art Quadratic Program based whole-body controller on a variety of whole-body control tasks on run-time and reactivity metrics in our experiments. Our open-source implementation of Kinodynamic Fabrics as well as robot demonstration videos can be found at this url: https://adubredu.github.io/kinofabs keywords: {Measurement;Humanoid robots;Fabrics;Real-time systems;Task analysis;Manipulator dynamics;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342091&isnumber=10341342

C. Perrot and O. Stasse, "Step Toward Deploying the Torque-Controlled Robot TALOS on Industrial Operations," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10405-10411, doi: 10.1109/IROS55552.2023.10342428.Abstract: This paper tackles the use of torque controlled humanoid robot TALOS in the context of industrial manufacturing. It demonstrates that it is possible to use Whole Body Model Predictive Control (WBMPC) to reliably insert a tool in the holes of an aircraft structure with an accuracy of few millimeters. This result is based on the use of Crocoddyl, an optimal control library that exploits differential dynamic programming (DDP) to achieve high numerical efficiency. The focus of this article is put on the procedure that was undertaken to shape the cost function of the optimal controller. Our approach has first been validate in a low performance setting on the humanoid robot TALOS. Then, a strategy to improve the performances by reinjecting information about the posture of the robot from previous experiments is showed in simulation. keywords: {Torque;Service robots;Shape;Humanoid robots;Optimal control;Cost function;Manufacturing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342428&isnumber=10341342

A. Razmjoo, T. Brecelj, K. Savevska, A. Ude, T. Petrič and S. Calinon, "Learning Joint Space Reference Manifold for Reliable Physical Assistance," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10412-10417, doi: 10.1109/IROS55552.2023.10342173.Abstract: This paper presents a study on the use of the Talos humanoid robot for performing assistive sit-to-stand or stand-to-sit tasks. In such tasks, the human exerts a large amount of force (100–200 N) within a very short time (2–8 s), posing significant challenges in terms of human unpredictability and robot stability control. To address these challenges, we propose an approach for finding a spatial reference for the robot, which allows the robot to move according to the force exerted by the human and control its stability during the task. Specifically, we focus on the problem of finding a 1D manifold for the robot, while assuming a simple controller to guide its movement on this manifold. To achieve this, we use a functional representation to parameterize the manifold and solve an optimization problem that takes into account the robot's stability and the unpredictability of human behavior. We demonstrate the effectiveness of our approach through simulations and experiments with the Talos robot, showing robustness and adaptability. keywords: {Manifolds;Uncertainty;Robot kinematics;Force;Redundancy;Stability analysis;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342173&isnumber=10341342

R. Soni, G. A. Castillo, L. Krishna, A. Hereid and S. Kolathaya, "MELP: Model Embedded Linear Policies for Robust Bipedal Hopping," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10418-10424, doi: 10.1109/IROS55552.2023.10342023.Abstract: Linear policies are the simplest class of policies that can achieve stable bipedal walking behaviors in both simulation and hardware. However, a significant challenge in deploying them widely is the difficulty in extending them to more dynamic behaviors like hopping and running. Therefore, in this work, we propose a new class of linear policies in which template models can be embedded. In particular, we show how to embed Spring Loaded Inverted Pendulum (SLIP) model in the policy class and realize perpetual hopping in arbitrary directions. The spring constant of the template model is learned in addition to the remaining parameters of the policy. Given this spring constant, the goal is to realize hopping trajectories using the SLIP model, which are then tracked by the bipedal robot using the linear policy. Continuous hopping with adjustable heading direction was achieved across different terrains in simulation with heading and lateral velocities of up to O.5m/ sec and 0.05m/ sec, respectively. The policy was then transferred to the hardware, and preliminary results (> 10 steps) of hopping were achieved. keywords: {Solid modeling;Three-dimensional displays;Hardware;Real-time systems;Behavioral sciences;Trajectory;Springs;Humanoid and Bipedal Locomotion;Rein-forcement Learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342023&isnumber=10341342

P. Klokowski et al., "evoBOT – Design and Learning-Based Control of a Two-Wheeled Compound Inverted Pendulum Robot," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10425-10432, doi: 10.1109/IROS55552.2023.10342128.Abstract: This paper introduces evoBOT, a novel robot platform for research on highly dynamic locomotion and human-machine interaction. evoBOT is capable of performing complex tasks such as handovers or manipulation while moving at high speeds. We provide an overview of the robot's core features and the underlying design decisions on both the mechanical and the electronic level. Moreover, we propose a reinforcement learning (RL) based control approach for training highly dynamic motions that is evaluated on a first set of robotic tasks, including robust balancing and dynamic locomotion. Lastly, we conduct extensive benchmarking on the adopted sim-to-real methods and present an initial sim-to-real pipeline for first transfer of the trained policies to the real robot. To accelerate robotics research in this direction, the full simulation model of the robot is released as open-source. keywords: {Training;Dynamics;Pipelines;Neural networks;Reinforcement learning;Robot sensing systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342128&isnumber=10341342

M. Boukheddimi, R. Kumar, S. Kumar, J. Carpentier and F. Kirchner, "Investigations into Exploiting the Full Capabilities of a Series-Parallel Hybrid Humanoid Using Whole Body Trajectory Optimization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10433-10439, doi: 10.1109/IROS55552.2023.10341784.Abstract: Trajectory optimization methods have become ubiquitous for the motion planning and control of underactuated robots for e.g., quadrupeds, humanoids etc. While they have been extensively used in the case of serial or tree type robots, they are seldomly used for planning and control of robots with closed loops. Series-parallel hybrid topology is quite commonly used in the design of humanoid robots, but they are often neglected during trajectory optimization and the movements are computed for a serial abstraction of the system and then the solution is mapped to the actuator coordinates. As a consequence, the full capability of the robot cannot be exploited. This paper presents a case study of trajectory optimization for series-parallel hybrid robot by taking into account all the holonomic constraints imposed by the closed kinematic loops present in the system. We demonstrate the advantages of this consideration with a weightlifting task on RH5 Manus humanoid in both simulation and experiments. keywords: {Actuators;Robot kinematics;Humanoid robots;Kinematics;Planning;Topology;Quadrupedal robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341784&isnumber=10341342

Y. -M. Chen, G. Nelson, R. Griffin, M. Posa and J. Pratt, "Integrable Whole-Body Orientation Coordinates for Legged Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10440-10447, doi: 10.1109/IROS55552.2023.10341531.Abstract: Complex multibody legged robots can have complex rotational control challenges. In this paper, we propose a concise way to understand and formulate a whole-body orientation that (i) depends on system configuration only and not a history of motion, (ii) can be representative of the orientation of the entire system while not being attached to any specific link, and (iii) has a rate of change that approximates total system angular momentum. We relate this orientation coordinate to past work, and discuss and demonstrate, including on hardware, several different uses for it. keywords: {Legged locomotion;Robot kinematics;Hardware;History;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341531&isnumber=10341342

L. Krishna and Q. Nguyen, "Learning Multimodal Bipedal Locomotion and Implicit Transitions: A Versatile Policy Approach," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10448-10455, doi: 10.1109/IROS55552.2023.10342398.Abstract: In this paper, we propose a novel framework for synthesizing a single multimodal control policy capable of generating diverse behaviors (or modes) and emergent inherent transition maneuvers for bipedal locomotion. In our method, we first learn efficient latent encodings for each behavior by training an autoencoder from a dataset of rough reference motions. These latent encodings are used as commands to train a multimodal policy through an adaptive sampling of modes and transitions to ensure consistent performance across different behaviors. We validate the policy's performance in simulation for various distinct locomotion modes such as walking, leaping, jumping on a block, standing idle, and all possible combinations of inter-mode transitions. Finally, we integrate a task-based planner to rapidly generate open-loop mode plans for the trained multimodal policy to solve high-level tasks like reaching a goal position on a challenging terrain. Complex parkour-like motions by smoothly combining the discrete locomotion modes were generated in $\sim 3$ min. to traverse tracks with a gap of width 0.45 m, a plateau of height 0.2 m, and a block of height 0.4 m, which are all significant compared to the dimensions of our mini-biped platform. keywords: {Training;Legged locomotion;Tracking;Simulation;Encoding;Behavioral sciences;Steady-state},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342398&isnumber=10341342

J. Liu, M. Li, J. W. Grizzle and J. -K. Huang, "CLF-CBF Constraints for Real-Time Avoidance of Multiple Obstacles in Bipedal Locomotion and Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10497-10504, doi: 10.1109/IROS55552.2023.10341626.Abstract: This paper presents a reactive planning system that allows a Cassie-series bipedal robot to avoid multiple non-overlapping obstacles via a single, continuously differentiable control barrier function (CBF). The overall system detects an individual obstacle via a height map derived from a LiDAR point cloud and computes an elliptical outer approximation, which is then turned into a CBF. The QP-CLF-CBF formalism developed by Ames et al. is applied to ensure that safe trajectories are generated. Safe planning in environments with multiple obstacles is demonstrated both in simulation and experimentally on the Cassie biped. keywords: {Point cloud compression;Meters;Laser radar;Navigation;Green products;Robot sensing systems;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341626&isnumber=10341342

S. Aravecchia, A. Richard, M. Clausel and C. Pradalier, "Next-Best-View Selection from Observation Viewpoint Statistics," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10505-10510, doi: 10.1109/IROS55552.2023.10341982.Abstract: This paper discusses the problem of autonomously constructing a qualitative map of an unknown 3D environment using a 3D-Lidar. In this case, how can we effectively integrate the quality of the 3D-reconstruction into the selection of the Next-Best-View? Here, we address the challenge of estimating the quality of the currently reconstructed map in order to guide the exploration policy, in the absence of ground truth, which is typically the case in exploration scenarios. Our key contribution is a method to build a prior on the quality of the reconstruction from the data itself. Indeed, we not only prove that this quality depends on statistics from the observation viewpoints, but we also demonstrate that we can enhance the quality of the reconstruction by leveraging these statistics during the exploration. To do so, we propose to integrate them into Next-Best-View selection policies, in which the information gain is directly computed based on these statistics. Finally, we demonstrate the robustness of our approach, even in challenging environments, with noise in the robot localization, and we further validate it through a real-world experiment. keywords: {Deep learning;Three-dimensional displays;Statistical analysis;Buildings;Reinforcement learning;Robustness;Robot localization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341982&isnumber=10341342

A. İşleyen, N. van de Wouw and Ö. Arslan, "Feedback Motion Prediction for Safe Unicycle Robot Navigation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10511-10518, doi: 10.1109/IROS55552.2023.10341787.Abstract: As a simple and robust mobile robot base, differential drive robots that can be modelled as a kinematic unicycle find significant applications in logistics and service robotics in both industrial and domestic settings. Safe robot navigation around obstacles is an essential skill for such unicycle robots to perform diverse useful tasks in complex cluttered environments, especially around people and other robots. Fast and accurate safety assessment plays a key role in reactive and safe robot motion design. In this paper, as a more accurate and still simple alternative to the standard circular Lyapunov level sets, we introduce novel conic feedback motion prediction methods for bounding the close-loop motion trajectory of the kinematic unicycle robot model under a standard unicycle motion control approach. We present an application of unicycle feedback motion prediction for safe robot navigation around obstacles using reference governors, where the safety of a unicycle robot is continuously monitored based on the predicted future robot motion. We investigate the role of motion prediction on robot behaviour in numerical simulations and conclude that fast and accurate feedback motion prediction is key for fast, reactive, and safe robot navigation around obstacles. keywords: {Robot motion;Navigation;Service robots;Prediction methods;Kinematics;Robot sensing systems;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341787&isnumber=10341342

C. E. Denniston, G. Salhotra, A. Kangaslahti, D. A. Caron and G. S. Sukhatme, "Learned Parameter Selection for Robotic Information Gathering," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10519-10526, doi: 10.1109/IROS55552.2023.10342080.Abstract: When robots are deployed in the field for environmental monitoring they typically execute pre-programmed motions, such as lawnmower paths, instead of adaptive methods, such as informative path planning. One reason for this is that adaptive methods are dependent on parameter choices that are both critical to set correctly and difficult for the non-specialist to choose. Here, we show how to automatically configure a planner for informative path planning by training a reinforcement learning agent to select planner parameters at each iteration of informative path planning. We demonstrate our method with 37 instances of 3 distinct environments, and compare it against pure (end-to-end) reinforcement learning techniques, as well as approaches that do not use a learned model to change the planner parameters. Our method shows a 9.53% mean improvement in the cumulative reward across diverse environments when compared to end-to-end learning based methods; we also demonstrate via a field experiment how it can be readily used to facilitate high performance deployment of an information gathering robot. keywords: {Training;Learning systems;Reinforcement learning;Trajectory;Environmental monitoring;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342080&isnumber=10341342

S. Sun et al., "FISS+: Efficient and Focused Trajectory Generation and Refinement Using Fast Iterative Search and Sampling Strategy," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10527-10534, doi: 10.1109/IROS55552.2023.10341498.Abstract: Trajectory planning plays a crucial role in autonomous driving systems, as it is tasked to generate feasible trajectories under highly dynamic scenarios within the time constraint. This paper proposes a novel two-stage coarse-to-fine framework for efficient sampling-based trajectory planning. The proposed method is designed to iteratively generate new trajectory samples focused on the low-cost regions in the sampling space. Two trajectory exploration algorithms are well-designed for efficient search in discretized coarse global space and continuous fine local space, respectively. Experimental results on the first-of-its-kind planning benchmark tool CommonRoad show that our method significantly outperforms the baseline methods both in optimality and computational efficiency. Overall, our approach offers a promising solution for efficient and effective trajectory planning in more autonomous vehicle applications. keywords: {Trajectory planning;Benchmark testing;Trajectory;Computational efficiency;Space exploration;Iterative methods;Time factors;Motion and path planning;autonomous vehicle navigation;trajectory sampling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341498&isnumber=10341342

J. Hackett and C. Hubicki, "Real-Time Failure-Adaptive Control for Dynamic Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 10535-10542, doi: 10.1109/IROS55552.2023.10341946.Abstract: The human world is full of risks that threaten failure of robotic tasks. Dynamic robots, such as agile drones and walking bipeds, are particularly susceptible to failure because their time to make critical decisions is short. This work seeks a control algorithm which adapts to failures and reprioritizes robot behavior automatically, all at real-time speeds. Our failure-adaptive control framework learns failure probabilities from in situ experience and minimizes the risk of future failures using fast online planners (i.e. model predictive control). By reasoning about probabilities of failure, more imminent risks are automatically prioritized by the framework without manually tuning weighting factors. Further, our low-order probability model is learned using fast convex optimizations, allowing for immediate learning from triggered failures during operation. We demonstrate the framework's capability to learn and plan in real time (< 20 ms) in highly dynamic scenarios with micro-aerial vehicles (i.e. drones). We conduct two experiments: a chase-avoid task, and a chase-avoid - track task. In both scenarios, a single failure causes a categorical shift in robot behavior and the drone will adapt, plan, and execute a non-failing strategy within one second post-failure. keywords: {Legged locomotion;Heuristic algorithms;Prediction algorithms;Real-time systems;Vehicle dynamics;Task analysis;Tuning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341946&isnumber=10341342

