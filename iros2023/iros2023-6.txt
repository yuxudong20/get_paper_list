L. G. Fletcher, P. Perali, A. Beathard and J. M. O'Kane, "A Visibility-Based Escort Problem," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4804-4811, doi: 10.1109/IROS55552.2023.10341574.Abstract: This paper introduces and solves a visibility-based escort planning problem. This novel problem, which is closely related to the well-researched family of visibility-based pursuit-evasion problems in robotics, entails an escort agent tasked with escorting a vulnerable agent, called the VIP, in a 2-dimensional environment. The escort protects the VIP from adversaries that pose line-of-sight threats. We describe a correct and complete planning algorithm whose inputs are a simply-connected polygonal map of the environment, starting locations for the escort and the VIP, along with a goal location to which the VIP agent should be safely moved. The algorithm computes trajectories for the escort and VIP which allow the VIP to reach its goal without coming into the line-of-sight of the adversary at any time. During the execution of these trajectories, the adversary is allowed to move along any continuous path that does not enter into the line-of-sight of the escort. The algorithm proceeds by dividing the environment into a collection of conservative regions and planning the escort's movements as a sequence of these regions via breadth-first search over an information graph. The trajectory of the VIP can then be constructed by tracing the ‘safe zones' swept out by the escort's trajectory. We describe an implementation of this algorithm and present computed examples of escort agent strategies in diverse environments. keywords: {Green products;Line-of-sight propagation;Trajectory;Planning;Task analysis;Computational complexity;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341574&isnumber=10341342

Y. Hasan, A. M. Villegas-Suarez, E. C. Carter, A. Faust and L. Tapia, "Enhancing Value Estimation Policies by Post-Hoc Symmetry Exploitation in Motion Planning Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4812-4819, doi: 10.1109/IROS55552.2023.10341746.Abstract: Motion planning tasks are often innately invariant to certain geometric transformations, or in other words, symmetric. This property, however, is not always reflected in learned policies that are trained on these tasks. Although this asymmetry can be addressed through data augmentation or additional training samples, doing so comes at a cost of increased training time. Instead of trying to remedy this issue during the learning process, we leverage this disparity during execution. We propose the symmetry exploitation policy, an augmentation in the post-hoc execution stage of RL policies. During the planning stage, we present the learned policy with an invariant, geometrically transformed version of the observation as an alternate perspective of the state. This allows the policy to produce multiple possible actions for a single state, and choose the action with the highest estimated value. Unlike other symmetry exploitation methods for learning solutions in motion planning, this method completely bypasses the need for additional training. We show the effect of the symmetry exploitation policy on DQN, A2C, and PPO policies, in three motion problems with different dimensions, observation types, and symmetries. The results show that by exploiting the symmetry of the task, a trained model achieves improved performance and better generalization, and can achieve comparable results to retraining, augmentation, or extended training, without incurring any additional training time. The efficacy is most prominent in more complex tasks, as 89 of the 100 models involved in the case study improve when using the method. keywords: {Training;Costs;Dynamics;Estimation;Data augmentation;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341746&isnumber=10341342

J. Sláma, J. Herynek and J. Faigl, "Risk-Aware Emergency Landing Planning for Gliding Aircraft Model in Urban Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4820-4826, doi: 10.1109/IROS55552.2023.10341622.Abstract: An in-flight loss of thrust poses a risk to the aircraft, its passengers, and people on the ground. When a loss of thrust happens, the (auto)pilot is forced to perform an emergency landing, possibly toward one of the reachable airports. If none of the airports is reachable, the aircraft is forced to land at another location, which can be risky in urban environments. In this work, we present a generalization of the previous work on planning safe emergency landing in the case of in-flight loss of thrust such that the risk induced by the loss of thrust can be assessed if none of the airports are reachable. The proposed method relies on planning space discretization and efficient risk propagation through the risk map. The approach can find the least risky landing site and corresponding forced landing trajectory for any configuration in the planning space. The method has been empirically evaluated in a realistic urban scenario. The results support its suitability for risk-aware planning of an emergency landing in the case of in-flight loss of thrust. keywords: {Atmospheric modeling;Urban areas;Airports;Planning;Trajectory;Aircraft;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341622&isnumber=10341342

J. Arrizabalaga and M. Ryll, "SCTOMP: Spatially Constrained Time-Optimal Motion Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4827-4834, doi: 10.1109/IROS55552.2023.10341500.Abstract: This work focuses on spatial time-optimal motion planning, a generalization of the exact time-optimal path following problem that allows a system to plan within a predefined space. In contrast to state-of-the-art methods, we drop the assumption of a given collision-free geometric reference. Instead, we present a three-stage motion planning method that solely relies on start and goal locations and a geometric representation of the environment to compute a time-optimal trajectory that is compliant with system dynamics and constraints. The proposed scheme first finds collision-free navigation corridors, second computes an obstacle-free Pythagorean Hodograph parametric spline along each corridor, and third, solves a spatially reformulated minimum-time optimization problem at each of these corridors. The spline obtained in the second stage is not a geometric reference, but an extension of the free space associated with its corridor, and thus, time-optimality of the solution is guaranteed. The validity of the proposed approach is demonstrated by a well-established planar example and benchmarked in a spatial system against state-of-the-art methodologies across a wide range of scenarios in highly congested environments. Video: https://youtu.be/zGExvnUEfOY keywords: {System dynamics;Navigation;Benchmark testing;Minimization;Planning;Trajectory;Splines (mathematics)},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341500&isnumber=10341342

Y. Lu, B. Yang, J. Li, Y. Zhou, H. Chen and Y. Mo, "Consecutive Inertia Drift of Autonomous RC Car via Primitive-Based Planning and Data-Driven Control," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4835-4840, doi: 10.1109/IROS55552.2023.10341461.Abstract: Inertia drift is an aggressive transitional driving maneuver, which is challenging due to the high nonlinearity of the system and the stringent requirement on control and planning performance. This paper presents a solution for the consecutive inertia drift of an autonomous RC car based on primitive-based planning and data-driven control. The planner generates complex paths via the concatenation of path segments called primitives, and the controller eases the burden on feedback by interpolating between multiple real trajectories with different initial conditions into one near-feasible reference trajectory. The proposed strategy is capable of drifting through various paths containing consecutive turns, which is validated in both simulation and reality. keywords: {Interpolation;Planning;Trajectory;Automobiles;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341461&isnumber=10341342

E. -L. M. Ruud, M. S. Rundhovde, J. Sandrib and G. Bitar, "A Hybrid-State Path Planner for ASV Formations with Full-Scale Experiments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4841-4848, doi: 10.1109/IROS55552.2023.10341696.Abstract: We present a hybrid-state path planner for autonomous surface vehicles (ASVs) constrained by a min-imum turning radius. The work is motivated by the future Norwegian naval mine countermeasures (NMCM) concept, which includes mine sweeping operations with ASVs that operate alone or in a formation of two, with and without mine sweeping equipment attached. Our path-planning approach is a variant of hybrid A* search, with the goal of finding the shortest path for transit in an environment consisting of an a priori map, operator-defined avoid areas and static obstacles detected by the vehicle. The planner is capable of finding traversable paths for ASV formations and single vehicles using a simple geometric model based on an estimated minimum turning radius for a given configuration. The method is modular as it is uncoupled from the path-following control method, making it applicable to many different systems, including our ASV configurations for mine sweeping operations. We present results from path planning in simulation for a single vehicle and two vehicles in formation. The method has been validated through a full-scale experiment with one of our ASVs in our testing area for autonomous platforms. keywords: {Navigation;Geometric modeling;Kinematics;Turning;Path planning;Intelligent robots;Testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341696&isnumber=10341342

N. Nechyporenko, C. Escobedo, S. Kadekodi and A. Roncone, "CAT-RRT: Motion Planning that Admits Contact One Link at a Time," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4849-4856, doi: 10.1109/IROS55552.2023.10341668.Abstract: Current motion planning approaches rely on binary collision checking to evaluate the validity of a state and thereby dictate where the robot is allowed to move. This approach leaves little room for robots to engage in contact with an object, as is often necessary when operating in densely cluttered spaces. In this work, we propose an alternative method that considers contact states as high-cost states that the robot should avoid but can traverse if necessary to complete a task. More specifically, we introduce Contact Admissible Transition-based Rapidly exploring Random Trees (CAT-RRT)11Supplementary video and open source code [1]., a planner that uses a novel per-link cost heuristic to find a path by traversing high-cost obstacle regions. Through extensive testing, we find that state-of-the-art optimization planners tend to over-explore low-cost states, which leads to slow and inefficient convergence to contact regions. Conversely, CAT-RRT searches both low and high-cost regions simultaneously with an adaptive thresholding mechanism carried out at each robot link. This leads to paths with a balance between efficiency, path length, and contact cost. keywords: {Costs;Source coding;Planning;Collision avoidance;Task analysis;Optimization;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341668&isnumber=10341342

S. -W. Kim, C. g. Hwang, S. Yoo, Y. Ko and S. Kang, "Novel Gripper with Rotatable Distal Joints for Home Robots: Picking and Placing Tableware," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4865-4872, doi: 10.1109/IROS55552.2023.10342249.Abstract: A convenient situation can be realized if home robots replace housework. However, tasks in an actual home environment are challenging for robots. Particularly, cleaning the table after eating is challenging because of the cluttered environments and various tableware shapes. This study presents a new type of gripper appropriate for picking and placing various tableware in narrow and cluttered environments. The gripper comprises a 1-DOF gripper and 2-DOF distal joints. The rotatable distal joints enable reducing the effective workspace when reorientating tableware and accessing tableware in narrow spaces. In addition, the gripper can transform into three types of grasping modes by actively rotating the distal joints for handling three types of tableware, namely cutlery, cups, and dishes. The gripper is experimentally demonstrated to handle various tableware and the tables can be cleaned after eating in real-world environments. keywords: {Shape;2-DOF;Transforms;Grasping;Cleaning;Grippers;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342249&isnumber=10341342

Z. Wu, J. Tang, X. Chen, C. Ma, X. Lan and N. Zheng, "Prioritized Planning for Target-Oriented Manipulation via Hierarchical Stacking Relationship Prediction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4873-4880, doi: 10.1109/IROS55552.2023.10342196.Abstract: In scenarios involving grasping multiple targets, the learning of stacking relationships between objects is fundamental for robots to execute safely and efficiently. However, current methods lack subdivision for the hierarchy of stacking relationship types. In scenes where objects are mostly stacked in an orderly manner, they are incapable of performing human-like and high-efficient grasping decisions. This paper proposes a perception-planning method to distinguish different stacking forms between objects and generate prioritized manipulation sequences based on given target designations. We utilize a Hierarchical Stacking Relationship Network (HSRN) to discriminate the hierarchy of stacking and generate a refined Stacking Relationship Tree (SRT) for relationship description. Considering objects with high stacking stability can be processed together if necessary, we introduce an elaborate decision-making planner based on Partially Observable Markov Decision Process (POMDP), which leverages observations and generates the least grasp-consuming decision chain with robustness and is suitable for simultaneously specifying multiple targets. To verify our work, we set the scene to the dining table and augment REGRAD dataset for network training. Experiments show that our method effectively generates grasping decisions that conform to human requirements, and improves the implementation efficiency compared with existing methods on the basis of guaranteeing success rate. keywords: {Training;Stacking;Decision making;Grasping;Markov processes;Prediction algorithms;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342196&isnumber=10341342

C. Tang, D. Huang, L. Meng, W. Liu and H. Zhang, "Task-Oriented Grasp Prediction with Visual-Language Inputs," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4881-4888, doi: 10.1109/IROS55552.2023.10342268.Abstract: To perform household tasks, assistive robots receive commands in the form of user language instructions for tool manipulation. The initial stage involves selecting the intended tool (i.e., object grounding) and grasping it in a task-oriented manner (i.e., task grounding). Nevertheless, prior researches on visual-language grasping (VLG) focus on object grounding, while disregarding the fine-grained impact of tasks on object grasping. Task-incompatible grasping of a tool will inevitably limit the success of subsequent manipulation steps. Motivated by this problem, this paper proposes GraspCLIP, which addresses the challenge of task grounding in addition to object grounding to enable task-oriented grasp prediction with visual-language inputs. Evaluation on a custom dataset demonstrates that GraspCLIP achieves superior performance over established baselines with object grounding only. The effectiveness of the proposed method is further validated on an assistive robotic arm for grasping previously unseen kitchen tools given the task specification. Our presentation video is available at: https://www.youtube.com/watch?v=e1wfYQPeAXU. keywords: {Grounding;Grasping;Manipulators;Assistive robots;6-DOF;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342268&isnumber=10341342

W. Chen, D. Lee, D. Chappell and N. Rojas, "Learning to Grasp Clothing Structural Regions for Garment Manipulation Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4889-4895, doi: 10.1109/IROS55552.2023.10342086.Abstract: When performing cloth-related tasks, such as garment hanging, it is often important to identify and grasp certain structural regions—a shirt's collar as opposed to its sleeve, for instance. However, due to cloth deformability, these manipulation activities, which are essential in domestic, health care, and industrial contexts, remain challenging for robots. In this paper, we focus on how to segment and grasp structural regions of clothes to enable manipulation tasks, using hanging tasks as case study. To this end, a neural network-based perception system is proposed to segment a shirt's collar from areas that represent the rest of the scene in a depth image. With a 10-minute video of a human manipulating shirts to train it, our perception system is capable of generalizing to other shirts regardless of texture as well as to other types of collared garments. A novel grasping strategy is then proposed based on the segmentation to determine grasping pose. Experiments demonstrate that our proposed grasping strategy achieves 92%, 80%, and 50% grasping success rates with one folded garment, one crumpled garment and three crumpled garments, respectively. Our grasping strategy performs considerably better than tested baselines that do not take into account the structural nature of the garments. With the proposed region segmentation and grasping strategy, challenging garment hanging tasks are successfully implemented using an open-loop control policy. Supplementary material is available at https://sites.google.com/view/garment-hanging keywords: {Image segmentation;Service robots;Clothing;Grasping;Medical services;Task analysis;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342086&isnumber=10341342

H. T. L. Doan, H. Arita and K. Tahara, "External Sensor-Less in-Hand Object Position Manipulation for an Under-Actuated Hand Using Data-Driven-Based Methods to Compensate for the Nonlinearity of Self-Locking Mechanism," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4896-4903, doi: 10.1109/IROS55552.2023.10341517.Abstract: Dexterous manipulation using an under-actuated hand has been a challenging task due to its non-linear dynamical characteristics. For a linkage-based under-actuated hand designed to be used to grasp and manipulate large, heavy, and rigid objects stably, precision grasping is necessary, which makes the task even more difficult to deal with. While approaches based on external sensors have been introduced throughout the years, to create a robotic hand that can be used for various tasks in unstructured environments, this paper takes the standpoint that control techniques that do not fully depend on utilizing additional sensing elements need to be further developed. This paper applies the hybrid method using analytics models and data-driven-based approaches to analyze internal sensors' data during the operation of the robot and introduces novel data-driven-based techniques to compensate for the limitations of controlling a linkage-based under-actuated hand with a self-locking mechanism. Then, a within-hand object position manipulation framework with proposed methodologies is presented and experimented with to show its effectiveness. keywords: {Force;Estimation;Grasping;Sensor phenomena and characterization;Robot sensing systems;Real-time systems;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341517&isnumber=10341342

K. Chen et al., "Contact-Aware Shaping and Maintenance of Deformable Linear Objects With Fixtures," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341726.Abstract: Studying the manipulation of deformable linear objects has significant practical applications in industry, including car manufacturing, textile production, and electronics automation. However, deformable linear object manipulation poses a significant challenge in developing planning and control algorithms, due to the precise and continuous control required to effectively manipulate the deformable nature of these objects. In this paper, we propose a new framework to control and maintain the shape of deformable linear objects with two robot manipulators utilizing environmental contacts. The framework is composed of a shape planning algorithm which automatically generates appropriate positions to place fixtures, and an object-centered skill engine which includes task and motion planning to control the motion and force of both robots based on the object status. The status of the deformable linear object is estimated online utilizing visual as well as force information. The framework manages to handle a cable routing task in real-world experiments with two Panda robots and especially achieves contact-aware and flexible clip fixing with challenging fixtures. keywords: {Visualization;Shape;Fixtures;Force;Routing;Robot sensing systems;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341726&isnumber=10341342

A. Meixner, F. Krebs, N. Jaquier and T. Asfour, "An Evaluation of Action Segmentation Algorithms on Bimanual Manipulation Datasets," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4912-4919, doi: 10.1109/IROS55552.2023.10341956.Abstract: Humans naturally execute many everyday manipulation actions with both arms simultaneously. Similarly, endowing robots with bimanual manipulation task models is key to efficiently perform complex manipulation tasks. To do so, a promising approach is to learn a library of task models from human demonstrations. However, this requires human motions to be meaningfully segmented. In this paper, we propose to segment the motion of each hand individually to account for different bimanual coordination patterns and provide a thorough evaluation of state-of-the-art segmentation algorithms on bimanual manipulation datasets. In particular, we compare segmentation algorithms at trajectory and semantic level with hierarchical algorithms. Moreover, our evaluation extensively studies the performances of various segmentation algorithms over a novel extension of the KIT Bimanual Manipulation Dataset featuring ~ 176 minutes of human motion recordings in household scenarios. keywords: {Motion segmentation;Robot kinematics;Semantics;Libraries;Trajectory;Recording;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341956&isnumber=10341342

J. Liu, Z. Li, W. Lin, S. Calinon, K. C. Tan and F. Chen, "SoftGPT: Learn Goal-Oriented Soft Object Manipulation Skills by Generative Pre-Trained Heterogeneous Graph Transformer," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4920-4925, doi: 10.1109/IROS55552.2023.10341846.Abstract: Soft object manipulation tasks in domestic scenes pose a significant challenge for existing robotic skill learning techniques due to their complex dynamics and variable shape characteristics. Since learning new manipulation skills from human demonstration is an effective way for robot applications, developing prior knowledge of the representation and dynamics of soft objects is necessary. In this regard, we propose a pretrained soft object manipulation skill learning model, namely SoftGPT, that is trained using large amounts of exploration data, consisting of a three-dimensional heterogeneous graph representation and a GPT-based dynamics model. For each downstream task, a goal-oriented policy agent is trained to predict the subsequent actions, and SoftGPT generates the consequences of these actions. Integrating these two approaches establishes a thinking process in the robot's mind that provides rollout for facilitating policy learning. Our results demonstrate that leveraging prior knowledge through this thinking process can efficiently learn various soft object manipulation skills, with the potential for direct learning from human demonstrations. keywords: {Solid modeling;Shape;Transformers;Data models;Task analysis;Robots;Manipulator dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341846&isnumber=10341342

J. Yeom, G. Li and G. Loianno, "Geometric Fault-Tolerant Control of Quadrotors in Case of Rotor Failures: An Attitude Based Comparative Study," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4974-4980, doi: 10.1109/IROS55552.2023.10341669.Abstract: The ability of aerial robots to operate in the presence of failures is crucial in various applications that demand continuous operations, such as surveillance, monitoring, and inspection. In this paper, we propose a fault-tolerant control strategy for quadrotors that can adapt to single and dual complete rotor failures. Our approach augments a classic geometric tracking controller on $S{O}(3)\times \mathbb{R}^{3}$ to accommodate the effects of rotor failures. We provide an in-depth analysis of several attitude error metrics to identify the most appropriate design choice for fault-tolerant control strategies. To assess the effectiveness of these metrics, we evaluate trajectory tracking accuracies. Simulation results demonstrate the performance of the proposed approach. keywords: {Measurement;Fault tolerance;Attitude control;Trajectory tracking;Surveillance;Simulation;Fault tolerant systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341669&isnumber=10341342

J. Pak, B. Kim, C. Ju, S. H. You and H. I. Son, "UAV-Based Trilateration System for Localization and Tracking of Radio-Tagged Flying Insects: Development and Field Evaluation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-8, doi: 10.1109/IROS55552.2023.10341725.Abstract: As the interest in ecosystem protection increases, many researchers are focusing on tracking flying insects to preserve biodiversity. Invasive alien species (IAS) such as Vespa velutina nigrithorax require extensive consideration in this regard owing to size and weight limitations. In this paper, we propose and experimentally validate an unmanned aerial vehicle (UAV)-based trilateration system for localization and tracking of flying insects. The proposed tracking algorithm is based on the use of multiple omnidirectional antennas, and it improves the tracking accuracy and shortens the tracking time compared to those of the existing unidirectional antennas. In addition, we reduce the measurement error and noise by applying a finite impulse response (FIR) filter to the received signal strength (RSS) emitted from the radio-telemetry transmitter. A field experiment involving the proposed trilateration system was conducted in an apiary. This experiment was divided into three sub-experiments as follows: 1) behavior experiment, 2) ground truth experiment, and 3) localization and tracking experiment. The proposed system contributes to the research on UAV-based tracking systems that respond to the speed of spreading of invasive V. velutina. keywords: {Location awareness;Measurement errors;Target tracking;Finite impulse response filters;Insects;Radio transmitters;Omnidirectional antennas},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341725&isnumber=10341342

N. Pan, R. Jin, C. Xu and F. Gao, "Canfly: A Can-Sized Autonomous Mini Coaxial Helicopter," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4989-4996, doi: 10.1109/IROS55552.2023.10341490.Abstract: The development of autonomous rotary-wing UAVs has shown an evident tendency in miniaturization. However, the side effects brought by miniaturization, such as decreased load capability, shorter flight duration and reduced autonomous ability, seriously hinder its process. In this paper, we first investigate the configurations of different rotary-wing aircraft and optimize the configuration selection. Afterward, with several elaborate mechanisms contributing to the miniaturization, we present the hardware design and control strategy of a mini coaxial helicopter, which is 62% smaller than the state-of-the-art autonomous mini quadrotor so far in collision area [1]. Meanwhile, abundant experiments reveal that it achieves impressive traversability and is capable of conducting autonomous tasks in unknown dense scenarios, while maintaining satisfactory performance regarding loadability and flight duration. keywords: {Propellers;Atmospheric modeling;Stacking;Aerodynamics;Controllability;Hardware;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341490&isnumber=10341342

X. Meng, Y. He, H. Xi, J. Han and A. Song, "Aerial Manipulator Systems Gain a New Skill: Achieve Contact-based Landing on a Mobile Platform," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 4997-5002, doi: 10.1109/IROS55552.2023.10342395.Abstract: This paper studies a novel application of an aerial manipulator (AM)-the contact-based landing on a mobile platform. An AM is inherently unstable, under-actuated, and usually loses some DOFs while contacting environments. Meanwhile, the AM's flight state is susceptible to uncertain movements of the mobile platform, such as acceleration, sudden stopping, and reversing. To accomplish the contact-based landing mission, a robust controller is first designed to maintain a steady contact-based flight. Then a hierarchical control framework is applied, integrating the controllers in free-flight and restricted-flight stages. An AM and a mobile platform are developed for contact-based flight experiments. The proposed scheme is reliable and has good repeatability in experiments. To the best of our knowledge, this is the first time an AM has been implemented to conduct a contact-based landing, which is also an innovative landing approach for rotorcraft UAVs. keywords: {Trajectory planning;Laboratories;Force;Manipulators;Autonomous aerial vehicles;Reliability;State estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342395&isnumber=10341342

S. Hwang, B. D. W. Remes and G. C. H. E. De Croon, "AOSoar: Autonomous Orographic Soaring of a Micro Air Vehicle," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5003-5010, doi: 10.1109/IROS55552.2023.10341699.Abstract: Utilizing wind hovering techniques of soaring birds can save energy expenditure and improve the flight endurance of micro air vehicles (MAVs). Here, we present a novel method for fully autonomous orographic soaring without a priori knowledge of the wind field. Specifically, we devise an Incremental Nonlinear Dynamic Inversion (INDI) controller with control allocation, adapting it for autonomous soaring. This allows for both soaring and the use of the throttle if necessary, without changing any gain or parameter during the flight. Furthermore, we propose a simulated-annealing-based optimization method to search for soaring positions. This enables for the first time an MAV to autonomously find a feasible soaring position while minimizing throttle usage and other control efforts. Autonomous orographic soaring was performed in the wind tunnel. The wind speed and incline of a ramp were changed during the soaring flight. The MAV was able to perform autonomous orographic soaring for flight times of up to 30 minutes. The mean throttle usage was only 0.25% for the entire soaring flight, whereas normal powered flight requires 38%. Also, it was shown that the MAV can find a new soaring spot when the wind field changes during the flight. keywords: {Trajectory planning;Wind tunnels;Wind speed;Search methods;Optimization methods;Manuals;Autonomous aerial vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341699&isnumber=10341342

Y. Yin, Q. Yang and H. Fang, "Error-State Kalman Filter Based External Wrench Estimation for MAVs Under a Cascaded Architecture," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5019-5026, doi: 10.1109/IROS55552.2023.10342358.Abstract: In many applications such as aerial transportation, delivery, and manipulation, it is essential to know the external wrench exerted on multirotor aerial vehicles precisely. This paper presents an algorithm to estimate external wrench using a rotor speed measurement unit, an inertial measurement unit and a motion capture system. Under a cascaded architecture containing two sub-systems, one error-state Kalman Filter is designed to estimate velocity and attitude and eliminate the bias of the measurement from the inertial measurement unit, the other error-state Kalman Filter is designed to estimate the external wrench. Observability of the two estimation subsystems is verified by the Lie derivative method. The proposed algorithm has been tested in simulations and real-world experiments, which demonstrates its superiority in providing real-time and accurate external wrench estimation. keywords: {Measurement units;Estimation;Transportation;Rotors;Computer architecture;Inertial navigation;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342358&isnumber=10341342

D. Lee, S. Hwang, C. Kim, S. J. Lee and H. J. Kim, "Minimally Actuated Tiltrotor for Perching and Normal Force Exertion," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5027-5033, doi: 10.1109/IROS55552.2023.10341910.Abstract: This study presents a new hardware design and control of a minimally actuated 5 control degrees of freedom (CDoF) quadrotor-based tiltrotor. The proposed tiltrotor possesses several characteristics distinct from those found in existing works, including: 1) minimal number of actuators for 5 CDoF, 2) large margin to generate interaction force during aerial physical interaction (APhI), and 3) no mechanical obstruction in thrust direction rotation. Thanks to these properties, the proposed tiltrotor is suitable for perching-enabled APhI since it can hover parallel to an arbitrarily oriented surface and can freely adjust its thrust direction. To fully control the 5-CDoF of the designed tiltrotor, we construct an asymptotically stabilizing controller with stability analysis. The proposed tiltrotor design and controller are validated in experiments where the first two experiments of x, y position tracking and pitch tracking show controllability of the added CDoF compared to a conventional quadrotor. Finally, the last experiment of perching and cart pushing demonstrates the proposed tiltrotor's applicability to perching-enabled APhI. keywords: {Actuators;Force;Controllability;Stability analysis;Hardware;Intelligent robots;Quadrotors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341910&isnumber=10341342

G. Li and G. Loianno, "Nonlinear Model Predictive Control for Cooperative Transportation and Manipulation of Cable Suspended Payloads with Multiple Quadrotors," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5034-5041, doi: 10.1109/IROS55552.2023.10341785.Abstract: Autonomous Micro Aerial Vehicles (MAVs) such as quadrotors equipped with manipulation mechanisms have the potential to assist humans in tasks such as construction and package delivery. Cables are a promising option for manipulation mechanisms due to their low weight, low cost, and simple design. However, designing control and planning strategies for cable mechanisms presents challenges due to indirect load actuation, nonlinear configuration space, and highly coupled system dynamics. In this paper, we propose a novel Nonlinear Model Predictive Control (NMPC) method that enables a team of quadrotors to manipulate a rigid-body payload in all 6 degrees of freedom via suspended cables. Our approach can concurrently exploit, as part of the receding horizon optimization, the available mechanical system redundancies to perform additional tasks such as inter-robot separation and obstacle avoidance while respecting payload dynamics and actuator constraints. To address real-time computational requirements and scalability, we employ a lightweight state vector parametrization that includes only payload states in all six degrees of freedom. This also enables the planning of trajectories on the SE (3) manifold load configuration space, thereby also reducing planning complexity. We validate the proposed approach through simulation and real-world experiments. keywords: {Scalability;Termination of employment;Aerospace electronics;Planning;Task analysis;Vehicle dynamics;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341785&isnumber=10341342

G. He, Y. Jangir, J. Geng, M. Mousaei, D. Bai and S. Scherer, "Image-Based Visual Servo Control for Aerial Manipulation Using a Fully-Actuated UAV," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5042-5049, doi: 10.1109/IROS55552.2023.10342145.Abstract: Using Unmanned Aerial Vehicles (UAVs) to per-form high-altitude manipulation tasks beyond just passive visual application can reduce the time, cost, and risk of human workers. Prior research on aerial manipulation has relied on either ground truth state estimate or GPS/total station with some Simultaneous Localization and Mapping (SLAM) algorithms, which may not be practical for many applications close to infrastructure with degraded GPS signal or featureless environments. Visual servo can avoid the need to estimate robot pose. Existing works on visual servo for aerial manipulation either address solely end-effector position control or rely on precise velocity measurement and pre-defined visual visual marker with known pattern. Furthermore, most of previous work used under-actuated UAVs, resulting in complicated mechanical and hence control design for the end-effector. This paper develops an image-based visual servo control strategy for bridge maintenance using a fully-actuated UAV. The main components are (1) a visual line detection and tracking system, (2) a hybrid impedance force and motion control system. Our approach does not rely on either robot pose/velocity estimation from an external localization system or pre-defined visual markers. The complexity of the mechanical system and controller architecture is also minimized due to the fully-actuated nature. Experiments show that the system can effectively execute motion tracking and force holding using only the visual guidance for the bridge painting. To the best of our knowledge, this is one of the first studies on aerial manipulation using visual servo that is capable of achieving both motion and force control without the need of external pose/velocity information or pre-defined visual guidance. keywords: {Bridges;Visualization;Simultaneous localization and mapping;Tracking;Force;Autonomous aerial vehicles;End effectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342145&isnumber=10341342

D. Patel, P. Pham and A. Bera, "DroNeRF: Real-Time Multi-Agent Drone Pose Optimization for Computing Neural Radiance Fields," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5050-5055, doi: 10.1109/IROS55552.2023.10342420.Abstract: We present a novel optimization algorithm called DroNeRF for the autonomous positioning of monocular camera drones around an object for real-time 3D reconstruction using only a few images. Neural Radiance Fields, or NeRF, is a novel view synthesis technique used to generate new views of an object or scene from a set of input images. Using drones in conjunction with NeRF provides a unique and dynamic way to generate novel views of a scene, especially with limited scene capabilities of restricted movements. Our approach focuses on calculating optimized pose for individual drones while solely depending on the object geometry without using any external localization system. The unique camera positioning during the data capturing phase significantly impacts the quality of the 3D model. To evaluate the quality of our generated novel views, we compute different perceptual metrics like the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). Our work demonstrates the benefit of using an optimal placement of various drones with limited mobility to generate perceptually better results. keywords: {Measurement;Location awareness;Solid modeling;Three-dimensional displays;PSNR;Cameras;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342420&isnumber=10341342

A. Mohammadi and H. Malik, "Generation of Time-Varying Impedance Attacks Against Haptic Shared Control Steering Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5064-5071, doi: 10.1109/IROS55552.2023.10342459.Abstract: The safety-critical nature of vehicle steering is one of the main motivations for exploring the space of possible cyber-physical attacks against the steering systems of modern vehicles. This paper investigates the adversarial capabilities for destabilizing the interaction dynamics between human drivers and vehicle haptic shared control (HSC) steering systems. In contrast to the conventional robotics literature, where the main objective is to render the human-automation interaction dynamics stable by ensuring passivity, this paper takes the exact opposite route. In particular, to investigate the damaging capabilities of a successful cyber-physical attack, this paper demonstrates that an attacker who targets the HSC steering system can destabilize the interaction dynamics between the human driver and the vehicle HSC steering system through synthesis of time-varying impedance profiles. Specifically, it is shown that the adversary can utilize a properly designed non-passive and time-varying adversarial impedance target dynamics, which are fed with a linear combination of the human driver and the steering column torques. Using these target dynamics, it is possible for the adversary to generate in realtime a reference angular command for the driver input device and the directional control steering assembly of the vehicle. Furthermore, it is shown that the adversary can make the steering wheel and the vehicle steering column angular positions to follow the reference command generated by the time-varying impedance target dynamics using proper adaptive control strategies. Numerical simulations demonstrate the effectiveness of such time-varying impedance attacks, which result in a non-passive and inherently unstable interaction between the driver and the HSC steering system. keywords: {Space vehicles;Steering systems;Wheels;Control systems;Numerical simulation;Space exploration;Haptic interfaces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342459&isnumber=10341342

Z. Wang, X. Xu, D. Yang, Z. Wang, S. Shtaierman and E. Steinbach, "Haptic Dataset Augmentation with Subjective QoE Labels using Conditional Generative Adversarial Network," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5072-5078, doi: 10.1109/IROS55552.2023.10341967.Abstract: This paper proposes a novel Generative Adversarial Network (GAN)-based strategy to augment subjective haptic Quality of Experience (QoE) datasets for bilateral teleoperation with haptic feedback without conducting time-consuming subjective experiments. In our previous work, we proposed a multi-assessment fusion approach to predict subjective haptic quality using a collection of objective metrics. This method requires a sufficiently large haptic dataset with QoE labels. The proposed generative approach automatically expands the existing haptic quality dataset by combining a modified conditional GAN (CGAN) and Style GAN (StyleGAN) architecture. The most important feature of our method is that it learns from the labeled training data and focuses on synthesizing signals with artifacts according to new input labels containing the QoE score, time delay, control method, and data reduction information. Extensive experiments are conducted to validate the suitability of the expanded dataset. The results show that our approach is able to generate new data, which match the label and signal distribution of the original data with categorical rank and linear correlation of over 0.85. keywords: {Training;Measurement;Correlation;Training data;Generative adversarial networks;Real-time systems;Haptic interfaces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341967&isnumber=10341342

S. Heredia, H. Masuda, A. Miyamoto and Y. Kuroda, "A Physically Based Deformable Model with Haptic Feedback for Real-Time Robotic Surgery Simulation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5079-5086, doi: 10.1109/IROS55552.2023.10341694.Abstract: Surgical simulators have been under development for years, formerly intended for surgical training and recently applied for training machine learning models. These systems often employ computationally efficient methods such as mass-spring models or position-based dynamics that prioritize real-time execution over physical accuracy, while the use of the finite element method (FEM) has been limited due its computational cost. In consequence, there has been little improvement in the accuracy of the deformable models and the haptics, relying on hand-tuned stiffness parameters, and empirical solutions to estimate the contact forces. To solve these limitations, we propose to develop a new surgical simulator for laparoscopic cholecystectomy employing the extended position-based dynamics method in conjunction with FEM to compute physically based deformation and obtain accurate contact forces during the collision response. While dense organs like the liver are modeled using tetrahedrons and addressed the element inversion problem in FEM, we propose to simulate the gallbladder as a closed elastic membrane using two-dimensional FEM elements with volume preserving constraints to model the inner incompressible fluid. Because continuous position and contact force measurement on real materials to verify the simulation is challenging, we employ a bilateral robotic system equipped with Fiber Bragg Grating-based force sensors. keywords: {Deformable models;Training;Solid modeling;Force measurement;Computational modeling;Machine learning;Robot sensing systems;Simulation and Animation;Haptics and Haptic Interfaces;Surgical Robotics;Laparoscopy},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341694&isnumber=10341342

J. Pankert and M. Hutter, "Learning Contact-Based State Estimation for Assembly Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5087-5094, doi: 10.1109/IROS55552.2023.10342219.Abstract: Robotic object manipulation requires knowledge of the environment's state. In particular, the object poses of fixed elements in the environment relative to the robot and the in-hand poses of grasped objects are of interest. For insertion tasks with tight tolerances, the accuracy of vision systems to estimate the object and in-hand pose is not high enough. This work proposes a state estimation system that delivers precise estimates for both estimation problems. It uses contact detections and the precise forward kinematics that robot arms provide thanks to their high-resolution joint encoders. We propose a reinforcement-learning-based exploration strategy that decides how the robot should engage with the environment to reduce state uncertainty. The system is evaluated in several simulation and hardware experiments. We show that the learned policy can propose meaningful actions for object localization. In hardware experiments with precision-milled objects, sub-millimeter accuracy is achieved for the in-hand pose estimation task. With objects relevant to industrial tasks, i.e., a melting fuse and a fuse box, millimeter-level accuracy can be reached for both in-hand pose estimation and fixed object localization. In an integrated experiment, we show how a robot grasps a fuse, estimates the in-hand pose, and inserts it into a fuse box. keywords: {Location awareness;Solid modeling;Uncertainty;Fuses;Service robots;Pose estimation;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342219&isnumber=10341342

A. Noccaro, S. Buscaglione, M. Pinardi, G. Di Pino and D. Formica, "Evaluation of a 7-DoFs Robotic Manipulator as Haptic Interface During Planar Reaching Tasks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5095-5100, doi: 10.1109/IROS55552.2023.10342470.Abstract: In this work, we evaluated the suitability of using a 7 degrees of freedom robotic manipulator as a planar haptic interface for studies in motor neuroscience. In particular, we assessed to what extent it can measure human movement and forces without applying undesired perturbations. To this aim, we evaluated the amount of perturbation exerted by the robot during planar reaching movements when controlled to be as transparent as possible in the 2D task space, through an impedance control. Two planar specular configurations of the robot were tested, namely G1 and G2, which differ in the position of the “elbow joint” in the workspace. For both configurations, we estimated the inertial ellipsoids and simulated the forces for human-like forward movements. Performance was then experimentally assessed on 8 healthy participants, in 15 different positions in the workspace. The average handpath perturbation decreased and settled to 6 mm after 2 minutes of interaction. Interaction forces resulted specular for G1 and G2, with mean values below 5 N. Overall, the robotic manipulator resulted suitable for studies on planar reaching movements in both configurations, with a preference for the G1 configuration due to its symmetrical distribution of trajectory deviations, which anyway remain well below 1 cm for movements of 15 cm. keywords: {Neuroscience;Perturbation methods;Manipulators;Particle measurements;Haptic interfaces;Trajectory;Motion measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342470&isnumber=10341342

M. R. Devlin, T. Liu, M. Zhu, N. S. Usevitch, N. Colonnese and A. H. Memar, "Soft, Modular, Shape-Changing Displays with Hyperelastic Bubble Arrays," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5101-5106, doi: 10.1109/IROS55552.2023.10341591.Abstract: Incorporating compliance into shape-changing displays can improve their wearability and actuation modalities. While recent advances in soft actuators highlight promising paths for soft shape-changing displays, these displays currently face some practical challenges of device failure and limited actuator displacement. A monolithic fabrication processes means the device is challenging to repair, for a single point of failure often renders the whole device ineffective. We have leveraged a modular hyperelastic bubble array layer to create a soft shape-changing skin. The modularity of this device allows for rapid repair of individual bubbles and fast prototyping, and the spherical, hyperelastic actuators enable an increase in degrees of freedom due to bubble-to-bubble interactions. Furthermore, we present a forward kinematic description of our device, incorporating these bubble-to-bubble interactions and the nonlinear instabilities unique to hyperelastic actuator inflation. We demonstrate the utility of this soft shape-changing skin as a haptic display that can be worn comfortably or applied to passive interactive objects such as a computer mouse. keywords: {Fabrication;Actuators;Kinematics;Maintenance engineering;Soft robotics;Skin;Mice},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341591&isnumber=10341342

A. Gerald, J. Ye, R. Batliwala, P. Hsu, J. Pang and S. Russo, "Soft Optical Sensor and Haptic Feedback System for Remote and Robot-Assisted Palpation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5107-5113, doi: 10.1109/IROS55552.2023.10341754.Abstract: Robotic palpation shows significant potential to improve the accuracy and speed of tumor identification. How-ever, robotic palpation mechanisms often lack haptic feedback, making it difficult for the surgeon to identify variations in tissue stiffness. This paper presents a soft optical sensor integrated with a wearable haptic glove for tumor detection during robotic palpation. The sensor contains an array of optical waveguides that can detect the presence of tumors embedded within a tissue phantom. Detection of a tumor results in an optical loss from the waveguide signal, triggering proportional inflation of the soft microfluidic actuators in the glove. The glove consists of four modular actuators placed at the fingertips, each corresponding to a sensing location on the waveguide array. The inflation of each actuator is proportional to the incident loss on the palpation sensor array, which is dependent on tumor depth. Thus, the glove is capable of alerting the user to the location of tumors during remote palpation. keywords: {Optical losses;Actuators;Optical feedback;Optical device fabrication;Robot sensing systems;Haptic interfaces;Optical sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341754&isnumber=10341342

J. Lin et al., "TIMS: A Tactile Internet-Based Micromanipulation System with Haptic Guidance for Surgical Training," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5114-5121, doi: 10.1109/IROS55552.2023.10341980.Abstract: Microsurgery involves the dexterous manipulation of delicate tissue or fragile structures, such as small blood vessels and nerves, under a microscope. To address the limitations of imprecise manipulation of human hands, robotic systems have been developed to assist surgeons in performing complex microsurgical tasks with greater precision and safety. However, the steep learning curve for robot-assisted microsurgery (RAMS) and the shortage of well-trained surgeons pose significant challenges to the widespread adoption of RAMS. Therefore, the development of a versatile training system for RAMS is necessary, which can bring tangible benefits to both surgeons and patients. In this paper, we present a Tactile Internet-Based Micromanipulation System (TIMS) based on a ROS-Django web-based architecture for microsurgical training. This system can provide tactile feedback to operators via a wearable tactile display (WTD), while real-time data is transmitted through the internet via a ROS-Django framework. In addition, TIMS integrates haptic guidance to ‘guide’ the trainees to follow a desired trajectory provided by expert surgeons. Learning from demonstration based on Gaussian Process Regression (GPR) was used to generate the desired trajectory. We conducted user studies to verify the effectiveness of our proposed TIMS, comparing users' performance with and without tactile feedback and/or haptic guidance. For more details of this project, please view our website: https://sites.google.com/view/viewtims/home. keywords: {Training;Microscopy;Random access memory;Tactile sensors;Microsurgery;Real-time systems;Haptic interfaces},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341980&isnumber=10341342

E. Mendoza and J. P. Whitney, "A Teleoperated MR-Safe Haptic System for Magnetic Resonance Imaging-Guided Prostate Needle Biopsies," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5122-5127, doi: 10.1109/IROS55552.2023.10342113.Abstract: Real-time magnetic resonance imaging (MRI) in-terventions are significantly impacted by material compatibility problems and size constraints in the MRI bore. Limited physi-cian access to patients within the bore of the MRI necessitates iterative positioning and imaging, which prolongs the duration of the procedure and increases patient risk. We present a passive MR-safe haptic teleoperation device for prostate needle biopsy inside an MRI machine. The device uses a low-friction hydrostatic transmission based on paired rolling diaphragm actuators, linear and rotary. The device has two degrees of freedom, allowing needle insertion and rotation. The robot produces negligible MR imaging artifacts, has effective positioning tracking, and can reliably detect needle punctures in the clinically relevant range. We describe the design components, system transparency, and perform a needle insertion test. keywords: {Performance evaluation;Magnetic resonance imaging;Biopsy;Magnetic resonance;Needles;Real-time systems;Haptic interfaces;Haptics and Haptic Interfaces;Medical Robots and Systems;Telerobotics and Teleoperation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342113&isnumber=10341342

S. Lee, H. Kim and D. Lee, "Symmetry-Based Modeling and Hybrid Orientation-Force Control of Wearable Cutaneous Haptic Device," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5128-5134, doi: 10.1109/IROS55552.2023.10341859.Abstract: We propose novel symmetry-based modeling and hybrid orientation-force control frameworks for cutaneous haptic device (CHD) to generate precise three degree-of-freedom (DoF) contact force on the fingertip robustly against user variability. The CHD hardware is designed in a form of an underactuated cable-driven parallel mechanism, with springs placed along the tendon to stabilize the pose. We analyze the kinematics of the CHD and propose a pose estimator by exploiting the symmetrical nature of the mechanism. We then devise a hybrid orientation-force controller to track the direction and magnitude of the desired contact force simultaneously in a feedback manner for control accuracy and robustness. We also adopt a tension regulator to mitigate friction effect during the actuation. Experimental validation and demonstration show the efficacy of the CHD with our proposed estimation and control framework. keywords: {Performance evaluation;Regulators;Force;Kinematics;Systems modeling;Hybrid power systems;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341859&isnumber=10341342

Y. Li, A. Debnath, G. J. Stein and J. Košecká, "Learning-Augmented Model-Based Planning for Visual Exploration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5165-5171, doi: 10.1109/IROS55552.2023.10341773.Abstract: We consider the problem of time-limited robotic exploration in previously unseen environments where exploration is limited by a predefined amount of time. We propose a novel exploration approach using learning-augmented model-based planning. We generate a set of sub goals associated with frontiers on the current map and derive a Bellman Equation for exploration with these subgoals. Visual sensing and advances in semantic mapping of indoor scenes are exploited for training a deep convolutional neural network to estimate properties associated with each frontier: the expected unobserved area beyond the frontier and the expected time steps (discretized actions) required to explore it. The proposed model-based planner is guaranteed to explore the whole scene if time permits. We thoroughly evaluate our approach on a large-scale pseudo-realistic indoor dataset (Matterport3D) with the Habitat simulator. We compare our approach with classical and more recent RL-based exploration methods. Our approach surpasses the greedy strategies by 2.1% and the RL-based exploration methods by 8.4% in terms of coverage. keywords: {Training;Visualization;Three-dimensional displays;Semantics;Robot sensing systems;Mathematical models;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341773&isnumber=10341342

J. Jiang, P. Li, X. Lv and Y. Yang, "DMCL: Robot Autonomous Navigation via Depth Image Masked Contrastive Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5172-5178, doi: 10.1109/IROS55552.2023.10341836.Abstract: Achieving high performance in deep reinforcement learning relies heavily on the ability to obtain good state representations from pixel inputs. However, learning an observation-space-to-action-space mapping from high-dimensional inputs is challenging in reinforcement learning, particularly when dealing with consecutive depth images as input states. In addition, we observe that the consecutive inputs of depth images are highly correlated for the autonomous navigation of a mobile robot, which inspires us to capture temporal correlations between consecutive inputs and infer scene change relationships. To this end, we propose a novel end-to-end robot vision navigation method dubbed DMCL, which obtains good spatial-temporal state representation via Depth image Masked Contrastive Learning. It reconstructs the latent representation from consecutive depth images masked in both spatial and temporal dimensions, resulting in a complete environment state representation. To obtain the optimal navigation policy, we leverage the Soft Actor-Critic reinforcement learning in conjunction with the above representation learning. Extensive experiments demonstrate that the proposed DMCL outperforms representative state-of-the-art methods. The source code will be made publicly available. keywords: {Representation learning;Deep learning;Navigation;Source coding;Robot vision systems;Reinforcement learning;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341836&isnumber=10341342

H. -G. Cao et al., "Image-based Regularization for Action Smoothness in Autonomous Miniature Racing Car with Deep Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5179-5186, doi: 10.1109/IROS55552.2023.10342029.Abstract: Deep reinforcement learning has achieved signif-icant results in low-level controlling tasks. However, for some applications like autonomous driving and drone flying, it is difficult to control behavior stably since the agent may suddenly change its actions which often lowers the controlling sys-tem's efficiency, induces excessive mechanical wear, and causes uncontrollable, dangerous behavior to the vehicle. Recently, a method called conditioning for action policy smoothness (CAPS) was proposed to solve the problem of jerkiness in low-dimensional features for applications such as quadrotor drones. To cope with high-dimensional features, this paper proposes image-based regularization for action smoothness (1-RAS) for solving jerky control in autonomous miniature car racing. We also introduce a control based on impact ratio, an adaptive regularization weight to control the smoothness constraint, called IR control. In the experiment, an agent with 1- RAS and IR control significantly improves the success rate from 59% to 95%. In the real-world-track experiment, the agent also outperforms other methods, namely reducing the average finish lap time, while also improving the completion rate even without real world training. This is also justified by an agent based on I-RAS winning the 2022 AWS DeepRacer Final Championship Cup. keywords: {Deep learning;Training;Reinforcement learning;Robustness;Trajectory;Behavioral sciences;Automobiles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342029&isnumber=10341342

M. Ibrahim, N. Akhtar, S. Anwar and A. Mian, "UnLoc: A Universal Localization Method for Autonomous Vehicles using LiDAR, Radar and/or Camera Input," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5187-5194, doi: 10.1109/IROS55552.2023.10342046.Abstract: Localization is a fundamental task in robotics for autonomous navigation. Existing localization methods rely on a single input data modality or train several computational models to process different modalities. This leads to stringent computational requirements and sub-optimal results that fail to capitalize on the complementary information in other data streams. This paper proposes UnLoc, a novel unified neural modeling approach for localization with multi-sensor input in all weather conditions. Our multi-stream network can handle LiDAR, Camera and RADAR inputs for localization on demand, i.e., it can work with one or more input sensors, making it robust to sensor failure. UnLoc uses 3D sparse convolutions and cylindrical partitioning of the space to process LiDAR frames and implements ResNet blocks with a slot attention-based feature filtering module for the Radar and image modalities. We introduce a unique learnable modality encoding scheme to distinguish between the input sensor data. Our method is extensively evaluated on Oxford Radar RobotCar, ApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our technique. The dataset, results, and codes are available at https://github.com/IbrahimUWA/UnLoc keywords: {Location awareness;Laser radar;Three-dimensional displays;Filtering;Radar;Radar imaging;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342046&isnumber=10341342

M. Waheed, S. Waheed, M. Milford, K. McDonald-Maier and S. Ehsan, "A Complementarity-Based Switch-Fuse System for Improved Visual Place Recognition," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5195-5202, doi: 10.1109/IROS55552.2023.10341876.Abstract: Recently several fusion and switching based approaches have been presented to solve the problem of Visual Place Recognition. In spite of these systems demonstrating significant boost in VPR performance they each have their own set of limitations. The multi-process fusion systems usually involve employing brute force and running all available VPR techniques simultaneously while the switching method attempts to negate this practise by only selecting the best suited VPR technique for given query image. But switching does fail at times when no available suitable technique can be identified. An innovative solution would be an amalgamation of the two otherwise discrete approaches to combine their competitive advantages while negating their shortcomings. The proposed, Switch-Fuse system, is an interesting way to combine both the robustness of switching VPR techniques based on complementarity and the force of fusing the carefully selected techniques to significantly improve performance. Our system holds a structure superior to the basic fusion methods as instead of simply fusing all or any random techniques, it is structured to first select the best possible VPR techniques for fusion, according to the query image. The system combines two significant processes, switching and fusing VPR techniques, which together as a hybrid model substantially improve performance on all major VPR data sets illustrated using PR curves. keywords: {Visualization;Force;Switches;Robustness;Data models;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341876&isnumber=10341342

G. A. Sigurdsson, J. Thomason, G. S. Sukhatme and R. Piramuthu, "RREx-BoT: Remote Referring Expressions with a Bag of Tricks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5203-5210, doi: 10.1109/IROS55552.2023.10342093.Abstract: Household robots operate in the same space for years. Such robots incrementally build dynamic maps that can be used for tasks requiring remote object localization. However, benchmarks in robot learning often test generalization through inference on tasks in unobserved environments. In an observed environment, locating an object is reduced to choosing from among all object proposals in the environment, which may number in the 100,000s. Armed with this intuition, using only a generic vision-language scoring model with minor modifications for 3d encoding and operating in an embodied environment, we demonstrate an absolute performance gain of 9.84% on remote object grounding above state of the art models for REVERIE and of 5.04% on FAO. When allowed to pre-explore an environment, we also exceed the previous state of the art pre-exploration method on REVERIE. Additionally, we demonstrate our model on a real-world TurtleBot platform, highlighting the simplicity and usefulness of the approach. Our analysis outlines a “bag of tricks” essential for accomplishing this task, from utilizing 3d coordinates and context, to gener-alizing vision-language models to large 3d search spaces. keywords: {Training;Location awareness;Solid modeling;Three-dimensional displays;Grounding;Robot kinematics;Performance gain},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342093&isnumber=10341342

Z. Xu et al., "PLPL-VIO: A Novel Probabilistic Line Measurement Model for Point-Line-Based Visual-Inertial Odometry," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5211-5218, doi: 10.1109/IROS55552.2023.10342387.Abstract: Point and line features are complementary in Visual-Inertial Odometry (VIO) or Visual-Inertial Simultaneous Localization And Mapping (VI-SLAM) systems. The advantage of combining these two types of features relies on their proper weighting in the cost function, usually set by their uncertainty. Compared with point features, setting line segment endpoints' uncertainty with isotropic distribution is unreasonable. But the uncertainty of line feature observation, especially for the endpoints' uncertainty along the line, is difficult to set due to occlusion and fragmentation problems. In this article, we use infinite lines as the line feature observations and prove that the uncertainty of these observations is only related to the vertical uncertainty of the endpoints, thus avoiding setting the parallel uncertainty of the endpoints. Besides, we introduce a novel consistent measurement model for line features. Furthermore, for long-time constraints, we add 3D line segments into the state vector and derive how to update them properly. Finally, we construct a point-line-based VIO system that takes into account the uncertainty of line feature observations and the consistency of line feature measurements. The proposed VIO system is validated on two public datasets. The results show that the proposed method obtains the best accuracy compared with the state-of-the-art point-based VIO systems (OpenVINS, VINS-Mono), a point-line-based VIO system (PL-VINS), and a structural line-based system (StructVIO). keywords: {Weight measurement;Measurement errors;Uncertainty;Three-dimensional displays;Simultaneous localization and mapping;Measurement uncertainty;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342387&isnumber=10341342

H. Kondoh and A. Kanezaki, "Multi-Goal Audio-Visual Navigation Using Sound Direction Map," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5219-5226, doi: 10.1109/IROS55552.2023.10341819.Abstract: Over the past few years, there has been a great deal of research on navigation tasks in indoor environments using deep reinforcement learning agents. Most of these tasks use only visual information in the form of first-person images to navigate to a single goal. More recently, tasks that simultaneously use visual and auditory information to navigate to the sound source and even navigation tasks with multiple goals instead of one have been proposed. However, there has been no proposal for a generalized navigation task combining these two types of tasks and using both visual and auditory information in a situation where multiple sound sources are goals. In this paper, we propose a new framework for this generalized task: multi-goal audio-visual navigation. We first define the task in detail, and then we investigate the difficulty of the multi-goal audio-visual navigation task relative to the current navigation tasks by conducting experiments in various situations. The research shows that multi-goal audio-visual navigation has the difficulty of the implicit need to separate the sources of sound. Next, to mitigate the difficulties in this new task, we propose a method named sound direction map (SDM), which dynamically localizes multiple sound sources in a learning-based manner while making use of past memories. Experimental results show that the use of SDM significantly improves the performance of multiple baseline methods, regardless of the number of goals. keywords: {Degradation;Deep learning;Visualization;Source separation;Navigation;Reinforcement learning;Indoor environment},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341819&isnumber=10341342

M. Hutsebaut-Buysse et al., "Directed Real-World Learned Exploration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5227-5234, doi: 10.1109/IROS55552.2023.10341504.Abstract: Automated Guided Vehicles (AGV) are omnipresent, and are able to carry out various kind of preprogrammed tasks. Unfortunately, a lot of manual configuration is still required in order to make these systems operational, and configuration needs to be re-done when the environment or task is changed. As an alternative to current inflexible methods, we employ a learning based method in order to perform directed exploration of a previously unseen environment. Instead of relying on handcrafted heuristic representations, the agent learns its own environmental representation through its embodiment. Our method offers loose coupling between the Reinforcement Learning (RL) agent, which is trained in simulation, and a separate, on real-world images trained task module. The uncertainty of the task module is used to direct the exploration behavior. As an example, we use a warehouse inventory task, and we show how directed exploration can improve the task performance through active data collection. We also propose a novel environment representation to efficiently tackle the sim2real gap in both sensing and actuation. We empirically evaluate the approach both in simulated environments and a real-world warehouse. keywords: {Training;Learning systems;Uncertainty;Remotely guided vehicles;Navigation;Robot vision systems;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341504&isnumber=10341342

S. Dey, A. Sadek, G. Monaci, B. Chidlovskii and C. Wolf, "Learning Whom to Trust in Navigation: Dynamically Switching Between Classical and Neural Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5235-5242, doi: 10.1109/IROS55552.2023.10342308.Abstract: Navigation of terrestrial robots is typically addressed either with localization and mapping (SLAM) followed by classical planning on the dynamically created maps, or by machine learning (ML), often through end-to-end training with reinforcement learning (RL) or imitation learning (IL). Recently, modular designs have achieved promising results, and hybrid algorithms that combine ML with classical planning have been proposed. Existing methods implement these combinations with handcrafted functions, which cannot fully exploit the complementary nature of the policies and the complex regularities between scene structure and planning performance. Our work builds on the hypothesis that the strengths and weaknesses of neural planners and classical planners follow some regularities, which can be learned from training data, in particular from interactions. This is grounded on the assumption that, both, trained planners and the mapping algorithms underlying classical planning are subject to failure cases depending on the semantics of the scene and that this dependence is learnable: for instance, certain areas, objects or scene structures can be reconstructed easier than others. We propose a hierarchical method composed of a high-level planner dynamically switching between a classical and a neural planner. We fully train all neural policies in simulation and evaluate the method in both simulation and real experiments with a LoCoBot robot, showing significant gains in performance, in particular in the real environment. We also qualitatively conjecture on the nature of data regularities exploited by the high-level planner. keywords: {Training;Three-dimensional displays;Simultaneous localization and mapping;Navigation;Heuristic algorithms;Semantics;Training data},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342308&isnumber=10341342

J. Fu, Y. Song, Y. Wu, F. Yu and D. Scaramuzza, "Learning Deep Sensorimotor Policies for Vision-Based Autonomous Drone Racing," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5243-5250, doi: 10.1109/IROS55552.2023.10341805.Abstract: The development of effective vision-based algorithms has been a significant challenge in achieving autonomous drones, which promise to offer immense potential for many real-world applications. This paper investigates learning deep sensorimotor policies for vision-based drone racing, which is a particularly demanding setting for testing the limits of an algorithm. Our method combines feature representation learning to extract task-relevant feature representations from high-dimensional image inputs with a learning-by-cheating framework to train a deep sensorimotor policy for vision-based drone racing. This approach eliminates the need for globally-consistent state estimation, trajectory planning, and handcrafted control design, allowing the policy to directly infer control commands from raw images, similar to human pilots. We conduct experiments using a realistic simulator and show that our vision-based policy can achieve state-of-the-art racing performance while being robust against unseen visual disturbances. Our study suggests that consistent feature embeddings are essential for achieving robust control performance in the presence of visual disturbances. The key to acquiring consistent feature embeddings is utilizing contrastive learning along with data augmentation. Video: https://youtu.be/AX_fcnW9yqE keywords: {Robust control;Visualization;Trajectory planning;Feature extraction;Robot sensing systems;Robustness;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341805&isnumber=10341342

N. Pavlasek, C. C. Cossette, D. Roy-Guay and J. R. Forbes, "Magnetic Navigation Using Attitude-Invariant Magnetic Field Information for Loop Closure Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5251-5257, doi: 10.1109/IROS55552.2023.10342466.Abstract: Indoor magnetic fields are a combination of Earth's magnetic field and disruptions induced by ferromag-netic objects, such as steel structural components in buildings. As a result of these disruptions, pervasive in indoor spaces, mag-netic field data is often omitted from navigation algorithms in indoor environments. This paper leverages the spatially-varying disruptions to Earth's magnetic field to extract positional information for use in indoor navigation algorithms. The algorithm uses a rate gyro and an array of four magnetometers to estimate the robot's pose. Additionally, the magnetometer array is used to compute attitude-invariant measurements associated with the magnetic field and its gradient. These measurements are used to detect loop closure points. Experimental results indicate that the proposed approach can estimate the pose of a ground robot in an indoor environment within meter accuracy. keywords: {Earth;Meters;Magnetic field measurement;Magnetometers;Indoor navigation;Indoor environment;Magnetic fields},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342466&isnumber=10341342

S. Hausler, S. Garg, P. Chakravarty, S. Shrivastava, A. Vora and M. Milford, "Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5258-5265, doi: 10.1109/IROS55552.2023.10341635.Abstract: Most 6-DoF localization and SLAM systems use static landmarks but ignore dynamic objects because they cannot be usefully incorporated into a typical pipeline. Where dynamic objects have been incorporated, typical approaches have attempted relatively sophisticated identification and localization of these objects, limiting their robustness or general utility. In this research, we propose a middle ground, demonstrated in the context of autonomous vehicles, using dynamic vehicles to provide limited pose constraint information in a 6-DoF frame-by-frame PnP-RANSAC localization pipeline. We refine initial pose estimates with a motion model and propose a method for calculating the predicted quality of future pose estimates, triggered by whether or not the autonomous vehicle's motion is constrained by the relative frame-to-frame location of dynamic vehicles in the environment. Our approach detects and identifies suitable dynamic vehicles to define these pose constraints to modify a pose filter, resulting in improved recall across a range of localization tolerances from 0.25m to 5m, compared to a state-of-the-art baseline single image PnP method and its vanilla pose filtering. Our constraint detection system is active for approximately 35% of the time on the Ford AV dataset and localization is particularly improved when the constraint detection is active. keywords: {Location awareness;Visualization;Tracking;Dynamics;Pipelines;6-DOF;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341635&isnumber=10341342

W. Zhao, A. Goudar, M. Tang, X. Qiao and A. P. Schoellig, "Uncertainty-Aware Gaussian Mixture Model for UWB Time Difference of Arrival Localization in Cluttered Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5266-5273, doi: 10.1109/IROS55552.2023.10342365.Abstract: Ultra-wideband (UWB) time difference of arrival (TDOA)-based localization has emerged as a low-cost and scalable indoor positioning solution. However, in cluttered environments, the performance of UWB TDOA-based localization deteriorates due to the biased and non-Gaussian noise distributions induced by obstacles. In this work, we present a bi-level optimization-based joint localization and noise model learning algorithm to address this problem. In particular, we use a Gaussian mixture model (GMM) to approximate the measurement noise distribution. We explicitly incorporate the estimated state's uncertainty into the GMM noise model learning, referred to as uncertainty-aware GMM, to improve both noise modeling and localization performance. We first evaluate the GMM noise model learning and localization performance in numerous simulation scenarios. We then demonstrate the effectiveness of our algorithm in extensive real-world experiments using two different cluttered environments. We show that our algorithm provides accurate position estimates with low-cost UWB sensors, no prior knowledge about the obstacles in the space, and a significant amount of UWB radios occluded. keywords: {Location awareness;Uncertainty;Time difference of arrival;Atmospheric measurements;Particle measurements;Sensors;Noise measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342365&isnumber=10341342

Z. Xun et al., "CREPES: Cooperative RElative Pose Estimation System," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5274-5281, doi: 10.1109/IROS55552.2023.10342523.Abstract: Mutual localization plays a crucial role in multi-robot cooperation. CREPES, a novel system that focuses on six degrees of freedom (DOF) relative pose estimation for multi-robot systems, is proposed in this paper. CREPES has a compact hardware design using active infrared (IR) LEDs, an IR fish-eye camera, an ultra-wideband (UWB) module and an inertial measurement unit (IMU). By leveraging IR light communication, the system solves data association between visual detection and UWB ranging. Ranging measurements from the UWB and directional information from the camera offer relative 3-DOF position estimation. Combining the mutual relative position with neighbors and the gravity constraints provided by IMUs, we can estimate the 6-DOF relative pose from a single frame of sensor measurements. In addition, we design an estimator based on the error-state Kalman filter (ESKF) to enhance system accuracy and robustness. When multiple neighbors are available, a Pose Graph Optimization (PGO) algorithm is applied to further improve system accuracy. We conduct enormous experiments to demonstrate CREPES’ accuracy between robot pairs and a team of robots, as well as performance under challenging conditions. keywords: {Location awareness;Visualization;Pose estimation;Position measurement;Robot sensing systems;Cameras;Hardware;Distance measurement;Multi-robot systems;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342523&isnumber=10341342

C. C. Cossette, M. Cohen, V. Korotkine, A. Del Castillo Bernal, M. A. Shalaby and J. R. Forbes, "navlie: A Python Package for State Estimation on Lie Groups," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5282-5287, doi: 10.1109/IROS55552.2023.10342362.Abstract: The ability to rapidly test a variety of algorithms for an arbitrary state estimation task is valuable in the prototyping phase of navigation systems. Lie group theory is now mainstream in the robotics community, and hence estimation prototyping tools should allow state definitions that belong to manifolds. A new package, called navlie, provides a framework that allows a user to model a large class of problems by implementing a set of classes complying with a generic interface. Once accomplished, navlie provides a variety of on-manifold estimation algorithms that can run directly on these classes. The package also provides a built-in library of common models, as well as many useful utilities. The open-source project can be found at https://github.com/decargroup/navlie keywords: {Manifolds;Estimation error;Navigation;Prototypes;Libraries;State estimation;Task analysis;Localization;Sensor Fusion;Software Tools for Robot Programming},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342362&isnumber=10341342

D. Schindler, V. Niculescu, T. Polonelli, D. Palossi, L. Benini and M. Magno, "A Relative Infrastructure-less Localization Algorithm for Decentralized and Autonomous Swarm Formation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5288-5295, doi: 10.1109/IROS55552.2023.10342168.Abstract: Decentralized and autonomous control of Unmanned Aerial Vehicle (UAV) swarms is a key enabler for cooperative systems and infrastructure-less formation flights. However, UAVs often lack reliable heading angle measurements, especially in indoor scenarios, space, and GNSS-denied environments, posing an additional observability challenge on range-based relative localization. We tackle this problem by proposing a novel solution enhancing the classical tag-and-anchor trilateration. The proposed solution relies on Ultra-wideband range measurements and addresses the relative pose estimation between pairs of UAVs under relative motion. Furthermore, it does not require any explicit motion pattern or initialization procedure and leverages an approximate maximum-likelihood algorithm to recursively solve the relative localization problem with constant computational complexity. The method has been implemented and demonstrated through field experiments, where a swarm of nano-UAVs positioned themselves with respect to a leader in a nearly-static formation with an average error of 38.5 cm and a convergence time of 25 s. The achieved formation accuracy is similar to the one achieved by the state-of-the-art EKF-based leader-follower methods. keywords: {Location awareness;Pose estimation;Autonomous aerial vehicles;Robustness;Motion measurement;Noise measurement;Observability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342168&isnumber=10341342

C. Thirgood, O. Mendez, E. C. Ling, J. Storey and S. Hadfield, "RaSpectLoc: RAman SPECTroscopy-dependent robot LOCalisation," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5296-5303, doi: 10.1109/IROS55552.2023.10342198.Abstract: This paper presents a new information source for supporting robot localisation: material composition. The proposed method complements the existing visual, structural, and semantic cues utilized in the literature. However, it has a distinct advantage in its ability to differentiate structurally [23], visually [25] or categorically [1] similar objects such as different doors, by using Raman spectrometers. Such devices can identify the material of objects it probes through the bonds between the material's molecules. Unlike similar sensors, such as mass spectroscopy, it does so without damaging the material or environment. In addition to introducing the first material-based localisation algorithm, this paper supports the future growth of the field by presenting a gazebo plugin for Raman spectrometers, material sensing demonstrations, as well as the first-ever localisation data-set with benchmarks for material-based localisation. This benchmarking shows that the proposed technique results in a significant improvement over current state-of-the-art localisation techniques, achieving 16 % more accurate localisation than the leading baseline. The code and dataset will be released at: https://github.com/ThirgoodC/RaSpectLoc keywords: {Visualization;Simultaneous localization and mapping;Navigation;Semantics;Benchmark testing;Mass spectroscopy;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342198&isnumber=10341342

D. J. Yoon et al., "Need for Speed: Fast Correspondence-Free Lidar-Inertial Odometry Using Doppler Velocity," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5304-5310, doi: 10.1109/IROS55552.2023.10341596.Abstract: In this paper, we present a fast, lightweight odometry method that uses the Doppler velocity measurements from a Frequency-Modulated Continuous-Wave (FMCW) lidar without data association. FMCW lidar is a recently emerging technology that enables per-return relative radial velocity measurements via the Doppler effect. Since the Doppler measurement model is linear with respect to the 6-degrees-of-freedom (DOF) vehicle velocity, we can formulate a linear continuous-time estimation problem for the velocity and numerically integrate for the 6-DOF pose estimate afterward. The caveat is that angular velocity is not observable with a single FMCW lidar. We address this limitation by also incorporating the angular velocity measurements from a gyroscope. This results in an extremely efficient odometry method that processes lidar frames at an average wall-clock time of 5.64ms on a single thread, well below the 10Hz operating rate of the lidar we tested. We show experimental results on real-world driving sequences and compare against state-of-the-art Iterative Closest Point (ICP)-based odometry methods, presenting a compelling tradeoff between accuracy and computation. We also present an algebraic observability study, where we demonstrate in theory that the Doppler measurements from multiple FMCW lidars are capable of observing all 6 degrees of freedom (translational and angular velocity). keywords: {Laser radar;Angular velocity;6-DOF;Gyroscopes;Sensors;Velocity measurement;Odometry},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341596&isnumber=10341342

M. Vaidis, W. Dubois, E. Daum, D. LaRocque and F. Pomerleau, "Uncertainty Analysis for Accurate Ground Truth Trajectories with Robotic Total Stations," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5312-5319, doi: 10.1109/IROS55552.2023.10341529.Abstract: In the context of robotics, accurate ground truth positioning is essential for the development of Simultaneous Localization and Mapping (SLAM) and control algorithms. Robotic Total Stations (RTSs) provide accurate and precise reference positions in different types of outdoor environments, especially when compared to the limited accuracy of Global Navigation Satellite System (GNSS) in cluttered areas. Three RTSs give the possibility to obtain the six-Degrees Of Freedom (DOF) reference pose of a robotic platform. However, the uncertainty of every pose is rarely computed for trajectory evaluation. As evaluation algorithms are getting increasingly precise, it becomes crucial to take into account this uncertainty. We propose a method to compute this six-DOF uncertainty from the fusion of three RTSs based on Monte Carlo (MC) methods. This solution relies on point-to-point minimization to propagate the noise of RTSs on the pose of the robotic platform. Five main noise sources are identified to model this uncertainty: noise inherent to the instrument, tilt noise, atmospheric factors, time synchronization noise, and extrinsic calibration noise. Based on extensive experimental work, we compare the impact of each noise source on the prism uncertainty and the final estimated pose. Tested on more than 50 km of trajectories, our comparison highlighted the importance of the calibration noise and the measurement distance, which should be ideally under 75 m. Moreover, it has been noted that the uncertainty on the pose of the robot is not prominently affected by one particular noise source, compared to the others. keywords: {Global navigation satellite system;Uncertainty;Simultaneous localization and mapping;Atmospheric modeling;Instruments;Computational modeling;Measurement uncertainty},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341529&isnumber=10341342

Q. Wu et al., "Graph Matching Optimization Network for Point Cloud Registration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5320-5325, doi: 10.1109/IROS55552.2023.10342346.Abstract: Point Cloud Registration is a fundamental and challenging problem in 3D computer vision. Recent works often utilize geometric structure features in downsampled points (patches) to seek correspondences, then propagate these sparse patch correspondences to the dense level in the corresponding patches' neighborhood. However, they neglect the explicit global scale rigid constraint at the dense level point matching. We claim that the explicit isometry-preserving constraint in the dense level on a global scale is also important for improving feature representation in the training stage. To this end, we propose a Graph Matching Optimization based Network (GMONet for short), which utilizes the graph-matching optimizer to explicitly exert the isometry preserving constraints in the point feature training to improve the point feature representation. Specifically, we exploit a partial graph-matching optimizer to enhance the super point (i.e., down-sampled key points) features and a full graph-matching optimizer to improve the dense level point features in the overlap region. Meanwhile, we leverage the inexact proximal point method and the mini-batch sampling technique to accelerate these two graph-matching optimizers. Given high discriminative point features in the evaluation stage, we utilize the RANSAC approach to estimate the transformation between the scanned pairs. The proposed method has been evaluated on the 3DMatch/3DLoMatch and the KITTI datasets. The experimental results show that our method performs competitively compared to state-of-the-art baselines. keywords: {Point cloud compression;Training;Representation learning;Computer vision;Three-dimensional displays;Task analysis;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342346&isnumber=10341342

E. Latif and R. Parasuraman, "SEAL: Simultaneous Exploration and Localization for Multi-Robot Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5358-5365, doi: 10.1109/IROS55552.2023.10342157.Abstract: The availability of accurate localization is critical for multi-robot exploration strategies; noisy or inconsistent localization causes failure in meeting exploration objectives. We aim to achieve high localization accuracy with contemporary exploration map belief and vice versa without needing global localization information. This paper proposes a novel simultaneous exploration and localization (SEAL) approach, which uses Gaussian Processes (GP)-based information fusion for maximum exploration while performing communication graph optimization for relative localization. Both these cross-dependent objectives were integrated through the Rao-Blackwellization technique. Distributed linearized convex hull optimization is used to select the next-best unexplored region for distributed exploration. SEAL outperformed cutting-edge methods on exploration and localization performance in extensive ROS-Gazebo simulations, illustrating the practicality of the approach in real-world applications. keywords: {Location awareness;Seals;Gaussian processes;Sensor phenomena and characterization;Robot sensing systems;Multi-robot systems;Noise measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342157&isnumber=10341342

I. A. Aly and K. M. Dogan, "Discrete-Time Adaptive Control Algorithm for Coordination of Multiagent Systems in the Presence of Coupled Dynamics," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5366-5371, doi: 10.1109/IROS55552.2023.10341404.Abstract: Discrete-time architectures have an advantage over their continuous counterparts as they can be directly executed on embedded hardware without the need for dis-cretization, and discretization can result in a loss of stability margin. This paper presents a discrete-time adaptive control architecture for uncertain scalar multi agent systems with coupled dynamics. Our strategy includes observer dynamics to handle the unmeasurable coupled dynamics and a user-assigned Laplacian matrix for coordination of the multiagent system. An illustrative example is presented to show the efficacy of the proposed control architectures. keywords: {Asymptotic stability;Laplace equations;Heuristic algorithms;Observers;Hardware;Adaptive control;Numerical stability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341404&isnumber=10341342

J. Park, A. Messing, H. Ravichandar and S. Hutchinson, "Risk-Tolerant Task Allocation and Scheduling in Heterogeneous Multi-Robot Teams," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5372-5379, doi: 10.1109/IROS55552.2023.10341837.Abstract: Effective coordination of heterogeneous multi-robot teams requires optimizing allocations, schedules, and motion plans in order to satisfy complex multi-dimensional task requirements. This challenge is exacerbated by the fact that real-world applications inevitably introduce uncertainties into robot capabilities and task requirements. In this paper, we extend our previous work on trait-based time-extended task allocation to account for such uncertainties. Specifically, we leverage the Sequential Probability Ratio Test to develop an algorithm that can guarantee that the probability of failing to satisfy task requirements is below a user-specified threshold. We also improve upon our prior approach by accounting for temporal deadlines in addition to synchronization and precedence constraints in a Mixed-Integer Linear Programming model. We evaluate our approach by benchmarking it against three baselines in a simulated battle domain in a city environment and compare its performance against a state-of-the-art framework in a pandemic-inspired multi-robot service coordination problem. Results demonstrate the effectiveness and advantages of our approach, which leverages redundancies to manage risk while simultaneously minimizing makespan. keywords: {Schedules;Uncertainty;Robot kinematics;Urban areas;Redundancy;Robot sensing systems;Mixed integer linear programming},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341837&isnumber=10341342

S. Talia, A. Thareja, C. Mavrogiannis, M. Schmittle and S. S. Srinivasa, "PuSHR: A Multirobot System for Nonprehensile Rearrangement," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5380-5387, doi: 10.1109/IROS55552.2023.10341853.Abstract: We focus on the problem of rearranging a set of objects with a team of car-like robot pushers built using off-the-shelf components. Maintaining control of pushed objects while avoiding collisions in a tight space demands highly coordinated motion that is challenging to execute on constrained hardware. Centralized replanning approaches become intractable even for small-sized problems whereas decentralized approaches often get stuck in deadlocks. Our key insight is that by carefully assigning pushing tasks to robots, we could reduce the complexity of the rearrangement task, enabling robust performance via scalable decentralized control. Based on this insight, we built PuSHR, a system that optimally assigns pushing tasks and trajectories to robots offline, and performs trajectory tracking via decentralized control online. Through an ablation study in simulation, we demonstrate that PuSHR dominates baselines ranging from purely centralized to fully decentralized in terms of success rate and time efficiency across challenging tasks with up to 4 robots. Hardware experiments demonstrate the transfer of our system to the real world and highlight its robustness to model inaccuracies. Our code can be found at https://github.com/prl-mushr/pushr, and videos from our experiments at https://youtu.be/nyUn9mHoR8Y. keywords: {Trajectory tracking;Robot kinematics;Decentralized control;System recovery;Hardware;Robustness;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341853&isnumber=10341342

S. Chen, T. X. Lin and F. Zhang, "Game-Theoretical Approach to Multi-Robot Task Allocation Using a Bio-Inspired Optimization Strategy," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5388-5393, doi: 10.1109/IROS55552.2023.10341947.Abstract: This paper introduces a game-theoretical approach to the multi-robot task allocation problem, where each robot is considered as self-interested and cannot share its personal utility functions. We consider the case where each robot can execute multiple tasks and each task requires only one robot. For real-world applications with mobile robots, we design a utility function that includes both assignment conflict penalties and path-dependent execution cost. For a robot to maximize its own utility, it needs to select a subset of conflict-free tasks that minimizes its total travel distance. Our approaches utilize a consensus communication scheme to share robots' task selection and the Speeding-Up and Slowing-Down (SUSD) strategy to search in a combinatorial action (task selection) space for a subset of tasks that can achieve a higher utility at each iteration. The SUSD strategy can perform a gradient-like search without calculating the derivatives, which allows robots to improve upon their current task selections. Simulation results show that robots using the proposed algorithms can successfully find Nash equilibria for effective coordination. keywords: {Robot kinematics;Simulation;Nash equilibrium;Space exploration;Resource management;Mobile robots;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341947&isnumber=10341342

C. A. Dimmig, K. C. Wolfe and J. Moore, "Multi-Robot Planning on Dynamic Topological Graphs Using Mixed- Integer Programming," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5394-5401, doi: 10.1109/IROS55552.2023.10341497.Abstract: Planning for multi-robot teams in complex environments is a challenging problem, especially when these teams must coordinate to accomplish a common objective. In general, optimal solutions to these planning problems are computationally intractable, since the decision space grows exponentially with the number of robots. In this paper, we present a novel approach for multi-robot planning on topological graphs using mixed-integer programming. Central to our approach is the notion of a dynamic topological graph, where edge weights vary dynamically based on the locations of the robots in the graph. We construct this graph using the critical features of the planning problem and the relationships between robots; we then leverage mixed-integer programming to minimize a shared cost that depends on the paths of all robots through the graph. To improve computational tractability, we formulated our optimization problem with a fully convex relaxation and designed our decision space around eliminating the exponential dependence on the number of robots. We test our approach on a multi-robot reconnaissance scenario, where robots must coordinate to minimize detectability and maximize safety while gathering information. We demonstrate that our approach is able to scale to a series of representative scenarios and is capable of computing optimal coordinated strategic behaviors for autonomous multi-robot teams in seconds. keywords: {Costs;Robot kinematics;Reconnaissance;Programming;Search problems;Planning;Dynamic programming},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341497&isnumber=10341342

A. Aswale and C. Pinciroli, "Heterogeneous Coalition Formation and Scheduling with Multi-Skilled Robots," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5402-5409, doi: 10.1109/IROS55552.2023.10342489.Abstract: We present an approach to task scheduling in heterogeneous multi-robot systems. In our setting, the tasks to complete require diverse skills. We assume that each robot is multi-skilled, i.e., each robot offers a subset of the possible skills. This makes the formation of heterogeneous teams (coalitions) a requirement for task completion. We present two centralized algorithms to schedule robots across tasks and to form suitable coalitions, assuming stochastic travel times across tasks. The coalitions are dynamic, in that the robots form and disband coalitions as the schedule is executed. The first algorithm we propose guarantees optimality, but its runtime is acceptable only for small problem instances. The second algorithm we propose can tackle large problems with short runtimes, and is based on a heuristic approach that typically reaches 1x-2x of the optimal solution cost. keywords: {Schedules;Runtime;Costs;Heuristic algorithms;Parallel processing;Multi-robot systems;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342489&isnumber=10341342

K. Popović, M. Schlafly, A. Prabhakar, C. Kim and T. D. Murphey, "Measuring Human-Robot Team Benefits Under Time Pressure in a Virtual Reality Testbed," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5410-5417, doi: 10.1109/IROS55552.2023.10341794.Abstract: During a natural disaster such as hurricane, earthquake, or fire, robots have the potential to explore vast areas and provide valuable aid in search & rescue efforts. These scenarios are often high-pressure and time-critical with dynamically-changing task goals. One limitation to these large scale deployments is effective human-robot interaction. Prior work shows that collaboration between one human and one robot benefits from shared control. Here we evaluate the efficacy of shared control for human-swarm teaming in an immersive virtual reality environment. Although there are many human-swarm interaction paradigms, few are evaluated in high-pressure settings representative of their intended end use. We have developed an open-source virtual reality testbed for realistic evaluation of human-swarm teaming performance under pressure. We conduct a user study ($\mathrm{n}=16$) comparing four human-swarm paradigms to a baseline condition with no robotic assistance. Shared control significantly reduces the number of instructions needed to operate the robots. While shared control leads to marginally improved team performance in experienced participants, novices perform best when the robots are fully autonomous. Our experimental results suggest that in immersive, high-pressure settings, the benefits of robotic assistance may depend on how the human and robots interact, and the human operator's expertise. keywords: {Human-robot interaction;Virtual reality;Particle measurements;Time measurement;Hurricanes;Pressure measurement;Time factors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341794&isnumber=10341342

S. He, S. Han and F. Miao, "Robust Electric Vehicle Balancing of Autonomous Mobility-on-Demand System: A Multi-Agent Reinforcement Learning Approach," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5471-5478, doi: 10.1109/IROS55552.2023.10342263.Abstract: Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust EAVs balancing policy to balance both the supply-demand ratio and charging utilization rate across the whole city. Experiments show that our proposed robust method performs better compared with a non-robust MARL method that does not consider state uncertainties; it improves the reward, charging utilization fairness, and supply-demand fairness by 19.28%, 28.18%, and 3.97%, respectively. Compared with a robust optimization-based method, the proposed MARL algorithm can improve the reward, charging utilization fairness, and supply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively. keywords: {Training;Time-frequency analysis;Uncertainty;Supply and demand;Urban areas;Reinforcement learning;Prediction algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342263&isnumber=10341342

C. Tanama, K. Peng, Z. Marinov, R. Stiefelhagen and A. Roitberg, "Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5479-5486, doi: 10.1109/IROS55552.2023.10342203.Abstract: Deep learning-based models are at the top of most driver observation benchmarks due to their remarkable accuracies but come with a high computational cost, while the resources are often limited in real-world driving scenarios. This paper presents a lightweight framework for resource- efficient driver activity recognition. We enhance 3D MobileNet, a speed-optimized neural architecture for video classification, with two paradigms for improving the trade-off between model accuracy and computational efficiency: knowledge distillation and model quantization. Knowledge distillation prevents large drops in accuracy when reducing the model size by harvesting knowledge from a large teacher model (I3D) via soft labels instead of using the original ground truth. Quantization further drastically reduces the memory and computation requirements by representing the model weights and activations using lower precision integers. Extensive experiments on a public dataset for in-vehicle monitoring during autonomous driving show that our proposed framework leads to an 3- fold reduction in model size and 1.4-fold improvement in inference time compared to an already speed-optimized architecture. Our code is available at https://github.com/calvintanama/qd-driver-activity-reco. keywords: {Solid modeling;Quantization (signal);Three-dimensional displays;Computational modeling;Memory management;Activity recognition;Computational efficiency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342203&isnumber=10341342

J. Wiederer, J. Schmidt, U. Kressel, K. Dietmayer and V. Belagiannis, "Joint Out-of-Distribution Detection and Uncertainty Estimation for Trajectory Prediction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5487-5494, doi: 10.1109/IROS55552.2023.10341616.Abstract: Despite the significant research efforts on trajectory prediction for automated driving, limited work exists on assessing the prediction reliability. To address this limitation we propose an approach that covers two sources of error, namely novel situations with out-of-distribution (OOD) detection and the complexity in in-distribution (ID) situations with uncertainty estimation. We introduce two modules next to an encoder-decoder network for trajectory prediction. Firstly, a Gaussian mixture model learns the probability density function of the ID encoder features during training, and then it is used to detect the OOD samples in regions of the feature space with low likelihood. Secondly, an error regression network is applied to the encoder, which learns to estimate the trajectory prediction error in supervised training. During inference, the estimated prediction error is used as the uncertainty. In our experiments, the combination of both modules outperforms the prior work in OOD detection and uncertainty estimation, on the Shifts robust trajectory prediction dataset by 2.8 % and 10.1%, respectively. The code is publicly available44project page: https://github.com/againerju/joodu. keywords: {Training;Uncertainty;Computational modeling;Estimation;Predictive models;Probability density function;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341616&isnumber=10341342

P. Agand, A. Iskrov and M. Chen, "Deep Reinforcement Learning-Based Intelligent Traffic Signal Controls with Optimized CO2 Emissions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5495-5500, doi: 10.1109/IROS55552.2023.10341972.Abstract: Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, waiting time, and stopped time. Our evaluation considers multiple scenarios that encompass a range of road users (trucks, buses, cars) with varying pollution levels. keywords: {Measurement;Q-learning;Sensitivity analysis;Roads;Urban areas;Transportation;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341972&isnumber=10341342

W. Xiong, J. Chen, X. Zhang, Q. Wang and Z. Qi, "Hierarchical Attention Network for Planning-Informed Multi-Agent Trajectory Prediction," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5501-5506, doi: 10.1109/IROS55552.2023.10341557.Abstract: The accurate prediction of the neighboring vehicles' trajectories affects the security of autonomous driving vehicles. However, it is challenging for existing methods to anticipating the trajectories of vehicles in the vicinity due to the uncertainty of driving behaviors and the complex interaction patterns of traffic flows. In this study, incorporating the planning information of the ego vehicle, we propose a novel trajectory prediction approach based on the hierarchical attention mechanism. Firstly, a spatio-temporary attention module is presented to extract the social interaction of surrounding vehicles and capture the temporal dependence of continuous frame historical information and planning information. Then, a hard-soft attention module is designed to perform two tasks: weighing the importance of both historical and future information, and learning different location information about the target vehicles. Our method is evaluated on two national highway datasets. The experimental results show that our algorithm achieves the state-of-the-art performance. keywords: {Road transportation;TV;Uncertainty;Prediction algorithms;Trajectory;Planning;Data mining},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341557&isnumber=10341342

J. Xue, D. Zhang, R. Xiong, Y. Wang and E. Liu, "A Two-Stage Based Social Preference Recognition in Multi-Agent Autonomous Driving System," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5507-5513, doi: 10.1109/IROS55552.2023.10341803.Abstract: Multi-Agent Reinforcement Learning (MARL) has become a promising solution for constructing a multi-agent autonomous driving system (MADS) in complex and dense scenarios. But most methods consider agents acting selfishly, which leads to conflict behaviors. Some existing works incorporate the concept of social value orientation (SVO) to promote coordination, but they lack the knowledge of other agents' SVOs, resulting in conservative maneuvers. In this paper, we aim to tackle the mentioned problem by enabling the agents to understand other agents' SVOs. To accomplish this, we propose a two-stage system framework. Firstly, we train a policy by allowing the agents to share their ground truth SVOs to establish a coordinated traffic flow. Secondly, we develop a recognition network that estimates agents' SVOs and integrates it with the policy trained in the first stage. Experiments demonstrate that our developed method significantly improves the performance of the driving policy in MADS compared to two state-of-the-art MARL algorithms. keywords: {Reinforcement learning;Behavioral sciences;Autonomous vehicles;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341803&isnumber=10341342

S. R. Yadavalli, L. C. Das and M. Won, "RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5514-5521, doi: 10.1109/IROS55552.2023.10341918.Abstract: A platoon refers to a group of vehicles traveling together in very close proximity using automated driving technology. Owing to its immense capacity to improve fuel efficiency, driving safety, and driver comfort, platooning technology has garnered substantial attention from the autonomous vehicle research community. Although highly advantageous, recent research has uncovered that an excessively small intra-platoon gap can impede traffic flow during highway on-ramp merging. While existing control-based methods allow for adaptation of the intra-platoon gap to improve traffic flow, making an optimal control decision under the complex dynamics of traffic conditions remains a challenge due to the massive computational complexity. In this paper, we present the design, implementation, and evaluation of a novel reinforcement learning framework that adaptively adjusts the intra-platoon gap of an individual platoon member to maximize traffic flow in response to dynamically changing, complex traffic conditions for highway on-ramp merging. The framework's state space has been meticulously designed in consultation with the transportation literature to take into account critical traffic parameters that bear direct relevance to merging efficiency. An intra-platoon gap decision making method based on the deep deterministic policy gradient algorithm is created to incorporate the continuous action space to ensure precise and continuous adaptation of the intra-platoon gap. An extensive simulation study demonstrates the effectiveness of the reinforcement learning-based approach for significantly improving traffic flow in various highway on-ramp merging scenarios. keywords: {Adaptation models;Roads;Merging;Optimal control;Reinforcement learning;Safety;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341918&isnumber=10341342

Q. Sun, X. Huang, B. C. Williams and H. Zhao, "P4P: Conflict-Aware Motion Prediction for Planning in Autonomous Driving," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5530-5536, doi: 10.1109/IROS55552.2023.10342247.Abstract: Motion prediction is crucial in enabling safe motion planning for autonomous vehicles in interactive scenarios. It allows the planner to identify potential conflicts with other traffic agents and generate safe plans. Existing motion predictors often focus on reducing prediction errors, yet it remains an open question on how well they help identify conflicts for the planner, which are critical to the safety of autonomous vehicles. In this paper, we evaluate state-of-the-art predictors through novel conflict-related metrics, such as the success rate of identifying conflicts. Surprisingly, the predictors suffer from a low success rate and thus lead to a large percentage of collisions when we test the prediction-planning system in an interactive simulator. To fill the gap, we propose a simple but effective alternative that combines a physics-based trajectory generator and a learning-based relation predictor to identify conflicts and infer conflict relations. We demonstrate that our predictor, P4P, achieves superior performance over existing learning-based predictors in realistic interactive driving scenarios from Waymo Open Motion Dataset. keywords: {Measurement;Generators;Trajectory;Safety;Planning;Autonomous vehicles;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342247&isnumber=10341342

S. Parra, A. Ortega, S. Schneider and N. Hochgeschwender, "A Thousand Worlds: Scenery Specification and Generation for Simulation-Based Testing of Mobile Robot Navigation Stacks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5537-5544, doi: 10.1109/IROS55552.2023.10342315.Abstract: Is mobile robot navigation a solved problem? We asked this question to 14 professional robot software engineers who work with navigation stacks of mobile, wheeled robots on a daily basis. They unanimously report that it remains challenging to ensure the performance of their mobile robots. We find that the method of choice to verify a robot's performance is to expose it to different environments under varying conditions. Unfortunately, these field tests are costly to set up and often too risky to execute. Therefore, robot software engineers want to replicate real-world environments in simulated sceneries (i) to test their navigation stack or parts of it prior to deployment; and (ii) to reproduce erroneous behaviour observed in the real world. Motivated by these insights, we have developed a domain-specific language and associated tooling which enables engineers (i) to specify sceneries of indoor environments; and (ii) to automatically generate variants thereof that resemble changes in the real world. We demonstrate how our approach enables the simulation-based replication of real-world environments and allows us to recreate erroneous robot behaviour as reported in the interviews. While performing simulation-based tests, we discovered an 8-year dormant bug in the ROS navigation stack. keywords: {Navigation;Computer bugs;Software;Indoor environment;Mobile robots;Interviews;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342315&isnumber=10341342

V. Suman, P. Pham and A. Bera, "RAIST: Learning Risk Aware Traffic Interactions via Spatio-Temporal Graph Convolutional Networks," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5545-5550, doi: 10.1109/IROS55552.2023.10341578.Abstract: A key aspect of driving a road vehicle is to interact with other road users, assess their intentions and make riskaware tactical decisions. An intuitive approach to enabling an intelligent automated driving system would be incorporating some aspects of human driving behavior. To this end, we propose a novel driving framework for egocentric views based on spatio-temporal traffic graphs. The traffic graphs model not only the spatial interactions amongst the road users but also their individual intentions through temporally associated message passing. We leverage a spatio-temporal graph convolutional network (ST-GCN) to train the graph edges. These edges are formulated using parameterized functions of 3D positions and scene-aware appearance features of road agents. Along with tactical behavior prediction, it is crucial to evaluate the risk-assessing ability of the proposed framework. We claim that our framework learns risk-aware representations by improving on the task of risk object identification, especially in identifying objects with vulnerable interactions like pedestrians and cyclists. keywords: {Three-dimensional displays;Pedestrians;Roads;Message passing;Road vehicles;Behavioral sciences;Object recognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341578&isnumber=10341342

Y. Fan, X. Liu, Y. Li and S. Wang, "Look Before You Drive: Boosting Trajectory Forecasting via Imagining Future," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5551-5558, doi: 10.1109/IROS55552.2023.10341509.Abstract: Predicting the future trajectories of other agents in the scene fast and effectively is crucial for autonomous driving systems. We note that high-quality predictions require us to take into account the subjective initiative of the target agents, which is reflected by the fact that they themselves make decisions based on their own predictions about the future, just like our ego vehicle's prediction-planning system. However, this characteristic has been neglected in previous studies. We introduce Look Before You Drive (LBYD), a two-stage approach that explicitly incorporates both past observations and future estimates to make predictions. To get a preliminary estimate of the future, we propose a neat and effective baseline capable of making predictions for multiple agents simultaneously. We use only the most basic structures, mainly Transformer, to ensure sufficient inference speed and room for expansion. On this basis, we cooperatively train two networks to enable the coarse estimates to boost final forecasting. Our experiments demonstrate that LBYD can significantly surpass the baseline performance. Moreover, while state-of-the-art methods rely on considering heterogeneity and artificially designed inductive biases for attention modeling, LBYD performs on par with SOTA without them on both the Argoverse 1 and the large scale Argoverse 2 datasets, and can run at 67 FPS on an RTX 3090 GPU. keywords: {Training;Scalability;Predictive models;Transformers;Trajectory;Proposals;History},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341509&isnumber=10341342

P. Schafhalter, S. Kalra, L. Xu, J. E. Gonzalez and I. Stoica, "Leveraging Cloud Computing to Make Autonomous Vehicles Safer," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5559-5566, doi: 10.1109/IROS55552.2023.10341821.Abstract: The safety of autonomous vehicles (AVs) depends on their ability to perform complex computations on high-volume sensor data in a timely manner. Their ability to run these computations with state-of-the-art models is limited by the processing power and slow update cycles of their onboard hardware. In contrast, cloud computing offers the ability to burst computation to vast amounts of the latest generation of hardware. However, accessing these cloud resources requires traversing wireless networks that are often considered to be too unreliable for real-time AV driving applications. Our work seeks to harness this unreliable cloud to enhance the accuracy of an AV's decisions, while ensuring that it can always fall back to its on-board computational capabilities. We identify three mechanisms that can be used by AVs to safely leverage the cloud for accuracy enhancements, and elaborate why current execution systems fail to enable these mechanisms. To address these limitations, we provide a system design based on the speculative execution of an AV's pipeline in the cloud, and show the efficacy of this approach in simulations of complex real-world scenarios that apply these mechanisms. keywords: {Cloud computing;Computational modeling;Wireless networks;Pipelines;Robot sensing systems;Hardware;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341821&isnumber=10341342

J. Ma and F. Wu, "Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 1-7, doi: 10.1109/IROS55552.2023.10341718.Abstract: Reinforcement learning (RL) has emerged as a promising approach for optimizing traffic signal control (TSC) to ensure the efficient operation of transportation networks. However, the traditional trial-and-error technique in RL is usually impractical in real-world applications. Offline RL, which trains models using pre-collected datasets, is a more practical approach. However, this presents challenges such as suboptimal datasets and limited generalization of pre-trained models. To address this, we propose an offline-to-online RL framework for TSC that pre-trains a generalized model and quickly adapts to new traffic scenarios through online refinement. In the offline stage, we augment the pre-collected datasets to cover a diverse set of possible scenarios and use an offline RL method to pretrain a control model. To ensure generalization, we use FRAP-like network as our base model, which is designed to learn the basic logic for signal control. In the online stage, we introduce a discrepancy measure to tackle inconsistencies between offline pre-trained models and online scenarios and prioritize samples based on it. In the experiments, the proposed approach achieves competitive performance and reduces the training time needed for learning in new scenarios, compared to several baselines. keywords: {Training;Adaptation models;Transportation;Reinforcement learning;Computational efficiency;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341718&isnumber=10341342

X. Yao et al., "Learning from Symmetry: Meta-Reinforcement Learning with Symmetrical Behaviors and Language Instructions," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5574-5581, doi: 10.1109/IROS55552.2023.10341769.Abstract: Meta-reinforcement learning (meta-RL) is a promising approach that enables the agent to learn new tasks quickly. However, most meta-RL algorithms show poor generalization in multi-task scenarios due to the insufficient task information provided only by rewards. Language-conditioned meta-RL improves the generalization capability by matching language instructions with the agent's behaviors. While both behaviors and language instructions have symmetry, which can speed up human learning of new knowledge. Thus, combining symmetry and language instructions into meta-RL can help improve the algorithm's generalization and learning efficiency. We propose a dual-MDP meta-reinforcement learning method that enables learning new tasks efficiently with symmetrical behav-iors and language instructions. We evaluate our method in mul-tiple challenging manipulation tasks, and experimental results show that our method can greatly improve the generalization and learning efficiency of meta-reinforcement learning. Videos are available at https://tumi6robot.wixsite.com/symmetry/. keywords: {Learning systems;Multitasking;Behavioral sciences;Task analysis;Intelligent robots;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341769&isnumber=10341342

N. Bührer, Z. Zhang, A. Liniger, F. Yu and L. Van Gool, "A Multiplicative Value Function for Safe and Efficient Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5582-5589, doi: 10.1109/IROS55552.2023.10342288.Abstract: An emerging field of sequential decision problems is safe Reinforcement Learning (RL), where the objective is to maximize the reward while obeying safety constraints. Being able to handle constraints is essential for deploying RL agents in real-world environments, where constraint violations can harm the agent and the environment. To this end, we propose a safe model-free RL algorithm with a novel multiplicative value function consisting of a safety critic and a reward critic. The safety critic predicts the probability of constraint violation and discounts the reward critic that only estimates constraint-free returns. By splitting responsibilities, we facilitate the learning task leading to increased sample efficiency. We integrate our approach into two popular RL algorithms, Proximal Policy Optimization and Soft Actor-Critic, and evaluate our method in four safety-focused environments, including classical RL benchmarks augmented with safety constraints and robot navigation tasks with images and raw Lidar scans as observations. Finally, we make the zero-shot sim-to-real transfer where a differential drive robot has to navigate through a cluttered room. Our code can be found at https://github.com/nikeke19/Safe-Mult-RL. keywords: {Laser radar;Codes;Navigation;Reinforcement learning;Benchmark testing;Prediction algorithms;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342288&isnumber=10341342

C. Zhao, J. Liu, S. -U. Yoon, X. Li, H. Li and Z. Zhang, "Energy Constrained Multi-Agent Reinforcement Learning for Coverage Path Planning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5590-5597, doi: 10.1109/IROS55552.2023.10341412.Abstract: For multi-agent area coverage path planning problem, existing researches regard it as a combination of Traveling Salesman Problem (TSP) and Coverage Path Planning (CPP). However, these approaches have disadvantages of poor observation ability in online phase and high computational cost in offline phase, making it difficult to be applied to energy-constrained Unmanned Aerial Vehicles (UAVs) and adjust strategy dynamically. In this paper, we decompose the task into two sub-problems: multi-agent path planning and sub-region CPP. We model the multi-agent path planning problem as a Collective Markov Decision Process (C-MDP), and design an Energy Constrained Multi-Agent Reinforcement Learning (ECMARL) algorithm based on the centralized training and distributed execution concept. Taking into account energy constraint of UAVs, the UAV propulsion power model is established to measure the energy consumption of UAVs, and load balancing strategy is applied to dynamically allocate target areas for each UAV. If the UAV is under energy-depleted situation, ECMARL can adjust the mission strategy in real time according to environmental information and energy storage conditions of other UAVs. When UAVs reach each sub-region of interest, Back-an-Forth Paths (BFPs) are adopted to solve CPP problem, which can ensure full coverage, optimality and complexity of the sub-problem. Comprehensive theoretical analysis and experiments demonstrate that ECMARL is superior to the traditional offline TSP-CPP strategy in terms of solution quality and computational time, and can effectively deal with the energy-constrained UAVs. keywords: {Training;Power measurement;Reinforcement learning;Traveling salesman problems;Propulsion;Markov processes;Autonomous aerial vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341412&isnumber=10341342

J. Lou, W. Wu, S. Liao and R. Shi, "Air-M: A Visual Reality Many-Agent Reinforcement Learning Platform for Large-Scale Aerial Unmanned System," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5598-5605, doi: 10.1109/IROS55552.2023.10341405.Abstract: Reinforcement learning for swarms of flying robots is a challenging task that requires a large number of data samples. Moreover, the problem of sim-to-real transfer has long been a challenge in robotics algorithm deployment. To address these issues, we propose Air-M, a platform that facilitates large-scale drone swarm learning in a distributed docker container environment and deployment in a virtual reality setting. Air-M trains the policy network using physics engines and creates replicas of agents in docker containers, which helps amortize the computational cost. In addition, Air-M establishes an intermediate link between the simulation and the real world, allowing real drones to interact with virtual objects via virtual sensors. This enables the policy network to be trained using virtual agents and seamlessly transferred to real drones. Air-Mis highly scalable, accommodating hundreds of agents with dynamic models and virtual sensors. We evaluate the effectiveness of our approach by conducting experiments in three representative virtual scenarios with an increasing number of agents. Our results demonstrate that our method outperforms the state-of- the-art in terms of training efficiency and transferability, making it a promising platform for swarm robotics applications. keywords: {Training;Soft sensors;Reinforcement learning;Containers;Autonomous aerial vehicles;Libraries;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341405&isnumber=10341342

J. Del Aguila Ferrandis, J. Moura and S. Vijayakumar, "Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5606-5613, doi: 10.1109/IROS55552.2023.10341629.Abstract: Developing robot controllers capable of achieving dexterous nonprehensile manipulation, such as pushing an object on a table, is challenging. The underactuated and hybrid-dynamics nature of the problem, further complicated by the uncertainty resulting from the frictional interactions, requires sophisticated control behaviors. Reinforcement Learning (RL) is a powerful framework for developing such robot controllers. However, previous RL literature addressing the nonprehensile pushing task achieves low accuracy, non-smooth trajectories, and only simple motions, i.e. without rotation of the manipulated object. We conjecture that previously used unimodal exploration strategies fail to capture the inherent hybrid-dynamics of the task, arising from the different possible contact interaction modes between the robot and the object, such as sticking, sliding, and separation. In this work, we propose a multimodal exploration approach through categorical distributions, which enables us to train planar pushing RL policies for arbitrary starting and target object poses, i.e. positions and orientations, and with improved accuracy. We show that the learned policies are robust to external disturbances and observation noise, and scale to tasks with multiple pushers. Furthermore, we validate the transferability of the learned policies, trained entirely in simulation, to a physical robot hardware using the KUKA iiwa robot arm. See our supplemental video: https://youtu.be/vTdvalmgrk4. keywords: {Uncertainty;Reinforcement learning;Manipulators;Hardware;Trajectory;Behavioral sciences;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341629&isnumber=10341342

X. Zhao, R. C. Fetecau and M. Chen, "Efficient Domain Coverage for Vehicles with Second-Order Dynamics via Multi-Agent Reinforcement Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5614-5621, doi: 10.1109/IROS55552.2023.10341748.Abstract: Collaborative autonomous multi-agent systems covering a specified area have many potential applications. Traditional approaches for such problems involve designing model-based control policies; however, state-of-the-art classical control policy still exhibits a large degree of sub-optimality. We present a combined reinforcement learning (RL) and control approach for the multi-agent coverage problem involving agents with second-order dynamics, with the RL component being based on the Multi-Agent Proximal Policy Optimization Algorithm (MAPPO). Our proposed network architecture includes the incorporation of LSTM and self-attention, which allows the trained policy to adapt to a variable number of agents. Our trained policy significantly outperforms the state-of-the-art classical control policy. We demonstrate our proposed method in a variety of simulated experiments. keywords: {Heuristic algorithms;Collaboration;Reinforcement learning;Network architecture;Vehicle dynamics;Task analysis;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341748&isnumber=10341342

W. E. L. Ilboudo, T. Kobayashi and T. Matsubara, "Domains as Objectives: Multi-Domain Reinforcement Learning with Convex-Coverage Set Learning for Domain Uncertainty Awareness," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5622-5629, doi: 10.1109/IROS55552.2023.10342236.Abstract: Domain randomization (DR) is a powerful framework that has allowed the transfer of policies from randomized domain (a.k.a. simulation) to real robots with little to no retraining requirement. However, because the policy has to perform well for many different domain conditions, DR tends to produce sub-optimal policies that can be too conservative on the target real system. This problem is further exacerbated the larger the randomized domain is. To tackle this issue, recent works have proposed to learn universal policies (UP) with domain knowledge such that they can adapt their behavior to each domain when paired with an online system identifier (OSI). However, in most applications, perfect identifications of the target domain can be impossible. In this paper, by drawing similarities between DR as a multi-domain reinforcement learning and multi-objective reinforcement learning (MORL), we propose to learn a UP over the convex coverage set borrowed from the MORL theory. Thanks to this, our method learns a UP that effectively captures different sub-domains of the uncertainty set and can therefore adapt its behavior based on an OSI uncertainty, unlocking the power of stochastic system identification with no retraining requirement. This pseudo-MORL framework also contains previous works in DR and robust reinforcement learning. We conduct simulations on Mujoco tasks and experiments on a real D'Claw robot, revealing the effectiveness of our domain-uncertainty-aware UP for sim-to-real transfer. keywords: {Uncertainty;Stochastic systems;Neural networks;Reinforcement learning;Open systems;Behavioral sciences;System identification},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342236&isnumber=10341342

S. Nikkhoo, Z. Li, A. Samanta, Y. Li and C. Liu, "PIMbot: Policy and Incentive Manipulation for Multi-Robot Reinforcement Learning in Social Dilemmas," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5630-5636, doi: 10.1109/IROS55552.2023.10341884.Abstract: Recent research has demonstrated the potential of reinforcement learning (RL) in enabling effective multi-robot collaboration, particularly in social dilemmas where robots face a trade-off between self-interests and collective benefits. However, environmental factors such as miscommunication and adversarial robots can impact cooperation, making it crucial to explore how multi-robot communication can be manipulated to achieve different outcomes. This paper presents a novel approach, namely PIMbot, to manipulating the reward function in multi-robot collaboration through two distinct forms of manipulation: policy and incentive manipulation. Our work introduces a new angle for manipulation in recent multi-agent RL social dilemmas that utilize a unique reward function for incentivization. By utilizing our proposed PIMbot mechanisms, a robot is able to manipulate the social dilemma environment effectively. PIMbot has the potential for both positive and negative impacts on the task outcome, where positive impacts lead to faster convergence to the global optimum and maximized rewards for any chosen robot. Conversely, negative impacts can have a detrimental effect on the overall task performance. We present comprehensive experimental results that demonstrate the effectiveness of our proposed methods in the Gazebo-simulated multi-robot environment. Our work provides insights into how inter-robot communication can be manipulated and has implications for various robotic applications. keywords: {Collaboration;Reinforcement learning;Environmental factors;Behavioral sciences;Multi-robot systems;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341884&isnumber=10341342

S. He, Y. Wang, S. Han, S. Zou and F. Miao, "A Robust and Constrained Multi-Agent Reinforcement Learning Electric Vehicle Rebalancing Method in AMoD Systems," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5637-5644, doi: 10.1109/IROS55552.2023.10342342.Abstract: Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand (AMoD) systems, but their unique charging patterns increase the model uncertainties in AMoD systems (e.g. state transition probability). Since there usually exists a mismatch between the training and test/true environments, incorporating model uncertainty into system design is of critical importance in real-world applications. However, model uncertainties have not been considered explicitly in EV AMoD system rebalancing by existing literature yet, and the coexistence of model uncertainties and constraints that the decision should satisfy makes the problem even more challenging. In this work, we design a robust and constrained multi-agent reinforcement learning (MARL) framework with state transition kernel uncertainty for EV AMoD systems. We then propose a robust and constrained MARL algorithm (ROCOMA) with robust natural policy gradients (RNPG) that trains a robust EV rebalancing policy to balance the supply-demand ratio and the charging utilization rate across the city under model uncertainty. Experiments show that the ROCOMA can learn an effective and robust rebalancing policy. It outperforms non-robust MARL methods in the presence of model uncertainties. It increases the system fairness by 19.6% and decreases the rebalancing costs by 75.8%. keywords: {Training;Uncertainty;Heuristic algorithms;Urban areas;Reinforcement learning;Electric vehicles;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342342&isnumber=10341342

R. Sambhus, A. Gokce, S. Welch, C. W. Herron and A. Leonessa, "Real-Time Model-Free Deep Reinforcement Learning for Force Control of a Series Elastic Actuator," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5645-5652, doi: 10.1109/IROS55552.2023.10341751.Abstract: Many state-of-the-art robotic applications utilize series elastic actuators (SEAs) with closed-loop force control to achieve complex tasks such as walking, lifting, and manipulation. Model-free PID control methods are more prone to instability due to nonlinearities in the SEA where cascaded model-based robust controllers can remove these effects to achieve stable force control. However, these model-based methods require detailed investigations to characterize the system accurately. Deep reinforcement learning (DRL) has proved to be an effective model-free method for continuous control tasks, where few works deal with hardware learning. This paper describes the training process of a DRL policy on the hardware of an SEA pendulum system for tracking force control trajectories from 0.05 - 0.35 Hz at 50 N amplitude using the Proximal Policy Optimization (PPO) algorithm. Safety mechanisms are developed and utilized for training the policy for over 21 hours (including overnight) without an operator present. The tracking performance is evaluated showing improvements of 25 N in mean absolute error when comparing the first 18 minutes of training to the full 21 hours for a 50 N amplitude, 0.1 Hz sinusoid desired force trajectory. Finally, the DRL policy exhibits better tracking and stability margins when compared to a model-free PID controller for a 50 N chirp force trajectory. keywords: {Training;Legged locomotion;Force;Reinforcement learning;Hardware;Stability analysis;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341751&isnumber=10341342

J. Kim, H. -J. Jung, D. H. Sim, J. -H. Yoo, S. W. Kim and H. U. Yoon, "An Approach to Design a Biomechanically-Inspired Reward Function to Solve a Patience Cube Under Reinforcement Learning Framework," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5653-5660, doi: 10.1109/IROS55552.2023.10341831.Abstract: This paper presents an approach to design a reward function by adopting both control theoretic and biomechanical perspectives. In reinforcement learning (RL), a reward function plays a crucial role for an RL agent training; especially, a task learning time and a task performance. Accordingly, designing a reward function becomes a key issue to train an RL agent generating human-like policy/strategy to perform dexterous manipulation. Since human beings are good at producing heuristic approaches to complete a given task, determining a set of basis functions as well as corresponding weights used not to be so straightforward. In this study, we consider solving a patience cube as an example of a dexterous manipulation task. In our approach, we first employed a quadratic regulator form as a backbone of a desired reward function. Next, the kinematic data of a controlled object and the sEMG data of a human expert were measured while performing a demonstration to solve a patience cube. Then, from the measured data, the weights of the basis functions were determined by utilizing muscle synergy extraction and inverse optimal control as two key tools. Finally, an RL agent was trained by the designed reward function and comparative analysis versus the other RL agents trained by prototypical weight settings was followed. The result showed that the RL agent trained by our approach yielded human-like learning curve as well as policy successfully and outperformed the others in terms of a task success rate and a task completion time. These findings substantiated the feasibility of extending our approach to an assistive robotic manipulator or prosthesis design to perform the activities of daily living. keywords: {Biomechanics;Weight measurement;Training;Optimal control;Reinforcement learning;Kinematics;Muscles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341831&isnumber=10341342

W. Guimont-Martin, J. -M. Fortin, F. Pomerleau and P. Giguère, "MaskBEV: Joint Object Detection and Footprint Completion for Bird's-Eye View 3D Point Clouds," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5677-5684, doi: 10.1109/IROS55552.2023.10342294.Abstract: Recent works in object detection in LiDAR point clouds mostly focus on predicting bounding boxes around objects. This prediction is commonly achieved using anchor-based or anchor-free detectors that predict bounding boxes, requiring significant explicit prior knowledge about the objects to work properly. To remedy these limitations, we propose MaskBEV, a bird's-eye view (BEV) mask-based object detector neural architecture. MaskBEV predicts a set of BEV instance masks that represent the footprints of detected objects. Moreover, our approach allows object detection and footprint completion in a single pass. MaskBEV also reformulates the detection problem purely in terms of classification, doing away with regression usually done to predict bounding boxes. We evaluate the performance of MaskBEV on both SemanticKITTI and KITTI datasets while analyzing the architecture advantages and limitations. keywords: {Point cloud compression;Training;Three-dimensional displays;Laser radar;Shape;Training data;Object detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342294&isnumber=10341342

Y. Zhu, P. Guo, H. Wei, X. Zhao and X. Wu, "Disentangled Discriminator for Unsupervised Domain Adaptation on Object Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5685-5691, doi: 10.1109/IROS55552.2023.10341878.Abstract: Object detection plays an important role in computer vision tasks such as autonomous driving, robotics, etc. Typically, a detection model is firstly trained on collected data and then deployed in real world. However, the discrepancy exists between training (source) and testing (target) data, which degrades the detection model's performance in the real world. To mitigate the negative effects, Unsupervised Domain Adaptation (UDA) methods learn the features of a shared domain via a discriminator. However, existing discriminators consider only the in-distribution adversarial learning, which ignore the out-of-distribution data of individual domains. In this paper, we propose a disentangled discriminator to consider the in-distribution and outliers separately. It aligns the source and target data with split branches under a gated strategy. We combine the disentangled discriminator with a Teacher-Student (T-S) framework that trains the student using labeled source data and unlabeled target data under a self-training mechanism. Specifically, the teacher network, that is updated with the parameters of student network via the exponential moving average, predicts pseudo labels for unlabeled data. The quality of pseudo labels can be improved after alleviating the domain discrepancy thanks to the disentangled discriminator. Extensive experiments on benchmarks demonstrate the superiority of the proposed method. Specifically, we achieve 53.9% mAP on Foggy Cityscapes, which is 7.2% higher than the Oracle. keywords: {Training;Adaptation models;Computational modeling;Object detection;Logic gates;Data models;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341878&isnumber=10341342

T. Nguyen et al., "Open-Vocabulary Affordance Detection in 3D Point Clouds," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5692-5698, doi: 10.1109/IROS55552.2023.10341553.Abstract: Affordance detection is a challenging problem with a wide variety of robotic applications. Traditional affordance detection methods are limited to a predefined set of affordance labels, hence potentially restricting the adaptability of intelligent robots in complex and dynamic environments. In this paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method, which is capable of detecting an unbounded number of affordances in 3D point clouds. By simultaneously learning the affordance text and the point feature, OpenAD successfully exploits the semantic relationships between affordances. Therefore, our proposed method enables zero-shot detection and can be able to detect previously unseen affordances without a single annotation example. Intensive experimental results show that OpenAD works effectively on a wide range of affordance detection setups and outperforms other baselines by a large margin. Additionally, we demonstrate the practicality of the proposed OpenAD in real-world robotic applications with a fast inference speed. Our project is available at https://openad2023.github.io. keywords: {Point cloud compression;Three-dimensional displays;Annotations;Affordances;Semantics;Usability;Intelligent robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341553&isnumber=10341342

M. R. Nallapareddy, K. Sirohi, P. L. J. Drews, W. Burgard, C. -H. Cheng and A. Valada, "EvCenterNet: Uncertainty Estimation for Object Detection Using Evidential Learning," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5699-5706, doi: 10.1109/IROS55552.2023.10341826.Abstract: Uncertainty estimation is crucial in safety-critical settings such as automated driving as it provides valuable information for several downstream tasks including high-level decision making and path planning. In this work, we propose EvCenterNet, a novel uncertainty-aware 2D object detection framework using evidential learning to directly estimate both classification and regression uncertainties. To employ evidential learning for object detection, we devise a combination of evidential and focal loss functions for the sparse heatmap inputs. We introduce class-balanced weighting for regression and heatmap prediction to tackle the class imbalance encountered by evidential learning. Moreover, we propose a learning scheme to actively utilize the predicted heatmap uncertainties to improve the detection performance by focusing on the most uncertain points. We train our model on the KITTI dataset and evaluate it on challenging out-of-distribution datasets including BDD100K and nuImages. Our experiments demonstrate that our approach improves the precision and minimizes the execution time loss in relation to the base model. keywords: {Heating systems;Uncertainty;Three-dimensional displays;Decision making;Estimation;Focusing;Object detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341826&isnumber=10341342

Q. Jiang and H. Sun, "SemanticBEVFusion: Rethinking LiDAR-Camera Fusion in Unified Bird's-Eye View Representation for 3D Object Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5707-5714, doi: 10.1109/IROS55552.2023.10342368.Abstract: LiDAR and cameras are two essential sensors for 3D object detection in autonomous driving. LiDAR provides accurate and reliable 3D geometry information while the camera provides rich texture with color. Despite the increasing popularity of fusing these two complementary sensors, the challenge remains in how to effectively fuse 3D LiDAR point cloud with 2D camera images. Recent methods focus on point-level fusion which paints the LiDAR point cloud with camera features in the perspective view or bird's-eye view (BEV)-level fusion which unifies multi-modality features in the BEV representation. In this paper, we rethink these previous fusion strategies and analyze their information loss and influences on geometric and semantic features. We present SemanticBEVFusion to deeply fuse camera features with LiDAR features in a unified BEV representation while maintaining per-modality strengths for 3D object detection. Our method achieves state-of-the-art performance on the large-scale nuScenes dataset, especially for challenging distant objects. The code will be made publicly available. keywords: {Point cloud compression;Three-dimensional displays;Laser radar;Fuses;Semantics;Object detection;Sensor fusion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342368&isnumber=10341342

H. Chang, L. Wang and J. Cheng, "RFDNet: Real-Time 3D Object Detection Via Range Feature Decoration," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5715-5721, doi: 10.1109/IROS55552.2023.10341482.Abstract: High-performance real-time 3D object detection is crucial in autonomous driving perception systems. Voxel-or point-based 3D object detectors are highly accurate but inefficient and difficult to deploy, while other methods use 2D projection views to improve efficiency, but information loss usually degrades performance. To balance effectiveness and efficiency, we propose a scheme called RFDNet that uses range features to decorate points. Specifically, RFDNet adaptively aggregates point features projected to independent grids and nearby regions via Dilated Grid Feature Encoding (DGFE) to generate a range view, which can handle occlusion and multi-frame inputs while the established geometric correlation between grid with surrounding space weakens the effects of scale distortion. We also propose a Soft Box Regression (SBR) strategy that supervises 3D box regression on a more extensive range than conventional methods to enhance model robustness. In addition, RFDNet benefits from our designed Semantic-assisted Ground-truth Sample (SA-GTS) data augmentation, which additionally considers collisions and spatial distributions of objects. Experiments on the nuScenes benchmark show that RFDNet outperforms all LiDAR-only non-ensemble 3D object detectors and runs at high speed of 20 FPS, achieving a better effectiveness-efficiency trade-off. Code is available at https://github.com/wy17646051/RFDNet. keywords: {Solid modeling;Three-dimensional displays;Graphical models;Detectors;Object detection;Feature extraction;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341482&isnumber=10341342

C. -Y. Huang, C. -T. Chen, Y. -A. Chen and K. -W. Chen, "Object-Level Unknown Obstacle Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5722-5729, doi: 10.1109/IROS55552.2023.10342306.Abstract: This paper presents a novel method for object-level unknown obstacle detection in driving scenes that reduces false positives. The proposed method combines existing anomaly detectors, depth estimation, and object detection techniques to achieve object-level predictions. Our method can predict anomalies as bound-box instance detections. These bounding boxes can then be used to refine anomaly detection by suppressing false positives outside of the bounding boxes. The proposed method has several advantages, including object-level detections that are more practical than pixel-level detections, and the ability to find and refine region proposals for obstacle detection. The paper provides a detailed explanation of all components of the system and includes an ablation study on the usage of depth estimation, as well as execution time averages on different hardware. The proposed method is evaluated using different metrics and benchmarks, demonstrating the effectiveness and relevance of the existing proposed methods. Overall, our proposed method has the potential to significantly improve object-level anomaly detection making it suitable for real-world applications. keywords: {Measurement;Location awareness;Roads;Neural networks;Estimation;Object detection;Proposals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342306&isnumber=10341342

Y. Shen et al., "BSH-Det3D: Improving 3D Object Detection with BEV Shape Heatmap," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5730-5737, doi: 10.1109/IROS55552.2023.10341930.Abstract: The progress of LiDAR-based 3D object detection has significantly enhanced developments in autonomous driving and robotics. However, due to the limitations of LiDAR sensors, object shapes suffer from deterioration in occluded and distant areas, which creates a fundamental challenge to 3D perception. Existing methods estimate specific 3D shapes and achieve remarkable performance. However, these methods rely on extensive computation and memory, causing imbalances between accuracy and real-time performance. To tackle this challenge, we propose a novel LiDAR-based 3D object detection model named BSH-Det3D, which applies an effective way to enhance spatial features by estimating complete shapes from a bird's eye view (BEV). Specifically, we design the Pillar-based Shape Completion (PSC) module to predict the probability of occupancy whether a pillar contains object shapes. The PSC module generates a BEV shape heatmap for each scene. After integrating with heatmaps, BSH-Det3D can provide additional information in shape deterioration areas and generate high-quality 3D proposals. We also design an attention-based densification fusion module (ADF) to adaptively associate the sparse features with heatmaps and raw points. The ADF module integrates the advantages of points and shapes knowledge with negligible overheads. Extensive experiments on the KITTI benchmark achieve state-of-the-art (SOTA) performance in terms of accuracy and speed, demonstrating the efficiency and flexibility of BSH-Det3D. The source code is available on https://github.com/mystorm16/BSH-Det3D. keywords: {Heating systems;Solid modeling;Three-dimensional displays;Shape;Source coding;Object detection;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341930&isnumber=10341342

W. Liang, G. Sun, C. Liu, J. Dong and K. Wang, "I3DOD: Towards Incremental 3D Object Detection via Prompting," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5738-5743, doi: 10.1109/IROS55552.2023.10341834.Abstract: 3D object detection have achieved significant performance in many fields, e.g., robotics system, autonomous driving, and augmented reality. However, most existing methods could cause catastrophic forgetting of old classes when performing on the class-incremental scenarios. Meanwhile, the current class-incremental 3D object detection methods neglect the relationships between the object localization information and category semantic information, and assume all the knowledge of old model is reliable. To address the above challenge, we present a novel Incremental 3D Object Detection framework with the guidance of prompting, i.e., I3DOD. Specifically, we propose a task-shared prompts mechanism to learn the matching relationships between the object localization information and category semantic information. After training on the current task, these prompts will be stored in our prompt pool, and perform the relationship of old classes in the next task. Moreover, we design a reliable distillation strategy to transfer knowledge from two aspects: a reliable dynamic distillation is developed to filter out the negative knowledge and transfer the reliable 3D knowledge to new detection model; the relation feature is proposed to capture the responses relation in feature space and protect plasticity of the model when learning novel 3D classes. To the end, we conduct comprehensive experiments on two benchmark datasets and our method outperforms the state-of-the-art object detection methods by 0.6% ∼ 2.7% in terms of mAP@0.25. keywords: {Location awareness;Training;Solid modeling;Three-dimensional displays;Semantics;Object detection;Reliability engineering},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341834&isnumber=10341342

T. Gossard, J. Tebbe, A. Ziegler and A. Zell, "SpinDOE: A Ball Spin Estimation Method for Table Tennis Robot," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5744-5750, doi: 10.1109/IROS55552.2023.10342178.Abstract: Spin plays a considerable role in table tennis, making a shot's trajectory harder to read and predict. However, the spin is challenging to measure because of the ball's high velocity and the magnitude of the spin values. Existing methods either require extremely high framerate cameras or are unreliable because they use the ball's logo, which may not always be visible. Because of this, many table tennis-playing robots ignore the spin, which severely limits their capabilities. This paper proposes an easily implementable and reliable spin estimation method. We developed a dotted-ball orientation estimation (DOE) method, that can then be used to estimate the spin. The dots are first localized on the image using a CNN and then identified using geometric hashing. The spin is finally regressed from the estimated orientations. Using our algorithm, the ball's orientation can be estimated with a mean error of 2.4° and the spin estimation has an relative error lower than 1%. Spins up to 175 rps are measurable with a camera of 350 fps in real time. Using our method, we generated a dataset of table tennis ball trajectories with position and spin, available on our project page. Project page: https://cogsys-tuebingen.github.io/spindoe/. keywords: {Torque;Sports equipment;Robot vision systems;Estimation;Cameras;Time measurement;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342178&isnumber=10341342

S. Xiong, G. Tziafas and H. Kasaei, "Enhancing Fine-Grained 3D Object Recognition Using Hybrid Multi-Modal Vision Transformer-CNN Models," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5751-5757, doi: 10.1109/IROS55552.2023.10342235.Abstract: Robots operating in human-centered environments, such as retail stores, restaurants, and households, are often required to distinguish between similar objects in different contexts with a high degree of accuracy. However, fine-grained object recognition remains a challenge in robotics due to the high intra-category and low inter-category dissimilarities. In addition, the limited number of fine-grained 3D datasets poses a significant problem in addressing this issue effectively. In this paper, we propose a hybrid multi-modal Vision Transformer (ViT) and Convolutional Neural Networks (CNN) approach to improve the performance of fine-grained visual classification (FGVC). To address the shortage of FGVC 3D datasets, we generated two synthetic datasets. The first dataset consists of 20 categories related to restaurants with a total of 100 instances, while the second dataset contains 120 shoe instances. Our approach was evaluated on both datasets, and the results indicate that our hybrid multi-modal model outperforms both CNN-only and ViT-only baselines, achieving a recognition accuracy of 94.50% and 93.51% on the restaurant and shoe datasets, respectively. Additionally, we have made our FGVC RGB-D datasets available to the research community to enable further experimentation and advancement. Furthermore, we integrated our proposed method with a robot framework and demonstrated its potential as a fine-grained perception tool in both simulated and real-world robotic scenarios. keywords: {Visualization;Solid modeling;Three-dimensional displays;Footwear;Transformers;Object recognition;Convolutional neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342235&isnumber=10341342

X. Lu and H. Radha, "ScAR: Scaling Adversarial Robustness for LiDAR Object Detection," 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Detroit, MI, USA, 2023, pp. 5758-5764, doi: 10.1109/IROS55552.2023.10341583.Abstract: The adversarial robustness of a model is its ability to resist adversarial attacks in the form of small perturbations to input data. Universal adversarial attack methods such as Fast Sign Gradient Method (FSGM) [1] and Projected Gradient Descend (PGD) [2] are popular for LiDAR object detection, but they are often deficient compared to task-specific adversarial attacks. Additionally, these universal methods typically require unrestricted access to the model's information, which is difficult to obtain in real-world applications. To address these limitations, we present a black-box Scaling Adversarial Robustness (ScAR) method for LiDAR object detection. By analyzing the statistical characteristics of 3D object detection datasets such as KITTI, Waymo, and nuScenes, we have found that the model's prediction is sensitive to scaling of 3D instances. We propose three black-box scaling adversarial attack methods based on the available information: model-aware attack, distribution-aware attack, and blind attack. We also introduce a strategy for generating scaling adversarial examples to improve the model's robustness against these three scaling adversarial attacks. Comparison with other methods on public datasets under different 3D object detection architectures demonstrates the effectiveness of our proposed method. keywords: {Solid modeling;Three-dimensional displays;Laser radar;Sensitivity;Closed box;Object detection;Resists},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341583&isnumber=10341342

