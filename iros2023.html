
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Papers</title>
        <style>
            table {
                width: 100%;
                border-collapse: collapse;
            }
            th, td {
                border: 1px solid #ddd;
                padding: 8px;
                word-wrap: break-word;
                max-width: 200px;
            }
            th {
                background-color: #f2f2f2;
            }
            td:nth-child(4) {
                width: 50%; /* 使abstract列的宽度是其他列的两倍 */
            }
        </style>
    </head>
    <body>
        <h1>Papers</h1>
        <table>
            <tr>
                <th>Title</th>
                <th>Authors</th>
                <th>Year</th>
                <th>Abstract</th>
                <th>Keywords</th>
                <th>URL</th>
            </tr>
    
            <tr>
                <td>Boosting Lidar 3D Object Detection with Point Cloud Semantic Segmentation</td>
                <td>X. Zhang, C. Min, Y. Jia, L. Chen, J. Zhang and H. Sun</td>
                <td>2023</td>
                <td>The integration of semantic information can effectively enhance the performance of 3D object detection based on lidar point cloud. Most of previous researches utilize camera-lidar fusion to improve detection accuracy for distant or small objects. However, this approach is typically unsuitable for real-time applications due to the large amount of input data. Recently, a multi-task framework using only Iidar has emerged as an alternative that employs the same feature extraction backbone with different heads to simultaneously output detection and semantic segmentation results for lidar point clouds. Nonetheless, some previous works have failed to achieve an optimal balance between accuracy and speed. To address this issue, we propose a multi-task framework which leverages the Cartesian pillar and a multi-scale semantic segmentation head to overcome the shortcomings of existing works and improve the detection accuracy. We evaluate the proposed method using typical pillar-based and voxel-based detection models on the nuScenes dataset. The experimental results demonstrate that the proposed design achieves better performance especially on small objects, compared to single-task models. Moreover, the proposed network increases mAP and NDS by 3.1 % and 2.5 % respectively on the nuScenes test set, compared to the representative multi-task network.</td>
                <td>Point cloud compression, Training, Three-dimensional displays, Laser radar, Semantic segmentation, Semantics, Object detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341613&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341613&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Generalized Few-shot Semantic Segmentation for LiDAR Point Clouds</td>
                <td>P. Wu, J. Mei, X. Zhao and Y. Hu</td>
                <td>2023</td>
                <td>Semantic segmentation of LiDAR point clouds can provide assistance for precise perception in autonomous driving, but traditional segmentation methods face challenges such as unbalanced class distribution and insufficient labeling. Generalized few-shot learning has been researched on image data, but these methods are difficult to apply directly to LiDAR point clouds. To tackle these challenges, we propose a generalized few-shot semantic segmentation method based on LiDAR point cloud data, enabling us to predict base and novel classes simultaneously. To improve the performance with limited novel class samples, we integrate semantic vectors and leverage the intrinsic relationship between base and novel class vectors to facilitate learning. We conduct comprehensive comparisons with other methods on the SemanticKITTI and constantly surpass them with higher mIoU, demonstrating the effectiveness of our method.</td>
                <td>Point cloud compression, Laser radar, Semantic segmentation, Semantics, Labeling, Intelligent robots, Faces</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341978&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341978&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Baking in the Feature: Accelerating Volumetric Segmentation by Rendering Feature Maps</td>
                <td>K. Blomqvist, L. Ott, J. J. Chung and R. Siegwart</td>
                <td>2023</td>
                <td>Methods have recently been proposed that densely segment 3D volumes into classes using only color images and expert supervision in the form of sparse semantically annotated pixels. While impressive, these methods still require a relatively large amount of supervision and segmenting an object can take several minutes in practice. Such systems typically only optimize the representation on the scene they are fitting, without leveraging prior information from previously seen images. In this paper, we propose to use features extracted with models pre-trained on large existing datasets to improve segmentation performance on novel scenes. We bake this feature representation into a Neural Radiance Field (NeRF) by volu-metrically rendering feature maps and supervising on features extracted from each input image. We show that by baking this representation into the NeRF, we make the subsequent classification task much easier. Our experiments show that our method achieves higher segmentation accuracy with fewer semantic annotations than existing methods over a wide range of scenes.</td>
                <td>Image segmentation, Three-dimensional displays, Annotations, Shape, Semantics, Feature extraction, Rendering (computer graphics)</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342071&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342071&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Optical Flow Boosts Unsupervised Localization and Segmentation</td>
                <td>X. Zhang and A. Boularias</td>
                <td>2023</td>
                <td>Unsupervised localization and segmentation are long-standing robot vision challenges that describe the critical ability for an autonomous robot to learn to decompose images into individual objects without labeled data. These tasks are important because of the limited availability of dense image manual annotation and the promising vision of adapting to an evolving set of object categories in lifelong learning. Most recent methods focus on using visual appearance continuity as object cues by spatially clustering features obtained from self-supervised vision transformers (ViT). In this work, we leverage motion cues, inspired by the common fate principle that pixels that share similar movements tend to belong to the same object. We propose a new loss term formulation that uses optical flow in unlabeled videos to encourage self-supervised ViT features to become closer to each other if their corresponding spatial locations share similar movements, and vice versa. We use the proposed loss function to finetune vision transformers that were originally trained on static images. Our fine-tuning procedure outperforms state-of-the-art techniques for unsupervised semantic segmentation through linear probing, without the use of any labeled data. This procedure also demonstrates increased performance over original ViT networks across unsupervised object localization and semantic segmentation benchmarks. Our code is available at https://github.com/mlzxy/flowdino.</td>
                <td>Location awareness, Optical losses, Visualization, Semantic segmentation, Robot vision systems, Object segmentation, Transformers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342195&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342195&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>T-UDA: Temporal Unsupervised Domain Adaptation in Sequential Point Clouds</td>
                <td>A. H. Gebrehiwot, D. Hurych, K. Zimmermann, P. Pérez and T. Svoboda</td>
                <td>2023</td>
                <td>Deep perception models have to reliably cope with an open-world setting of domain shifts induced by different geographic regions, sensor properties, mounting positions, and several other reasons. Since covering all domains with annotated data is technically intractable due to the endless possible variations, researchers focus on unsupervised domain adaptation (UDA) methods that adapt models trained on one (source) domain with annotations available to another (target) domain for which only unannotated data are available. Current predominant methods either leverage semi-supervised approaches, e.g., teacher-student setup, or exploit privileged data, such as other sensor modalities or temporal data consistency. We introduce a novel domain adaptation method that leverages the best of both approaches. Our approach combines input data's temporal and cross-sensor geometric consistency with the mean teacher method. Dubbed T-UDA for “temporal UDA”, such a combination yields massive performance gains for the task of 3D semantic segmentation of driving scenes. Experiments are conducted on Waymo Open Dataset, nuScenes, and SemanticKITTI, for two popular 3D point cloud architectures, Cylinder3D and MinkowskiNet. Our codes are publicly available on https://github.com/ctu-vras/T-UDA.</td>
                <td>Point cloud compression, Measurement, Adaptation models, Three-dimensional displays, Semantic segmentation, Performance gain, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341446&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341446&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Superpixel Transformers for Efficient Semantic Segmentation</td>
                <td>A. Z. Zhu et al.</td>
                <td>2023</td>
                <td>Semantic segmentation, which aims to classify every pixel in an image, is a key task in machine perception, with many applications across robotics and autonomous driving. Due to the high dimensionality of this task, most existing approaches use local operations, such as convolutions, to generate per-pixel features. However, these methods are typically unable to effectively leverage global context information due to the high computational costs of operating on a dense image. In this work, we propose a solution to this issue by leveraging the idea of superpixels, an over-segmentation of the image, and applying them with a modern transformer framework. In particular, our model learns to decompose the pixel space into a spatially low dimensional superpixel space via a series of local cross-attentions. We then apply multi-head self-attention to the superpixels to enrich the superpixel features with global context and then directly produce a class prediction for each superpixel. Finally, we directly project the superpixel class predictions back into the pixel space using the associations between the superpixels and the image pixel features. Reasoning in the superpixel space allows our method to be substantially more computationally efficient compared to convolution-based decoder methods. Yet, our method achieves state-of-the-art performance in semantic segmentation due to the rich superpixel features generated by the global self-attention mechanism. Our experiments on Cityscapes and ADE20K demonstrate that our method matches the state of the art in terms of accuracy, while outperforming in terms of model parameters and latency.</td>
                <td>Semantic segmentation, Computational modeling, Network architecture, Transformers, Cognition, Computational efficiency, Decoding</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341519&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341519&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hierarchical Semi-Supervised Learning Framework for Surgical Gesture Segmentation and Recognition Based on Multi-Modality Data</td>
                <td>Z. Yuan, J. Lin and D. Zhang</td>
                <td>2023</td>
                <td>Segmenting and recognizing surgical operation trajectories into distinct, meaningful gestures is a critical preliminary step in surgical workflow analysis for robot-assisted surgery. This step is necessary for facilitating learning from demonstrations for autonomous robotic surgery, evaluating surgical skills, and so on. In this work, we develop a hierarchical semi-supervised learning framework for surgical gesture segmentation using multi-modality data (i.e. kinematics and vision data). More specifically, surgical tasks are initially segmented based on distance characteristics-based profiles and variance characteristics-based profiles constructed using kinematics data. Subsequently, a Transformer-based network with a pre-trained ‘ResNet-18’ backbone is used to extract visual features from the surgical operation videos. By combining the potential segmentation points obtained from both modalities, we can determine the final segmentation points. Furthermore, gesture recognition can be implemented based on supervised learning. The proposed approach has been evaluated using data from the publicly available JIGSAWS database, including Suturing, Needle Passing, and Knot Tying tasks. The results reveal an average F1 score of 0.623 for segmentation and an accuracy of 0.856 for recognition. For more details about this paper, please visit our website: https://sites.google.com/view/surseg/home.</td>
                <td>Visualization, Surgery, Kinematics, Semisupervised learning, Feature extraction, Transformers, Needles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341673&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341673&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lidar Panoptic Segmentation and Tracking without Bells and Whistles</td>
                <td>A. Agarwalla et al.</td>
                <td>2023</td>
                <td>State-of-the-art lidar panoptic segmentation (LPS) methods follow “bottom-up” segmentation-centric fashion wherein they build upon semantic segmentation networks by utilizing clustering to obtain object instances. In this paper, we re-think this approach and propose a surprisingly simple yet effective detection-centric network for both LPS and tracking. Our network is modular by design and optimized for all aspects of both the panoptic segmentation and tracking task. One of the core components of our network is the object instance detection branch, which we train using point-level (modal) annotations, as available in segmentation-centric datasets. In the absence of amodal (cuboid) annotations, we regress modal centroids and object extent using trajectory-level supervision that provides information about object size, which cannot be inferred from single scans due to occlusions and the sparse nature of the lidar data. We obtain fine-grained instance segments by learning to associate lidar points with detected centroids. We evaluate our method on several 3D/4D LPS benchmarks and observe that our model establishes a new state-of-the-art among open-sourced models, outperforming recent query-based models.</td>
                <td>Laser radar, Annotations, Semantic segmentation, Benchmark testing, Spatiotemporal phenomena, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341415&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341415&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CompUDA: Compositional Unsupervised Domain Adaptation for Semantic Segmentation Under Adverse Conditions</td>
                <td>K. Yeung</td>
                <td>2023</td>
                <td>In autonomous driving, performing robust semantic segmentation under adverse weather conditions is a long-standing challenge. Imperfect camera observations under adverse conditions result in images with reduced visibility, which hinders label annotation and semantic scene understanding based on these images. A common solution is to adopt semantic segmentation models trained in a source domain with ground truth labels and perform unsupervised domain adaptation (UDA) from the source domain to an unlabeled target domain that has adverse conditions. Due to imperfect visual observations in the target domain, such adaptation needs special treatment to achieve good performance. In this paper, we propose a new compositional unsupervised domain adaptation (CompUDA) method that disentangles the domain gap based on multiple factors including style, visibility, and image quality. The domain gaps caused by these individual factors can then be addressed separately by introducing the intermediate domains. Specifically, 1) to address the style gap, we perform source-to-intermediate domain adaptation and generate pseudo-labels for self-training in the target domain; 2) to address the visibility gap, we perform a geometry-aligned normal-to-adverse image translation and introduce a synthetic domain; 3) finally, to address the image quality gap between the synthetic and target domain, we perform a synthetic-to-real adaptation based on the generated pseudo-labels. Our compositional unsupervised domain adaptation can be used in conjunction with a wide variety of semantic segmentation methods and result in significant performance improvement across datasets. The codes are available at https://github.com/zhengziqiang/CompUDA.</td>
                <td>Image quality, Visualization, Codes, Image synthesis, Semantic segmentation, Semantics, Performance gain</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342102&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342102&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Transparent Object Tracking with Enhanced Fusion Module</td>
                <td>K. Garigapati, E. Blasch, J. Wei and H. Ling</td>
                <td>2023</td>
                <td>Accurate tracking of transparent objects, such as glasses, plays a critical role in many robotic tasks such as robot-assisted living. Due to the adaptive and often reflective texture of such objects, traditional tracking algorithms that rely on general-purpose learned features suffer from reduced performance. Recent research has proposed to instill trans-parency awareness into existing general object trackers by fusing purpose-built features. However, with the existing fusion techniques, the addition of new features causes a change in the latent space making it impossible to incorporate transparency awareness on trackers with fixed latent spaces. For example, many of the current days' transformer-based trackers are fully pre-trained and are sensitive to any latent space perturbations. In this paper, we present a new feature fusion technique that integrates transparency information into a fixed feature space, enabling its use in a broader range of trackers. Our proposed fusion module, composed of a transformer encoder and an MLP module, leverages key query-based transformations to embed the transparency information into the tracking pipeline. We also present a new two-step training strategy for our fusion module to effectively merge transparency features. We propose a new tracker architecture that uses our fusion techniques to achieve superior results for transparent object tracking. Our proposed method achieves competitive results with state-of-the-art trackers on TOTB, which is the largest transparent object tracking benchmark recently released. Our results and the implementation of code will be made publicly available at https://github.com/kalyan0510/TOTEM.</td>
                <td>Training, Perturbation methods, Pipelines, Object segmentation, Glass, Transformers, Object tracking</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341597&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341597&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Self-Supervised Event-Based Monocular Depth Estimation Using Cross-Modal Consistency</td>
                <td>J. Zhu et al.</td>
                <td>2023</td>
                <td>An event camera is a novel vision sensor that can capture per-pixel brightness changes and output a stream of asynchronous “events”. It has advantages over conventional cameras in those scenes with high-speed motions and challenging lighting conditions because of the high temporal resolution, high dynamic range, low bandwidth, low power consumption, and no motion blur. Therefore, several supervised monocular depth estimation from events is proposed to address scenes difficult for conventional cameras. However, depth annotation is costly and time-consuming. In this paper, to lower the annotation cost, we propose a self-supervised event-based monocular depth estimation framework named EMoDepth. EMoDepth constrains the training process using the cross-modal consistency from intensity frames that are aligned with events in the pixel coordinate. Moreover, in inference, only events are used for monocular depth prediction. Additionally, we design a multi-scale skip-connection architecture to effectively fuse features for depth estimation while maintaining high inference speed. Experiments on MVSEC and DSEC datasets demonstrate that our contributions are effective and that the accuracy can outperform existing supervised event-based and unsupervised frame-based methods.</td>
                <td>Training, Power demand, Fuses, Annotations, Estimation, Lighting, Vision sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342434&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342434&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sim-to-Real Vision-Depth Fusion CNNs for Robust Pose Estimation Aboard Autonomous Nano-quadcopters</td>
                <td>L. Crupi, E. Cereda, A. Giusti and D. Palossi</td>
                <td>2023</td>
                <td>Nano-quadcopters are versatile platforms attracting the interest of both academia and industry. Their tiny form factor, i.e., ~ 10 cm diameter, makes them particularly useful in narrow scenarios and harmless in human proximity. However, these advantages come at the price of ultra-constrained onboard computational and sensorial resources for autonomous operations. This work addresses the task of estimating human pose aboard nano-drones by fusing depth and images in a novel CNN exclusively trained in simulation yet capable of robust predictions in the real world. We extend a commercial off-the-shelf (COTS) Crazyflie nano-drone - equipped with a 320x240 px camera and an ultra-low-power System-on-Chip - with a novel multi-zone (8 x 8) depth sensor. We design and compare different deep-learning models that fuse depth and image inputs. Our models are trained exclusively on simulated data for both inputs, and transfer well to the real world: field testing shows an improvement of 58% and 51 % of our depth+camera system w.r.t. a camera-only State-of-the-Art baseline on the horizontal and angular mean pose errors, respectively. Our prototype is based on COTS components, which facilitates reproducibility and adoption of this novel class of systems.</td>
                <td>Fuses, Pose estimation, Prototypes, Training data, Cameras, Robot sensing systems, Reproducibility of results</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342162&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342162&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SSC-RS: Elevate LiDAR Semantic Scene Completion with Representation Separation and BEV Fusion</td>
                <td>J. Mei, Y. Yang, M. Wang, T. Huang, X. Yang and Y. Liu</td>
                <td>2023</td>
                <td>Semantic scene completion (SSC) jointly predicts the semantics and geometry of the entire 3D scene, which plays an essential role in 3D scene understanding for autonomous driving systems. SSC has achieved rapid progress with the help of semantic context in segmentation. However, how to effectively exploit the relationships between the semantic context in semantic segmentation and geometric structure in scene completion remains under exploration. In this paper, we propose to solve outdoor SSC from the perspective of representation separation and BEV fusion. Specifically, we present the network, named SSC-RS, which uses separate branches with deep supervision to explicitly disentangle the learning procedure of the semantic and geometric representations. And a BEV fusion network equipped with the proposed Adaptive Representation Fusion (ARF) module is presented to aggregate the multi-scale features effectively and efficiently. Due to the low computational burden and powerful representation ability, our model has good generality while running in real-time. Extensive experiments on SemanticKITTI demonstrate our SSC-RS achieves state-of-the-art performance. Code is available at https://github.com/Jieqianyu/SSC-RS.git.</td>
                <td>Geometry, Visualization, Three-dimensional displays, Adaptive systems, Laser radar, Fuses, Semantic segmentation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341742&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341742&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PANet: LiDAR Panoptic Segmentation with Sparse Instance Proposal and Aggregation</td>
                <td>J. Mei, Y. Yang, M. Wang, X. Hou, L. Li and Y. Liu</td>
                <td>2023</td>
                <td>Reliable LiDAR panoptic segmentation (LPS), including both semantic and instance segmentation, is vital for many robotic applications, such as autonomous driving. This work proposes a new LPS framework named PANet to eliminate the dependency on the offset branch and improve the performance on large objects, which are always over-segmented by clustering algorithms. Firstly, we propose a non-learning Sparse Instance Proposal (SIP) module with the “sampling-shifting-grouping” scheme to directly group thing points into instances from the raw point cloud efficiently. More specifically, balanced point sampling is introduced to generate sparse seed points with more uniform point distribution over the distance range. And a shift module, termed bubble shifting, is proposed to shrink the seed points to the clustered centers. Then we utilize the connected component label algorithm to generate instance proposals. Furthermore, an instance aggregation module is devised to integrate potentially fragmented instances, improving the performance of the SIP module on large objects. Extensive experiments show that PANet achieves state-of-the-art performance among published works on the SemanticKITII validation and nuScenes validation for the panoptic segmentation task. Code is available at https://github.com/Jieqianyu/PANet.git.</td>
                <td>Point cloud compression, Training, Instance segmentation, Laser radar, Semantics, Clustering algorithms, Proposals</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342468&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342468&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles</td>
                <td>J. Chung</td>
                <td>2023</td>
                <td>We present a new method to adapt an RGB-trained water segmentation network to target-domain aerial thermal imagery using online self-supervision by leveraging texture and motion cues as supervisory signals. This new thermal capability enables current autonomous aerial robots operating in near-shore environments to perform tasks such as visual navigation, bathymetry, and flow tracking at night. Our method overcomes the problem of scarce and difficult-to-obtain near-shore thermal data that prevents the application of conventional supervised and unsupervised methods. In this work, we curate the first aerial thermal near-shore dataset, show that our approach outperforms fully-supervised segmentation models trained on limited target-domain thermal data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded computing platform. Code and datasets used in this work will be available at: https://github.com/connorlee77/uav-thermal-water-segmentation.</td>
                <td>Training, Visualization, Target tracking, Navigation, Motion segmentation, Autonomous aerial vehicles, Bathymetry</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342016&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342016&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lightweight, Uncertainty-Aware Conformalized Visual Odometry</td>
                <td>A. C. Stutts, D. Erricolo, T. Tulabandhula and A. R. Trivedi</td>
                <td>2023</td>
                <td>Data-driven visual odometry (VO) is a critical subroutine for autonomous edge robotics, and recent progress in the field has produced highly accurate point predictions in complex environments. However, emerging autonomous edge robotics devices like insect-scale drones and surgical robots lack a computationally efficient framework to estimate VO's predictive uncertainties. Meanwhile, as edge robotics continue to proliferate into mission-critical application spaces, awareness of the model's predictive uncertainties has become crucial for risk-aware decision-making. This paper addresses this challenge by presenting a novel, lightweight, and statistically robust framework that leverages conformal inference (CI) to extract VO's uncertainty bands. Our approach represents the uncertainties using flexible, adaptable, and adjustable prediction intervals that, on average, guarantee the inclusion of the ground truth across all degrees of freedom (DOF) of pose estimation. We discuss the architectures of generative deep neural networks for estimating multivariate uncertainty bands along with point (mean) prediction. We also present techniques to improve the uncertainty estimation accuracy, such as leveraging Monte Carlo dropout (MC-dropout) for data augmentation. Finally, we propose a novel training loss function that combines interval scoring and calibration loss with traditional training metrics-mean-squared error and KL-divergence-to improve uncertainty-aware learning. Our simulation results demonstrate that the presented framework consistently captures true uncertainty in pose estimations across different datasets, estimation models, and applied noise types, indicating its wide applicability.</td>
                <td>Training, Measurement, Uncertainty, Computational modeling, Pose estimation, Computer architecture, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341924&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341924&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LiDAR Meta Depth Completion</td>
                <td>W. Boettcher, L. Hoyer, O. Unal, K. Li and D. Dai</td>
                <td>2023</td>
                <td>Depth estimation is one of the essential tasks to be addressed when creating mobile autonomous systems. While monocular depth estimation methods have improved in recent times, depth completion provides more accurate and reliable depth maps by additionally using sparse depth information from other sensors such as LiDAR. However, current methods are specifically trained for a single LiDAR sensor. As the scanning pattern differs between sensors, every new sensor would require re- training a specialized depth completion model, which is computationally inefficient and not flexible. Therefore, we propose to dynamically adapt the depth completion model to the used sensor type enabling LiDAR adaptive depth completion. Specifically, we propose a meta depth completion network that uses data patterns derived from the data to learn a task network to alter weights of the main depth completion network to solve a given depth completion task effectively. The method demonstrates a strong capability to work on multiple LiDAR scanning patterns and can also generalize to scanning patterns that are unseen during training. While using a single model, our method yields significantly better results than a non-adaptive baseline trained on different LiDAR patterns. It outperforms LiDAR-specific expert models for very sparse cases. These advantages allow flexible deployment of a single depth completion model on different sensors, which could also prove valuable to process the input of nascent LiDAR technology with adaptive instead of fixed scanning patterns. The source code is available at github.com/wbkit/ResLAN</td>
                <td>Training, Adaptation models, Laser radar, Computational modeling, Source coding, Estimation, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341349&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341349&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Primitive Skill-Based Robot Learning from Human Evaluative Feedback</td>
                <td>A. Hiranaka et al.</td>
                <td>2023</td>
                <td>Reinforcement learning (RL) algorithms face significant challenges when dealing with long-horizon robot manipulation tasks in real-world environments due to sample inefficiency and safety issues. To overcome these challenges, we propose a novel framework, SEED, which leverages two approaches: reinforcement learning from human feedback (RLHF) and primitive skill-based reinforcement learning. Both approaches are particularly effective in addressing sparse reward issues and the complexities involved in long-horizon tasks. By combining them, SEED reduces the human effort required in RLHF and increases safety in training robot manipulation with RL in real-world settings. Additionally, parameterized skills provide a clear view of the agent's high-level intentions, allowing humans to evaluate skill choices before they are executed. This feature makes the training process even safer and more efficient. To evaluate the performance of SEED, we conducted extensive experiments on five manipulation tasks with varying levels of complexity. Our results show that SEED significantly outperforms state-of-the-art RL algorithms in sample efficiency and safety. In addition, SEED also exhibits a substantial reduction of human effort compared to other RLHF methods. Further details and video results can be found at https://seediros23.github.io/.</td>
                <td>Training, Costs, Reinforcement learning, Robot learning, Safety, Complexity theory, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341912&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341912&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Autocomplete of 3D Motions for UAV Teleoperation</td>
                <td>B. Ibrahim, M. H. Hussein, I. H. Elhajj and D. Asmar</td>
                <td>2023</td>
                <td>Tele-operating aerial vehicles without any automated assistance is challenging due to various limitations, especially for inexperienced users. Autocomplete addresses this problem by automatically identifying and completing the user's intended motion. Such a framework uses machine learning to recognize and classify human inputs as one of a set of motion primitives, and then, if the human operator accepts, synthesizes the motion in order to complete the desired motion. This has been shown to improve the performance of the system and reduce operator workload. Previous Autocomplete systems focused on different 2D motions (line, arc, sine,..). However, since most UAVs tasks are in a 3D world, this paper introduces 3D Autocomplete for 3D motions. Moreover, the proposed framework presents just-in-time prediction of the 3D motions by proposing a change point detection technique, which allows the framework to autonomously identify when to conduct a prediction. Also, it deals with variable motion sizes. Real time simulation results show that the proposed framework is capable of predicting the user intentions after change point detection.</td>
                <td>Deep learning, Three-dimensional displays, Simulation, Autonomous aerial vehicles, Real-time systems, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342390&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342390&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Exploiting Spatio-Temporal Human-Object Relations Using Graph Neural Networks for Human Action Recognition and 3D Motion Forecasting</td>
                <td>D. Lagamtzis, F. Schmidt, J. Seyler, T. Dang and S. Schober</td>
                <td>2023</td>
                <td>Human action recognition and motion forecasting is becoming increasingly successful, in particular with utilizing graphs. We aim to transfer this success into the context of industrial Human-Robot Collaboration (HRC), where humans work closely with robots and interact with workpieces in defined workspaces. For this purpose, it is necessary to use all the available information extractable in such a workspace and represent it with a natural structure, such as graphs, that can be used for learning. Since humans are the center of HRC, it is mandatory to construct the graph in a human-centered way and use real-world 3D information as well as object labels to represent their environment. Therefore, we present a novel Graph Neural Network (GNN) architecture which combines, human action recognition and motion forecasting for industrial HRC environments. We evaluate our method with two different and publicly available human action datasets, including one that is a particularly realistic representation of the industrial HRC, and compare the results with baseline methods for classifying the current human action and predicting the human motion. Our experiments show that our combined GNN approach improves the accuracy of action recognition compared to previous work, and significantly on the CoAx dataset by up to 20%. Further, our motion forecasting approach performs better than existing baselines, predicting human trajectories with a Final Displacement Error (FDE) of less than 10cm for a prediction horizon of 1s.</td>
                <td>Three-dimensional displays, Service robots, Collaboration, Graph neural networks, Trajectory, Human activity recognition, Data mining</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342491&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342491&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Improving Human-Robot Interaction Effectiveness in Human-Robot Collaborative Object Transportation Using Force Prediction</td>
                <td>Vidal and A. Sanfeliu</td>
                <td>2023</td>
                <td>In this work, we analyse the use of a prediction of the human's force in a Human-Robot collaborative object transportation task at a middle distance. We check that this force prediction can improve multiple parameters associated with effective Human-Robot Interaction (HRI) such as perception of the robot's contribution to the task, comfort or trust in the robot in a physical Human Robot Interaction (pHRI). We present a Deep Learning model that allows to predict the force that a human will exert in the next 1 $s$ using as inputs the force previously exerted by the human, the robot's velocity and environment information obtained from the robot's LiDAR. Its success rate is up to 92.3% in testset and up to 89.1 % in real experiments. We demonstrate that this force prediction, in addition to being able to be used directly to detect changes in the human's intention, can be processed to obtain an estimate of the human's desired trajectory. We have validated this approach with a user study involving 18 volunteers.</td>
                <td>Deep learning, Force, Human-robot interaction, Collaboration, Transportation, Predictive models, Transformers, Physical Human-Robot Interaction, Object Transportation, Human-in-the-Loop, Force Prediction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342517&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342517&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Leveraging Saliency-Aware Gaze Heatmaps for Multiperspective Teaching of Unknown Objects</td>
                <td>D. Weber, V. Bolz, A. Zell and E. Kasneci</td>
                <td>2023</td>
                <td>As robots become increasingly prevalent amidst diverse environments, their ability to adapt to novel scenarios and objects is essential. Advances in modern object detection have also paved the way for robots to identify interaction entities within their immediate vicinity. One drawback is that the robot's operational domain must be known at the time of training, which hinders the robot's ability to adapt to unexpected environments outside the preselected classes. However, when encountering such challenges a human can provide support to a robot by teaching it about the new, yet unknown objects on an ad hoc basis. In this work, we merge augmented reality and human gaze in the context of multimodal human-robot interaction to compose saliency-aware gaze heatmaps leveraged by a robot to learn emerging objects of interest. Our results show that our proposed method exceeds the capabilities of the current state of the art and outperforms it in terms of commonly used object detection metrics.</td>
                <td>Heating systems, Measurement, Training, Human-robot interaction, Object detection, Object recognition, Usability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342312&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342312&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Language Guided Temporally Adaptive Perception for Efficient Natural Language Grounding in Cluttered Dynamic Worlds</td>
                <td>S. Patki, J. Arkin, N. Raicevic and T. M. Howard</td>
                <td>2023</td>
                <td>As robots operate alongside humans in shared spaces, such as homes and offices, it is essential to have an effective mechanism for interacting with them. Natural language offers an intuitive interface for communicating with robots, but most of the recent approaches to grounded language understanding reason only in the context of an instantaneous state of the world. Though this allows for interpreting a variety of utterances in the current context of the world, these models fail to interpret utterances which require the knowledge of past dynamics of the world, thereby hindering effective human-robot collaboration in dynamic environments. Constructing a comprehensive model of the world that tracks the dynamics of all objects in the robot's workspace is computationally expensive and difficult to scale with increasingly complex environments. To address this challenge, we propose a learned model of language and perception that facilitates the construction of temporally compact models of dynamic worlds through closed-loop grounding and perception. Our experimental results on the task of grounding referring expressions demonstrate more accurate interpretation of robot instructions in cluttered and dynamic table-top environments without a significant increase in runtime as compared to an open-loop baseline.</td>
                <td>Adaptation models, Runtime, Grounding, Computational modeling, Natural languages, Collaboration, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341527&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341527&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>T-Top, an Open Source Tabletop Robot with Advanced Onboard Audio, Vision and Deep Learning Capabilities</td>
                <td>A. Maheux, A. M. Panchea, P. Warren, D. Létourneau and F. Michaud</td>
                <td>2023</td>
                <td>In recent years, studies on Socially Assistive Robots (SARs) examine how to improve the quality of life of people living with dementia and older adults (OAs) in general. However, most SARs have somewhat limited perception capabilities or interact using simple pre-programmed responses, providing limited or repetitive interaction modalities. Integrating more advanced perceptual capabilities with deep learning processing would help move beyond such limitations. This paper presents T-Top, a tabletop robot designed with advanced audio and vision processing using deep learning neural networks. T-Top is made available as an open source platform with the goal of providing an experimental SAR platform that can implement richer interaction modalities with OAs.</td>
                <td>Deep learning, Neural networks, Human-robot interaction, Assistive robots, Older adults, Intelligent robots, Dementia</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342252&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342252&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Human Motion Intention for pHRI Assistive Control</td>
                <td>P. Franceschi, F. Bertini, F. Braghin, L. Roveda, N. Pedrocchi and M. Beschi</td>
                <td>2023</td>
                <td>This work addresses human intention identification during physical Human-Robot Interaction (pHRI) tasks to include this information in an assistive controller. To this purpose, human intention is defined as the desired trajectory that the human wants to follow over a finite rolling prediction horizon so that the robot can assist in pursuing it. This work investigates a Recurrent Neural Network (RNN), specifically, Long-Short Term Memory (LSTM) cascaded with a Fully Connected layer. In particular, we propose an iterative training procedure to adapt the model. Such an iterative procedure is powerful in reducing the prediction error. Still, it has the drawback that it is time-consuming and does not generalize to different users or different co-manipulated objects. To overcome this issue, Transfer Learning (TL) adapts the pre-trained model to new trajectories, users, and co-manipulated objects by freezing the LSTM layer and fine-tuning the last FC layer, which makes the procedure faster. Experiments show that the iterative procedure adapts the model and reduces prediction error. Experiments also show that TL adapts to different users and to the co-manipulation of a large object. Finally, to check the utility of adopting the proposed method, we compare the proposed controller enhanced by the intention prediction with the other two standard controllers of pHRI.</td>
                <td>Training, Adaptation models, Recurrent neural networks, Computational modeling, Transfer learning, Predictive models, Trajectory, physical human-robot interaction, human intention prediction, LSTM, transfer learning, differential game theory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342014&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342014&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>VARIQuery: VAE Segment-Based Active Learning for Query Selection in Preference-Based Reinforcement Learning</td>
                <td>D. Marta, S. Holk, C. Pek, J. Tumova and I. Leite</td>
                <td>2023</td>
                <td>Human-in-the-loop reinforcement learning (RL) methods actively integrate human knowledge to create reward functions for various robotic tasks. Learning from preferences shows promise as alleviates the requirement of demonstrations by querying humans on state-action sequences. However, the limited granularity of sequence-based approaches complicates temporal credit assignment. The amount of human querying is contingent on query quality, as redundant queries result in excessive human involvement. This paper addresses the often-overlooked aspect of query selection, which is closely related to active learning (AL). We propose a novel query selection approach that leverages variational autoencoder (VAE) representations of state sequences. In this manner, we formulate queries that are diverse in nature while simultaneously taking into account reward model estimations. We compare our approach to the current state-of-the-art query selection methods in preference-based RL, and find ours to be either on-par or more sample efficient through extensive benchmarking on simulated environments relevant to robotics. Lastly, we conduct an online study to verify the effectiveness of our query selection approach with real human feedback and examine several metrics related to human effort.</td>
                <td>Measurement, Visualization, Estimation, Clustering algorithms, Reinforcement learning, Market research, Human in the loop</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341795&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341795&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Interactive Spatiotemporal Token Attention Network for Skeleton-Based General Interactive Action Recognition</td>
                <td>Y. Wen, Z. Tang, Y. Pang, B. Ding and M. Liu</td>
                <td>2023</td>
                <td>Recognizing interactive action plays an important role in human-robot interaction and collaboration. Previous methods use late fusion and co-attention mechanism to capture interactive relations, which have limited learning capability or inefficiency to adapt to more interacting entities. With assumption that priors of each entity are already known, they also lack evaluations on a more general setting addressing the diversity of subjects. To address these problems, we propose an Interactive Spatiotemporal Token Attention Network (ISTA-Net), which simultaneously model spatial, temporal, and interactive relations. Specifically, our network contains a tokenizer to partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to represent motions of multiple diverse entities. By extending the entity dimension, ISTs provide better interactive representations. To jointly learn along three dimensions in ISTs, multi-head self-attention blocks integrated with 3D convolutions are designed to capture inter-token correlations. When modeling correlations, a strict entity ordering is usually irrelevant for recognizing interactive actions. To this end, Entity Rearrangement is proposed to eliminate the orderliness in ISTs for interchangeable entities. Extensive experiments on four datasets verify the effectiveness of ISTA-Net by outperforming state-of-the-art methods. Our code is publicly available at https://github.com/Necolizer/ISTA-Net.</td>
                <td>Knowledge engineering, Convolutional codes, Correlation, Three-dimensional displays, Human-robot interaction, Collaboration, Benchmark testing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342472&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342472&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Joint Policies for Human-Robot Dialog and Co-Navigation</td>
                <td>Y. Hayamizu, Z. Yu and S. Zhang</td>
                <td>2023</td>
                <td>Service robots need language capabilities for communicating with people, and navigation skills for beyond-proximity interaction in the real world. When the robot explores the real world with people side by side, there is the compound problem of human-robot dialog and co-navigation. The human-robot team uses dialog to decide where to go, and their shared spatial awareness affects the dialog state. In this paper, we develop a framework that learns a joint policy for human-robot dialog and co-navigation toward efficiently and accurately completing tour guide and information delivery tasks. We show that our approach outperforms baselines from the literature in task completion rate and execution time, and demonstrate our approach in the real world.</td>
                <td>Service robots, Navigation, Compounds, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341663&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341663&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Initial Task Allocation for Multi-Human Multi-Robot Teams with Attention-Based Deep Reinforcement Learning</td>
                <td>C. Min</td>
                <td>2023</td>
                <td>Multi-human multi-robot teams have great potential for complex and large-scale tasks through the collaboration of humans and robots with diverse capabilities and expertise. To efficiently operate such highly heterogeneous teams and maximize team performance timely, sophisticated initial task allocation strategies that consider individual differences across team members and tasks are required. While existing works have shown promising results in reallocating tasks based on agent state and performance, the neglect of the inherent heterogeneity of the team hinders their effectiveness in realistic scenarios. In this paper, we present a novel formulation of the initial task allocation problem in multi-human multi-robot teams as a contextual multi-attribute decision-make process and propose an attention-based deep reinforcement learning approach. We introduce a cross-attribute attention module to encode the latent and complex dependencies of multiple attributes in the state representation. We conduct a case study in a massive threat surveillance scenario and demonstrate the strengths of our model.</td>
                <td>Deep learning, Representation learning, Adaptation models, Surveillance, Decision making, Collaboration, Reinforcement learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341410&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341410&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Human-Robot Collaboration for Unknown Flexible Surface Exploration and Treatment Based on Mesh Iterative Learning Control</td>
                <td>J. Xia et al.</td>
                <td>2023</td>
                <td>Contact tooling operations like sanding and polishing have been high in demand for robotics and automation, as manual operations are labour-intensive with inconsistent quality. However, automating these operations remains a challenge since they are highly dependent on prior knowledge about the geometry of the workpiece. While several methods have been developed in existing research to automate the geometry learning process and adjust the contact force, human supervision is heavily required in the calibration of workpieces and the path planning of robot motion in such methods. Furthermore, the stiffness identification of the workpiece is not considered in most of these methods. This paper presents a human-robot collaboration (HRC) framework, which is able to perform surface exploration on an unknown object combining the operator's flexibility with the control precision of the robot. The operator moves the robot along the surface of the target object, and the robot recognizes the surface geometry and surface stiffness while exerting a desired contact force through control. For this purpose, a mesh iterative learning control (MILC) is developed to learn the surface stiffness, plan the exploration path, and adjust contact force through repetitive online correction based on HRC. The proof of learning convergence and the results of the simulation and experiments performed using a 7-DOF Sawyer robot demonstrate the validity of the proposed controller.</td>
                <td>Geometry, Uncertainty, Target recognition, Force, Collaboration, Stability analysis, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341612&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341612&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Projecting Robot Intentions Through Visual Cues: Static vs. Dynamic Signaling</td>
                <td>S. Sonawani, Y. Zhou and H. B. Amor</td>
                <td>2023</td>
                <td>Augmented and mixed-reality techniques harbor a great potential for improving human-robot collaboration. Visual signals and cues may be projected to a human partner in order to explicitly communicate robot intentions and goals. However, it is unclear what type of signals support such a process and whether signals can be combined without adding additional cognitive stress to the partner. This paper focuses on identifying the effective types of visual signals and quantify their impact through empirical evaluations. In particular, the study compares static and dynamic visual signals within a collaborative object sorting task and assesses their ability to shape human behavior. Furthermore, an information-theoretic analysis is performed to numerically quantify the degree of information transfer between visual signals and human behavior. The results of a human subject experiment show that there are significant advantages to combining multiple visual signals within a single task, i.e., increased task efficiency and reduced cognitive load.</td>
                <td>Visualization, Shape, Collaboration, Virtual reality, Behavioral sciences, Task analysis, Stress</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342222&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342222&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robottheory Fitness: GoBot's Engagement Edge for Spurring Physical Activity in Young Children</td>
                <td>T. Warren, S. W. Logan and N. T. Fitter</td>
                <td>2023</td>
                <td>Children around the world are growing more sedentary over time, which leads to considerable accompanying wellness challenges. Pilot results from our research group have shown that robots may offer something different or better than other developmentally appropriate toys when it comes to motivating physical activity. However, the foundations of this work involved larger-group interactions in which it was difficult to tease apart potential causes of motion, or one-time sessions during which the impact of the robot may have been due to novelty. Accordingly, the work in this paper covers more controlled interactions focused on one robot and one child participant, in addition to considering interactions over longitudinal observation. We discuss the results of a deployment during which $N=8$ participants interacted with our custom GoBot robot over two months of weekly sessions. Within each session, the child users experienced a teleoperated robot mode, a semi-autonomous robot mode, and a control condition during which the robot was present but inactive. Results showed that children tended to be more active when the robot was active and the teleoperated mode did not yield significantly different results than the semi-autonomous mode. These insights can guide future application of assistive robots in child motor interventions, in addition to informing how these robots can be equipped to assist busy human clinicians.</td>
                <td>Toy manufacturing industry, Assistive robots, Robots, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341442&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341442&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Implicit Projection: Improving Team Situation Awareness for Tacit Human-Robot Interaction via Virtual Shadows</td>
                <td>A. Boateng, W. Zhang and Y. Zhang</td>
                <td>2023</td>
                <td>Fluent teaming is characterized by tacit interaction without explicit communication. Such interaction requires team situation awareness (TSA) to facilitate. However, existing approaches often rely on explicit communication (such as visual projection) to support TSA, resulting in a paradox. In this paper, we consider implicit projection (IP) to improve TSA for tacit human-robot interaction. IP minimizes interruption and can thus reduce the cognitive demand to maintain TSA in teaming. We introduce a novel process for achieving IP via virtual shadows (referred to as IPS). We compare our method with two baselines that use explicit projection to maintain TSA. Results via human factors studies demonstrate that IPS supports better TSA and significantly improves unsolicited human responsiveness to robots, a key feature of fluent teaming. Participants acknowledged robots implementing IPS more favorable as a teammate. Simultaneously, our results also demonstrate that IPS is comparable to, and sometimes better than, the best-performing baselines on information accuracy.</td>
                <td>Visualization, Human-robot interaction, Human factors, IP networks, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341641&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341641&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>User Interactions and Negative Examples to Improve the Learning of Semantic Rules in a Cognitive Exercise Scenario</td>
                <td>Hernández, A. Andriella, C. Torras and G. Alenyà</td>
                <td>2023</td>
                <td>Enabling a robot to perform new tasks is a complex endeavor, usually beyond the reach of non-technical users. For this reason, research efforts that aim at empowering end-users to teach robots new abilities using intuitive modes of interaction are valuable. In this article, we present INtuitive PROgramming 2 (INPRO2), a learning framework that allows inferring planning actions from demonstrations given by a human teacher. INPRO2 operates in an assistive scenario, in which the robot may learn from a healthcare professional (a therapist or caregiver) new cognitive exercises that can be later administered to patients with cognitive impairment. INPRO2 features significant improvements over previous work, namely: (1) exploitation of negative examples; (2) proactive interaction with the teacher to ask questions about the legality of certain movements; and (3) learning goals in addition to legal actions. Through simulations, we show the performance of different proactive strategies for gathering negative examples. Real-world experiments with human teachers and a TIAGo robot are also presented to qualitatively illustrate INPRO2.</td>
                <td>Industries, Technological innovation, Law, Semantics, Europe, Medical services, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341942&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341942&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Large Language Models as Zero-Shot Human Models for Human-Robot Interaction</td>
                <td>B. Zhang and H. Soh</td>
                <td>2023</td>
                <td>Human models play a crucial role in human-robot interaction (HRI), enabling robots to consider the impact of their actions on people and plan their behavior accordingly. However, crafting good human models is challenging; capturing context-dependent human behavior requires significant prior knowledge and/or large amounts of interaction data, both of which are difficult to obtain. In this work, we explore the potential of large language models (LLMs) — which have consumed vast amounts of human-generated text data — to act as zero-shot human models for HRI. Our experiments on three social datasets yield promising results; the LLMs are able to achieve performance comparable to purpose-built models. That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our findings, we demonstrate how LLM-based human models can be integrated into a social robot's planning process and applied in HRI scenarios focused on the important element of trust. Specifically, we present one case study on a simulated trust-based table-clearing task and replicate past results that relied on custom models. Next, we conduct a new robot utensil-passing experiment ($n=65$) where preliminary results show that planning with an LLM-based human model can achieve gains over a basic myopic plan. In summary, our results show that LLMs offer a promising (but incomplete) approach to human modeling for HRI.</td>
                <td>Sensitivity, Human-robot interaction, Robot sensing systems, Data models, Cognition, Planning, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341488&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341488&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MPC-Based Human-Accompanying Control Strategy for Improving the Motion Coordination Between the Target Person and the Robot</td>
                <td>J. Peng, Z. Liao, H. Yao, Z. Su, Y. Zeng and H. Dai</td>
                <td>2023</td>
                <td>Social robots have gained widespread attention for their potential to assist people in diverse domains, such as living assistance and logistics transportation. Human-accompanying, i.e., walking side-by-side with a person, is an expected and essential capability for social robots. However, due to the complexity of motion coordination between the target person and the mobile robot, the accompanying action is still unstable. In this study, we propose a human-accompanying control strategy to improve the motion coordination for better practicability of the human-accompanying robot. Our approach allows the robot to adapt to the motion variations of the target person and avoid obstacles while accompanying them. First, a human-robot interaction model based on the separation-bearing-orientation scheme is developed to ascertain the relative position and orientation between the robot and the target person. Then, a human-accompanying controller based on behavioral dynamics and model predictive control (MPC) is designed to avoid obstacles and simultaneously track the direction and velocity of the target person. Experimental results indicate that the proposed method can effectively achieve side-by-side accompanying by simultaneously controlling the relative position, direction, and velocity between the target person and robot.</td>
                <td>Adaptation models, Target tracking, Robot kinematics, Social robots, Transportation, Behavioral sciences, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342246&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342246&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Improved Inference of Human Intent by Combining Plan Recognition and Language Feedback</td>
                <td>I. Idrees et al.</td>
                <td>2023</td>
                <td>Conversational assistive robots can aid people, especially those with cognitive impairments, to accomplish various tasks such as cooking meals, performing exercises, or operating machines. However, to interact with people effectively, robots must recognize human plans and goals from noisy observations of human actions, even when the user acts sub-optimally. Previous works on Plan and Goal Recognition (PGR) as planning have used hierarchical task networks (HTN) to model the actor/human. However, these techniques are insufficient as they do not have user engagement via natural modes of interaction such as language. Moreover, they have no mechanisms to let users, especially those with cognitive impairments, know of a deviation from their original plan or about any sub-optimal actions taken towards their goal. We propose a novel framework for plan and goal recognition in partially observable domains—Dialogue for Goal Recognition (D4GR) enabling a robot to rectify its belief in human progress by asking clarification questions about noisy sensor data and sub-optimal human actions. We evaluate the performance of D4GR over two simulated domains—kitchen and blocks domain. With language feedback and the world state information in a hierarchical task model, we show that D4GR framework for the highest sensor noise performs 1% better than HTN in goal accuracy in both domains. For plan accuracy, D4GR outperforms by 4% in the kitchen domain and 2% in the blocks domain in comparison to HTN. The ALWAYS-ASK oracle outperforms our policy by 3% in goal recognition and 7% in plan recognition. D4GR does so by asking 68% fewer questions than an oracle baseline. We also demonstrate a real-world robot scenario in the kitchen domain, validating the improved plan and goal recognition of D4GR in a realistic setting.</td>
                <td>Uncertainty, Target tracking, Sociology, Social robots, Robot sensing systems, Planning, Noise measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342380&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342380&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Online Human Capability Estimation Through Reinforcement Learning and Interaction</td>
                <td>C. Sun, A. G. Cohn and M. Leonetti</td>
                <td>2023</td>
                <td>Service robots are expected to assist users in a constantly growing range of environments and tasks. People may be unique in many ways, and online adaptation of robots is central to personalized assistance. We focus on collaborative tasks in which the human collaborator may not be fully ablebodied, with the aim for the robot to automatically determine the best level of support. We propose a methodology for online adaptation based on Reinforcement Learning and Bayesian inference. As the Reinforcement Learning process continuously adjusts the robot's behavior, the actions that become part of the improved policy are used by the Bayesian inference module as local evidence of human capability, which can be generalized across the state space. The estimated capabilities are then used as pre-conditions to collaborative actions, so that the robot can quickly disable actions that the person seems unable to perform. We demonstrate and validate our approach on two simulated tasks and one real-world collaborative task across a range of motion and sensing capabilities.</td>
                <td>Q-learning, Service robots, Estimation, Collaboration, Robot sensing systems, Bayes methods, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341868&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341868&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Cognitive Approach to Hierarchical Task Selection for Human-Robot Interaction in Dynamic Environments</td>
                <td>Seifer and W. M. Qazi</td>
                <td>2023</td>
                <td>In an efficient and flexible human-robot collaborative work environment, a robot team member must be able to recognize both explicit requests and implied actions from human users. Identifying “what to do” in such cases requires an agent to have the ability to construct associations between objects, their actions, and the effect of actions on the environment. In this regard, semantic memory is being introduced to understand the explicit cues and their relationships with available objects and required skills to make “tea” and “sandwich”. We have extended our previous hierarchical robot control architecture to add the capability to execute the most appropriate task based on both feedback from the user and the environmental context. To validate this system, two types of skills were implemented in the hierarchical task tree: 1) Tea making skills and 2) Sandwich making skills. During the conversation between the robot and the human, the robot was able to determine the hidden context using ontology and began to act accordingly. For instance, if the person says “I am thirsty” or “It is cold outside” the robot will start to perform the tea-making skill. In contrast, if the person says, “I am hungry” or “I need something to eat”, the robot will make the sandwich. A humanoid robot Baxter was used for this experiment. We tested three scenarios with objects at different positions on the table for each skill. We observed that in all cases, the robot used only objects that were relevant to the skill.</td>
                <td>Semantics, Robot control, Memory management, Humanoid robots, Oral communication, Ontologies, Object recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341768&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341768&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Reward Shaping for Building Trustworthy Robots in Sequential Human-Robot Interaction</td>
                <td>Y. Guo, X. J. Yang and C. Shi</td>
                <td>2023</td>
                <td>Trust-aware human-robot interaction (HRI) has received increasing research attention, as trust has been shown to be a crucial factor for effective HRI. Research in trust-aware HRI discovered a dilemma - maximizing task rewards often leads to decreased human trust, while maximizing human trust would compromise task performance. In this work, we address this dilemma by formulating the HRI process as a two-player Markov game and utilizing the reward-shaping technique to improve human trust while limiting performance loss. Specifically, we show that when the shaping reward is potential-based, the performance loss can be bounded by the potential functions evaluated at the final states of the Markov game. We apply the proposed framework to the experience-based trust model, resulting in a linear program that can be efficiently solved and deployed in real-world applications. We evaluate the proposed framework in a simulation scenario where a human-robot team performs a search-and-rescue mission. The results demonstrate that the proposed framework successfully modifies the robot's optimal policy, enabling it to increase human trust at a minimal task performance cost.</td>
                <td>Sufficient conditions, Costs, Limiting, Buildings, Human-robot interaction, Games, Markov processes</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341904&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341904&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Latent Emission-Augmented Perspective-Taking (LEAPT) for Human-Robot Interaction</td>
                <td>K. Chen, J. Y. Lim, K. Kuan and H. Soh</td>
                <td>2023</td>
                <td>Perspective-taking is the ability to perceive or understand a situation or concept from another individual's point of view, and is crucial in daily human interactions. Enabling robots to perform perspective-taking remains an unsolved problem; existing approaches that use deterministic or handcrafted methods are unable to accurately account for uncertainty in partially-observable settings. This work proposes to address this limitation via a deep world model that enables a robot to perform both perception and conceptual perspective taking, i.e., the robot is able to infer what a human sees and believes. The key innovation is a decomposed multi-modal latent state space model able to generate and augment fictitious observations/emissions. Optimizing the ELBO that arises from this probabilistic graphical model enables the learning of uncertainty in latent space, which facilitates uncertainty estimation from high-dimensional observations. We tasked our model to predict human observations and beliefs on three partially-observable HRI tasks. Experiments show that our method significantly outperforms existing baselines and is able to infer visual observations available to other agent and their internal beliefs.</td>
                <td>Visualization, Technological innovation, Uncertainty, Human-robot interaction, Training data, State-space methods, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342126&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342126&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Attentional Recurrent Neural Network for Occlusion-Aware Proactive Anomaly Detection in Field Robot Navigation</td>
                <td>Campbell</td>
                <td>2023</td>
                <td>The use of mobile robots in unstructured environments like the agricultural field is becoming increasingly common. The ability for such field robots to proactively identify and avoid failures is thus crucial for ensuring efficiency and avoiding damage. However, the cluttered field environment introduces various sources of noise (such as sensor occlusions) that make proactive anomaly detection difficult. Existing approaches can show poor performance in sensor occlusion scenarios as they typically do not explicitly model occlusions and only leverage current sensory inputs. In this work, we present an attention-based recurrent neural network architecture for proactive anomaly detection that fuses current sensory inputs and planned control actions with a latent representation of prior robot state. We enhance our model with an explicitly-learned model of sensor occlusion that is used to modulate the use of our latent representation of prior robot state. Our method shows improved anomaly detection performance and enables mobile field robots to display increased resilience to predicting false positives regarding navigation failure during periods of sensor occlusion, particularly in cases where all sensors are briefly occluded. Our code is available at: https://github.com/andreschreiber/roar.</td>
                <td>Recurrent neural networks, Navigation, Fuses, Supervised learning, Robot sensing systems, Robustness, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341852&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341852&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Collaborative Trolley Transportation System with Autonomous Nonholonomic Robots</td>
                <td>B. Xia et al.</td>
                <td>2023</td>
                <td>Cooperative object transportation using multiple robots has been intensively studied in the control and robotics literature, but most approaches are either only applicable to omnidirectional robots or lack a complete navigation and decision-making framework that operates in real time. This paper presents an autonomous nonholonomic multi-robot system and an end-to-end hierarchical autonomy framework for collaborative luggage trolley transportation. This framework finds kinematic-feasible paths, computes online motion plans, and provides feedback that enables the multi-robot system to handle long lines of luggage trolleys and navigate obstacles and pedestrians while dealing with multiple inherently complex and coupled constraints. We demonstrate the designed collaborative trolley transportation system through practical transportation tasks, and the experiment results reveal their effectiveness and reliability in complex and dynamic environments. (Video11Video demonstration: https://youtu.be/efnPERm0Rco.)</td>
                <td>Pedestrians, Navigation, Decision making, Transportation, Collaboration, Reliability engineering, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341508&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341508&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Deconfounded Opponent Intention Inference for Football Multi-Player Policy Learning</td>
                <td>S. Wang, Y. Pan, Z. Pu, B. Liu and J. Yi</td>
                <td>2023</td>
                <td>Due to the high complexity of a football match, the opponents' strategies are variable and unknown. Thus predicting the opponents' future intentions accurately based on current situation is crucial for football players' decision-making. To better anticipate the opponents and learn more effective strategies, a deconfounded opponent intention inference (DOII) method for football multi-player policy learning is proposed in this paper. Specifically, opponents' intentions are inferred by an opponent intention supervising module. Furthermore, for some confounders which affect the causal relationship among the players and the opponents, a decon-founded trajectory graph module is designed to mitigate the influence of these confounders and increase the accuracy of the inferences about opponents' intentions. Besides, an opponent-based incentive module is designed to improve the players' sensitivity to the opponents' intentions and further to train reasonable players' strategies. Representative results indicate that DOII can effectively improve the performance of players' strategies in the Google Research Football environment, which validates the superiority of the proposed method.</td>
                <td>Sensitivity, Simulation, Decision making, Trajectory, Internet, Complexity theory, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341469&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341469&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Stroke-Based Rendering and Planning for Robotic Performance of Artistic Drawing</td>
                <td>I. Ilinkin, D. Song and Y. J. Kim</td>
                <td>2023</td>
                <td>We present a new robotic drawing system based on stroke-based rendering (SBR). Our motivation is the artistic quality of the whole performance. Not only should the generated strokes in the final drawing resemble the input image, but the stroke sequence should also exhibit a human artist's planning process. Thus, when a robot executes the drawing task, both the drawing results and the way the robot executes would look artistic. Our SBR system is based on image segmentation and depth estimation. It generates the drawing strokes in an order that allows for the intended shape to be perceived quickly and for its detailed features to be filled in and emerge gradually when observed by the human. This ordering represents a stroke plan that the drawing robot should follow to create an artistic rendering of images. We experimentally demonstrate that our SBR-based drawing makes visually pleasing artistic images, and our robotic system can replicate the result with proper sequences of stroke drawing.</td>
                <td>Image segmentation, Shape, Estimation, Rendering (computer graphics), Planning, Task analysis, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341808&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341808&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Heterogeneous Robot-Assisted Services in Isolation Wards: A System Development and Usability Study</td>
                <td>Y. Kwon et al.</td>
                <td>2023</td>
                <td>Isolation wards operate in quarantine rooms to prevent cross-contamination caused by infectious diseases. Behind the benefits, medical personnel can have the infection risk from patients and the heavy workload due to the isolation. This work proposes a robot-assisted system to alleviate these problems in isolation wards. We conducted a survey about the medical staff's difficulties and envisioning robots. Using the investigation result, we devised three valuable services using two kinds of heterogeneous robots: telemedicine, emergency alert, and delivery services by care robots and delivery robots. Our system also provides user-interactive components such as a dashboard for medical staff and a patient app for inpatients. To manage the services efficiently, we suggest the robotic system based on a central control server and a hierarchical management architecture. Through a user study, we reviewed the usability of the developed system and its future directions.</td>
                <td>Surveys, Infectious diseases, Telemedicine, Personnel, Servers, Usability, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341857&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341857&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Irregular Change Detection in Sparse Bi-Temporal Point Clouds Using Learned Place Recognition Descriptors and Point-to-Voxel Comparison</td>
                <td>N. Stathoulopoulos, A. Koval and G. Nikolakopoulos</td>
                <td>2023</td>
                <td>Change detection and irregular object extraction in 3D point clouds is a challenging task that is of high importance not only for autonomous navigation but also for updating existing digital twin models of various industrial environments. This article proposes an innovative approach for change detection in 3D point clouds using deep learned place recognition descriptors and irregular object extraction based on voxel-to-point comparison. The proposed method first aligns the bi-temporal point clouds using a map-merging algorithm in order to establish a common coordinate frame. Then, it utilizes deep learning techniques to extract robust and discriminative features from the 3D point cloud scans, which are used to detect changes between consecutive point cloud frames and therefore find the changed areas. Finally, the altered areas are sampled and compared between the two time instances to extract any obstructions that caused the area to change. The proposed method was successfully evaluated in real-world field experiments, where it was able to detect different types of changes in 3D point clouds, such as object or muck-pile addition and displacement, showcasing the effectiveness of the approach. The results of this study demonstrate important implications for various applications, including safety and security monitoring in construction sites, mapping and exploration and suggests potential future research directions in this field.</td>
                <td>Point cloud compression, Deep learning, Solid modeling, Three-dimensional displays, Feature extraction, Real-time systems, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342248&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342248&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Magnetically Controlled Cell Robots with Immune-Enhancing Potential</td>
                <td>H. Sun et al.</td>
                <td>2023</td>
                <td>Magnetic microrobots exhibit enormous potential in targeted drug delivery owing to the remote wireless manipulation and minimum invasion for medical treatment. High degree of freedom offers the magnetic propelled robots extraordinary application prospect since they can be controlled precisely when different magnetic fields sources working cooperatively. However, the biocompatibility of microrobots have attracted sustained and general concern. Therefore, it is highly necessary to develop a promising carrier with high biocompatibility and investigate the mechanism of drug loading-release triggered by special microenvironment in the targeted region. In this paper, we proposed a magnetically controlled cell robots (MCRs) based on macrophages propelled by a rotating magnetic field. The innovative MCRs exhibit good biocompatibility and low toxicity by optimizing the concentration of polylysine-coated Fe nanoparticles (PLL@FeNPs) to 40 µg/mL. These MCRs loaded with murine interleukin-12 (IL-12), murine chemokine (C-C motif) ligand 5 (CCL-5), and murine C-X-C motif chemokine ligand 10 (CXCL-10) which can stimulate T cell differentiation and recruitment of monocytes, respectively. The macrophages showed an obvious M1-polarization tendency of macrophages to phagocytose intracellular pathogens and resist the growth of tumor cells. Under the control of a magnetic propelling system composed of 3 pairs of Helmholtz coil, the cell robot can be propelled wirelessly and moved along a predefined path with high accuracy. Moreover, the MCRs could approach to cancer cells and stop at places of interest in vitro. In conclusion, we have accomplished the preliminary construction of a targeted drug delivery system which displays great immune-enhancing potential for targeted drug delivery.</td>
                <td>Drugs, Wireless communication, Targeted drug delivery, Toxicology, Propulsion, Magnetic fields, In vitro</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341753&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341753&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Tightly-Coupled Visual-DVL Fusion For Accurate Localization of Underwater Robots</td>
                <td>Y. Huang et al.</td>
                <td>2023</td>
                <td>This paper proposes a tightly-coupled visual-Doppler-Velocity-Log (visual-DVL) fusion method for underwater robot localization through integrating the velocity measurements from a DVL into a visual odometry (VO). Considering that employing the DVL measurements in dead-reckoning systems easily leads to error accumulation and suboptimal results in previous works, we directly integrate them into the visual tracking process. Specifically, the velocity measurements are utilized to improve the initial estimation of camera pose during visual tracking, aiming to provide a better initial value for pose optimization. Thereafter, these velocity measurements are also directly employed to constrain the position change of the camera between two adjacent frames by constructing a novel DVL error term, which is optimized jointly with the visual constrains to obtain a more accurate camera pose. Various experiments are carried out in the datasets collected from several scenarios of the underwater simulation environment HoloOcean, and the results illustrate that the proposed fusion method can effectively improve the localization accuracy for underwater robots by about 20% compared to pure visual odometry. The proposed method provides valuable guidance for the accurate localization of underwater robots.</td>
                <td>Location awareness, Autonomous underwater vehicles, Visualization, Robot vision systems, Measurement uncertainty, Cameras, Velocity measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342197&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342197&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Fully Proprioceptive Slip-Velocity-Aware State Estimation for Mobile Robots via Invariant Kalman Filtering and Disturbance Observer</td>
                <td>X. Yu et al.</td>
                <td>2023</td>
                <td>This paper develops a novel slip estimator using the invariant observer design theory and Disturbance Observer (DOB). The proposed state estimator for mobile robots is fully proprioceptive and combines data from an inertial measurement unit and body velocity within a Right Invariant Extended Kalman Filter (RI-EKF). By embedding the slip velocity into SE3 (3) matrix Lie group, the developed DOB-based RI-EKF provides real-time velocity and slip velocity estimates on different terrains. Experimental results using a Husky wheeled robot confirm the mathematical derivations and effectiveness of the proposed method in estimating the observable state variables. Open-source software is available for download and reproducing the presented results.</td>
                <td>Measurement units, Propioception, Real-time systems, Disturbance observers, Mobile robots, Kalman filters, Standards</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342519&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342519&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Predicting Energy Consumption and Traversal Time of Ground Robots for Outdoor Navigation on Multiple Types of Terrain</td>
                <td>Wagner</td>
                <td>2023</td>
                <td>The outdoor navigation capabilities of ground robots have improved significantly in recent years, opening up new potential applications in a variety of settings. Cost-based representations of the environment are frequently used in the path planning domain to obtain an optimized path based on various objectives, such as traversal time or energy consumption. However, obtaining such cost representations is still cumbersome, particularly in outdoor settings with diverse terrain types and slope angles. In this paper, we address this problem by using a data-driven approach to develop a cost representation for various outdoor terrain types that supports two optimization objectives, namely energy consumption and traversal time. We train a supervised machine learning model whose inputs consists of extracted environment data along a path and whose outputs are the predicted energy consumption and traversal time. The model is based on a ResNet neural network architecture and trained using field-recorded data. The error of the proposed method on different types of terrain is within 11% of the ground truth data. To show that it performs and generalizes better than currently existing approaches on various types of terrain, a comparison to a baseline method is made.</td>
                <td>Energy consumption, Adaptation models, Visualization, Costs, Navigation, Machine learning, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341716&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341716&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Informative Path Planning for Scalar Dynamic Reconstruction Using Coregionalized Gaussian Processes and a Spatiotemporal Kernel</td>
                <td>L. Booth and S. Carpin</td>
                <td>2023</td>
                <td>The proliferation of unmanned vehicles offers many opportunities for solving environmental sampling tasks with applications in resource monitoring and precision agriculture. Informative path planning (IPP) includes a family of methods which offer improvements over traditional surveying techniques for suggesting locations for observation collection. In this work, we present a novel solution to the IPP problem by using a coregionalized Gaussian processes to estimate a dynamic scalar field that varies in space and time. Our method improves previous approaches by using a composite kernel accounting for spatiotemporal correlations and at the same time, can be readily incorporated in existing IPP algorithms. Through extensive simulations, we show that our novel modeling approach leads to more accurate estimations when compared with formerly proposed methods that do not account for the temporal dimension.</td>
                <td>Gaussian processes, Robot sensing systems, Path planning, Spatiotemporal phenomena, Planning, Multi-robot systems, Kernel</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341858&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341858&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Probabilistic Guarantees for Nonlinear Safety-Critical Optimal Control</td>
                <td>P. Akella, W. Ubellacker and A. D. Ames</td>
                <td>2023</td>
                <td>Leveraging recent developments in black-box risk-aware verification, we provide three algorithms that generate probabilistic guarantees on (1) optimality of solutions, (2) recursive feasibility, and (3) maximum controller runtimes for general nonlinear safety-critical finite-time optimal controllers. These methods forego the usual (perhaps) restrictive assumptions required for typical theoretical guarantees, e.g. terminal set calculation for recursive feasibility in Nonlinear Model Predictive Control, or convexification of optimal controllers to ensure optimality. Furthermore, we show that these methods can directly be applied to hardware systems to generate controller guarantees on their respective systems.</td>
                <td>Runtime, Optimal control, Closed box, Probabilistic logic, Prediction algorithms, Hardware, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342474&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342474&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Non-Gaussian Uncertainty Minimization Based Control of Stochastic Nonlinear Robotic Systems</td>
                <td>W. Han, A. Jasour and B. Williams</td>
                <td>2023</td>
                <td>In this paper, we consider the closed-loop control problem of nonlinear robotic systems in the presence of probabilistic uncertainties and disturbances. More precisely, we design a state feedback controller that minimizes deviations of the states of the system from the nominal state trajectories due to uncertainties and disturbances. Existing approaches to address the control problem of probabilistic systems are limited to particular classes of uncertainties and systems such as Gaussian uncertainties and processes and linearized systems. We present an approach that deals with nonlinear dynamics models and arbitrary known probabilistic uncertainties. We formulate the controller design problem as an optimization problem in terms of statistics of the probability distributions including moments and characteristic functions. In particular, in the provided optimization problem, we use moments and characteristic functions to propagate uncertainties throughout the nonlinear motion model of robotic systems. In order to reduce the tracking deviations, we minimize the uncertainty of the probabilistic states around the nominal trajectory by minimizing the trace and the determinant of the covariance matrix of the probabilistic states. To obtain the state feedback gains, we solve deterministic optimization problems in terms of moments, characteristic functions, and state feedback gains using off-the-shelf interior-point optimization solvers. To illustrate the performance of the proposed method, we compare our method with existing probabilistic control methods.</td>
                <td>State feedback, Uncertainty, Tracking, Stochastic processes, Process control, Probabilistic logic, Probability distribution</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342065&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342065&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Compliant Stiffness by Impedance Control-Aware Task Segmentation and Multi-Objective Bayesian Optimization with Priors</td>
                <td>M. Okada, M. Komatsu, R. Okumura and T. Taniguchi</td>
                <td>2023</td>
                <td>Rather than traditional position control, impedance control is preferred to ensure the safe operation of industrial robots programmed from demonstrations. However, variable stiffness learning studies have focused on task performance rather than safety (or compliance). Thus, this paper proposes a novel stiffness learning method to satisfy both task performance and compliance requirements. The proposed method optimizes the task and compliance objectives ($T/C$ objectives) simultaneously via multi-objective Bayesian optimization. We define the stiffness search space by segmenting a demonstration into task phases, each with constant responsible stiffness. The segmentation is performed by identifying impedance control-aware switching linear dynamics (IC-SLD) from the demonstration. We also utilize the stiffness obtained by proposed IC-SLD as priors for efficient optimization. Experiments on simulated tasks and a real robot demonstrate that IC-SLD-based segmentation and the use of priors improve the optimization efficiency compared to existing baseline methods.</td>
                <td>Learning systems, Position control, Switches, Aerospace electronics, Control systems, Bayes methods, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342096&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342096&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Exploration Using Extra Safety Budget in Constrained Policy Optimization</td>
                <td>H. Xu et al.</td>
                <td>2023</td>
                <td>Reinforcement learning (RL) has achieved promising results on most robotic control tasks. Safety of learning-based controllers is an essential notion of ensuring the effectiveness of the controllers. Current methods adopt whole consistency constraints during the training, thus resulting in inefficient exploration in the early stage. In this paper, we propose an algorithm named Constrained Policy Optimization with Extra Safety Budget (ESB-CPO) to strike a balance between the exploration efficiency and the constraints satis-faction. In the early stage, our method loosens the practical constraints of unsafe transitions (adding extra safety bud-get) with the aid of a new metric we propose. With the training process, the constraints in our optimization problem become tighter. Meanwhile, theoretical analysis and practical experiments demonstrate that our method gradually meets the cost limit's demand in the final training stage. When evaluated on Safety-Gym and Bullet-Safety-Gym benchmarks, our method has shown its advantages over baseline algorithms in terms of safety and optimality. Remarkably, our method gains remarkable performance improvement under the same cost limit compared with baselines.</td>
                <td>Training, Measurement, Costs, Estimation, Reinforcement learning, Stability analysis, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342149&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342149&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Quadratic Dynamic Matrix Control for Fast Cloth Manipulation</td>
                <td>Martinez and C. Torras</td>
                <td>2023</td>
                <td>Robotic cloth manipulation is an increasingly relevant area of research, challenging classic control algorithms due to the deformable nature of cloth. While it is possible to apply linear model predictive control to make the robot move the cloth according to a given reference, this approach suffers from a large dimensionality of the state-space representation of the cloth models. To address this issue, in this work we study the application of an input-output model predictive control strategy, based on quadratic dynamic matrix control, to robotic cloth manipulation. To account for uncertain disturbances on the cloth's motion, we further extend the algorithm with suitable chance constraints. In extensive simulated experiments, involving disturbances and obstacle avoidance, we show that quadratic dynamic matrix control can be successfully applied in different cloth manipulation scenarios, with significant gains in optimization speed compared to standard model predictive control strategies. The experiments further demonstrate that the closed-loop model used by quadratic dynamic matrix control can be beneficial to the tracking accuracy, leading to improvements over the standard predictive control strategy. Moreover, a preliminary experiment on a real robot shows that quadratic dynamic matrix control can indeed be employed in real settings.</td>
                <td>Heuristic algorithms, Dynamics, Stochastic processes, Predictive models, Prediction algorithms, Solids, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341850&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341850&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Gaussian Process Model for Opponent Prediction in Autonomous Racing</td>
                <td>E. L. Zhu, F. L. Busch, J. Johnson and F. Borrelli</td>
                <td>2023</td>
                <td>In head-to-head racing, performing tightly con-strained, but highly rewarding maneuvers, such as overtaking, require an accurate model of interactive behavior of the opposing target vehicle (TV). We propose to construct a prediction model given data of the TV from previous races. In particular, a one-step Gaussian process (GP) model is trained on closed-loop interaction data to learn the behavior of a TV driven by an unknown policy. Predictions of the nominal trajectory and associated uncertainty are rolled out via a sampling-based approach and are used in a model predictive control (MPC) policy for the ego vehicle in order to intelligently trade-off between safety and performance when racing against a TV. In a Monte Carlo study, we compare the GP-based predictor in closed-loop with the MPC policy against several predictors from literature and observe that the GP-based predictor achieves similar win rates while maintaining safety in up to 3x more races. Through experiments, we demonstrate the approach in real-time on a 1/10th scale racecar platform operating at speeds of around 2.8 m/s, and show a significant level of improvement when using the GP-based predictor over a baseline MPC predictor. Videos of the experiments can be found at https://voutu.be/KMSs4ofDfIs.</td>
                <td>TV, Uncertainty, Gaussian processes, Predictive models, Data models, Real-time systems, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341566&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341566&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Optimal Energy Tank Initialization for Minimum Sensitivity to Model Uncertainties</td>
                <td>A. Pupa, P. R. Giordano and C. Secchi</td>
                <td>2023</td>
                <td>Energy tanks have gained popularity inside the robotics and control communities over the last years, since they represent a formidable tool to enforce passivity (and, thus, input/output stability) of a controlled robot, possibly interacting with uncertain environments. One weak point of passification strategies based on energy tanks concerns, however, their initialization. Indeed, a too large initial energy can cause practical unstable behaviors, while a too low initial energy level can prevent the correct execution of the task. This shortcoming becomes even more relevant in presence of uncertainties in the robot model and/or environment, since it may be hard to predict in advance the correct (safe) amount of initial tank energy for a successful task execution. In this paper we then propose a new strategy for addressing this issue. The recent notion of closed-loop state sensitivity is exploited to derive precise bounds (tubes) on the tank energy behavior by assuming parametric uncertainty in the robot model. These tubes are then exploited in a novel nonlinear optimization problem aiming at finding both the best trajectory and the minimal initial tank energy that allow executing a positioning task for any value of the uncertain parameters in a given range. The approach is finally validated via a statistical analysis in simulation and experiments on real robot hardware.</td>
                <td>Uncertainty, Sensitivity, Statistical analysis, Predictive models, Robot sensing systems, Stability analysis, Electron tubes</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341568&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341568&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Data-Driven Approach to Synthesizing Dynamics-Aware Trajectories for Underactuated Robotic Systems</td>
                <td>A. Srikanthan, F. Yang, I. Spasojevic, D. Thakur, V. Kumar and N. Matni</td>
                <td>2023</td>
                <td>We consider joint trajectory generation and tracking control for under-actuated robotic systems. A common solution is to use a layered control architecture, where the top layer uses a simplified model of system dynamics for trajectory generation, and the low layer ensures approximate tracking of this trajectory via feedback control. While such layered control architectures are standard and work well in practice, selecting the simplified model used for trajectory generation typically relies on engineering intuition and experience. In this paper, we propose an alternative data-driven approach to dynamicsaware trajectory generation. We show that a suitable augmented Lagrangian reformulation of a global nonlinear optimal control problem results in a layered decomposition of the overall problem into trajectory planning and feedback control layers. Crucially, the resulting trajectory optimization is dynamicsaware, in that, it is modified with a tracking penalty regularizer encoding the dynamic feasibility of the generated trajectory. We show that this tracking penalty regularizer can be learned from system rollouts for independently-designed low layer feedback control policies, and instantiate our framework in the context of a unicycle and a quadrotor control problem in simulation. Further, we show that our approach handles the sim-to-real gap through experiments on the quadrotor hardware platform without any additional training. For both the synthetic unicycle example and the quadrotor system, our framework shows significant improvements in both computation time and dynamic feasibility in simulation and hardware experiments.</td>
                <td>Training, Costs, Systematics, Trajectory planning, Computer architecture, Hardware, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341651&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341651&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Time-Optimal Control via Heaviside Step-Function Approximation</td>
                <td>C. Pham</td>
                <td>2023</td>
                <td>Least-squares programming is a popular tool in robotics due to its simplicity and availability of open-source solvers. However, certain problems like sparse programming in the $\ell_{0}$- or $\ell_{0}-\mathbf{norm}$ for time-optimal control are not equivalently solvable. In this work, we propose a non-linear hierarchical least-squares programming (NL-HLSP) for time-optimal control of non-linear discrete dynamic systems. We use a continuous approximation of the heaviside step function with an additional term that avoids vanishing gradients. We use a simple discretization method by keeping states and controls piece-wise constant between discretization steps. This way, we obtain a comparatively easily implementable NL-HLSP in contrast to direct transcription approaches of optimal control. We show that the NL-HLSP indeed recovers the discrete time-optimal control in the limit for resting goal points. We confirm the results in simulation for linear and non-linear control scenarios.</td>
                <td>Bang-bang control, Humanoid robots, Programming, Behavioral sciences, Motion control, Dynamical systems, Computational complexity</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342255&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342255&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bi-Component Silicone 3D Printing with Dynamic Mix Ratio Modification for Soft Robotic Actuators</td>
                <td>E. Nicolae, T. Batigne, C. Duhart and M. Serrano</td>
                <td>2023</td>
                <td>Pneumatically operated soft actuators are increasingly researched due to their fabrication simplicity, actuation capabilities, and low production cost. Depending on the Soft Pneumatic Actuator (SPA) objective, its design can be modified to reach new bending angles or increase its actuation strength. However, increasing the abilities of Soft Pneumatic Actuators (SPAs) requires increasing the complexity of their air cavities or using multiple materials with different mechanical stiffness. Both solutions complexify the fabrication of SPAs, reducing their primary benefits of manufacturing simplicity and low production cost. This paper presents a novel additive manufacturing fabrication process incorporating multiple mechanical stiffnesses using a single bicomponent soft material. This process aims to integrate multiple bending angles with multi-channel SPAs without increasing their manufacturing complexity. Our process uses a dynamic modification of the bi-component silicone mix ratio to generate the desired mechanical properties of the material. Modifying the mix ratio allows us to control the material's cure time and mechanical properties, such as its final stiffness. We found that using a single 30 shore-A bi-component silicone, we could achieve several stiffness values with different reticulation times and levels of stickiness. Using these shore ranges and our fabrication process, we built several SPAs. We explored how the printing orientation of the SPAs modifies its bending actuation using our fabrication process to illustrate the capabilities of our approach.</td>
                <td>Fabrication, Pneumatic actuators, Costs, Deformation, Production, Soft robotics, Bending</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342125&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342125&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Two-stage Train Components Defect Detection Based on Prior Knowledge</td>
                <td>G. Peng, Z. Li, S. Wan and Z. Deng</td>
                <td>2023</td>
                <td>The existing method of detecting defects in train components, which relies on visual identification, requires extensive involvement from inspectors and presents certain limitations. In this study, a two-stage defect detection based on prior knowledge was developed, which first detects the types and positions of components, and then conducts targeted detection of possible existing defect types. The algorithm introduces the prior knowledge of the relative spatial position relationship of components and optimizes the detection of sub-components by cascaded convolutional neural networks and local scale-up. In this study, three methods were used, including deep learning, template matching, and quantitative evaluation based on prior knowledge, to perform targeted detection of defect types that may occur in components. Experiments have verified the adaptability and accuracy of the method, demonstrating its high value for engineering applications.</td>
                <td>Knowledge engineering, Deep learning, Rails, Visualization, Costs, Inspection, Rail transportation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341915&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341915&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Complete Coverage Path Planning for Omnidirectional Expand and Collapse Robot Panthera</td>
                <td>L. Yi, A. W. Y. Sang, A. A. Hayat, Q. Tang, A. V. Le and M. R. Elara</td>
                <td>2023</td>
                <td>Autonomous mobile robots (AMRs) face challenges in efficiently covering complex environments. To navigate narrow and expansive areas, AMRs must have two essential attributes: compact size for confined spaces and larger size with omnidirectional locomotion for broader spaces. This study utilizes omnidirectional expand and collapse robots (OECRs) to demonstrate efficient area coverage. OECRs can collapse to navigate through confined spaces and expand for efficient coverage in broad spaces. However, current complete coverage path planning (CCPP) methods do not account for the expanded and collapsed states of OECRs. To address this, a depth-first search (DFS) approach is proposed for OECRs' CCPP, which can adjust the robotic footprint along the CCPP path to reduce path length. The proposed DFS outperforms the state-of-the-art CCPP in terms of increased area coverage and reduced distance traveled on a selected map.</td>
                <td>Navigation, Path planning, Mobile robots, Intelligent robots, Faces</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342525&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342525&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Monte-Carlo Tree Search with Prioritized Node Expansion for Multi-Goal Task Planning</td>
                <td>C. Pham</td>
                <td>2023</td>
                <td>Symbolic task planning for robots is computationally challenging due to the combinatorial complexity of the possible action space. This fact is amplified if there are several sub-goals to be achieved due to the increased length of the action sequences. In this work, we propose a multi-goal symbolic task planner for deterministic decision processes based on Monte Carlo Tree Search. We augment the algorithm by prioritized node expansion which prioritizes nodes that already have fulfilled some sub-goals. Due to its linear complexity in the number of sub-goals, our algorithm is able to identify symbolic action sequences of 145 elements to reach the desired goal state with up to 48 sub-goals while the search tree is limited to under 6500 nodes. We use action reduction based on a kinematic reachability criterion to further ease computational complexity. We combine our algorithm with object localization and motion planning and apply it to a real-robot demonstration with two manipulators in an industrial bearing inspection setting.</td>
                <td>Location awareness, Monte Carlo methods, Machine learning, Kinematics, Inspection, Manipulators, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342430&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342430&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning</td>
                <td>M. Atad, J. Feng, I. Rodríguez, M. Durner and R. Triebel</td>
                <td>2023</td>
                <td>Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. With GRACE, we are able to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a dual-armed robotic system. We further demonstrate that our method is capable of detecting infeasible assemblies, substantially alleviating the undesirable impacts from false predictions, and hence facilitating real-world deployment soon. Code and training data are available at https://github.com/DLR-RM/GRACE.</td>
                <td>Robotic assembly, Representation learning, Three-dimensional displays, Aluminum, Training data, Solids, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342352&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342352&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Statistical Characterization of Position-Dependent Behavior Using Frequency-Aware B-Spline</td>
                <td>Rawashdeh, M. Heertjes and M. Al Janaideh</td>
                <td>2023</td>
                <td>Stretching the definition of the standard Sine profile allows building a generalized symmetric frequency-aware basis function that can be used to generate reference motion trajectories. Other profiles such as polynomials, sigmoid, and harmonic-based models can be equally used under the proposed technique. Despite being suitable at the level of any higher-order time derivative, in this study, the generic basis function is realized at the jerk level such that the generated signals adhere to the limitations of the driven motion system. Introducing suitable time shifts, replicas of basis functions can be obtained giving rise to B-spline like frequency-aware profiles that can be used to realize the actual motion under any desired kinematical constraints, which are neatly written to reduces the computation burden at the motion controller side. Utilizing mainly the frequency-aware B-spline profiles, frequency-dependent random walk motion is presented and used to collect information about the driven motion system to help in characterizing any position-dependent errors through the statistical means, i.e. Analysis of Variance, and Design of Experiments. This allows dividing the working space in which motion takes place into several spatial regions with preferred frequency contents. The effectiveness of these proposed profiles is shown through hardware experiments using a precision motion system.</td>
                <td>Time-frequency analysis, Switches, Frequency conversion, Hardware, Behavioral sciences, Trajectory, Splines (mathematics)</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341845&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341845&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Flexible Gear Assembly with Visual Servoing and Force Feedback</td>
                <td>J. Ming, D. Bargmann, H. Cao and M. Caccamo</td>
                <td>2023</td>
                <td>This paper presents a vision-guided two-stage approach with force feedback to achieve high-precision and flexible gear assembly. The proposed approach integrates YOLO to coarsely localize the target workpiece in a searching phase and deep reinforcement learning (DRL) to complete the insertion. Specifically, DRL addresses the challenge of partial visibility when the on-wrist camera is too close to the workpiece of a small size. Moreover, we use force feedback to improve the robustness of the vision-guided assembly process. To reduce the effort of collecting training data on real robots, we use synthetic RGB images for training YOLO and construct an offline interaction environment leveraging sampled real-world data for training DRL agents. The proposed approach was evaluated in an industrial gear assembly experiment, which requires an assembly clearance of 0.3 mm, demonstrating high robustness and efficiency in gear searching and insertion from arbitrary positions.</td>
                <td>YOLO, Training, Deep learning, Three-dimensional displays, Gears, Force feedback, Reinforcement learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341833&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341833&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robotic Powder Grinding with Audio-Visual Feedback for Laboratory Automation in Materials Science</td>
                <td>Y. Nakajima et al.</td>
                <td>2023</td>
                <td>This study focuses on the powder grinding process, which is a necessary step for material synthesis in materials science experiments. In material science, powder grinding is a time-consuming process that is typically executed by hand, as commercial grinding machines are unsuitable for samples of small size. Robotic powder grinding would solve this problem, but it is a challenging task for robots, as it requires observing the powder state and generating appropriate motions. Our previous study proposed a robotic powder grinding system using visual feedback. Although visual feedback is helpful for observing the powder distribution, the particle size during the grinding process remains invisible, leading to suboptimal robot actions. In some cases, the robot chose to gather the powder even though continuing to grind instead would have produced finer powder. In this paper, we present a multi-modal robotic grinding system that utilizes both audio and visual feedback. It makes use of the grinding sound which carries information about the grinding progress, as the particle size strongly affects the audio intensity. The audio feedback enables the robot to grind until the powder is sufficiently fine. In our experiments, the robot ground 80.5% of the powder to a particle size smaller than $250\ \mu\mathrm{m}$ with audio and visual feedback and 68% without audio feedback, indicating that multi-modal feedback is an effective tool to produce finer powder. We conclude that the addition of audio feedback provides crucial information to the robot, allowing it to better understand the progress of the grinding process and make more optimal decisions. This robot system can be used to prepare samples in material science experiments and analyze the grinding process.</td>
                <td>Visualization, Materials science and technology, Powders, Mortar, Robot vision systems, Timing, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341526&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341526&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Deep Learning-Based Leaf Detection for Robotic Physical Sampling with P-AgBot</td>
                <td>A. Deb, K. Kim and D. J. Cappelleri</td>
                <td>2023</td>
                <td>Automating leaf detection and physical leaf sample collection using Internet of Things (IoT) technologies is a crucial task in precision agriculture. In this paper, we present a deep learning-based approach for detecting and segmenting crop leaves for robotic physical sampling. We discuss a method for generating a physical dataset of agricultural crops. Our proposed pipeline incorporates using an RGB-D camera for dataset collection, fusing the depth frame along with RGB images to train Mask R-CNN and YOLOv5 models. We also propose our novel leaf pose estimating algorithm for physical sampling and maximizing leaf sample area while using a robotic arm integrated to the P-AgBot platform. The proposed approach has been experimentally validated on corn and sorghum, in both indoor and outdoor environments. Our method has achieved a best-case detection rate of 90.6%, a 9% smaller error compared to our previous method, and approximately 80% smaller error compared to other state-of-the-art methods in estimating the leaf position.</td>
                <td>YOLO, Deep learning, Three-dimensional displays, Robot kinematics, Robot vision systems, Crops, Grasping</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341516&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341516&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>In-Situ Measurement of Extrusion Width for Fused Filament Fabrication Process Using Vision and Machine Learning Models</td>
                <td>Hernandez</td>
                <td>2023</td>
                <td>Measuring geometry of the printing road is key for detection of anomalies in 3D printing processes. Although commercial 3D printers can measure the extrusion height using various distance sensors, measuring of the width in real-time remains a challenge. This paper presents a visual in-situ monitoring system to measure width of the printing filament road in 2D patterns. The proposed system is composed of a printable shroud with embedded camera setup and a visual detection approach based on a two-stage instance segmentation method. Each of the segmentation and localization stages can use multiple computational approaches including Gaussian mixture model, color filter, and deep neural network models. The visual monitoring system is mounted on a standard 3D printer and validated with the measurement of printed filament roads of sub-millimeter widths. The results on accuracy and robustness reveal that combinations of deep models for both segmentation and localization stages have better performance. Particularly, fully connected CNN segmentation model combined with YOLO object detector can measure sub-millimeter extrusion width with 90 μm accuracy at 125 ms speed. This visual monitoring system has potential to improve the control of printing processes by the real-time measurement of printed filament geometry.</td>
                <td>Location awareness, Instance segmentation, Visualization, Three-dimensional displays, Atmospheric measurements, Roads, Particle measurements, Computer vision, Additive manufacturing, Instance segmentation, Machine learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341406&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341406&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Motion Orchestration in Dual-Stage Wafer Scanners</td>
                <td>Rawashdeh, M. Heertjes and M. Al Janaideh</td>
                <td>2023</td>
                <td>In semiconductor manufacturing, lithography machines are becoming more and more sophisticated system of systems. As an example, a TWINSCAN wafer scanner machine is composed of a wafer, and reticle handlers, reticle, optics, and two wafer chains or systems. In previous studies, we covered the interactions between the reticle, optics, and wafer chains during the step-and-scan cycle. In this study, we focus on the interaction between the additional wafer chain responsible for aligning the wafer substrate and taking its height map during the measurement cycle, and the other chains that are active during the step-and-scan cycle. Working in parallel to increase machine throughput, the inertial forces associated motion of the two cycles induce vibration that may propagate throughout the chains in the machine if no appropriate measures are taken. In this investigation, we look at the reference trajectories responsible for steering the chains throughout the two cycles, and propose two reference trajectory orchestrations that factor in the machine design, geometry, mass distribution, and functions. Theoretically, these orchestrations lead to suppressing the induced vibration without sacrificing the machine throughput while keeping the involved control loops intact.</td>
                <td>Vibrations, Semiconductor device measurement, Upper bound, Vibration measurement, Throughput, Optics, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342526&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342526&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Packaging Unit Detection for Automated Palletizing Tasks</td>
                <td>M. Völk, K. Kleeberger, W. Kraus and R. Bormann</td>
                <td>2023</td>
                <td>For various automated palletizing tasks, the detection of packaging units is a crucial step preceding the actual handling of the packaging units by an industrial robot. We propose an approach to this challenging problem that is fully trained on synthetically generated data and can be robustly applied to arbitrary real world packaging units without further training or setup effort. The proposed approach is able to handle sparse and low quality sensor data, can exploit prior knowledge if available and generalizes well to a wide range of products and application scenarios. To demonstrate the practical use of our approach, we conduct an extensive evaluation on real-world data with a wide range of different retail products. Further, we integrated our approach in a lab demonstrator and a commercial solution will be marketed through an industrial partner.</td>
                <td>Training, Packaging, Robot sensing systems, Industrial robots, Task analysis, Pallets, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342076&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342076&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robotic Barrier Construction through Weaved, Inflatable Tubes</td>
                <td>H. J. Hee Kim et al.</td>
                <td>2023</td>
                <td>In this article, we present a mechanism and related path planning algorithm to construct light-duty barriers out of extruded, inflated tubes weaved around existing environmental features. Our extruded tubes are based on everted vine-robots and in this context, we present a new method to steer their growth. We characterize the mechanism in terms of accuracy resilience, and, towards their use as barriers, the ability of the tubes to withstand distributed loads. We further explore an algorithm which, given a feature map and the size and direction of the external load, can determine where and how to extrude the barrier. Finally, we showcase the potential of this method in an autonomously extruded two-layer wall weaved around three pipes. While preliminary, our work indicates that this method has potential for barrier construction in cluttered environments, e.g. shelters against wind or snow. Future work may show how to achieve tighter weaves, how to leverage weave friction for improved strength, how to assess barrier performance for feedback control, and how to operate the extrusion mechanism off of a mobile robot.</td>
                <td>Snow, Friction, Path planning, Electron tubes, Feedback control, Mobile robots, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342190&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342190&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bistable Tensegrity Robot with Jumping Repeatability Based on Rigid Plate-Shaped Compressors</td>
                <td>K. Shimura, N. Iwamoto and T. Umedachi</td>
                <td>2023</td>
                <td>This study presents a bistable tensegrity robot that can perform repetitive jumps using one motor. This robot is based on a tensegrity structure that uses rigid plate-shaped compressors. To achieve bistability in this structure, we optimized the position of additional springs using a physics simulator that considers geometric constraints attributed to the collision between compression materials. A prototype was constructed based on the simulation model. To achieve jumping repeatability, we used one motor to control three tendons, each used; to control the additional spring strain, trigger the snap-through motion, and reform the structure to its original form. The prototype could jump using snap-through motion and reform back to its original form based on motor rotation. Furthermore, the robot demonstrated its ability to jump over flights of stairs by attaching a stand with a slight angle and using jumping repeatability.</td>
                <td>Prototypes, Stairs, Compressors, Springs, Collision avoidance, Physics, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342069&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342069&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Accessible Soft Robotics Education with Re-Configurable Balloon Robots</td>
                <td>S. Wu, K. Gilday and J. Hughes</td>
                <td>2023</td>
                <td>Soft robotics requires effective tools to educate the next generation of engineers and researchers. Stemming from a lack of universally accepted principles for education and with high barriers to entry in terms of fabrication and hardware, education to date has been highly ad hoc. We present a low-cost toolkit based on re-configurable balloon which allows rapid development of soft yet functional robots. This provides practical demonstrations of key soft robotic principles including: morphology, stiffness control, controller dependencies and modulation of environmental interactions, while grounding robot behaviours in fundamental mechani-cal models. We provide a framework for assembling balloon structures, incorporating actuation and exploring interactions. A diverse set of robots have been developed to show the potential to use this balloon-bots for educational activities for undergraduate teaching or below. In particular, different modes of locomotion are shown using robots each of which has an assembly time under 5 minutes. These robots can teach skills ranging from component integration and implementation, to key soft robotic design principles and embodied intelligence.</td>
                <td>Uncertainty, Three-dimensional displays, Educational robots, Grounding, Education, Morphology, Soft robotics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342299&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342299&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Fabrication and Simulation Recipe for Untethering Soft-Rigid Robots with Cable-Driven Stiffness Modulation</td>
                <td>J. M. Bern, Z. J. Patterson, L. Z. Yañez, K. K. Misquitta and D. Rus</td>
                <td>2023</td>
                <td>We explore the idea of robotic mechanisms that can shift between soft and rigid states, with the long-term goal of creating robots that marry the flexibility and robustness of soft robots with the strength and precision of rigid robots. We present a simple yet effective method to achieve large and rapid stiffness variations by compressing and relaxing a flexure using cables. Next, we provide a differentiable modeling framework that can be used for motion planning, which simultaneously reasons about the modulated stiffness joints, tendons, rigid joints, and basic hydrodynamics. We apply this stiffness tuning and simulation recipe to create SoRiTu, an untethered soft-rigid robotic sea turtle capable of various swimming maneuvers.</td>
                <td>Modulation, Mechanical cables, Soft robotics, Robustness, Planning, Robots, Tuning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341630&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341630&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Integrated Design of a Robotic Bio-Inspired Trunk</td>
                <td>T. Chevillon, S. Mbakop, G. Tagne and R. Merzouki</td>
                <td>2023</td>
                <td>Soft-Continuum Manipulators are of increasing interest to researchers for various non-destructive applications (minimally invasive surgery, fibroscopy, oncology, pipe exploration and many others). They are made with soft material or special arrangement of actuators allowing them to exhibit resilience and dexterity. The concept of Proprioceptive Soft-Continuum Manipulators still remains a major challenge for soft roboticists due to the big issues related to manufacturing process which becomes very expensive including sophisticated or experimental tools, highly skilled technicians and time. However, the manipulator proprioception is very usefull for enhancing the dexterity during their manipulation. Henceforth, this paper investigates a quick and simple approach for the integrated design of a proprioceptive Soft Robotic Bio- Inspired Trunk made with dragon skin 30 Material. This soft manipulator is made up of two segments composed of three independent physical control inputs each. It has an embedded electronics mainly composed of IMUs. The latter have allowed controlling the shape kinematics using a control-oriented modeling approach inspired from the kinematics control of a puppet toy. The developed modeling approach is a Reduced Order Modeling (ROM) which uses Pythagorean Hodograph (PH) curves which lowers in real time, the control dimension of the robot to virtual control points of its representative PH curve. The proposed investigation presents also a comprehensive approach for the manufacturing process of Soft-Continuum Manipulators with complex geometry.</td>
                <td>Geometry, Manufacturing processes, Shape, Toy manufacturing industry, Propioception, Kinematics, Integrated design</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341706&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341706&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A New Design of Multilayered String Jamming Mechanism with Three-Degree-of-Freedom</td>
                <td>R. Michikawa, K. Tadakuma and F. Matsuno</td>
                <td>2023</td>
                <td>A robot must exhibit softness so as not to accidentally damage its environment. However, stiffness is also necessary, so that the robot can transmit forces and perform tasks. In soft robotics, it is desirable to be able to switch between two states, namely a flexible state for adapt ion to the environment and a rigid state for the transmission of forces. String jamming mechanisms, which comprise many units connected in a bead-like pattern, have received attention for their ability to switch between flexible and rigid states. In this study, we propose a new design of the string jamming mechanism that enhances the maximum stiffness in the rigid state while maintaining a high fitting performance for the environment in the flexible state. We evaluate the fitting of the mechanism to the environment in a qualitative geometric discussion and compare the performance of the mechanism with that of existing string jamming mechanisms. The results of experiments measuring the maximum stiffness show the usefulness of the proposed mechanism from a quantitative point of view.</td>
                <td>Fitting, Switches, Soft robotics, Ions, Jamming, Task analysis, Intelligent robots, Soft Robot Materials and Design, Ten-don/Wire Mechanism, Mechanism Design</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342146&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342146&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Protective Skin Mechanism with an Exhaustive Arrangement of Tiny Rigid Bodies for Soft Robots: Evaluation of Puncture Resistance, Elasticity, and Descaling Resistance of the Scale Mechanism</td>
                <td>K. Tadakuma, M. Watanabe and S. Tadokoro</td>
                <td>2023</td>
                <td>Soft actuators have several advantages, including large deformation, safety and adaptability to the environment, and shock absorbance. However, they are weak against sharp objects owing to their soft bodies. This paper proposes a novel protective skin mechanism with an exhaustive arrangement of tiny rigid bodies. Small pieces were sewed on an elastic sheet using Kevlar strings. We conducted some measurements of puncture resistance, elasticity, and descaling resistance of the scale mechanisms. The results indicate that the elasticity of the proposed scale mechanism was just 150% larger than a simple silicone sheet. In addition, approximately 15 N was required for descaling, which is seven times larger than that of the glued scales. There was no puncture even when pricked with a needle.</td>
                <td>Resistance, Actuators, Electric shock, Soft robotics, Elasticity, Needles, Skin, Soft Robot Materials and Design, Mechanism Design, Biomimetics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341363&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341363&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Printable Bistable Structures for Programmable Frictional Skins of Soft-Bodied Robots</td>
                <td>T. D. Ta and Y. Kawahara</td>
                <td>2023</td>
                <td>Soft robots made of flexible materials are highly adaptive, easy to fabricate, and safer to interact with. One of the ways for soft robots to interact with the surrounding environment is through their deformable bodily characteristics including internal body stiffness and external body friction. Though the flexibility of soft-bodied robots has been rigorously studied, the frictional skin of such soft-bodied robots, acting as a mechanical interface between the robot and the environment, remains unexplored. Being able to design the frictional skin will make soft-bodied robots more versatile in environmental navigation, more dexterous in manipulation tasks, and more flexible in haptic feedback. In this paper, we propose a robotic skin that can be programmed dynamically to change the mode of friction. The robotic skin is based on bistable bellow structures that can be switched between two folding states to change the contact points between the robotic skin and the ground. Our robotic skin can dynamically change its anisotropic frictional behavior to add another dimension to the designing space of soft robotics.</td>
                <td>Navigation, Friction, Switches, Grasping, Soft robotics, Skin, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341445&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341445&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>mCLARI: A Shape-Morphing Insect-Scale Robot Capable of Omnidirectional Terrain-Adaptive Locomotion in Laterally Confined Spaces</td>
                <td>H. Kabutz, A. Hedrick, W. P. McDonnell and K. Jayaram</td>
                <td>2023</td>
                <td>Soft compliant microrobots have the potential to deliver significant societal impact when deployed in applications such as search and rescue. In this research we present mCLARI, a body compliant quadrupedal microrobot of 20mm neutral body length and 0.97g, improving on its larger predecessor, CLARI. This robot has four independently actuated leg modules with 2 degrees of freedom, each driven by piezoelectric actuators. The legs are interconnected in a closed kinematic chain via passive body joints, enabling passive body compliance for shape adaptation to external constraints. Despite scaling its larger predecessor down to 60 % in length and 38% in mass, mCLARI maintains 80% of the actuation power to achieve high agility. Additionally, we demonstrate the new capability of passively shape-morphing mCLARI - omnidirectional laterally confined locomotion - and experimentally quantify its running performance achieving a new unconstrained top speed of ~3 bodylengths/s (60 mms-1). Leveraging passive body compliance, mCLARI can navigate through narrow spaces with a body compression ratio of up to 1.5 × the neutral body shape.</td>
                <td>Legged locomotion, Shape, Navigation, Piezoelectric actuators, Kinematics, Quadrupedal robots, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341588&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341588&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FABRIKv: A Fast, Iterative Inverse Kinematics Solver for Surgical Continuum Robot with Variable Curvature Model</td>
                <td>F. Wang et al.</td>
                <td>2023</td>
                <td>Due to the advantages of high flexibility, large workspace, and good human-body compatibility, flexible tendon-driven surgical continuum robots have attracted a lot of attention in robot-assisted minimally invasive surgery. However, due to the coupling of the position and angle of the continuum robot, and the easy deformation of the external force, its inverse kinematics solution has always been a challenge. This paper proposes a fast inverse kinematics solver for surgical continuum robots with a variable curvature model. Firstly, the deformation of the continuum robot is analyzed, and a representation method of the variable curvature model is proposed. Next, to solve the inverse kinematics problem when the continuum robot deforms under load, FABRIKv is proposed by improving the Forward And Backward Reaching Inverse Kinematics (FABRIK). During the inverse kinematics solution, the algorithm preserves the real-time nature of FABRIK and corrects for deformation effects caused by the load. Finally, the experiment verifies the rationality and effectiveness of the variable curvature model representation method, as well as the fastness and accuracy of the FARIKv solver.</td>
                <td>Deformable models, Minimally invasive surgery, Deformation, Force, Kinematics, Real-time systems, Iterative methods</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342089&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342089&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Synchronous Control of a Magnetically-Actuated and Ultrasound-Guided Multi-Arm Robotic System</td>
                <td>Z. Li and Q. Xu</td>
                <td>2023</td>
                <td>This paper presents the design of a new multi-arm robotic system with mobile magnetic actuation and extracorpo-real ultrasound guidance dedicated to magnetic catheterization. The kinematic model of the external mobile actuation arm (EMAA) and extracorporeal ultrasound-integrated tracking arm (EUTA) are derived based on Denavit-Hartenberg (DH) parameters, including specially designed end-effectors. The synchronous control scheme for the mobile magnet and mobile ultrasound probe is introduced with polar coordinate-based magnetic actuation and visual servo-based ultrasound tracking method. Meanwhile, a denoising algorithm based on Speckle Reduction Anisotropic Diffusion (SRAD) is implemented. The effectiveness of the proposed robotic system has been verified by conducting several experimental studies, e.g., ex-vivo tests of catheter steering in endovascular phantom and soft tissue-imitating phantom with the average error of 0.32 mm and signal-to-noise-ratio (SNR) of 12.2 for the ultrasound imaging.</td>
                <td>Visualization, Ultrasonic imaging, Magnetic resonance imaging, Robot kinematics, Imaging phantoms, Robot sensing systems, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341399&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341399&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Shared-Control Dexterous Robotic System for Assisting Transoral Mandibular Fracture Reduction: Development and Cadaver Study</td>
                <td>Y. Wang et al.</td>
                <td>2023</td>
                <td>The rigid and straight nature of conventional surgical drills and screwdrivers makes it difficult to access the posterior mandible for fracture reduction without the creation of facial incisions. To assist transoral mandibular fracture reduction in hard-to-reach areas, we propose a shared-control dexterous robotic system. The end effector of this system is an articulated drilling/screwing tool to provide distal dexterity. This system uses an admittance-control-based approach to provide precision and stability during shared-control hole-drilling processes. A cadaver study showed the efficacy of the proposed system to assist plate fixation in the reduction of mandibular fractures. The proposed articulated surgical tool was capable of drilling holes in and driving screws into the mandible of a cadaver head. In addition, the shared-control robotic system ensured that the drill moved along its axial direction, leading to stable and precise hole drilling.</td>
                <td>Drilling, Torque, Instruments, Fasteners, End effectors, Robots, Cadaver</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342097&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342097&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Model-Based Bending Control of Magnetically-Actuated Robotic Endoscopes for Automatic Retroflexion in Confined Spaces</td>
                <td>Y. Sun et al.</td>
                <td>2023</td>
                <td>This paper is concerned with the issue of the kinematic model-based bending control for the magnetically actuated robotic endoscope and its application for automatic retroflexion. By the utilization of the Cosserat rod theory and the transformation in the magnetic tip of the endoscope, the comprehensive kinematic model of the magnetically-actuated robotic endoscope is established. Afterward, a magnetic control scheme for the bending motion is proposed by co-developing an error feedback PID control strategy and the model-based feedback approach. Moreover, as one unique kind of bending motion, retroflexion is taken into account, and the strategy aimed at the bid of compact space retroflexion is presented by virtue of the introduction of serial waypoints pursuing the position of the magnetic tip being close to the midline as possible. Eventually, the developed modeling and bending control scheme and the compact space retroflexion strategy are examined in a magnetically actuated robotic endoscope system to manifest the effectiveness and applicability of the theoretical approach. The experimental results indicate that the designed controller can drive the endoscope to bend to the desired pose and show a reduction of about 47.01% in the sweeping area and 79.25% in the last distance to midline achieved by conducting compact space retroflexion in comparison to “U” type one.</td>
                <td>PI control, Endoscopes, Magnetic confinement, Kinematics, Bending, Aerospace electronics, Drives</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342414&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342414&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>3D Laser-and-Tissue Agnostic Data-Driven Method for Robotic Laser Surgical Planning</td>
                <td>G. Ma, R. Prakash, B. Mann, W. Ross and P. Codd</td>
                <td>2023</td>
                <td>In robotic laser surgery, shape prediction of an one-shot ablation crater is an important problem for minimizing errant overcutting of healthy tissue during the course of pathological tissue resection and precise tumor removal. Since it is difficult to physically model the laser-tissue interaction due to the variety of optical tissue properties, complicated process of heat transfer, and uncertainty about the chemical reaction, we propose a 3D crater prediction model based on an entirely data-driven method without any assumptions of laser settings and tissue properties. Based on the crater prediction model, we formulate a novel robotic laser planning problem to determine the optimal laser incident configuration, which aims to create a crater that aligns with the surface target (e.g. tumor, pathological tissue). To solve the one-shot ablation crater prediction problem, we model the 3D geometric relation between the tissue surface and the laser energy profile as a non-linear regression problem that can be represented by a single-layer perceptron (SLP) network. The SLP network is encoded in a novel kinematic model to predict the shape of the post-ablation crater with an arbitrary laser input. To estimate the SLP network parameters, we formulate a dataset of one-shot laser-phantom craters reconstructed by the optical coherence tomography (OCT) B-scan images. To verify the method. The learned crater prediction model is applied to solve a simplified robotic laser planning problem modelled as a surface alignment error minimization problem. The initial results report about $(91.2\pm 3.0)\%$ 3D-crater-Intersection-over-Union (3D-crater-IoU) for the 3D crater prediction and an average of about 98.0% success rate for the simulated surface alignment experiments.</td>
                <td>Solid modeling, Pathology, Three-dimensional displays, Surface emitting lasers, Predictive models, Laser modes, Chemical lasers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341343&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341343&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Insertion, Retrieval and Performance Study of Miniature Magnetic Rotating Swimmers for the Treatment of Thrombi</td>
                <td>Y. Lu, J. Ramos, M. G. Ghosn, D. J. Shah, A. T. Becker and J. Leclerc</td>
                <td>2023</td>
                <td>Miniature Magnetic Rotating Swimmers (MMRSs) are untethered machines containing magnetic materials. An external rotating magnetic field produces a torque on the swimmers to make them rotate. MMRSs have propeller fins that convert the rotating motion into forward propulsion. This type of robot has been shown to have potential applications in the medical realm. This paper presents new MMRS designs with (1) an increased permanent magnet volume to increase the available torque and prevent the MMRS from becoming stuck inside a thrombus; (2) new helix designs that produce an increased force to compensate for the weight added by the larger permanent magnet volume; (3) different head drill shape designs that have different interactions with thrombi. The two best MMRS designs were tested experimentally by removing a partially dried 1-hour-old thrombus with flow in a bifurcating artery model. The first MMRS disrupted a large portion of the thrombus. The second MMRS retrieved a small remaining piece of the thrombus. In addition, a tool for inserting, retrieving, and switching MMRSs during an experiment is presented and demonstrated. Finally, this paper shows that the two selected MMRS designs can perform accurate 3D path-following.</td>
                <td>Three-dimensional displays, Torque, Shape, Velocity control, Fluid flow, Switches, Propulsion</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342355&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342355&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hybrid Tendon and Ball Chain Continuum Robots for Enhanced Dexterity in Medical Interventions</td>
                <td>G. Pittiglio, M. Mencattelli, A. Donder, Y. Chitalia and P. E. Dupont</td>
                <td>2023</td>
                <td>A hybrid continuum robot design is introduced that combines a proximal tendon-actuated section with a distal telescoping section comprised of permanent-magnet spheres actuated using an external magnet. While, individually, each section can approach a point in its workspace from one or at most several orientations, the two-section combination possesses a dexterous workspace. The paper describes kinematic modeling of the hybrid design and provides a description of the dexterous workspace. We present experimental validation which shows that a simplified kinematic model produces tip position mean and maximum errors of 3% and 7% of total robot length, respectively.</td>
                <td>Shape, Kinematics, Switches, Feedback control, Electron tubes, Task analysis, Feedforward systems, Medical Rebots and Systems, Steerable Catheters, Flexible Robotics, Magnetic Actuation, Continuum robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341686&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341686&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Semi-Autonomous Assistance for Telesurgery Under Communication Loss</td>
                <td>H. Ishida, A. Munawar, R. H. Taylor and P. Kazanzides</td>
                <td>2023</td>
                <td>Telesurgery has a clear potential for providing high-quality surgery to medically underserved areas like rural areas, battlefields, and spacecraft; nevertheless, effective methods to overcome unreliable communication systems are still lacking. Furthermore, it is not well understood how users react at the moment of communication loss and also during the loss. In this paper, we aim to analyze human response by proposing a telesurgery simulation framework that models an environment incorporating local and remote sites. Furthermore, this framework generates structural data for human behavior analysis and can provide different forms of assistance during the communication failure and at the communication recovery. We investigated three different types of assistance: User-centered, Robot-centered and Hybrid. A 12-person user-study was carried out using the proposed telesurgery simulation where participants completed a peg transfer task with random communication loss. The collected data was used to analyze the human response to a communication failure. The proposed Hybrid method reduced temporal demand with no increase in completion time compared to the baseline control method where users were unable to move the input device during the communication loss. The Hybrid method also significantly reduced both the task completion time and workload compared to the other two proposed methods (User-centered and Robot-centered).</td>
                <td>Space vehicles, Analytical models, Communication systems, Robot control, Surgery, Input devices, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341450&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341450&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Improving Surgical Situational Awareness with Signed Distance Field: A Pilot Study in Virtual Reality</td>
                <td>H. Ishida et al.</td>
                <td>2023</td>
                <td>The introduction of image-guided surgical navigation (IGSN) has greatly benefited technically demanding surgical procedures by providing real-time support and guidance to the surgeon during surgery. To develop effective IGSN, a careful selection of the surgical information and the medium to present this information to the surgeon is needed. However, this is not a trivial task due to the broad array of available options. To address this problem, we have developed an open-source library that facilitates the development of multimodal navigation systems in a wide range of surgical procedures relying on medical imaging data. To provide guidance, our system calculates the minimum distance between the surgical instrument and the anatomy and then presents this information to the user through different mechanisms. The real-time performance of our approach is achieved by calculating Signed Distance Fields at initialization from segmented anatomical volumes. Using this framework, we developed a multimodal surgical navigation system to help surgeons navigate anatomical variability in a skull base surgery simulation environment. Three different feedback modalities were explored: visual, auditory, and haptic. To evaluate the proposed system, a pilot user study was conducted in which four clinicians performed mastoidectomy procedures with and without guidance. Each condition was assessed using objective performance and subjective workload metrics. This pilot user study showed improvements in procedural safety without additional time or workload. These results demonstrate our pipeline's successful use case in the context of mastoidectomy.</td>
                <td>Measurement, Visualization, Navigation, Surgery, Virtual reality, Skull, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342004&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342004&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Development and Evaluation of a Single-arm Robotic System for Autonomous Suturing</td>
                <td>J. Liu et al.</td>
                <td>2023</td>
                <td>This article introduces a novel suture managing device (SMD) and new suture management controller to enable single-arm suture management during autonomous suturing with the Smart Tissue Autonomous Robot (STAR). The primary function of the SMD is to tension and manage the suture thread, a task that was previously carried out by a second manipulator or a human assistant. The SMD and its controller are integrated into STAR's autonomous suturing workflow. Experiments were conducted to quantify the tensioning force of SMD and to evaluate the suture quality of the new single-arm system. The prototype of SMD achieves 1.67N tensioning force with suturing time of 29.1±0.42 seconds per stitch. Our study results demonstrate that the single-arm STAR system with SMD achieves equivalent performance to our previous works in suturing efficiency where suture management was performed with either a dual-armed robotic system or by a human surgical assistant. The study's findings contribute to the field of medical robotics and to our knowledge represent the first known instance of single-arm suturing with suture management during autonomous anastomosis.</td>
                <td>Laparoscopes, Medical robotics, Force, Stars, Surgery, Prototypes, Manipulators</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341999&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341999&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>End-to-End Learning of Deep Visuomotor Policy for Needle Picking</td>
                <td>H. Lin, B. Li, X. Chu, Q. Dou, Y. Liu and K. W. Samuel Au</td>
                <td>2023</td>
                <td>Needle picking is a challenging manipulation task in robot-assisted surgery due to the characteristics of small slender shapes of needles, needles' variations in shapes and sizes, and demands for millimeter-level control. Prior works, heavily relying on the prior of needles (e.g., geometric models), are hard to scale to unseen needles' variations. In this paper, we present the first end- to-end learning method to train deep visuomotor policy for needle picking. Concretely, we propose DreamerfD to maximally leverage demonstrations to improve the learning efficiency of a state-of-the-art model-based reinforcement learning method, DreamerV2; Since Variational Auto-Encoder (VAE) in DreamerV2 is difficult to scale to high-resolution images, we propose Dynamic Spotlight Adaptation to represent control-related visual signals in a low-resolution image space; Virtual Clutch is also proposed to reduce per-formance degradation due to significant error between prior and posterior encoded states at the beginning of a rollout. We conducted extensive experiments in simulation to evaluate the performance, robustness, in-domain variation adaptation, and effectiveness of individual components of our method. Our method, trained by 8k demonstration timesteps and 140k online policy timesteps, can achieve a remarkable success rate of 80%. Furthermore, our method effectively demonstrated its superiority in generalization to unseen in-domain variations including needle variations and image disturbance, highlighting its robustness and versatility. Codes and videos are available at https://sites.google.com/view/DreamerfD.</td>
                <td>Learning systems, Visualization, Shape, Surgery, Reinforcement learning, Needles, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342194&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342194&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot</td>
                <td>T. Huang, K. Chen, W. Wei, J. Li, Y. Long and Q. Dou</td>
                <td>2023</td>
                <td>Reinforcement learning is still struggling with solving long-horizon surgical robot tasks which involve multiple steps over an extended duration of time due to the policy exploration challenge. Recent methods try to tackle this problem by skill chaining, in which the long-horizon task is decomposed into multiple subtasks for easing the exploration burden and subtask policies are temporally connected to complete the whole long-horizon task. However, smoothly connecting all subtask policies is difficult for surgical robot scenarios. Not all states are equally suitable for connecting two adjacent subtasks. An undesired terminate state of the previous subtask would make the current subtask policy unstable and result in a failed execution. In this work, we introduce value-informed skill chaining (ViSkill), a novel reinforcement learning framework for long-horizon surgical robot tasks. The core idea is to distinguish which terminal state is suitable for starting all the following subtask policies. To achieve this target, we introduce a state value function that estimates the expected success probability of the entire task given a state. Based on this value function, a chaining policy is learned to instruct subtask policies to terminate at the state with the highest value so that all subsequent policies are more likely to be connected for accomplishing the task. We demonstrate the effectiveness of our method on three complex surgical robot tasks from SurRoL, a comprehensive surgical simulation platform, achieving high task success rates and execution efficiency. Code is available at https: / /github. com/med-air/ViSkill.</td>
                <td>Medical robotics, Codes, Reinforcement learning, Task analysis, Intelligent robots, Software development management</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342180&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342180&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design of a Jumping Control Framework with Heuristic Landing for Bipedal Robots</td>
                <td>J. Zhang, J. Shen, Y. Liu and D. Hong</td>
                <td>2023</td>
                <td>Generating dynamic jumping motions on legged robots remains a challenging control problem as the full flight phase and large landing impact are expected. Compared to quadrupedal robots or other multi-legged robots, bipedal robots place higher requirements for the control strategy given a much smaller support polygon. To solve this problem, a novel heuristic landing planner is proposed in this paper. With the momentum feedback during the flight phase, landing locations can be updated to minimize the influence of uncertainties from tracking errors or external disturbances when landing. To the best of our knowledge, this is the first approach to take advantage of the flight phase to reduce the impact of the jump landing which is implemented in the actual robot. By integrating it with a modified kino-dynamics motion planner with centroidal momentum and a low-level controller which explores the whole-body dynamics to hierarchically handle multiple tasks, a complete and versatile jumping control framework is designed in this paper. Extensive results of simulation and hardware jumping experiments on a miniature bipedal robot with proprioceptive actuation are provided to demonstrate that the proposed framework is able to achieve human-like efficient and robust jumping tasks, including directional jump, twisting jump, step jump, and somersaults.</td>
                <td>Legged locomotion, Uncertainty, Tracking, Dynamics, Propioception, Hardware, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342265&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342265&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion</td>
                <td>J. Kim, J. Cha, D. Kim and J. Park</td>
                <td>2023</td>
                <td>The estimation of external joint torque and contact wrench is essential for achieving stable locomotion of humanoids and safety-oriented robots. Although the contact wrench on the foot of humanoids can be measured using a force-torque sensor (FTS), FTS increases the cost, inertia, complexity, and failure possibility of the system. This paper introduces a method for learning external joint torque solely using proprioceptive sensors (encoders and IMUs) for a floating base robot. For learning, the GRU network is used and random walking data is collected. Real robot experiments demonstrate that the network can estimate the external torque and contact wrench with significantly smaller errors compared to the model-based method, momentum observer (MOB) with friction modeling. The study also validates that the estimated contact wrench can be utilized for zero moment point (ZMP) feedback control, enabling stable walking. Moreover, even when the robot's feet and the inertia of the upper body are changed, the trained network shows consistent performance with a model-based calibration. This result demonstrates the possibility of removing FTS on the robot, which reduces the disadvantages of hardware sensors.</td>
                <td>Legged locomotion, Learning systems, Torque, Costs, Humanoid robots, Propioception, Estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342530&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342530&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Time to Danger, an Alternative to Passive Safety for the Locomotion of a Biped Robot in a Crowd</td>
                <td>B. Wieber and T. Fraichard</td>
                <td>2023</td>
                <td>A biped robot walking in a crowd must avoid falls and collisions at the same time. The latter is usually addressed through Passive Safety (PS), which guarantees that the robot is at rest when a collision is inevitable. Since PS may limit the robot's mobility, the purpose of this work is to introduce and explore the novel concept of Time To Danger (TTD) as an alternative. For a given robot motion, TTD is the time where the robot enters the region that a person can potentially occupy in the future. After having studied the properties of TTD, a novel locomotion strategy is proposed, which computes an optimal locomotion plan that guarantees balance preservation and TTD maximization, following a receding horizon Model Predictive Control scheme. Controlled experiments in a challenging simulated crowd scenario demonstrate how the novel locomotion strategy outperforms a Passive Safety-based locomotion strategy from a collision avoidance point of view.</td>
                <td>Legged locomotion, Robot motion, Safety, Collision avoidance, Intelligent robots, Predictive control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342001&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342001&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ZMP Feedback Balance Control of Humanoid in Response to Ground Acceleration</td>
                <td>M. Konishi, K. Kojima, K. Okada, M. Inaba and K. Kawasaki</td>
                <td>2023</td>
                <td>In order for a humanoid robot to balance on the movable ground, balance feedback control in response to its unpredictable movement is required. However, feedback control in response to ground movement has the following two issues, (A) Interaction between the ground dynamics and the balance control may cause vibration. (B) The balance control may rather deteriorate the stability due to the response delay. To solve these problems, this study proposes the support foot acceleration term in the walking stabilizer and gives its gain by considering the following two conditions, (A) Avoiding steady-state vibration in a two-mass linear inverted pendulum model on an arbitrary ground, and (B) reducing the influence of inertial forces resulting from the delay of ZMP feedback. Experiments with a life-size humanoid JAXON verified the steady-state vibration phenomenon and improved the stability of acceleration and deceleration when boarding the Two-Wheeled Scooter.</td>
                <td>Vibrations, Phase measurement, Humanoid robots, Motorcycles, Vibration measurement, Stability analysis, Steady-state</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341851&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341851&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Manipulation of Center of Pressure for Bipedal Locomotion by Passive Twisting of Viscoelastic Trunk Joint and Asymmetrical Arm Swinging</td>
                <td>T. Takuma, I. Hashimoto, R. Andachi, Y. Sugimoto and S. Aoi</td>
                <td>2023</td>
                <td>To implement successful bipedal locomotion in a robot, its center of pressure (CoP) is placed on a supporting area. To achieve locomotion, many studies have focused on the lower body. Given that the human upper body has a large mass and its behavior influences locomotion even in the case of the robot, this study investigates the effect of the upper body, which contains moving arms and a twisting trunk, on CoP. The dynamics is analyzed using a simple model that has a passive viscoelastic trunk joint around the vertical axis and arms that oscillates back and forth. From the derivations of CoP and trunk joint trajectory, three important findings are made: (i) the CoP oscillates along the lateral direction only when the arm swings in an anterior-posterior asymmetric manner, (ii) the trajectories of CoP and trunk joint are in anti-phase, (iii) the phase of CoP along the lateral direction is influenced by the swinging cycle and viscoelasticity of the trunk joint, the mechanical elements of the upper body. An experiment using a robot verified the first and second findings, and simulation verified the last finding. The last finding will contribute to making a feedback controller that converges to the desired phase, which is an important factor for successful bipedal locomotion.</td>
                <td>Legged locomotion, Torso, Phase measurement, Simulation, Manipulators, Stability analysis, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342461&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342461&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Implantable Variable Length Actuator for Modulating in Vivo Musculo-Tendon Force in a Bipedal Animal Model</td>
                <td>S. Thomas et al.</td>
                <td>2023</td>
                <td>Mobility, a critical factor in quality of life, is often rehabilitated using simplistic solutions, such as walkers. Exoskeletons (wearable robotics) offer a more sophisticated rehabilitation approach. However, non-adherence to externally worn mobility aids limits their efficacy. Here, we present the concept of a fully implantable assistive limb actuator that overcomes non-adherence constraints, and which can provide high-precision assistive force. In a bipedal animal model (fowl), we have developed a variable length isometric actuator (measuring ϕ9 x 30 mm) that is able to be directly implanted within the leg via a bone anchor and tendon fixation, replacing the lateral gastrocnemius muscle belly. The actuator is able to generate isometric force similar to the in vivo force of the native muscle, designed to generate assistive torque at the ankle and reduce muscular demand at no additional energy cost. The device has a stroke of 10 mm that operates up to 770 mm/s (77 stroke lengths/s), capable of acting as a clutch (disengaging when needed) and with a tunable slack length to modulate the timing and level of assistive force during gait. Surgical techniques to attach the actuator to the biological system, the Achilles tendon and tibia, have been established and validated using survival surgeries and cadaveric specimens.</td>
                <td>Actuators, In vivo, Torque, Animals, Force, Surgery, Muscles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341584&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341584&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TemporalStereo: Efficient Spatial-Temporal Stereo Matching Network</td>
                <td>Y. Zhang, M. Poggi and S. Mattoccia</td>
                <td>2023</td>
                <td>We present TemporalStereo, a coarse-to-fine stereo matching network that is highly efficient, and able to effectively exploit the past geometry and context information to boost matching accuracy. Our network leverages sparse cost volume and proves to be effective when a single stereo pair is given. However, its peculiar ability to use spatio-temporal information across stereo sequences allows TemporalStereo to alleviate problems such as occlusions and reflective regions while enjoying high efficiency also in this latter case. Notably, our model - trained once with stereo videos - can run in both single-pair and temporal modes seamlessly. Experiments show that our network relying on camera motion is robust even to dynamic objects when running on videos. We validate TemporalStereo through extensive experiments on synthetic (SceneFlow, TartanAir) and real (KITTI 2012, KITTI 2015) datasets. Our model achieves state-of-the-art performance on any of these datasets.</td>
                <td>Geometry, Costs, Dynamics, Cameras, Intelligent robots, Videos</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341598&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341598&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Convolutional Occupancy Models for Dense Packing of Complex, Novel Objects</td>
                <td>N. Mishra, P. Abbeel, X. Chen and M. Sieb</td>
                <td>2023</td>
                <td>Dense packing in pick-and-place systems is an important feature in many warehouse and logistics applications. Prior work in this space has largely focused on planning algorithms in simulation, but real-world packing performance is often bottlenecked by the difficulty of perceiving 3D object geometry in highly occluded, partially observed scenes. In this work, we present a fully-convolutional shape completion model, F-CON, which can be easily combined with off-the-shelf planning methods for dense packing in the real world. We also release a simulated dataset, COB-3D-v2, that can be used to train shape completion models for real-word robotics applications, and use it to demonstrate that F-CON outperforms other state-of-the-art shape completion methods. Finally, we equip a real-world pick-and-place system with F-CON, and demonstrate dense packing of complex, unseen objects in cluttered scenes. Across multiple planning methods, F-CON enables substantially better dense packing than other shape completion methods.</td>
                <td>Training, Geometry, Three-dimensional displays, Shape, Planning, Intelligent robots, Logistics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341466&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341466&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Video Inpainting for RGB-D Pipeline Reconstruction</td>
                <td>L. Wang et al.</td>
                <td>2023</td>
                <td>This paper presents a Video Inpainting algorithm that enables monocular-camera-laser-based pipeline inspection robots to capture both color and 3D information using only one video stream. Conventional monocular-camera-laser inspection methods are limited to capture either 2D color images or 3D point clouds since the laser tends to overexpose the actual color of the scanning area. We propose a real-time Video Inpainting method to solve this problem with minimal hardware needs that can be easily integrated with conventional pipeline profiling robots. The algorithm is accelerated by two components: a lightweight network that directly predicts the complete optical flow and simplifies the algorithm pipeline, and the Polar coordinate transformation, which significantly reduces the image processing compexity. Real-world experiments demonstrate that our online algorithm has comparable or better color estimation accuracy against state-of-the-art offline algorithms, while is capable of running at 23 frames per second (FPS) on a laptop computer with a resolution of 1024 × 1024 pixels. In addition, we verify that this method can be used for video pre-processing for downstream tasks that require high-quality visual inputs, such as Simultaneously Localization and Mapping (SLAM). To the best of our knowledge, this is the first real-time Video Inpainting algorithm that can be used for in-pipe environments, serving as an important building block for highly compact RGB-D inspection sensors and robots for the pipeline industry.</td>
                <td>Visualization, Three-dimensional displays, Image color analysis, Robot kinematics, Pipelines, Streaming media, Inspection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341971&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341971&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MUFeat: Multi-Level CNN and Unsupervised Learning for Local Feature Detection and Description</td>
                <td>W. Chen</td>
                <td>2023</td>
                <td>Local feature detection and description are two essential steps in many visual applications. Most learned local feature methods require high-quality labeled data to achieve superior performance, but such labels are often expensive. To address this problem, we propose MUFeat, an unsupervised learning framework of jointly learning local feature detector and descriptor without requirement of ground-truth correspondences. MUFeat trains the network based on the putative matches from the pretrained model and two proposed unsupervised loss functions. Furthermore, the MUFeat framework includes a pyramidal feature hierarchy network to obtain keypoints and descriptors from feature maps. Experiments indicate that MUFeat outperforms most state-of-the-art supervised learning methods on image matching, medical image registration and visual localization tasks.</td>
                <td>Location awareness, Training, Visualization, Image matching, Feature detection, Supervised learning, Detectors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342482&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342482&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Early or Late Fusion Matters: Efficient RGB-D Fusion in Vision Transformers for 3D Object Recognition</td>
                <td>G. Tziafas and H. Kasaei</td>
                <td>2023</td>
                <td>The Vision Transformer (ViT) architecture has established its place in computer vision literature, however, training ViTs for RGB-D object recognition remains an understudied topic, viewed in recent literature only through the lens of multi-task pretraining in multiple vision modalities. Such approaches are often computationally intensive, relying on the scale of multiple pretraining datasets to align RGB with 3D information. In this work, we propose a simple yet strong recipe for transferring pretrained ViTs in RGB-D domains for 3D object recognition, focusing on fusing RGB and depth representations encoded jointly by the ViT. Compared to previous works in multimodal Transformers, the key challenge here is to use the attested flexibility of ViTs to capture cross-modal interactions at the downstream and not the pretraining stage. We explore which depth representation is better in terms of resulting accuracy and compare early and late fusion techniques for aligning the RGB and depth modalities within the ViT architecture. Experimental results in the Washington RGB-D Objects dataset (ROD) demonstrate that in such RGB → RGB-D scenarios, late fusion techniques work better than most popularly employed early fusion. With our transfer baseline, fusion ViTs score up to 95.4% top-1 accuracy in ROD, achieving new state-of-the-art results in this benchmark. We further show the benefits of using our multimodal fusion baseline over unimodal feature extractors in a synthetic-to-real visual adaptation as well as in an open-ended lifelong learning scenario in the ROD benchmark, where our model outperforms previous works by a margin of >8%. Finally, we integrate our method with a robot framework and demonstrate how it can serve as a perception utility in an interactive robot learning scenario, both in simulation and with a real robot.</td>
                <td>Training, Visualization, Three-dimensional displays, Computer architecture, Benchmark testing, Transformers, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341422&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341422&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TransTouch: Learning Transparent Objects Depth Sensing Through Sparse Touches</td>
                <td>L. Bian, P. Shi, W. Chen, J. Xu, L. Yi and R. Chen</td>
                <td>2023</td>
                <td>Transparent objects are common in daily life. However, depth sensing for transparent objects remains a challenging problem. While learning-based methods can leverage shape priors to improve the sensing quality, the labor-intensive data collection in real world and the sim-to-real domain gap restrict these methods' scalability. In this paper, we propose a method to finetune a stereo network with sparse depth labels automatically collected using a probing system with tactile feedback. We present a novel utility function to evaluate the benefit of touches. By approximating and optimizing the utility function, we can optimize the probing locations given a fixed touching budget to better improve the network's performance on real objects. We further combine tactile depth supervision with a confidence-based regularization to prevent over-fitting during finetuning. To evaluate the effectiveness of our method, we construct a real-world dataset including both diffuse and transparent objects. Experimental results on this dataset show that our method can significantly improve real-world depth sensing accuracy, especially for transparent objects.</td>
                <td>Learning systems, Shape, Scalability, Tactile sensors, Data collection, Sensors, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341417&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341417&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>WatchPed: Pedestrian Crossing Intention Prediction Using Embedded Sensors of Smartwatch</td>
                <td>J. A. Abbasi, N. M. Imran, L. C. Das and M. Won</td>
                <td>2023</td>
                <td>The pedestrian crossing intention prediction problem is to estimate whether or not the target pedestrian will cross the street. State-of-the-art techniques heavily depend on visual data acquired through the front camera of the ego-vehicle to make a prediction of the pedestrian's crossing intention. Hence, the efficiency of current methodologies tends to decrease notably in situations where visual input is imprecise, for instance, when the distance between the pedestrian and ego-vehicle is considerable or the illumination levels are inadequate. To address the limitation, in this paper, we present the design, implementation, and evaluation of the first-of-its-kind pedestrian crossing intention prediction model based on integration of motion sensor data gathered through the smartwatch (or smartphone) of the pedestrian. We propose an innovative machine learning framework that effectively integrates motion sensor data with visual input to enhance the predictive accuracy significantly, particularly in scenarios where visual data may be unreliable. Moreover, we perform an extensive data collection process and introduce the first pedestrian intention prediction dataset that features synchronized motion sensor data. The dataset comprises 255 video clips that encompass diverse distances and lighting conditions. We trained our model using the widely-used JAAD and our own datasets and compare the performance with a state-of-the-art model. The results demonstrate that our model outperforms the current state-of-the-art method, particularly in cases where the distance between the pedestrian and the observer is considerable (more than 70 meters) and the lighting conditions are inadequate.</td>
                <td>Visualization, Wearable Health Monitoring Systems, Pedestrians, Lighting, Predictive models, Data collection, Motion detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341607&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341607&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Object Detection Based on Raw Bayer Images</td>
                <td>G. Lu</td>
                <td>2023</td>
                <td>Bayer pattern is a widely used Color Filter Array (CFA) for digital image sensors, efficiently capturing different light wavelengths on different pixels without the need for a costly ISP pipeline. The resulting single-channel raw Bayer images offer benefits such as spectral wavelength sensitivity and low time latency. However, object detection based on Bayer images has been underexplored due to challenges in human observation and algorithm design caused by the discontinuous color channels in adjacent pixels. To address this issue, we propose the BayerDetect network, an end-to-end deep object detection framework that aims to achieve fast, accurate, and memory-efficient object detection. Unlike RGB color images, where each pixel encodes spectral context from adjacent pixels during ISP color interpolation, raw Bayer images lack spectral context. To enhance the spectral context, the BayerDetect network introduces a spectral frequency attention block, transforming the raw Bayer image pattern to the frequency domain. In object detection, clear object boundaries are essential for accurate bounding box predictions. To handle the challenges posed by alternating spectral channels and mitigate the influence of discontinuous boundaries, the BayerDetect network incorporates a spatial attention scheme that utilizes deformable convolutional kernels in multiple scales to explore spatial context effectively. The extracted convolutional features are then passed through a sparse set of proposal boxes for detection and classification. We conducted experiments on both public and self-collected raw Bayer images, and the results demonstrate the superb performance of the BayerDetect network in object detection tasks.</td>
                <td>Image sensors, Sensitivity, Image color analysis, Frequency-domain analysis, Object detection, Sensor phenomena and characterization, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342008&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342008&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SDFMAP: Neural Signed Distance Fields for Mapping and Positioning in Real-Time</td>
                <td>S. Liu and J. Zhu</td>
                <td>2023</td>
                <td>Neural surface reconstruction has recently gained a bit attention due to the promising result on scene rendering. Nevertheless, most of existing approaches either treat the camera parameters as the prior during training or indirectly estimate them through structure-from-motion. To tap the potential of implicit neural networks, we present a novel end-to-end neural network, termed SDFMAP, without any prior knowledge of the scene, like pre-computed camera parameters and pretrained geometric priors. Specifically, our method adopts a single multilayer perceptron to achieve simultaneously pose estimation and indoor scene reconstruction in real-time through learning the truncated signed distance function. Comparing to the recent neural implicit vSLAM systems, our approach achieves higher tracking speed via a lightweight network. Experiments on several challenging benchmark datasets show that our SDFMAP method achieves the state-of-the-art results on camera tracking and scene reconstruction.</td>
                <td>Training, Surface reconstruction, Image color analysis, Robot vision systems, Pose estimation, Neural networks, Benchmark testing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341429&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341429&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LocalViT: Analyzing Locality in Vision Transformers</td>
                <td>Y. Li et al.</td>
                <td>2023</td>
                <td>The aim of this paper is to study the influence of locality mechanisms in vision transformers. Transformers originated from machine translation and are particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking is a locality mechanism for infor-mation exchange within a local region. In this paper, locality mechanism is systematically investigated by carefully designed controlled experiments. We add locality to vision transformers into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to vision transformers with different architecture designs, which shows the generalization of the locality concept. For ImageNet2012 classification, the locality-enhanced transformers outperform the baselines Swin-T [1], DeiT-T [2] and PVT-T [3] by 1.0%, 2.6 % and 3.1 % with a negligible increase in the number of parameters and computational effort. Code is available at https://github.com/ofsoundof/LocalViT.</td>
                <td>Convolutional codes, Lattices, Computer architecture, Performance gain, Transformers, Machine translation, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342025&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342025&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>HistoDepth - Novel Depth Perception for Safe Collaborative Robots</td>
                <td>U. Scholl</td>
                <td>2023</td>
                <td>The trend towards Industry 4.0 demands an increasing flexibility of system configurations including more agile collaborative human-robot systems that can quickly adapt to new tasks and missions. At the same time, state-of-the-art safety solution for many industrial robots is the use of barriers around the robot to limit the exposure to human co-workers. As these barriers cannot be reconfigured easily, it considerably limits the flexibility of many robotic solutions. Thus, new more agile safety solutions are required, and these require new, robust perception systems that can ensure detection of all safety relevant objects. In this paper, we propose a novel depth perception approach called HistoDepth, which addresses this need. It uses depth sensors that are mounted at fixed, static positions outside of the workspace and tracks each pixel measurement statistically over time. This enables our solution to differentiate static scene elements from dynamic (potentially hazardous) elements and to enforce a robot safety maneuver upon detection of a critical object. Moreover, it can adapt automatically to changing noise or scene configurations without user input or the need for setup changes. Using an UR5 robot arm, we demonstrate that this solution can enable new agile and flexible safety concepts for collaborative robots.</td>
                <td>Service robots, Collaboration, Robot sensing systems, Manipulators, Time measurement, Safety, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341816&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341816&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Safe Active Learning and Probabilistic Design of Experiment for Autonomous Hydraulic Excavators</td>
                <td>M. Dio, O. Demir, A. Trachte and K. Graichen</td>
                <td>2023</td>
                <td>Recently, data-driven and hybrid control of hydraulic cylinders for excavator assistance functions have been in the focus of many research papers. To ensure an accurate behavior, data-driven controllers and models need a large amount of data to cover all relevant operation regions, which requires a time-consuming data generation process. In this work, we introduce two learning-based methods to enhance the efficiency of this procedure: a static learning method and an active learning method. Both methods reduce the amount of required data to learn a hydraulic inverse actuation model. Compared to previous collection methods, the required data was reduced by factor 7.5, while the information content of the dataset remains nearly the same.</td>
                <td>Learning systems, Tracking loops, Process control, Hydraulic systems, Data collection, Excavation, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342052&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342052&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SMART-Rain: A Degradation Evaluation Dataset for Autonomous Driving in Rain</td>
                <td>C. Zhang, Z. Huang, H. Guo, L. Qin, M. H. Ang and D. Rus</td>
                <td>2023</td>
                <td>Autonomous driving in the rain remains a challenge. One main problem is performance degradation caused by rain. This work introduces a new dataset to study this problem. Our dataset is collected from a full-scale vehicle equipped with a 3D LiDAR sensor and multiple forward-facing cameras under various rainy conditions. In addition, rainfall intensity is recorded in real-time from a rain sensor. The combination of sensor and rainfall intensity measurement is designed for studying algorithm performance under different levels of rainfall. In this work, in addition to presenting dataset creation details, we also introduce three degradation evaluation tasks with baseline results, including rainfall intensity estimation, LiDAR degradation estimation, and 2D object detection evaluation. This dataset, development kit, and baseline codes will be made available at https://smart-rain-dataset.github.io/</td>
                <td>Degradation, Rain, Laser radar, Three-dimensional displays, Urban areas, Estimation, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342015&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342015&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Representation for Anomaly Detection of Vehicle Trajectories</td>
                <td>R. Jiao et al.</td>
                <td>2023</td>
                <td>Predicting the future trajectories of surrounding vehicles based on their history trajectories is a critical task in autonomous driving. However, when small crafted perturbations are introduced to those history trajectories, the resulting anomalous (or adversarial) trajectories can significantly mislead the future trajectory prediction module of the ego vehicle, which may result in unsafe planning and even fatal accidents. Therefore, it is of great importance to detect such anomalous trajectories of the surrounding vehicles for system safety, but few works have addressed this issue. In this work, we propose two novel methods for learning effective and efficient representations for online anomaly detection of vehicle trajectories. Different from general time-series anomaly detection, anomalous vehicle trajectory detection deals with much richer contexts on the road and fewer observable patterns on the anomalous trajectories themselves. To address these challenges, our methods exploit contrastive learning techniques and trajectory semantics to capture the patterns underlying the driving scenarios for effective anomaly detection under supervised and unsupervised settings, respectively. We conduct extensive experiments to demonstrate that our supervised method based on contrastive learning and unsupervised method based on reconstruction with semantic latent space can significantly improve the performance of anomalous trajectory detection in their corresponding settings over various baseline methods. We also demonstrate our methods' generalization ability to detect unseen patterns of anomalies.</td>
                <td>Roads, Perturbation methods, Semantics, Trajectory, Safety, Planning, History</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342070&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342070&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lateral-Direction Localization Attack in High-Level Autonomous Driving: Domain-Specific Defense Opportunity via Lane Detection</td>
                <td>J. Shen, Y. Luo, Z. Wan and Q. A. Chen</td>
                <td>2023</td>
                <td>Localization in high-level Autonomous Driving (AD) systems is highly security critical. Recently, researchers found that state-of-the-art Multi-Sensor Fusion (MSF) based localization is vulnerable to GPS spoofing, which can cause road hazards such as driving off road or onto the wrong way. In this work, we perform the first exploration of using Lane Detection (LD) to detect and correct deviations caused by such attacks and design a novel LD-based system-level defense, LD3. We evaluate LD3 on real-world sensor traces and find that it can achieve effective and timely detection against the state-of-the-art attack with 100% true positive rates and 0% false positive rates. Results show that LD3 can be highly effective at steering the AD vehicle to safely stop within the current traffic lane. We implement LD3 on 2 open-source AD systems and validate its end-to-end defense capability using an industry-grade AD simulator and also in the physical world with a real vehicle-sized AD R&D vehicle.</td>
                <td>Location awareness, Lane detection, Roads, Robot sensing systems, Hazards, Security, Autonomous vehicles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342017&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342017&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Safety-Assured Speculative Planning with Adaptive Prediction</td>
                <td>X. Liu, R. Jiao, Y. Wang, Y. Han, B. Zheng and Q. Zhu</td>
                <td>2023</td>
                <td>Recently significant progress has been made in vehicle prediction and planning algorithms for autonomous driving. However, it remains quite challenging for an autonomous vehicle to plan its trajectory in complex scenarios when it is difficult to accurately predict its surrounding vehicles' behaviors and trajectories. In this work, to maximize performance while ensuring safety, we propose a novel speculative planning framework based on a prediction-planning interface that quantifies both the behavior-level and trajectory-level uncertainties of surrounding vehicles. Our framework leverages recent prediction algorithms that can provide one or more possible behaviors and trajectories of the surrounding vehicles with probability estimation. It adapts those predictions based on the latest system states and traffic environment, and conducts planning to maximize the expected reward of the ego vehicle by considering the probabilistic predictions of all scenarios and ensure system safety by ruling out actions that may be unsafe in worst case. We demonstrate the effectiveness of our approach in improving system performance and ensuring system safety over other baseline methods, via extensive simulations in SUMO on a challenging multi-lane highway lane-changing case study.</td>
                <td>Road transportation, Uncertainty, System performance, Prediction algorithms, Probabilistic logic, Planning, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341530&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341530&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Decision Tree-based Monitoring and Recovery Framework for Autonomous Robots with Decision Uncertainties</td>
                <td>R. Peddi and N. Bezzo</td>
                <td>2023</td>
                <td>Autonomous mobile robots (AMR) operating in the real world often need to make critical decisions that directly impact their own safety and the safety of their surroundings. Learning-based approaches for decision making have gained popularity in recent years, since decisions can be made very quickly and with reasonable levels of accuracy for many applications. These approaches, however, typically return only one decision, and if the learner is poorly trained or observations are noisy, the decision may be incorrect. This problem is further exacerbated when the robot is making decisions about its own failures, such as faulty actuators or sensors and external disturbances, when a wrong decision can immediately cause damage to the robot. In this paper, we consider this very case study: a robot dealing with such failures must quickly assess uncertainties and make safe decisions. We propose an uncertainty aware learning-based failure detection and recovery approach, in which we leverage Decision Tree theory along with Model Predictive Control to detect and explain which failure is compromising the system, assess uncertainties associated with the failure, and lastly, find and validate corrective controls to recover the system. Our approach is validated with simulations and real experiments on a faulty unmanned ground vehicle (UGV) navigation case study, demonstrating recovery to safety under uncertainties.</td>
                <td>Uncertainty, Navigation, Decision making, Robot sensing systems, Safety, Sensors, Decision trees</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342207&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342207&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Safety Filter for Realizing Safe Robot Navigation in Crowds</td>
                <td>K. Feng, Z. Lu, J. Xu, H. Chen and Y. Lou</td>
                <td>2023</td>
                <td>It is challenging to realize the safe navigation of mobile robots in crowds. Most of the previous studies may lead to unsafe robot navigation in crowds, as safety guarantee is lacked. To solve this problem, we devise a safety filter (SF) that enables realization of safe robot navigation in crowds, and provides safety guarantees by verifying whether the optimal action recommended by an unsafe method is safe and, if not, corrects the action. The three main processes performed by the SF applied to given robot are (1) construction of the safe state constraints of the robot using a safe set; (2) construction of the safe action constraints of the robot based on discrete-time generalized velocity obstacles (DGVOs); and (3) determination of a feasible solution of the SF design problem, or, if none can be found, replacement of the above hard constraints with heuristic soft constraints. We used the SF with a reaction-based method and three learning-based methods in simulation experiments of random and non-random crowds, and the results showed that the SF decreases the collision rates and danger rates and thereby increases the success rates of these methods. We also deployed the SF with three learning-based methods on an mr1000 robot in real-world experiments, and the results showed that the SF enabled the robot using learning-based methods to navigate to its goal without colliding with humans.</td>
                <td>Learning systems, Navigation, Safety, Mobile robots, Collision avoidance, Robots, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342054&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342054&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Risk-Aware Safe Control for Decentralized Multi-Agent Systems via Dynamic Responsibility Allocation</td>
                <td>Y. Lyu, W. Luo and J. M. Dolan</td>
                <td>2023</td>
                <td>Decentralized control schemes are increasingly favored in various domains that involve multi-agent systems due to the need for computational efficiency as well as general applicability to large-scale systems. However, in the absence of an explicit global coordinator, it is hard for distributed agents to determine how to efficiently interact with others. In this paper, we present a risk-aware decentralized control framework that provides guidance on how much relative responsibility share (a percentage) an individual agent should take to avoid collisions with others while moving efficiently without direct communications. We propose a novel Control Barrier Function (CBF)-inspired risk measurement to characterize the aggregate risk agents face from potential collisions under motion uncertainty. We use this measurement to allocate responsibility shares among agents dynamically and develop risk-aware decentralized safe controllers. In this way, we are able to leverage the flexibility of robots with lower risk to improve the motion flexibility for those with higher risk, thus achieving improved collective safety. We demonstrate the validity and efficiency of our proposed approach through two examples: ramp merging in autonomous driving and a multi-agent position-swapping game.</td>
                <td>Visualization, Uncertainty, Robot kinematics, Measurement uncertainty, Dynamics, Decentralized control, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341720&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341720&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Autonomous Exploration Using Ground Robots with Safety Guarantees</td>
                <td>D. S. Sundarsingh, J. Bhagiya, J. Chatrola and P. Jagtap</td>
                <td>2023</td>
                <td>Autonomous exploration in an unknown environment is widely studied, and many exploration strategies exist. However, in most of the works, safety is not usually given top priority. The reason behind the violation of safety by most of the exploration algorithms in real-world applications is the ignorance of some or all of the following factors (i) the mathematical model of the robot, (ii) practical constraints on states (like constrained steering angle, speed, etc.) and inputs (like actuator saturation), and (iii) hardware constraints such as sampling time, sensor noise, modelling uncertainties, etc. In this work, we propose an autonomous exploration framework for the 2-D exploration problem that considers the factors above to provide safety guarantees for the robot and the environment. The effectiveness of this method is shown in a high-fidelity simulation of different robots with onboard sensors in different simulation environments and real-world implementation.</td>
                <td>Actuators, Uncertainty, Robot sensing systems, Mathematical models, Hardware, Safety, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341929&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341929&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Exploiting the Kinematic Redundancy of a Backdrivable Parallel Manipulator for Sensing During Physical Human-Robot Interaction</td>
                <td>S. Nguyen and C. Gosselin</td>
                <td>2023</td>
                <td>Robots need to adapt their behaviour while physically interacting with an operator to guarantee safety and provide intuitiveness. Inferring the intentions of the operator is a challenging problem that can be addressed by introducing sensors, in addition to motor encoders. Also, kinematic redundancy can be used to avoid issues such as singularities or mechanical interference, and the redundant coordinates can be controlled freely. In this work, we propose to use the redundant degrees of freedom to infer the intentions of an operator interacting with a backdrivable kinematically redundant parallel robot, without introducing any additional sensors. The proposed approach is based on the fact that, in mechanically backdrivable robots, the operator can control the redundant degrees of freedom, and this can be sensed using solely motor encoders through the solution of the forward kinematics. This approach is implemented to switch between a position controller and a controller that allows the operator to guide the robot freely thanks to gravity compensation. Experiments are carried out to compare this approach with an existing one and show that it improves intuitiveness during interaction by reducing false mode change detections.</td>
                <td>Mechanical sensors, Parallel robots, Robot kinematics, Redundancy, Kinematics, Switches, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341495&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341495&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Measuring People's Boredom and Indifference to the Robot's Explanation in a Museum Scenario</td>
                <td>R. Nagaya, S. H. Seo and T. Kanda</td>
                <td>2023</td>
                <td>To personalize the robot guide experience, the robot needs to detect a person's indifference and adjust its explanation toward the person's interest in topics. However, detecting the person's indifference is challenging in a museum, as we cannot use a bulky wearable or facial expression recognition due to unexpected light condition or standing position. We propose to observe people's behaviors and movements on detecting people's indifference. To prove its feasibility, we invited 11 participants to our in-lab museum-like environment. Our robot explains exhibits while videorecording the interaction. Then, we asked participants to watch the recordings and report when they felt bored or indifferent to the explanation. We labelled their movement and matched them to their report so that we know which behaviors and movements hint the person's indifference. We used the decision tree and random forest methods to understand the common pattern when people are indifferent during the explanation in a museum scenario. From our observation experiment, we found that if the listener nods their heads many times or looks at the exhibit for a long time, they are likely interested in the topic, fewer overall movements or looking elsewhere hint that the listener may be indifferent, and if the explanation goes longer than three minutes, the listener would be likely bored.</td>
                <td>Face recognition, Museums, Real-time systems, Behavioral sciences, Recording, Decision trees, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341996&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341996&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Is Weakly-Supervised Action Segmentation Ready for Human-Robot Interaction? No, Let's Improve It with Action-Union Learning</td>
                <td>F. Yang, S. Odashima, S. Masui and S. Jiang</td>
                <td>2023</td>
                <td>Action segmentation plays an important role in enabling robots to automatically understand human activities. To train the action recognition model, while obtaining action labels for all frames is costly, annotating timestamp labels for weak supervision is cost-effective. However, existing methods may not fully utilize timestamp labels, which leads to insufficient performance. To alleviate this issue, we proposed a novel learning pattern in our training stage, which maximizes the probability of action union of surrounding timestamps for unlabeled frames. In our inference stage, we provided a new refinement solution to generate better hard-assigned action classes from soft-assigned predictions. Importantly, our methods are model-agnostic and can be applied to existing frameworks. On three commonly used action-segmentation data, our method outperforms previous timestamp-supervision methods and achieves new state-of-the-art performance. More-over, our method uses less than 1% of fully-supervised labels to obtain comparable or even better results.</td>
                <td>Training, Human-robot interaction, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341931&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341931&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Co-Speech Gesture Synthesis using Discrete Gesture Token Learning</td>
                <td>S. Lu, Y. Yoon and A. Feng</td>
                <td>2023</td>
                <td>Synthesizing realistic co-speech gestures is an important and yet unsolved problem for creating believable motions that can drive a humanoid robot to interact and communicate with human users. Such capability will improve the impressions of the robots by human users and will find applications in education, training, and medical services. One challenge in learning the co-speech gesture model is that there may be multiple viable gesture motions for the same speech utterance. The deterministic regression methods can not resolve the conflicting samples and may produce over-smoothed or damped motions. We proposed a two-stage model to address this uncertainty issue in gesture synthesis by modeling the gesture segments as discrete latent codes. Our method utilizes RQ- VAE in the first stage to learn a discrete codebook consisting of gesture tokens from training data. In the second stage, a two-level autoregressive transformer model is used to learn the prior distribution of residual codes conditioned on input speech context. Since the inference is formulated as token sampling, multiple gesture sequences could be generated given the same speech input using top-k sampling. The quantitative results and the user study showed the proposed method outperforms the previous methods and is able to generate realistic and diverse gesture motions.</td>
                <td>Training, Codes, Uncertainty, Speech coding, Motion segmentation, Training data, Medical services</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342027&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342027&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>WSCFER: Improving Facial Expression Representations by Weak Supervised Contrastive Learning</td>
                <td>W. Nie, B. Chen, W. Wu, X. Xu, W. Ren and H. Liu</td>
                <td>2023</td>
                <td>The major challenge of Facial Expression Recog-nition (FER) is to learn class discriminative representations, and the existing works mainly address it by designing various classification networks from class level. However, learning representations at class level is limited due to the inconspicuous class discrimination among different facial expressions. Thus, in this paper, we propose a Weak Supervised Contrastive learning FER (WSCFER) method to improve facial expression representations by simultaneously learning instance-level representations which are highly complementary to the general class-level representations. Specifically, our proposed WSCFER consists of three components: a major task for FER classification, an auxiliary task for Weak Supervised Contrastive (WSC) learning which pulls augmented samples of the same image together while pushing apart instance samples from different classes, and a Partial Consistency Loss (PCL) for optimizing the two embedding spaces from both the class level and the instance level. We compare WSC with some state-of-the-art contrastive methods and find that it can efficiently learn instance-level representations but avoid overemphasizing irrelevant parts, which is crucial for FER. WSCFER achieves superior performance on several in-the-wild databases, and it also shows the promising potential for learning representations under noisy annotations.</td>
                <td>Learning systems, Databases, Annotations, Reliability, Noise measurement, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342450&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342450&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Expressing and Inferring Action Carefulness in Human-to-Robot Handovers</td>
                <td>L. Lastrico et al.</td>
                <td>2023</td>
                <td>Implicit communication plays such a crucial role during social exchanges that it must be considered for a good experience in human-robot interaction. This work addresses implicit communication associated with the detection of physical properties, transport, and manipulation of objects. We propose an ecological approach to infer object characteristics from subtle modulations of the natural kinematics occurring during human object manipulation. Similarly, we take inspiration from human strategies to shape robot movements to be communica-tive of the object properties while pursuing the action goals. In a realistic HRI scenario, participants handed over cups - filled with water or empty - to a robotic manipulator that sorted them. We implemented an online classifier to differentiate careful/not careful human movements, associated with the cups' content. We compared our proposed “expressive” controller, which modulates the movements according to the cup filling, against a neutral motion controller. Results show that human kinematics is adjusted during the task, as a function of the cup content, even in reach-to-grasp motion. Moreover, the carefulness during the handover of full cups can be reliably inferred online, well before action completion. Finally, although questionnaires did not reveal explicit preferences from partici-pants, the expressive robot condition improved task efficiency.</td>
                <td>Robot motion, Shape, Human-robot interaction, Transportation, Kinematics, Handover, Filling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342111&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342111&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Open-Loop Saccadic Control of a 3D Biomimetic Eye Using the Actor-Critic Algorithm</td>
                <td>H. Granado, R. J. Alitappeh, A. John, A. J. Van Opstal and A. Bernardino</td>
                <td>2023</td>
                <td>The application of reinforcement learning algorithms to robotics has increased over the last decade, especially for the control of robots with non-linear dynamics and a redundant number of degrees of freedom using classic control techniques. Here we study the control of a biomimetic robotic eye with three extraocular muscle pairs as a prime example. Using an actor-critic algorithm, this paper aims to link reinforcement learning to this control problem, and create a framework that will learn the open-loop control of saccadic movements of the robotic eye. The basis for the implemented control is inspired by the primate physiological pulsed control signal, which is generated, integrated and sent to the appropriate muscles to perform the saccade. The metric that evaluates the saccadic output is also inspired by the primate oculomotor system and is used to shape the reward function. This methodology was applied to a simplified 3D physical model of the human eye as a proof of concept. The algorithm managed to learn a saccadic control strategy in 3D. The trajectories obtained, have similar non-linear dynamics as those recorded in humans and their 3D rotational kinematics are constrained by Listing's law.</td>
                <td>Measurement, Weight measurement, Solid modeling, Three-dimensional displays, Heuristic algorithms, Biomimetics, Biological system modeling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341913&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341913&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Communicating human intent to a robotic companion by multi-type gesture sentences</td>
                <td>P. Vanc, J. K. Behrens, K. Stepanova and V. Hlavac</td>
                <td>2023</td>
                <td>Human-Robot collaboration in home and industrial workspaces is on the rise. However, the communication between robots and humans is a bottleneck. Although people use a combination of different types of gestures to complement speech, only a few robotic systems utilize gestures for communication. In this paper, we propose a gesture pseudo-language and show how multiple types of gestures can be combined to express human intent to a robot (i.e., expressing both the desired action and its parameters - e.g., pointing to an object and showing that the object should be emptied into a bowl). The demonstrated gestures and the perceived tabletop scene (object poses detected by CosyPose) are processed in real-time) to extract the human's intent. We utilize behavior trees to generate reactive robot behavior that handles various possible states of the world (e.g., a drawer has to be opened before an object is placed into it) and recovers from errors (e.g., when the scene changes). Furthermore, our system enables switching between direct teleoperation of the end-effector and high-level operation using the proposed gesture sentences. The system is evaluated on increasingly complex tasks using a real 7-DoF Franka Emika Panda manipulator. Controlling the robot via action gestures lowered the execution time by up to 60%, compared to direct teleoperation.</td>
                <td>Service robots, Collaboration, Switches, Real-time systems, End effectors, Behavioral sciences, Task analysis, Human-robot collaboration, Intent recognition, Scene awareness, Gesture detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341944&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341944&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MoEmo Vision Transformer: Integrating Cross-Attention and Movement Vectors in 3D Pose Estimation for HRI Emotion Detection</td>
                <td>D. C. Jeong et al.</td>
                <td>2023</td>
                <td>Emotion detection presents challenges to intelligent human-robot interaction (HRI). Foundational deep learning techniques used in emotion detection are limited by information-constrained datasets or models that lack the necessary complexity to learn interactions between input data elements, such as the the variance of human emotions across different contexts. In the current effort, we introduce 1) MoEmo (Motion to Emotion), a cross-attention vision transformer (ViT) for human emotion detection within robotics systems based on 3D human pose estimations across various contexts, and 2) a data set that offers full-body videos of human movement and corresponding emotion labels based on human gestures and environmental contexts. Compared to existing approaches, our method effectively leverages the subtle connections between movement vectors of gestures and environmental contexts through the use of cross-attention on the extracted movement vectors of full-body human gestures/poses and feature maps of environmental contexts. We implement a cross-attention fusion model to combine movement vectors and environment contexts into a joint representation to derive emotion estimation. Leveraging our Naturalistic Motion Database, we train the MoEmo system to jointly analyze motion and context, yielding emotion detection that outperforms the current state-of-the-art.</td>
                <td>Emotion recognition, Three-dimensional displays, Databases, Pose estimation, Human-robot interaction, Feature extraction, Transformers, Data mining, Task analysis, Context modeling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342417&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342417&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>How Do Humans Provide Motion Assistance for a Robotic Shape-Tracing Task?</td>
                <td>T. M. Higgins and A. M. Fey</td>
                <td>2023</td>
                <td>Often in the field of haptic guidance, an important question is how the robotic device should assist some imperfect human movement. While many control strategies have been suggested to help improve human performance in particular tasks, structuring guidance in a generalizable way remains elusive. Many assistive controllers rely on predicting a user's goal movement or knowing some idealized trajectory a-priori but may fail to assist during an arbitrary task. In this study, we propose a ‘flipped’ approach to studying human-robot collaborative behavior - we ask humans to assist a robotic device whose movements are in some way imperfect. We conducted an experiment during which subjects assisted a haptic device performing a shape-following task autonomously but with different types of error-prone controllers. For each shape, we evaluated a simple trajectory-following controller as well as one with human-like motion constraints. We also evaluated the role of visual feedback on a user's ability to help the robot accomplish the unknown task. We found that the human was generally able to improve the robotic error in all trajectories when the robotic motion did not include gravity compensation; however, error reduction was primarily in the vertical direction. When the robotic controller included gravity compensation, the human user was not able to improve errors significantly, except for vertical errors when provided visual feedback. In the no visual feedback conditions, the human user contributed to significantly greater error for most paths compared to a robot with gravity compensation, indicating an inability to provide assistance in that case.</td>
                <td>Performance evaluation, Robot motion, Visualization, Shape, Trajectory, Haptic interfaces, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341499&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341499&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>The Effects of Robot Motion on Comfort Dynamics of Novice Users in Close-Proximity Human-Robot Interaction</td>
                <td>P. Howell, J. Kolb, Y. Liu and H. Ravichandar</td>
                <td>2023</td>
                <td>Effective and fluent close-proximity human-robot interaction requires understanding how humans get habituated to robots and how robot motion affects human comfort. While prior work has identified humans' preferences over robot motion characteristics and studied their influence on comfort, we are yet to understand how novice first-time robot users get habituated to robots and how robot motion impacts the dynamics of comfort over repeated interactions. To take the first step towards such understanding, we carry out a user study to investigate the connections between robot motion and user comfort and habituation. Specifically, we study the influence of workspace overlap, end-effector speed, and robot motion legibility on overall comfort and its evolution over repeated interactions. Our analyses reveal that workspace overlap, in contrast to speed and legibility, has a significant impact on users' perceived comfort and habituation. In particular, lower workspace overlap leads to users reporting significantly higher overall comfort, lower variations in comfort, and fewer fluctuations in comfort levels during habituation.</td>
                <td>Robot motion, Measurement, Fluctuations, Dynamics, Sociology, Human-robot interaction, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341487&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341487&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Estimating Human Comfort Levels in Autonomous Vehicles Based on Vehicular Behaviors and Physiological Signals</td>
                <td>H. Su and Y. Jia</td>
                <td>2023</td>
                <td>In this study, we proposed a dynamic model that could quantify human comfort in autonomous vehicles (AVs) based on vehicular behaviors and a Kalman filter (KF) based approach to further refine comfort level estimation by leveraging physiological signals. The dynamic model could capture the dynamics in human comfort when the passenger was exposed to a continuous sequence of vehicular behaviors during an AV journey. The KF-based comfort estimation approach could fuse comfort level estimations based on physiological signals and the dynamic model. A simulator-based user study was conducted to evaluate the comfort estimation approaches in which the participants experienced a set of virtual AV journeys on a high-fidelity driving simulator with 6-degree-of-freedom motions. Experimental results show that the proposed approaches could quantify human comfort levels and the KF-based approach outperforms the others.</td>
                <td>Fuses, Heuristic algorithms, Dynamics, Estimation, Particle measurements, Physiology, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342480&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342480&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>EELS: Towards Autonomous Mobility in Extreme Terrain with a Versatile Snake Robot with Resilience to Exteroception Failures</td>
                <td>R. Thakker et al.</td>
                <td>2023</td>
                <td>The discovery of ocean worlds such as Enceladus, Titan, and Europa motivates the development of versatile autonomous mobility systems to enable the next era of space exploration where there is large uncertainty in terrain specifications due to a lack of prior surface reconnaissance missions. To explore these environments, we propose Exobiology Extant Life Surveyor (EELS): the first large-scale (4 lm long with 400 Nm peak torque) snake robot. The large scale is achieved by using a screw-based active skin mechanism to decouple motion and shape control. Autonomous mobility for such a system remains an open problem due to its many Degrees of Freedom (DoFs), complex terrain interactions, and intermittent localization failures in GPS-denied perceptually degraded environments due to the presence of fog, dust, featureless terrains, etc. We propose NEO, an autonomy architecture that scales to large DoFs to generate a versatile set of gaits to achieve mobility in unknown extreme environments. We also discuss the resilience capabilities of NEO that achieves closed-loop tracking performance by leveraging exteroception when available but can also operate with proprioception only, leading to resiliency against localization failures via graceful degradation in performance rather than unsafe behaviors. A quantitative hardware evaluation of exteroceptive leader-follower gait is performed indoors on synthetic ice along with qualitative results of field deployment of the proprioceptive leader-follower and sidewinding gaits in extreme environments of icy and sandy terrains with mobility-stressing elements such as trenches, undulations, and steep slopes (up to 35 degrees). We present a set of lessons learned from field deployments with a summary of challenges and open research problems. Video: www.rohanthakker.in/eels-neo-autonomy.html</td>
                <td>Location awareness, Sea surface, Uncertainty, Torque, Shape control, Saturn, Snake robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341448&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341448&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Track, Stop, and Eliminate: an Algorithm to Solve Stochastic Orienteering Problems Using MCTS</td>
                <td>C. D. Alvarenga and S. Carpin</td>
                <td>2023</td>
                <td>We present a novel algorithm to solve the stochastic orienteering problem with chance constraints that combines Monte Carlo Tree Search (MCTS) with a best arm identification (BAI) algorithm. This method extends our recently proposed solution that builds a search planning tree considering both an objective function to maximize, as well as a chance constraint on the failure probability, i.e., the probability of violating the assigned budget constraint. By combining these two approaches, we obtain a new planner that tunes the amount of tree search at run time. Extensive simulation results on our benchmark problems show that the new approach is significantly faster than the previous one, while incurring in just marginal decrements in terms of performance.</td>
                <td>Monte Carlo methods, Simulation, Benchmark testing, Search problems, Linear programming, Planning, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341892&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341892&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DVL-Based Odometry for Autonomous Underwater Gliders</td>
                <td>G. Billings, A. Phung and R. Camilli</td>
                <td>2023</td>
                <td>Autonomous underwater gliders (AUGs) are capable of traversing basin scale distances but lack sufficient localization accuracy to operate without periodic surfacing to obtain GPS fixes and constrain localization drift. Conventionally, AUGs use a dynamic flight model with depth averaged current correction (DACC) to dead-reckon their position while subsurface. However, these flight models become unstable at shallow pitch angles and DACC is inaccurate in dynamic and highly sheared water column currents. We present a method and preliminary results from field trials for improved real-time AUG localization using a Doppler Velocity Logger to estimate vehicle velocity and dynamically profile water column currents. This improved localization reduces the need for periodic surfacing, while independence from a dynamic flight model makes it particularly suited for shallow-yo profile missions.</td>
                <td>Location awareness, Surveys, Uncertainty, Computational modeling, Sea measurements, Real-time systems, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341554&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341554&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Terrain-Aware Kinodynamic Planning with Efficiently Adaptive State Lattices for Mobile Robot Navigation in Off-Road Environments</td>
                <td>E. R. Damm, J. M. Gregory, E. S. Lancaster, F. A. Sanchez, D. M. Sahu and T. M. Howard</td>
                <td>2023</td>
                <td>To safely traverse non-flat terrain, robots must account for the influence of terrain shape in their planned motions. Terrain-aware motion planners use an estimate of the vehicle roll and pitch as a function of pose, vehicle suspension, and ground elevation map to weigh the cost of edges in the search space. Encoding such information in a traditional two-dimensional cost map is limiting because it is unable to capture the influence of orientation on the roll and pitch estimates from sloped terrain. The research presented herein addresses this problem by encoding kinodynamic information in the edges of a recombinant motion planning search space based on the Efficiently Adaptive State Lattice (EASL). This approach, which we describe as a Kinodynamic Efficiently Adaptive State Lattice (KEASL), differs from the prior representation in two ways. First, this method uses a novel encoding of velocity and acceleration constraints and vehicle direction at expanded nodes in the motion planning graph. Second, this approach describes additional steps for evaluating the roll, pitch, constraints, and velocities associated with poses along each edge during search in a manner that still enables the graph to remain recombinant. Velocities are computed using an iterative bidirectional method using Eulerian integration that more accurately estimates the duration of edges that are subject to terrain-dependent velocity limits. Real-world experiments on a Clearpath Robotics Warthog Unmanned Ground Vehicle were performed in a non-flat, unstructured environment. Results from 2093 planning queries from these experiments showed that KEASL provided a more efficient route than EASL in 83.72% of cases when EASL plans were adjusted to satisfy terrain-dependent velocity constraints. An analysis of relative runtimes and differences between planned routes is additionally presented. These results reinforce the importance of considering kinodynamic constraints for motion planning in non-flat environments and illustrate how such information can be encoded in an adaptive recombinant motion planning search space.</td>
                <td>Space vehicles, Suspensions (mechanical systems), Costs, Navigation, Shape, Government, Lattices</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341537&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341537&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Evaluation of Underwater AprilTag Localization for Highly Agile Micro Underwater Robots</td>
                <td>N. Bauschmann, D. A. Duecker, T. L. Alff and R. Seifried</td>
                <td>2023</td>
                <td>Accurate localization systems are still a bottleneck for Unmanned Underwater Vehicles (UUVs). In recent years, fiducial markers have become a readily available, low-cost option. However, an in-depth analysis of marker detection accuracy in the underwater domain has yet to be performed. We propose a methodology to evaluate fiducial marker systems, namely the popular AprilTag system, in experiments. Our study especially focuses on aspects crucial for highly agile micro underwater robots, such as dynamic motions and the calibration medium. This class of robots is typically extensively studied in research tanks which motivates a first focus on clear-water settings. However, the proposed method and the findings can be transferred to similar scenarios. We demonstrate the importance of calibrating underwater and that the detection accuracy decreases linearly with camera distance and could therefore easily be compensated for. Moreover, we identify a suitable camera that maximizes the detection rate during highly dynamic motions. In sum, this work is an initial step towards application-relevant design strategies for designing low-cost, accessible localization systems for agile, mobile robots.</td>
                <td>Location awareness, Autonomous underwater vehicles, Visualization, Robot vision systems, Dynamics, Cameras, Fiducial markers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341764&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341764&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Weakly Supervised Caveline Detection for AUV Navigation Inside Underwater Caves</td>
                <td>B. Yu, R. Tibbetts, T. Barna, A. Morales, I. Rekleitis and M. J. Islam</td>
                <td>2023</td>
                <td>Underwater caves are challenging environments that are crucial for water resource management, and for our understanding of hydro-geology and history. Mapping underwater caves is a time-consuming, labor-intensive, and hazardous operation. For autonomous cave mapping by underwater robots, the major challenge lies in vision-based estimation in the complete absence of ambient light, which results in constantly moving shadows due to the motion of the camera-light setup. Thus, detecting and following the caveline as navigation guidance is paramount for robots in autonomous cave mapping missions. In this paper, we present a computationally light caveline detection model based on a novel Vision Transformer (ViT)-based learning pipeline. We address the problem of scarce annotated training data by a weakly supervised formulation where the learning is reinforced through a series of noisy predictions from intermediate sub-optimal models. We validate the utility and effectiveness of such weak supervision for caveline detection and tracking in three different cave locations: USA, Mexico, and Spain. Experimental results demonstrate that our proposed model, CL-ViT, balances the robustness-efficiency trade-off, ensuring good generalization performance while offering 10+ FPS on single-board (Jetson TX2) devices.</td>
                <td>Navigation, Computational modeling, Pipelines, Training data, Predictive models, Transformers, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342435&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342435&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Wireless Network Demands of Data Products from Small Uncrewed Aerial Systems at Hurricane Ian</td>
                <td>T. Manzini, R. Murphy, D. Merrick and J. Adams</td>
                <td>2023</td>
                <td>Data collected at Hurricane Ian (2022) quantifies the demands that small uncrewed aerial systems (UAS), or drones, place on the network communication infrastructure and identifies gaps in the field. Drones have been increasingly used since Hurricane Katrina (2005) for disaster response, however getting the data from the drone to the appropriate decision makers throughout incident command in a timely fashion has been problematic. These delays have persisted even as countries such as the USA have made significant investments in wireless infrastructure, rapidly deployable nodes, and an increase in commercial satellite solutions. Hurricane Ian serves as a case study of the mismatch between communications needs and capabilities. In the first four days of the response, nine drone teams flew 34 missions under the direction of the State of Florida FL-UAS1, generating 636GB of data. The teams had access to six different wireless communications networks but had to resort to physically transferring data to the nearest intact emergency operations center in order to make the data available to the relevant agencies. The analysis of the mismatch contributes a model of the drone data-to-decision workflow in a disaster and quantifies wireless network communication requirements throughout the workflow in five factors. Four of the factors-availability, bandwidth, burstiness, and spatial distribution-were previously identified from analyses of Hurricanes Harvey (2017) and Michael (2018). This work adds upload rate as a fifth attribute. The analysis is expected to improve drone design and edge computing schemes as well as inform wireless communication research and development.</td>
                <td>Cloud computing, Satellites, Wireless networks, Bandwidth, Streaming media, Autonomous aerial vehicles, Hurricanes</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342413&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342413&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robot Learning to Mop Like Humans Using Video Demonstrations</td>
                <td>S. Gaurav et al.</td>
                <td>2023</td>
                <td>Though mopping the floor is a mundane and tedious daily task, enabling robots to perform it comparably to humans remains a challenge. Hand-coding desired mopping behaviors for variable surfaces and situations is particularly difficult. In this paper, we develop a robotic system for mopping the floor by mimicking the human behavior demonstrated in videos. Our baseline robotic system uses traditional computer vision techniques for tracking and inverse kinematics. Our proposed robot mop learning system comprises advanced computer vision techniques, Time Contrastive Network (TCN), and reinforcement learning. Using these, we devise a reward function for the mopping task. We use a Universal 10e robotic arm attached to a mop to perform the mopping task and a first-person camera attached on top of the robotic arm to provide feedback for robotic learning. We evaluate our proposed robot mop learning system's imitative similarity using optical flow, distance in mop location, and force applied to the floor, as well as cleaning efficiency using a white glove method.</td>
                <td>Computer vision, Robot vision systems, Reinforcement learning, Manipulators, Robot learning, Behavioral sciences, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342231&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342231&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>AcTag: Opti-Acoustic Fiducial Markers for Underwater Localization and Mapping</td>
                <td>K. Norman, D. Butterfield and J. G. Mangelson</td>
                <td>2023</td>
                <td>Fiducial markers are important tools for robotic navigation and imaging, enabling accurate localization and tracking of objects in challenging environments. In this paper, we present AcTag, a new fiducial marker design for use underwater with imaging sonar and cameras, as well as a method for the detection of AcTags within acoustic images. High amounts of noise and a nonlinear projection model make it difficult to use imaging sonar in autonomous localization and mapping. In order to expand the use of imaging sonar in autonomous underwater vehicles, our markerb design and detection algorithm for sonar images facilitate the identification of four unique landmarks per tag, and provide relative range and azimuth values to each landmark. We evaluate our marker and detection algorithm with simulated and real-world sonar data, reporting on the false positive and true positive rates, as well as the estimated error for the range and azimuth estimates per landmark. We also release an open-source library for generating tag families and detecting the tags.</td>
                <td>Location awareness, Azimuth, Aluminum, Sonar, Sonar navigation, Cameras, Fiducial markers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341885&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341885&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Agent Collective Construction Using 3D Decomposition</td>
                <td>A. K. Srinivasan, S. Singh, G. Gutow, H. Choset and B. Vundurthy</td>
                <td>2023</td>
                <td>Consider a Multi-Agent Collective Construction (MACC) problem that aims to generate a plan for fictitious cubic robots to build a three-dimensional structure comprised of cubic blocks. These cubic robots can carry one cubic block at a time; robots may move left, right, forwards, backward, or climb up or down one block. To construct structures taller than one cube, the robots must build supporting scaffolding made of blocks and remove the scaffolding once the structure is built. Prior works sought to create a planner that considered the structure as one monolithic assembly, which becomes intractable for larger workspaces and complex structures. To this end, we present a decomposition algorithm that breaks the structure into substructures that can be planned for independently. We use Mixed Integer Linear Programming (MILP) to plan for each of these substructures and then aggregate the solutions to construct the entire structure. Extensive testing on 200 randomly generated structures shows an order of magnitude improvement in the solution computation time compared to an MILP approach without decomposition. Finally, we leverage the independence between substructures to detect which substructures can be built in parallel.</td>
                <td>Measurement, Three-dimensional displays, Aggregates, Parallel processing, Mixed integer linear programming, Optimization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341964&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341964&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Daily Assistive Modular Robot Design Based on Multi-Objective Black-Box Optimization</td>
                <td>K. Kawaharazuka, T. Makabe, K. Okada and M. Inaba</td>
                <td>2023</td>
                <td>The range of robot activities is expanding from industries with fixed environments to diverse and changing environments, such as nursing care support and daily life support. In particular, autonomous construction of robots that are personalized for each user and task is required. Therefore, we develop an actuator module that can be reconfigured to various link configurations, can carry heavy objects using a locking mechanism, and can be easily operated by human teaching using a releasing mechanism. Given multiple target coordinates, a modular robot configuration that satisfies these coordinates and minimizes the required torque is automatically generated by Tree-structured Parzen Estimator (TPE), a type of black-box optimization. Based on the obtained results, we show that the robot can be reconfigured to perform various functions such as moving monitors and lights, serving food, and so on.</td>
                <td>Actuators, Torque, Service robots, Robot kinematics, Education, Closed box, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342041&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342041&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Unified Perspective on Multiple Shooting In Differential Dynamic Programming</td>
                <td>H. Li, W. Yu, T. Zhang and P. M. Wensing</td>
                <td>2023</td>
                <td>Differential Dynamic Programming (DDP) is an efficient computational tool for solving nonlinear optimal control problems. It was originally designed as a single shooting method and thus is sensitive to the initial guess supplied. This work considers the extension of DDP to multiple shooting (MS), improving its robustness to initial guesses. A novel derivation is proposed that accounts for the defect between shooting segments during the DDP backward pass, while still maintaining quadratic convergence locally. The derivation enables unifying multiple previous MS algorithms, and opens the door to many smaller algorithmic improvements. A penalty method is introduced to strategically control the step size, further improving the convergence performance. An adaptive merit function and a more reliable acceptance condition are employed for globalization. The effects of these improvements are benchmarked for trajectory optimization with a quadrotor, an acrobot, and a manipulator. MS-DDP is also demonstrated for use in Model Predictive Control (MPC) for dynamic jumping with a quadruped robot, showing its benefits over a single shooting approach.</td>
                <td>Heuristic algorithms, Optimal control, Robustness, Dynamic programming, Quadrupedal robots, Trajectory optimization, Manipulator dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342217&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342217&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Data-Driven Distributionally Robust Optimal Control with State-Dependent Noise</td>
                <td>R. Liu, G. Shi and P. Tokekar</td>
                <td>2023</td>
                <td>Distributionally Robust Optimal Control (DROC) is a technique that enables robust control in a stochastic setting when the true distribution is not known. Traditional DROC approaches require given ambiguity sets or a KL divergence bound to represent the distributional uncertainty. These may not be known a priori and may require hand-crafting. In this paper, we lift this assumption by introducing a data-driven technique for estimating the uncertainty and a bound for the KL divergence. We call this technique D3ROC. To evaluate the effectiveness of our approach, we consider a navigation problem for a car-like robot with unknown noise distributions. The results demonstrate that D3ROC provides robust and efficient control policies that outperform the iterative Linear Quadratic Gaussian (iLQG) control. The results also show the effectiveness of our proposed approach in handling different noise distributions.</td>
                <td>Robust control, Maximum likelihood estimation, Uncertainty, Navigation, Robot kinematics, Optimal control, Iterative methods</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342392&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342392&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Mangasarian-Soldov Function Based Neural Network for Constrained Control of Parallel and Serial Robots</td>
                <td>W. Li, Y. Zou, Z. Yi, H. Wu and Y. Pan</td>
                <td>2023</td>
                <td>Zeroing neural networks (ZNNs) are powerful alternatives to solving quadratic programming (QP) for constrained control of parallel and serial robots. A recent study showed that a ZNN solver designed based on a perturbed Fischer-Burmeister function (pFB-ZNN) achieves more satisfactory performance than other ZNN solvers. The pFB-ZNN solver suffers from manual tuning of an extra hyper-parameter and may encounter residual error peaks. To tackle the above issues, this paper proposes a new Mangasarian-Solodov function-based ZNN (MS-ZNN) solver. The MS-ZNN solver has no extra hyper-parameter to be tuned and it can eliminate residual error peaks appeared in the pFB-ZNN solver, ensuring a higher solution accuracy. Mathematically, this paper details the design and convergence analysis of the MS-ZNN solver, demonstrating its convergence in the sense of Lyapunov. Numerical studies are comparatively performed, verifying the effectiveness and superiority of the MS-ZNN solver. The MS-ZNN solver is then successfully applied to kinematic control of a parallel robot and a serial robot under joint constraints. Both simulative and experimental results demonstrate that the proposed MS-ZNN solver is applicable to constrained control of parallel and serial robots with joint-limit avoidance achieved.</td>
                <td>Parallel robots, Neural networks, Robot control, Manuals, Kinematics, Robot sensing systems, Quadratic programming</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341741&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341741&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Controller and Trajectory Optimization for a Quadrotor UAV with Parametric Uncertainty</td>
                <td>A. Srour, A. Franchi and P. R. Giordano</td>
                <td>2023</td>
                <td>In this work, we exploit the recent notion of closed-loop state sensitivity to critically compare three typical controllers for a quadrotor UAV with the goal of evaluating the impact of controller choice, gain tuning and shape of the reference trajectory in minimizing the sensitivity of the closed-loop system against uncertainties in the model parameters. To this end, we propose a novel optimization problem that takes into account both the shape of the reference trajectory and the controller gains. We then run a large statistical campaign for comparing the performance of the three controllers which provides some interesting insight for the goal of increasing closed-loop robustness against parametric uncertainties.</td>
                <td>Uncertainty, Sensitivity, Shape, Statistical analysis, Autonomous aerial vehicles, Robustness, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341739&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341739&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time NMPC for an Automated Valet Parking with Load-Based Safety Constraints and a Path-Parametric Model</td>
                <td>B. B. Carlos, M. Williams and B. Pelourdeau</td>
                <td>2023</td>
                <td>As global living standards continue to rise and urbanization accelerates, cities worldwide face high demands for parking. To tackle this issue and foster the efficient use of limited parking capacity, automated valet parking (AVP) solutions have emerged as a resource optimization and hassle-free alternative. This paper addresses the problem of reducing retrieval time in AVP systems while ensuring the integrity of both the robot and the car through critical adherence conditions. The key novelty of our approach is that we dovetail these conditions as acceleration constraints into a multi-stage motion generation framework formulated as a real-time nonlinear model predictive control (NMPC) scheme that approximates time-optimal behavior by maximizing progress on a path. The NMPC generates trajectories based on the motion direction, allowing for convenient parameterization of controller instances and unlimited parking maneuvers. Thanks to high-performance software implementations, the resulting quadratic subprograms can be solved in the order of milliseconds. Comparative analysis against the currently implemented pure pursuit algorithm and an experimental validation on Stanley Robotics' robot have shown a 31% time improvement following a path and constraint satisfaction in the loaded scenario for the proposed controller.</td>
                <td>Urban areas, Real-time systems, Software, Trajectory, Safety, Standards, Pursuit algorithms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342085&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342085&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Time-Optimal Point-To-Point Motion Planning and Assembly Mode Change of Cuspidal Manipulators: Application to 3R and 6R Robots</td>
                <td>T. Marauli, D. H. Salunkhe, H. Gattringer, A. Müller, D. Chablat and P. Wenger</td>
                <td>2023</td>
                <td>The kinematics of cuspidal 3R regional robots was studied extensively in the past. Moreover, certain industrial 6R robots were found to be cuspidal (e.g. Fanuc CRX series, Kinova GEN2), which makes cuspidal robots finally interesting for practical applications. This necessitates optimal trajectory planning, respecting the dynamics and technical limits of the particular robot. In this paper, a method for singularity-free time-optimal point-to-point trajectory (PtP) trajectory planning is proposed. As a special case, this method is applicable to time-optimal singularity-free assembly mode changing. Results are shown for 3R robots and a 6R Fanuc CRX10iA/L.</td>
                <td>Trajectory planning, Service robots, Welding, Kinematics, Manipulators, Trajectory, Planning, Singularities, Cuspidality, Optimal Control, Dynamics, Kinematics, Industrial Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341420&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341420&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>End-to-End Learning of Behavioural Inputs for Autonomous Driving in Dense Traffic</td>
                <td>J. Shrestha, S. Idoko, B. Sharma and A. K. Singh</td>
                <td>2023</td>
                <td>Trajectory sampling in the Frenet(road-aligned) frame, is one of the most popular methods for motion planning of autonomous vehicles. It operates by sampling a set of behavioral inputs, such as lane offset and forward speed, before solving a trajectory optimization problem conditioned on the sampled inputs. The sampling is handcrafted based on simple heuristics, does not adapt to driving scenarios, and is oblivious to the capabilities of downstream trajectory planners. In this paper, we propose an end-to-end learning of behavioral input distribution from expert demonstrations or in a self-supervised manner. We embed a novel differentiable trajectory optimizer as a layer in neural networks, allowing us to update behavioral inputs by considering the optimizer's feedback. Moreover, our end-to-end approach also ensures that the learned behavioral inputs aid the convergence of the optimizer. We improve the state-of-the-art in the following aspects. First, we show that learned behavioral inputs substantially decrease collision rate while improving driving efficiency over handcrafted approaches. Second, our approach outperforms model predictive control methods based on sampling-based optimization.</td>
                <td>Neural networks, Behavioral sciences, Planning, Autonomous vehicles, Trajectory optimization, Intelligent robots, Predictive control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341439&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341439&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Gaussian Max-Value Entropy Search for Multi-Agent Bayesian Optimization</td>
                <td>H. Ma, T. Zhang, Y. Wu, F. P. Calmon and N. Li</td>
                <td>2023</td>
                <td>We study the multi-agent Bayesian optimization (BO) problem, where multiple agents maximize a black-box function via iterative queries. We focus on Entropy Search (ES), a sample-efficient BO algorithm that selects queries to maximize the mutual information about the maximum of the black-box function. One of the main challenges of ES is that calculating the mutual information requires computationallycostly approximation techniques. For multi-agent BO problems, the computational cost of ES is exponential in the number of agents. To address this challenge, we propose the Gaussian Max-value Entropy Search, a multi-agent BO algorithm with favorable sample and computational efficiency. The key to our idea is to use a normal distribution to approximate the function maximum and calculate its mutual information accordingly. The resulting approximation allows queries to be cast as the solution of a closed-form optimization problem which, in turn, can be solved via a modified gradient ascent algorithm and scaled to a large number of agents. We demonstrate the effectiveness of Gaussian max-value Entropy Search through numerical experiments on standard test functions and real-robot experiments on the source seeking problem. Results show that the proposed algorithm outperforms the multi-agent BO baselines in the numerical experiments and can stably seek the source with a limited number of noisy observations on real robots.</td>
                <td>Closed box, Gaussian distribution, Approximation algorithms, Search problems, Entropy, Computational efficiency, Bayes methods</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341675&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341675&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FATROP: A Fast Constrained Optimal Control Problem Solver for Robot Trajectory Optimization and Control</td>
                <td>L. Vanroye, A. Sathya, J. De Schutter and W. Decré</td>
                <td>2023</td>
                <td>Trajectory optimization is a powerful tool for robot motion planning and control. State-of-the-art general-purpose nonlinear programming solvers are versatile, handle constraints effectively and provide a high numerical robustness, but they are slow because they do not fully exploit the optimal control problem structure at hand. Existing structure-exploiting solvers are fast, but they often lack techniques to deal with nonlinearity or rely on penalty methods to enforce (equality or inequality) path constraints. This work presents FATROP: a trajectory optimization solver that is fast and benefits from the salient features of general-purpose nonlinear optimization solvers. The speed-up is mainly achieved through the integration of a specialized linear solver, based on a Riccati recursion that is generalized to also support stagewise equality constraints. To demonstrate the algorithm's potential, it is bench-marked on a set of robot problems that are challenging from a numerical perspective, including problems with a minimum-time objective and no-collision constraints. The solver is shown to solve problems for trajectory generation of a quadrotor, a robot manipulator and a truck-trailer problem in a few tens of milliseconds. The algorithm's C++-code implementation accompanies this work as open source software, released under the GNU Lesser General Public License (LGPL). This software framework may encourage and enable the robotics community to use trajectory optimization in more challenging applications.</td>
                <td>Robot motion, Software algorithms, Optimal control, Programming, Manipulators, Robustness, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342336&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342336&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CineTransfer: Controlling a Robot to Imitate Cinematographic Style from a Single Example</td>
                <td>P. Pueyo, E. Montijano, A. C. Murillo and M. Schwager</td>
                <td>2023</td>
                <td>This work presents CineTransfer, an algorithmic framework that drives a robot to record a video sequence that mimics the cinematographic style of an input video. We propose features that abstract the aesthetic style of the input video, so the robot can transfer this style to a scene with visual details that are significantly different from the input video. The framework builds upon CineMPC, a tool that allows users to control cinematographic features, like subjects' position on the image and the depth of field, by manipulating the intrinsics and extrinsics of a cinematographic camera. However, CineMPC requires a human expert to specify the desired style of the shot (composition, camera motion, zoom, focus, etc). CineTransfer bridges this gap, aiming a fully autonomous cinematographic platform. The user chooses a single input video as a style guide. CineTransfer extracts and optimizes two important style features, the composition of the subject in the image and the scene depth of field, and provides instructions for CineMPC to control the robot to record an output sequence that matches these features as closely as possible. In contrast with other style transfer methods, our approach is a lightweight and portable framework which does not require deep network training or extensive datasets. Experiments with real and simulated videos demonstrate the system's ability to analyze and transfer style between recordings, and are available in the supplementary video11https://youtu.be/_QzNz5WUtpk</td>
                <td>Training, Visualization, Video sequences, Robot vision systems, Feature extraction, Cameras, Recording</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342280&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342280&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Satisfaction of Joint Position and Velocity Bounds in Discrete-Time Acceleration Control of Robot Manipulators</td>
                <td>E. Zanolli and A. Del Prete</td>
                <td>2023</td>
                <td>This paper deals with the robust control of fully-actuated robots subject to joint position, velocity and acceleration bounds. Robotic systems are subject to disturbances, which may arise from modeling errors, sensor noises or communication delays. This work presents mathematical and computational tools to ensure the robust satisfaction of joint bounds in the control of robot manipulators. We consider a system subject to bounded additive disturbances on the control inputs, with constant joint position, velocity and acceleration bounds. We compute the robust viability kernel, which is the set of states such that, starting from any such state, it is possible to avoid violating the constraints in the future, despite the presence of disturbances. Then we develop an efficient algorithm to compute the range of feasible accelerations that allow the state to remain inside the robust viability kernel. Our derivation ensures the continuous-time robust satisfaction of the joint bounds, while considering the discrete-time nature of the control inputs. Tests are performed in simulation with a single joint and a 6-DOF robot manipulator, demonstrating the effectiveness of the proposed approach compared to other state-of-the-art methods.</td>
                <td>Robust control, Additives, Computational modeling, Robot sensing systems, Manipulators, Mathematical models, 6-DOF</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341667&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341667&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Toward Closed-Loop Additive Manufacturing: Paradigm Shift in Fabrication, Inspection, and Repair</td>
                <td>M. Singh et al.</td>
                <td>2023</td>
                <td>Increased usage of additive manufacturing (AM) in various industries has solidified its role as an advanced manufacturing technique. However, there is an inherent lack of reliability in AM processes, particularly common in extrusion or deposition-based methods due to the stochastic nature of ma-terial deposition. This necessitates an intelligent manufacturing solution to address the drawbacks of AM. Thus, we propose a novel layer-wise approach toward closed-loop AM, which is capable of in-situ monitoring and repairing geometric defects. In this paper, we present a system that uses a robotic AM experimental platform that mimics a conventional open-loop fabrication setup, which we augment into a closed-loop system using two add-ons: in-situ inspection subsystem and online process correction subsystem. The in-situ inspection subsystem collects 3D point cloud scans and compares them against a reference CAD model, categorizing geometric deviations as positive or negative defects. Then the subsequent online process correction subsystem uses a re-plan and/or repair strategy to address the positive and/or negative defects, respectively. To evaluate this idea, we conducted three experiments on parts with manually induced defects to investigate the system's ability to repair those parts, thereby reducing defects, improving part accuracy, and enhancing mechanical properties. Comparing the defective and repaired parts, we observe a reduction in defect percent by volume from 10.7% to 1.3%, an improvement in geometric tolerance from 3.86% error to 0.08% error, and an increase in the part's breaking load from 4.77 kN to 6.31 kN. These experiments prove that our layer-wise closed-loop additive manufacturing approach improves the quality, tolerance, and reliability of plastic 3D printed parts, with the potential to extend to other extrusion/deposition-based AM processes, or even subtractive manufacturing and hybrid manufacturing methods.</td>
                <td>Fabrication, Point cloud compression, Solid modeling, Three-dimensional displays, Stochastic processes, Maintenance engineering, Inspection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342148&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342148&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Optimal Decision Making in Robotic Assembly and Other Trial-and-Error Tasks</td>
                <td>J. Watson and N. Correll</td>
                <td>2023</td>
                <td>Uncertainty in perception, actuation, and the environment often require multiple attempts for a robotic task to be successful. We study a class of problems providing (1) low-entropy indicators of terminal success / failure, and (2) unreliable (high-entropy) data to predict the final outcome of an ongoing task. Examples include a robot trying to connect with a charging station, parallel parking, or assembling a tightly-fitting part. The ability to restart after predicting a failure early, versus simply running to failure, can significantly decrease the makespan, that is, the total time to completion, with the drawback of potentially short-cutting an otherwise successful operation. Assuming task running times to be Poisson distributed, and using a Markov Jump Process to capture the dynamics of the underlying Markov Decision Process, we derive a closed-form solution that predicts makespan based on the confusion matrix of the failure predictor. This allows the robot to learn failure prediction in a production environment, and only adopt a preemptive policy when it actually saves time. We demonstrate this approach using a robotic peg-in-hole assembly problem. Failures are predicted by a dilated convolutional network based on force-torque data, showing an average makespan reduction from 101s to 81s ($\mathrm{N}=120,\ \mathrm{p} < 0.05$). We posit that the proposed algorithm generalizes to any robotic behavior with an unambiguous terminal reward, with wide ranging applications on how robots can learn and improve their behaviors in the wild.</td>
                <td>Robotic assembly, Uncertainty, Production, Markov processes, Prediction algorithms, Distance measurement, Behavioral sciences, Failure Detection and Recovery, Assembly, Manipulation Planning, Industrial Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341863&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341863&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Cartesian Platform for Cooperative Multi-Robot Manipulation Tasks</td>
                <td>S. Müller, S. Ilić, V. Scamarcio and J. Hughes</td>
                <td>2023</td>
                <td>For many manipulation tasks in environments such as laboratory or a kitchen, the presence of two robot arms is important to enable collaborative tasks requiring two arms (e.g. lid removal or tool use) or to improve the efficiency of scheduling of tasks. Currently, the development of multi-arm manipulation solutions has largely focused on 6 degrees of freedom articulated robot arms. However, cartesian robots have many advantages, including their precision, reliability, efficiency, and simple path planning. By developing a cartesian platform such that the end effectors of two mirrored systems can interact freely without collisions in 5 degrees of freedom, we can leverage the advantages of cartesian robots (high precision, simple planning, and low-cost hardware) and show robot cooperation. We equip each robot with end-effectors with different skills to increase the range of tasks the robots can cooperatively complete. To exploit this robotic hardware, we have developed a task-allocation and path-planning algorithm that enables these two mirror robots to work together to solve tasks collaboratively, exploiting the different skills and workspace of the two robots. We show how this robot can be used for cooperative tasks in lab automation, including pick and place, unscrewing vial caps, liquid pouring, and weighing. These demonstrate the feasibility and capabilities of the proposed robotic system for cooperative automation using cartesian robots.</td>
                <td>Automation, Software algorithms, Hardware, End effectors, Software, Planning, Resource management</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342469&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342469&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robotic Crop Handling in Cluttered and Unstructured Environments using Simulated L-System Dynamic Plant Models</td>
                <td>Q. T. Barthelme and C. Lehnert</td>
                <td>2023</td>
                <td>This paper presents the development of a simulation for dynamic plant models, generated from L-system functional models. A key application of these dynamic plant models is to aid in developing new methods for robotic manipulation of plants that minimize damage due to physical interaction. We present a use case of the dynamic plant model by evaluating its performance against standard RRT and novel keyhole robot arm pruning algorithms in comparison with a physical plant. Through this paper, we show that the simulated plant model was able to predict the failure modes for each pruning algorithm. The dynamic plant model was also able to predict the performance difference between algorithms, simulated experiments predicting an increase in target point capture success rate from 57% RRT to 90% keyhole compared with 65% RRT to 86% keyhole when applied to a physical sample; thus validating it as a useful simulation tool for developing and testing novel robotic methods for plant handling within cluttered and unstructured environments.</td>
                <td>Heuristic algorithms, Predictive models, Prediction algorithms, End effectors, Reliability, Robots, Standards</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341658&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341658&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Understanding the Influence of Robot Motion on the Experimental Processes Present in Food Science Applications</td>
                <td>Delval, G. Marchesini and J. Hughes</td>
                <td>2023</td>
                <td>Laboratory experiments in modern food labs are human-driven and tedious processes which can have limited throughput, reliability, repeatability or robustness. Through repeatable motions and precise control of process parameters, robotic automation can provide significant improvements to the existing experimental processes, and also improve manual assessment of the sensory data. By developing a robotic automation system which performs the make, measure, adjust and clean processes for a milk beverage made from water and powdered milk, we explore how variation in different process parameters impacts quality of the beverage in terms of the measured pH value. Using collected data we also identify optimal process parameters from robustness and time-cost standpoint. By comparing performance of the robotic system to a human we demonstrate varied performance in the pH adjustment process and 3x better precision in the pH probe cleaning. We identify that designed robotic system requires 45% more time to perform the experiment when compared to a human, yet provides significant advances in terms of repeatability and reproducibility. These findings demonstrate feasibility and benefits of the robotic automation in the food lab environments, thus paving the way for the broader implementation.</td>
                <td>Viscosity, Automation, Dairy products, Robot sensing systems, Cleaning, Robustness, Space exploration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341710&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341710&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>High-Accuracy Injection Using a Mobile Manipulation Robot for Chemistry Lab Automation</td>
                <td>A. Angelopoulos, M. Verber, C. McKinney, J. Cahoon and R. Alterovitz</td>
                <td>2023</td>
                <td>Lab automation has the potential to accelerate scientific progress in the natural sciences, allowing tedious experiments that would require many hours of human time to be automated, enabling higher accuracy, efficiency, and repeatability. Mobile manipulation robots have the potential to work in chemistry labs designed for humans to complete tasks for which setting up customized factory-scale automation is premature or infeasible. We present a new method to enable a mobile manipulation robot to automate injections, a common task in chemistry labs when using equipment such as gas chromatographs (GCs) for analyzing the contents of a sample mixture. This task is challenging for a mobile manipulation robot due to the need to navigate to the equipment in the lab and then achieve millimeter-scale accuracy required for the syringe positioning. Our approach leverages deep learning to create a model capable of localizing the syringe with high accuracy using cameras mounted on the chemistry equipment, and then uses a visual servoing approach based on the syringe's needle localization to achieve the injection. We demonstrate that our approach is robust to uncertainty in navigation as well as uncertainty in the grasping position and orientation of the syringe, achieving errors sufficiently small to enable the mobile manipulation robot to automate injections in real chemistry equipment.</td>
                <td>Chemistry, Automation, Uncertainty, Navigation, Grasping, Needles, Visual servoing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341743&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341743&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robotic Defect Inspection with Visual and Tactile Perception for Large-Scale Components</td>
                <td>A. Agarwal et al.</td>
                <td>2023</td>
                <td>In manufacturing processes, surface inspection is a key requirement for quality assessment and damage localization. Due to this, automated surface anomaly detection has become a promising area of research in various industrial inspection systems. A particular challenge in industries with large-scale components, like aircraft and heavy machinery, is inspecting large parts with very small defect dimensions. Moreover, these parts can be of curved shapes. To address this challenge, we present a 2-stage multi-modal inspection pipeline with visual and tactile sensing. Our approach combines the best of both visual and tactile sensing by identifying and localizing defects using a global view (vision) and using the localized area for tactile scanning for identifying remaining defects. To benchmark our approach, we propose a novel real-world dataset with multiple metallic defect types per image, collected in the production environments on real aerospace manufacturing parts, as well as online robot experiments in two environments. Our approach is able to identify 85% defects using Stage I and identify 100% defects after Stage II.</td>
                <td>Visualization, Pipelines, Transfer learning, Tactile sensors, Lighting, Production, Inspection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341590&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341590&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Development of the Whole-Body Waterproof Shell Applying and Removing System Using Phase-Change Paraffin and Grease for the Multi-DOF Robot</td>
                <td>T. Makabe, K. Okada and M. Inaba</td>
                <td>2023</td>
                <td>We need to build robot systems that can operate in multiple environments, including underwater. For a robot to be waterproof, it needs to be covered all over its body and have a waterproof structure, but this is expensive to produce and consumes hardware resources such as weight. In this study, we propose a method of constructing a robot system in which paraffin, which can change its phase between solid and liquid at different temperatures, is cloaked on the robot's body surface, and grease is inserted into the robot's body to give it an acquired the waterproof shell for waterproofing purposes. We have studied an automated method of the waterproof shell application system by combining a small spider-shaped multi-legged robot that can measure the temperature of the environment as a robot to which we apply the waterproof shell and a life-size arm robot that performs the operation applying the waterproof shell to the small robot as a system. Through the addition of the waterproof shell to the spider-shaped robot and the removing the shell in unnecessary areas by controlling temperature, and the realization of operation in land and water environments, we used the proposed robot system to add acquired functions to operate underwater robots, thereby expanding the supported area of robots which have many DOFs and sensors from conventional ground to underwater.</td>
                <td>Temperature measurement, Temperature sensors, Water, Temperature, Robot sensing systems, Manipulators, Sensor systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341470&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341470&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning to Adapt the Parameters of Behavior Trees and Motion Generators (BTMGs) to Task Variations</td>
                <td>F. Ahmad, M. Mayr and V. Krueger</td>
                <td>2023</td>
                <td>The ability to learn new tasks and quickly adapt to different variations or dimensions is an important attribute in agile robotics. In our previous work, we have explored Behavior Trees and Motion Generators (BTMGs) as a robot arm policy representation to facilitate the learning and execution of assembly tasks. The current implementation of the BTMGs for a specific task may not be robust to the changes in the environment and may not generalize well to different variations of tasks. We propose to extend the BTMG policy representation with a module that predicts BTMG parameters for a new task variation. To achieve this, we propose a model that combines a Gaussian process and a weighted support vector machine classifier. This model predicts the performance measure and the feasibility of the predicted policy with BTMG parameters and task variations as inputs. Using the outputs of the model, we then construct a surrogate reward function that is utilized within an optimizer to maximize the performance of a task over BTMG parameters for a fixed task variation. To demonstrate the effectiveness of our proposed approach, we conducted experimental evaluations on push and obstacle avoidance tasks in simulation and with a real KUKA iiwa robot. Furthermore, we compared the performance of our approach with four baseline methods.</td>
                <td>Support vector machines, Gaussian processes, Predictive models, Manipulators, Generators, Behavioral sciences, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341636&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341636&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction</td>
                <td>U. Khalid, H. Iqbal, S. Vahidian, J. Hua and C. Chen</td>
                <td>2023</td>
                <td>Human-robot interaction (HRI) is a rapidly growing field that encompasses social and industrial applications. Machine learning plays a vital role in industrial HRI by enhancing the adaptability and autonomy of robots in complex environments. However, data privacy is a crucial concern in the interaction between humans and robots, as companies need to protect sensitive data while machine learning algorithms require access to large datasets. Federated Learning (FL) offers a solution by enabling the distributed training of models without sharing raw data. Despite extensive research on Federated learning (FL) for tasks such as natural language processing (NLP) and image classification, the question of how to use FL for HRI remains an open research problem. The traditional FL approach involves transmitting large neural network parameter matrices between the server and clients, which can lead to high communication costs and often becomes a bottleneck in FL. This paper proposes a communication-efficient FL framework for human-robot interaction (CEFHRI) to address the challenges of data heterogeneity and communication costs. The framework leverages pre-trained models and introduces a trainable spatiotemporal adapter for video understanding tasks in HRI. Experimental results on three human-robot interaction benchmark datasets: HRI30, InHARD, and COIN demonstrate the superiority of CEFHRI over full fine-tuning in terms of communication costs. The proposed methodology provides a secure and efficient approach to HRI federated learning, particularly in industrial environments with data privacy concerns and limited communication bandwidth. Our code is available at https://github.com/umarkhalidAI/CEFHRI-Efficient-Federated-Learning.</td>
                <td>Training, Data privacy, Adaptation models, Costs, Federated learning, Service robots, Human-robot interaction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341467&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341467&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>COMPlacent: A Compliant Whisker Manipulator for Object Tactile Exploration</td>
                <td>C. Xiao and J. Wachs</td>
                <td>2023</td>
                <td>Handling fragile objects requires minimally invasive interaction skills in order to avoid any permanent deformation, alternation or damages. Such need is often required in tactile exploration tasks. In this paper, we propose an innovative whisker manipulator (COMPlacent), which is designed to accomplish tactile exploration with minimum intrusiveness. The design is inspired by biological whiskers observed in animals, where whiskers are used as means of tactile exploration in an analogous way as fingers are. Artificial whiskers are compliant but robust, which mitigates contact forces by bending or conforming to the object surface. The intrusiveness is further reduced by reactive control, which is implemented based on tactile sensors and actuators installed on each whisker. This allows the whisker to be retracted from the object surface, so that the energy transferred by contacts is minimized. The tactile sensor is designed to be ultrasensitive, which allows it to gather contact information with high fidelity. By modeling contact pressure as a time-series signal, a machine learning framework is leveraged to discriminate object properties including shape and texture. Evaluation experiments were conducted on real objects, which successfully demonstrates object classification at an accuracy of 97.3%, and texture discrimination accuracy of 92.1%.</td>
                <td>Pressure sensors, Minimally invasive surgery, Microorganisms, Shape, Time series analysis, Tactile sensors, Machine learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341547&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341547&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Monolithic Microchannels in Miniature Pneumatic Soft Robots for Sequential Motions</td>
                <td>D. Fan, H. Liu, T. Wang, R. Zhu and H. Wang</td>
                <td>2023</td>
                <td>Miniature soft robots present great potential in delicate manipulations due to their gentle force, complaint structures, and flexible motions. Easy control and fast response make pneumatic actuation a prevalent method for driving soft robotics. In addition, sequential motions are also crucial for enhancing the grasping and moving abilities of soft robots. How-ever, existing miniature pneumatic soft robots are limited to one-dimensional geometries and simple motions due to the difficulties in designing and fabricating intricate small airways in miniature pneumatic soft robots, which restricts them from more versatile deformations. Here, we employ intricate monolithic microchannels embedded into miniature soft robots' mon-olithic bodies for sequential motions. After verifying the effects of the channel diameter, strain-limiting layer, and elastic modulus of the robot's body on the bending behaviors of the ID soft robots, we fabricated a soft flower robot capable of sequential and simultaneous 3D-to-3D shape morphing through five individual microchannels and a soft carnivorous plant robot containing 2D interconnected microchannels capable of sequential enclosed grasping through a single inlet.</td>
                <td>Statistical analysis, Shape, Grasping, Soft robotics, Pneumatic systems, Bending, Flowering plants</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341400&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341400&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Polymer-Based Self-Calibrated Optical Fiber Tactile Sensor</td>
                <td>W. Chen, Y. Yan, Z. Zhang, L. Yang and J. Pan</td>
                <td>2023</td>
                <td>Human skin can accurately sense the self-decoupled normal and shear forces when in contact with objects of different sizes. Although there exist many soft and conformable tactile sensors on robotic applications able to decouple the normal force and shear forces, the impact of the size of object in contact on the force calibration model has been commonly ignored. Here, using the principle that contact force can be derived from the light power loss in the soft optical fiber core, we present a soft tactile sensor that decouples normal and shear forces and calibrates the measurement results based on the object size, by designing a two-layered weaved polymer-based optical fiber anisotropic structure embedded in a soft elastomer. Based on the anisotropic response of optical fibers, we developed a linear calibration algorithm to simultaneously measure the size of the contact object and the decoupled normal and shear forces calibrated the object size. By calibrating the sensor at the robotic arm tip, we show that robots can reconstruct the force vector at an average accuracy of 0.15N for normal forces, 0.17N for shear forces in X-axis, and 0.18N for shear forces in Y-axis, within the sensing range of 0-2N in all directions, and the average accuracy of object size measurement of 0.4mm, within the test indenter diameter range of 5-12mm.</td>
                <td>Force measurement, Wearable computers, Force, Tactile sensors, Optical variables measurement, Size measurement, Loss measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341656&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341656&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>2-DOF Robot Arm with Variable Torque Limiters Realized by Electrostatic Film Motors</td>
                <td>M. Osada, G. Zhang, S. Yoshimoto and A. Yamamoto</td>
                <td>2023</td>
                <td>When a conventional robot arm, driven by magnetic motors and reduction gears, operates at high speed, it can produce an excessive force due to unexpected contact. Toward the development of a safe robot arm, the present study investigated a 2-degree-of-freedom robot arm driven by direct-drive electrostatic film motors. The motors operate synchronously, except when they are subjected to an excessive force. This loss of synchronicity is referred to as a “step-out” process, and in the present study it was used to produce a variable torque limiter. By setting appropriate limits, contact forces caused by unexpected collisions could be suppressed. The torque limiter was applied to a robot arm with a length of 50 cm. Experiments showed that the arm was impact-resistant; when it was struck by a hammer, the torque limiter was activated and suppressed the collision force, allowing the robot arm to successfully resume its motion. When the robot arm unexpectedly hit an obstacle while moving at 1.2 m/s, the collision force was maintained at 21 N by the torque limiter, which is sufficiently small for a high-speed collision. The experiments verified the effectiveness of the electrostatic film motor as a variable torque limiter to realize safe contact.</td>
                <td>Torque, Force, Prototypes, 2-DOF, Manipulators, Synchronous motors, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341997&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341997&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Stiffness Analysis of a Bio-Inspired Soft Actuator with Bi-Direction Tunable Stiffness Property</td>
                <td>J. Lin, R. Xiao and Z. Guo</td>
                <td>2023</td>
                <td>Modulating the stiffness of soft actuators is crucial for improving the efficiency of interaction with the environment. However, current stiffness modulation mechanisms are hard to achieve high lateral stiffness and a wide range of bending stiffness simultaneously. Here, we draw inspiration from the anatomical structure of the finger and propose a bi-directional tunable stiffness actuator (BTSA). BTSA is a soft-rigid hybrid structure that combines air-tendon hybrid actuation (ATA) and bone-like structures (BLS). We develop a corresponding fabrication method and a stiffness analysis model to support the design of BLS. The results show that the influence of the BLS on bending deformation is negligible, with a distal point distance error of less than 1.5 mm. Moreover, the bi-directional tunable stiffness is proved to be functional. The bending stiffness can be tuned by ATA from 0.23 N/mm to 0.70 N/mm, with a magnification of 3 times. The addition of BLS improves lateral stiffness up to 4.2 times compared with the one without BLS, and the lateral stiffness can be tuned decoupling within 1.2 to 2.1 times (e.g. from 0.35 N/mm to 0.46 N/mm when the bending angle is 45 deg). Finally, a four-BTSA gripper is developed to conduct horizontal lifting and grasping tasks to demonstrate the advantages of BTSA.</td>
                <td>Actuators, Deformation, Fingers, Modulation, Bidirectional control, Anatomical structure, Bending</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341730&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341730&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MIGHTY: Multi-Functional Suction Cup for Object Gripping and Surface Attachment</td>
                <td>E. Papadakis, M. Sigalas, M. Vangos and P. Trahanias</td>
                <td>2023</td>
                <td>The spectrum of applications of robotic systems is constantly being expanded in the research, industrial and even the defense sectors, ranging from manipulation and assembly to critical infrastructure monitoring and post-disaster response. Nevertheless, contemporary robotic capabilities are significantly hindered when traversing through or interacting with complex, unstructured and dynamic environments. To alleviate for that, we introduce in the current work MIGHTY (Multi-functional Intelligent Gripping with High Tolerance), a novel, lightweight, sensor-enhanced vacuum suction cup providing not only enhanced attachment capabilities on a plethora of surfaces of varying roughness, but also robust and accurate contact and force estimation. Its unique design facilitates multiple functionalities, from acting as a gripper for object manipulation to operating as a foot for stable walking and steep surface climbing. The proposed suction cup was extensively assessed under varying experimental setups, in order to validate its capacity to sense the applied force and torque and the torque's axis, as well as its ability to attach on a variety of surfaces. In all cases remarkable results were demonstrated, attesting for its effectiveness and robustness.</td>
                <td>Legged locomotion, Torque, Service robots, Force, Robot sensing systems, Surface roughness, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341732&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341732&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multiple-Contact Estimation for Tendon-Driven Continuum Robots with Proprioceptive Sensor Information by Contact Particle Filter and Kinetostatic Models</td>
                <td>D. Job, M. Bensch and M. Schappler</td>
                <td>2023</td>
                <td>This paper presents a new approach to determine single and multiple simultaneous contact forces on a tendon-driven continuum robot (CR). The estimation is based solely on the proprioceptive tendon force and length sensors that are already present. Unlike for rigid-body robots, only indirect measurements of the external forces' deflection is available. The required full kinetostatic model, which is prone to local minima due to the unknown contacts, is solved with a particle filter. The method is validated by simulative studies and experimental investigations on a new robot setup for visual inspection of aircraft engines. The algorithm allows the estimation of single contacts with an error up to 4.43 mm or 2.9 % of the robot's length. Multiple contacts can only be correctly determined at the two distal of the three segments.</td>
                <td>Visualization, Robot vision systems, Estimation, Propioception, Robot sensing systems, Particle filters, Sensor systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341897&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341897&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Eversion-Capable Fabric Robot Gripper with Novel Retraction Mechanism</td>
                <td>A. Hassan, T. Abrar, F. Aljaber, I. Vitanov and K. Althoefer</td>
                <td>2023</td>
                <td>Soft grippers have a number of advantages over their conventional stiff-bodied counterparts; not only do they surpass them in ease of fabrication and safety but also, in many cases, require less complex control strategies - due to natural compliance and form-fitting plasticity. Pneumatically actuated soft grippers made from non-extensible fabrics or polyethylene sheets have been shown to outperform soft silicone grippers capable of applying greater forces to the environment. Despite progress in the field, grasping a given object within a confined space proves challenging for both soft and conventional robotic grippers. A key issue is that most grippers use rotary or lateral translational motion when grasping an object, hence other objects in the scene may impede the closing motion as the gripper attempts to reach the target. In this study, we present a novel design for a soft robotic gripper equipped with a brace of fabric-based fingers capable, by way of eversion, of longitudinal extension, bending, and retraction, i.e. returning to a stowed state. Our experiments show that from the retracted to fully-extended state, the gripper fingers extend by up to 200% in length. To test the performance of the design, force characterisation experiments and grasping operations were carried out, demonstrating that each finger is capable of as much as 30 N of maximum tip force and can bend to an angle of 127°• At a bending pressure of 82.7 kPa (the maximum tested pressure in the bending chamber), a maximum (pullout) force of 16 N is needed to release an object that has been grasped by the finger. The experimental scenario detailed features all three mechanisms (eversion, bending and retraction) discussed.</td>
                <td>Polyethylene, Fingers, Force, Grasping, Bending, Soft robotics, Fabrics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342364&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342364&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Emergent Sequential Motion Through Compliant Auxetic Shells</td>
                <td>A. Sedal, M. Kohler, G. Agbofode, T. Y. Moore and S. Kota</td>
                <td>2023</td>
                <td>Though they are compliant, nimble and morpho-logically intelligent, fluidic soft robots often rely on bulky components for power and actuation. This work contributes a design methodology which enables development of soft fluidic robots that move in a sequenced fashion, enabling lightweight devices with embodied intelligence. Bezier-curved beams were introduced as a design building block whose antagonistic placement results in Representative Auxetic Element (RAE) that can be patterned on inflatable shells. Kinematics and loading behaviour of these design building blocks were studied through Finite Element Analysis (FEA). We give a methodology for patterning RAEs on cylindrical and conic shells to create soft fluidic components that move (motion components) and those that delay fluid flow (pinch components). We verify the physical concepts governing the design methodology through two prototype devices that produce sequenced motion under a single fluidic input. Devices using this framework have the potential to perform complicated sequenced motions with lightweight control components.</td>
                <td>Performance evaluation, Sequential analysis, Three-dimensional displays, Auxetic materials, Loading, Prototypes, Kinematics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341387&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341387&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Automated Gait Generation for Walking, Soft Robotic Quadrupeds</td>
                <td>J. Ketchum, S. Schiffer, M. Sun, P. Kaarthik, R. L. Truby and T. D. Murphey</td>
                <td>2023</td>
                <td>Gait generation for soft robots is challenging due to the nonlinear dynamics and high dimensional input spaces of soft actuators. Limitations in soft robotic control and perception force researchers to hand-craft open loop controllers for gait sequences, which is a non-trivial process. Moreover, short soft actuator lifespans and natural variations in actuator behavior limit machine learning techniques to settings that can be learned on the same time scales as robot deployment. Lastly, simulation is not always possible, due to heterogeneity and nonlinearity in soft robotic materials and their dynamics change due to wear. We present a sample-efficient, simulation free, method for self-generating soft robot gaits, using very minimal computation. This technique is demonstrated on a motorized soft robotic quadruped that walks using four legs constructed from 16 “handed shearing auxetic” (HSA) actuators. To manage the dimension of the search space, gaits are composed of two sequential sets of leg motions selected from 7 possible primitives. Pairs of primitives are executed on one leg at a time; we then select the best-performing pair to execute while moving on to subsequent legs. This method-which uses no simulation, sophisticated computation, or user input-consistently generates good translation and rotation gaits in as low as 4 minutes of hardware experimentation, outperforming hand-crafted gaits. This is the first demonstration of completely autonomous gait generation in a soft robot.</td>
                <td>Legged locomotion, Actuators, Navigation, Transfer learning, Process control, Soft robotics, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342059&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342059&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards MR-Safe Concentric Bellows-Based Hydrostatic Linear Actuator for a Needle Driver</td>
                <td>K. K. Lin, Y. Qiu, K. Yan, Q. Ding and S. S. Cheng</td>
                <td>2023</td>
                <td>Magnetic resonance imaging (MRI) is increasingly used for robotic needle-based clinical diagnosis and therapy due to its high resolution and high soft tissue contrast. A needle driver therefore becomes an essential part of these MRI -guided robotic systems to perform in-bore needle placement under continuous MR imaging. However, existing actuator designs for needle drivers are constrained by the high magnetic field and the bore size of the MR scanner, and most lack high- performance capability, especially in terms of large output force and long stroke. In this work, we introduce an MR-safe concentric bellows linear actuator (CBLA) with improved performance over the existing designs. This soft actuator achieves 45N output force, 77 % stroke-length ratio for the bellows, and around 250% stroke-diameter ratio. A mathematical model was built to characterize the behavior of the actuator to provide guidance for the actuator design. The fabrication and experimental evaluation of the actuator are also presented. The results demonstrate the effectiveness of the CBLA design and the strong potential of its application in MR-guided surgical procedures.</td>
                <td>Actuators, Bellows, Magnetic resonance imaging, Force, Medical treatment, Soft robotics, Needles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342078&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342078&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Analytical Computation of the Contact Force Jacobian for MRI-Actuated Robotic Catheter</td>
                <td>Y. Itsarachaiyot, R. Hao and M. C. Çavuşoğlu</td>
                <td>2023</td>
                <td>Contact force Jacobian relates the changes in the contact force to the changes in the actuation of a robotic catheter in contact with a surface. In this paper, we present an analytical method for calculating the contact force Jacobian for the Cosserat rod model of an MRI-actuated robotic catheter. First, the Cosserat rod model of the MRI-actuated robotic catheter under tip contact position constraint is introduced. For the analytical derivation of contact force Jacobian, the initial value problem parameter derivatives are defined and calculated analytically. Finally, simulation results show that the presented analytical method calculates the contact force Jacobian in significantly shorter computation time with comparable accuracy, compared to direct numerical computation.</td>
                <td>Jacobian matrices, Analytical models, Computational modeling, Simulation, Force, Benchmark testing, Numerical models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342418&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342418&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Magnet Array-Actuated Steerable Flexible Robot with Beacon-TFM Ultrasonic Position Sensing for Robotic Neurosurgery</td>
                <td>X. Gu, Q. Zhang, X. Sun, Z. Shen, B. W. Drinkwater and F. Ju</td>
                <td>2023</td>
                <td>A magnetically controlled steerable robot with the capability of flexible navigation and intraoperative ultrasonic imaging and position sensing in neurosurgery is introduced in this paper. The robot system uses a piezoelectric transducer as the ultrasonic imaging beacon and applies a permanent magnet as the actuation unit, which can steer the flexible robot following a planned complex path to access a target in the brain. In order to enable the robot to navigate flexibly within tissues, a method is proposed to enhance magnetic actuation force by utilizing an array of small magnetic blocks instead of using a large magnet. The improvement of the array for magnetic field distribution was verified by simulation. Simulation studies also involved modeling the ultrasonic transmitting and receiving functions of the piezoelectric transducer to verify the feasibility of using them for intraoperative imaging. In addition, a prototype of the robot is fabricated and tested. The feasibility of the proposed magnetically controlled steerable robot for navigation in soft tissue is verified.</td>
                <td>Ultrasonic imaging, Navigation, Magnetic resonance imaging, Piezoelectric transducers, Prototypes, Robot sensing systems, Acoustics, Permanent magnets, Sensors, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341991&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341991&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sunram 7: An MR Safe Robotic System for Breast Biopsy</td>
                <td>H. Ranjan et al.</td>
                <td>2023</td>
                <td>In breast cancer patients, some nodules are only visible on MRI, thus, requiring MRI-guidance to perform the biopsy. MRI interventions are cumbersome due to the magnetic field and the constrained working space. An MR safe robotic system actuated by pneumatic stepper motors may enable these procedures, improving both accuracy and image-guided navigation. A compact multipurpose pneumatic stepper motor has been designed with outer dimensions $(45 \times 40\times 15)\mathbf{mm}^{\mathbf{3}}$. This is configurable as a linear, rotational or curved stepper motor with a customizable step size and radius of curvature. Five copies of these motors actuate the Sunram 7 biopsy robot, of which the moving part (without protruding racks and tubes) measures $(130 \times 65\times 55)\mathbf{mm}^{\mathbf{3}}$. After manually choosing the target location and angle of approach, the needle is robotically inserted into the breast and the integrated pneumatic biopsy gun is fired to sample tissue from the lesion. The maximum torque of the presented motor is 0.61 N m at 6 bar which can be achieved using 13-teeth polycarbonate gears. Using 17-teeth gears for higher accuracy and a more convenient working pressure of 2 bar the maximum torque is 0.28 N m. The accuracy in free air of the Sunram 7 robot is 1.69mm and 1.72mm in X and Z-direction respectively, with a resulting 2-D error of 2.54 mm. The workspace volume is 4.1 L. When targeting 10 mm-sized lesions in phantoms under MRI guidance, Sunram 7 achieved a success rate of 68%. The minimum interval between two successive biopsies was 5:47 minutes. The presented multipurpose stepper motor has distinct advantages over previous designs in terms of robustness, customizability, printability and ease of integration in MR safe robotics. The Sunram 7 is able to perform accurate MRI-guided biopsies in a large workspace volume while reducing the intervention time when compared to the gold standard (i.e., MRI-guided free-hand biopsy).</td>
                <td>Torque, Gears, Robot kinematics, Magnetic resonance imaging, Biopsy, Phantoms, Breast biopsy</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342425&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342425&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Constrained Reinforcement Learning and Formal Verification for Safe Colonoscopy Navigation</td>
                <td>D. Corsi et al.</td>
                <td>2023</td>
                <td>The field of robotic Flexible Endoscopes (FEs) has progressed significantly, offering a promising solution to reduce patient discomfort. However, the limited autonomy of most robotic FEs results in non-intuitive and challenging manoeuvres, constraining their application in clinical settings. While previous studies have employed lumen tracking for autonomous navigation, they fail to adapt to the presence of obstructions and sharp turns when the endoscope faces the colon wall. In this work, we propose a Deep Reinforcement Learning (DRL)-based navigation strategy that eliminates the need for lumen tracking. However, the use of DRL methods poses safety risks as they do not account for potential hazards associated with the actions taken. To ensure safety, we exploit a Constrained Reinforcement Learning (CRL) method to restrict the policy in a predefined safety regime. Moreover, we present a model selection strategy that utilises Formal Verification (FV) to choose a policy that is entirely safe before deployment. We validate our approach in a virtual colonoscopy environment and report that out of the 300 trained policies, we could identify three policies that are entirely safe. Our work demonstrates that CRL, combined with model selection through FV, can improve the robustness and safety of robotic behaviour in surgical applications.</td>
                <td>Virtual colonoscopy, Navigation, Endoscopes, Reinforcement learning, Lumen, Iron, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341789&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341789&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Development of a Novel Soft and Inflatable Tactile Sensing Balloon for Early Diagnosis of Colorectal Cancer Polyps</td>
                <td>O. C. Kara et al.</td>
                <td>2023</td>
                <td>In this paper, with the goal of addressing the high early-detection miss rate of colorectal cancer (CRC) polyps during a colonoscopy procedure, we propose the design and fabrication of a unique inflatable vision-based tactile sensing balloon (VTSB). The proposed soft VTSB can readily be integrated with the existing colonoscopes and provide a radiation-free, safe, and high-resolution textural mapping and morphology characterization of CRC polyps. The performance of the proposed VTSB has been thoroughly characterized and evaluated on four different types of additively manufactured CRC polyp phantoms with three different stiffness levels. Additionally, we integrated the VTSB with a colonoscope and successfully performed a simulated colonoscopic procedure inside a tube with a few CRC polyp phantoms attached to its internal surface.</td>
                <td>Fabrication, Phantoms, Surface morphology, Morphology, Colonoscopy, Robot sensing systems, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342343&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342343&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Telescopic Tendon-Driven Needle Robot for Minimally Invasive Neurosurgery</td>
                <td>S. Rezaeian, B. Badie and J. Sheng</td>
                <td>2023</td>
                <td>This paper presents the design, characterization, and testing of a steerable needle robot for minimally invasive neurosurgery. The robot consists of a rigid outer tube and two telescopic tendon-driven steerable tubes. Through the rotation, translation, and bending of individual tubes, this telescopic tendon-driven needle robot can perform dexterous motion and follow the path of the tip. We presented the design of the needle robot and its actuation system, modeling of the robotic kinematics, characterization of the robot motion, results of the open-loop kinematic control, and demonstration of the follow-the-leader motion. The position error of the robot tip is 0.92 mm, and follow-the-leader motion error is 1.1 mm. Due to its small footprint and unique motion ability, the robot has the potential to be manipulated inside human brain and used for minimally invasive neurosurgery.</td>
                <td>Robot motion, Minimally invasive surgery, Kinematics, Systems modeling, Needles, Neurosurgery, Electron tubes</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341660&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341660&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Characteristics of Permanent Magnet Coupling Based Wireless Manipulation via Simulation</td>
                <td>T. Zhang, J. Li, T. Cheng, C. S. H. Ng, P. W. Y. Chiu and Z. Li</td>
                <td>2023</td>
                <td>Characteristics of wireless manipulation based on permanent magnet coupling, including anchoring distance, panning torque, and translational force, are assessed in this paper. The study focuses on a typical scenario where a slave robot embedded with a small permanent magnet can be remotely controlled within a constrained area by a master large permanent magnet placed outside the environment at a certain distance from it. The key parameters (force and torque) acting on the slave robot are quantified and evaluated. In this article, several combinations of permanent magnets with various dimensions and configurations are studied using finite element methods. Based on the obtained results, we create a lookup table for each parameter, serving as a guideline to help interested researchers choose suitable magnetic combinations for their applications.</td>
                <td>Wireless communication, Couplings, Economics, Torque, Medical robotics, Force, Permanent magnets</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341587&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341587&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Evaluation of Bidirectional Continuous Rotation and Variable Curvature Needle Steering Algorithm</td>
                <td>F. Tavakkolmoghaddam, C. Bales, Y. Wang, Z. Zhao and G. S. Fischer</td>
                <td>2023</td>
                <td>The success rate of robotic-assisted needle-guided interventions for applications such as tissue biopsy and targeted drug delivery relies heavily on the accuracy of the needle placement. Tissue shift and needle tip deflection due to needle-tissue interaction are some factors that can adversely affect the outcome of these procedures. In this paper, we present a novel algorithm for robotically-steered bevel tip needles that provides variable needle curvatures by continuously controlling the rotation speed of the needle in a bidirectional manner. Our algorithm is an extension of the Continuous Rotation and Variable Curvature (CURV) algorithm and extends its use with wired sensorized needles. Additionally, we present algorithms for the implementation of our proposed method for closed-loop needle steering in robotic systems with image or sensor feedback. To validate our approach, we perform two benchtop needle insertion experiments in a gelatin phantom and ex vivo tissue. In the first experiment, we demonstrate the capability of our proposed algorithm in achieving variable curvatures and compare it with the CURV algorithm and our simulation results. The second experiment studies the effect of the unidirectional and bidirectional needle steering on the tissue wind-up using a novel force collection setup. Our results highlight the capability of the proposed algorithm in achieving variable curvature profiles and suggest a potential advantage compared to the original method in terms of reducing the imbalanced forces sensed at the load cell due to the needle-tissue friction buildup.</td>
                <td>Targeted drug delivery, Sensitivity, Simulation, Force, Pipelines, Phantoms, Needles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342431&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342431&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Development of a Whole-Body Work Imitation Learning System by a Biped and Bi-Armed Humanoid</td>
                <td>Y. Matsuura, K. Kawaharazuka, N. Hiraoka, K. Kojima, K. Okada and M. Inaba</td>
                <td>2023</td>
                <td>Imitation learning has been actively studied in recent years. In particular, skill acquisition by a robot with a fixed body, whose root link position and posture and camera angle of view do not change, has been realized in many cases. On the other hand, imitation of the behavior of robots with floating links, such as humanoid robots, is still a difficult task. In this study, we develop an imitation learning system using a biped robot with a floating link. There are two main problems in developing such a system. The first is a teleoperation device for humanoids, and the second is a control system that can withstand heavy workloads and long-term data collection. For the first point, we use the whole body control device TABLIS. It can control not only the arms but also the legs and can perform bilateral control with the robot. By connecting this TABLIS with the high-power humanoid robot JAXON, we construct a control system for imi-tation learning. For the second point, we will build a system that can collect long-term data based on posture optimization, and can simultaneously move the robot's limbs. We combine high-cycle posture generation with posture optimization methods, including whole-body joint torque minimization and contact force optimization. We designed an integrated system with the above two features to achieve various tasks through imitation learning. Finally, we demonstrate the effectiveness of this system by experiments of manipulating flexible fabrics such that not only the hands but also the head and waist move simultaneously, manipulating objects using legs characteristic of humanoids, and lifting heavy objects that require large forces.</td>
                <td>Legged locomotion, Learning systems, Torque, Robot vision systems, Humanoid robots, Data collection, Control systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342502&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342502&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Humanoid Walking System with CNN-Based Uneven Terrain Recognition and Landing Control with Swing-Leg Velocity Constraints</td>
                <td>S. Sato, K. Kojima, N. Hiraoka, K. Okada and M. Inaba</td>
                <td>2023</td>
                <td>In order for a humanoid robot to traverse uneven terrain without falling over, the robot must control its landing position appropriately. To determine the landing position, there are two difficulties in terrain recognition and leg motion control. In terrain recognition, it is difficult to recognize and avoid terrain such as steps and obstacles that cannot be landed on in real-time. In leg motion control, it is necessary to land at appropriate positions and times to control the CoG trajectory while limiting the velocity of the swing-leg to suppress the landing impact. For solving these problems, we propose a recognition and walking control system on uneven terrain. In terrain recognition, we improved the recognition accuracy while satisfying real-time performance by using a CNN that learns the relationship between the foot and the geometric information of the surrounding terrain. In the leg motion control, landing impact was reduced by modifying the landing position under not only (1) terrain constraint and (2) robot stability constraint, but also (3) leg velocity constraint. We verified the effectiveness of the proposed system through uneven terrain walking and push recovery experiments using the actual robot.</td>
                <td>Legged locomotion, Limiting, Humanoid robots, Control systems, Real-time systems, Trajectory, Motion control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342511&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342511&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Whole Body Control Formulation for Humanoid Robots with Closed/Parallel Kinematic Chains: Kangaroo Case Study</td>
                <td>S. Sovukluk, J. Englsberger and C. Ott</td>
                <td>2023</td>
                <td>This study extends the whole-body control (WBC) formulation for bipedal humanoid robots that include closed (parallel) kinematic chains in their structure. Along with general formulation, we also stress the implementation of this formulation on Kangaroo, which is a highly dynamic humanoid robot developed by PAL Robotics. This 76-DOF robot includes 24 independent closed-kinematic chains in its structure and constitutes a good case study for our approach. We discuss the WBC formulation for various control structures, including inverse dynamics control (IDC) and Modular Passive Tracking Control (MPTC). As a test scenario, we employ a 3D spring-loaded inverted pendulum (SLIP) jumping trajectory with disturbance rejection as the desired CoM trajectory.</td>
                <td>Couplings, Three-dimensional displays, Force, Humanoid robots, Kinematics, Trajectory, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341391&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341391&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Exploring Kinodynamic Fabrics for Reactive Whole-Body Control of Underactuated Humanoid Robots</td>
                <td>Bredu, G. Gibson and J. Grizzle</td>
                <td>2023</td>
                <td>For bipedal humanoid robots to successfully operate in the real world, they must be competent at simultaneously executing multiple motion tasks while reacting to unforeseen external disturbances in real-time. We propose Kinodynamic Fabrics as an approach for the specification, solution and simultaneous execution of multiple motion tasks in real-time while being reactive to dynamism in the environment. Kinodynamic Fabrics allows for the specification of prioritized motion tasks as forced spectral semi-sprays and solves for desired robot joint accelerations at real-time frequencies. We evaluate the capabilities of Kinodynamic fabrics on diverse physically-challenging whole-body control tasks with a bipedal humanoid robot both in simulation and in the real-world. Kinodynamic Fabrics outperforms the state-of-the-art Quadratic Program based whole-body controller on a variety of whole-body control tasks on run-time and reactivity metrics in our experiments. Our open-source implementation of Kinodynamic Fabrics as well as robot demonstration videos can be found at this url: https://adubredu.github.io/kinofabs</td>
                <td>Measurement, Humanoid robots, Fabrics, Real-time systems, Task analysis, Manipulator dynamics, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342091&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342091&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Step Toward Deploying the Torque-Controlled Robot TALOS on Industrial Operations</td>
                <td>C. Perrot and O. Stasse</td>
                <td>2023</td>
                <td>This paper tackles the use of torque controlled humanoid robot TALOS in the context of industrial manufacturing. It demonstrates that it is possible to use Whole Body Model Predictive Control (WBMPC) to reliably insert a tool in the holes of an aircraft structure with an accuracy of few millimeters. This result is based on the use of Crocoddyl, an optimal control library that exploits differential dynamic programming (DDP) to achieve high numerical efficiency. The focus of this article is put on the procedure that was undertaken to shape the cost function of the optimal controller. Our approach has first been validate in a low performance setting on the humanoid robot TALOS. Then, a strategy to improve the performances by reinjecting information about the posture of the robot from previous experiments is showed in simulation.</td>
                <td>Torque, Service robots, Shape, Humanoid robots, Optimal control, Cost function, Manufacturing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342428&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342428&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Joint Space Reference Manifold for Reliable Physical Assistance</td>
                <td>A. Razmjoo, T. Brecelj, K. Savevska, A. Ude, T. Petrič and S. Calinon</td>
                <td>2023</td>
                <td>This paper presents a study on the use of the Talos humanoid robot for performing assistive sit-to-stand or stand-to-sit tasks. In such tasks, the human exerts a large amount of force (100–200 N) within a very short time (2–8 s), posing significant challenges in terms of human unpredictability and robot stability control. To address these challenges, we propose an approach for finding a spatial reference for the robot, which allows the robot to move according to the force exerted by the human and control its stability during the task. Specifically, we focus on the problem of finding a 1D manifold for the robot, while assuming a simple controller to guide its movement on this manifold. To achieve this, we use a functional representation to parameterize the manifold and solve an optimization problem that takes into account the robot's stability and the unpredictability of human behavior. We demonstrate the effectiveness of our approach through simulations and experiments with the Talos robot, showing robustness and adaptability.</td>
                <td>Manifolds, Uncertainty, Robot kinematics, Force, Redundancy, Stability analysis, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342173&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342173&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MELP: Model Embedded Linear Policies for Robust Bipedal Hopping</td>
                <td>R. Soni, G. A. Castillo, L. Krishna, A. Hereid and S. Kolathaya</td>
                <td>2023</td>
                <td>Linear policies are the simplest class of policies that can achieve stable bipedal walking behaviors in both simulation and hardware. However, a significant challenge in deploying them widely is the difficulty in extending them to more dynamic behaviors like hopping and running. Therefore, in this work, we propose a new class of linear policies in which template models can be embedded. In particular, we show how to embed Spring Loaded Inverted Pendulum (SLIP) model in the policy class and realize perpetual hopping in arbitrary directions. The spring constant of the template model is learned in addition to the remaining parameters of the policy. Given this spring constant, the goal is to realize hopping trajectories using the SLIP model, which are then tracked by the bipedal robot using the linear policy. Continuous hopping with adjustable heading direction was achieved across different terrains in simulation with heading and lateral velocities of up to O.5m/ sec and 0.05m/ sec, respectively. The policy was then transferred to the hardware, and preliminary results (> 10 steps) of hopping were achieved.</td>
                <td>Solid modeling, Three-dimensional displays, Hardware, Real-time systems, Behavioral sciences, Trajectory, Springs, Humanoid and Bipedal Locomotion, Rein-forcement Learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342023&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342023&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>evoBOT – Design and Learning-Based Control of a Two-Wheeled Compound Inverted Pendulum Robot</td>
                <td>P. Klokowski et al.</td>
                <td>2023</td>
                <td>This paper introduces evoBOT, a novel robot platform for research on highly dynamic locomotion and human-machine interaction. evoBOT is capable of performing complex tasks such as handovers or manipulation while moving at high speeds. We provide an overview of the robot's core features and the underlying design decisions on both the mechanical and the electronic level. Moreover, we propose a reinforcement learning (RL) based control approach for training highly dynamic motions that is evaluated on a first set of robotic tasks, including robust balancing and dynamic locomotion. Lastly, we conduct extensive benchmarking on the adopted sim-to-real methods and present an initial sim-to-real pipeline for first transfer of the trained policies to the real robot. To accelerate robotics research in this direction, the full simulation model of the robot is released as open-source.</td>
                <td>Training, Dynamics, Pipelines, Neural networks, Reinforcement learning, Robot sensing systems, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342128&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342128&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Investigations into Exploiting the Full Capabilities of a Series-Parallel Hybrid Humanoid Using Whole Body Trajectory Optimization</td>
                <td>M. Boukheddimi, R. Kumar, S. Kumar, J. Carpentier and F. Kirchner</td>
                <td>2023</td>
                <td>Trajectory optimization methods have become ubiquitous for the motion planning and control of underactuated robots for e.g., quadrupeds, humanoids etc. While they have been extensively used in the case of serial or tree type robots, they are seldomly used for planning and control of robots with closed loops. Series-parallel hybrid topology is quite commonly used in the design of humanoid robots, but they are often neglected during trajectory optimization and the movements are computed for a serial abstraction of the system and then the solution is mapped to the actuator coordinates. As a consequence, the full capability of the robot cannot be exploited. This paper presents a case study of trajectory optimization for series-parallel hybrid robot by taking into account all the holonomic constraints imposed by the closed kinematic loops present in the system. We demonstrate the advantages of this consideration with a weightlifting task on RH5 Manus humanoid in both simulation and experiments.</td>
                <td>Actuators, Robot kinematics, Humanoid robots, Kinematics, Planning, Topology, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341784&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341784&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Integrable Whole-Body Orientation Coordinates for Legged Robots</td>
                <td>M. Chen, G. Nelson, R. Griffin, M. Posa and J. Pratt</td>
                <td>2023</td>
                <td>Complex multibody legged robots can have complex rotational control challenges. In this paper, we propose a concise way to understand and formulate a whole-body orientation that (i) depends on system configuration only and not a history of motion, (ii) can be representative of the orientation of the entire system while not being attached to any specific link, and (iii) has a rate of change that approximates total system angular momentum. We relate this orientation coordinate to past work, and discuss and demonstrate, including on hardware, several different uses for it.</td>
                <td>Legged locomotion, Robot kinematics, Hardware, History, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341531&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341531&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Multimodal Bipedal Locomotion and Implicit Transitions: A Versatile Policy Approach</td>
                <td>L. Krishna and Q. Nguyen</td>
                <td>2023</td>
                <td>In this paper, we propose a novel framework for synthesizing a single multimodal control policy capable of generating diverse behaviors (or modes) and emergent inherent transition maneuvers for bipedal locomotion. In our method, we first learn efficient latent encodings for each behavior by training an autoencoder from a dataset of rough reference motions. These latent encodings are used as commands to train a multimodal policy through an adaptive sampling of modes and transitions to ensure consistent performance across different behaviors. We validate the policy's performance in simulation for various distinct locomotion modes such as walking, leaping, jumping on a block, standing idle, and all possible combinations of inter-mode transitions. Finally, we integrate a task-based planner to rapidly generate open-loop mode plans for the trained multimodal policy to solve high-level tasks like reaching a goal position on a challenging terrain. Complex parkour-like motions by smoothly combining the discrete locomotion modes were generated in $\sim 3$ min. to traverse tracks with a gap of width 0.45 m, a plateau of height 0.2 m, and a block of height 0.4 m, which are all significant compared to the dimensions of our mini-biped platform.</td>
                <td>Training, Legged locomotion, Tracking, Simulation, Encoding, Behavioral sciences, Steady-state</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342398&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342398&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CLF-CBF Constraints for Real-Time Avoidance of Multiple Obstacles in Bipedal Locomotion and Navigation</td>
                <td>K. Huang</td>
                <td>2023</td>
                <td>This paper presents a reactive planning system that allows a Cassie-series bipedal robot to avoid multiple non-overlapping obstacles via a single, continuously differentiable control barrier function (CBF). The overall system detects an individual obstacle via a height map derived from a LiDAR point cloud and computes an elliptical outer approximation, which is then turned into a CBF. The QP-CLF-CBF formalism developed by Ames et al. is applied to ensure that safe trajectories are generated. Safe planning in environments with multiple obstacles is demonstrated both in simulation and experimentally on the Cassie biped.</td>
                <td>Point cloud compression, Meters, Laser radar, Navigation, Green products, Robot sensing systems, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341626&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341626&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Next-Best-View Selection from Observation Viewpoint Statistics</td>
                <td>S. Aravecchia, A. Richard, M. Clausel and C. Pradalier</td>
                <td>2023</td>
                <td>This paper discusses the problem of autonomously constructing a qualitative map of an unknown 3D environment using a 3D-Lidar. In this case, how can we effectively integrate the quality of the 3D-reconstruction into the selection of the Next-Best-View? Here, we address the challenge of estimating the quality of the currently reconstructed map in order to guide the exploration policy, in the absence of ground truth, which is typically the case in exploration scenarios. Our key contribution is a method to build a prior on the quality of the reconstruction from the data itself. Indeed, we not only prove that this quality depends on statistics from the observation viewpoints, but we also demonstrate that we can enhance the quality of the reconstruction by leveraging these statistics during the exploration. To do so, we propose to integrate them into Next-Best-View selection policies, in which the information gain is directly computed based on these statistics. Finally, we demonstrate the robustness of our approach, even in challenging environments, with noise in the robot localization, and we further validate it through a real-world experiment.</td>
                <td>Deep learning, Three-dimensional displays, Statistical analysis, Buildings, Reinforcement learning, Robustness, Robot localization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341982&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341982&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Feedback Motion Prediction for Safe Unicycle Robot Navigation</td>
                <td>A. İşleyen, N. van de Wouw and Ö. Arslan</td>
                <td>2023</td>
                <td>As a simple and robust mobile robot base, differential drive robots that can be modelled as a kinematic unicycle find significant applications in logistics and service robotics in both industrial and domestic settings. Safe robot navigation around obstacles is an essential skill for such unicycle robots to perform diverse useful tasks in complex cluttered environments, especially around people and other robots. Fast and accurate safety assessment plays a key role in reactive and safe robot motion design. In this paper, as a more accurate and still simple alternative to the standard circular Lyapunov level sets, we introduce novel conic feedback motion prediction methods for bounding the close-loop motion trajectory of the kinematic unicycle robot model under a standard unicycle motion control approach. We present an application of unicycle feedback motion prediction for safe robot navigation around obstacles using reference governors, where the safety of a unicycle robot is continuously monitored based on the predicted future robot motion. We investigate the role of motion prediction on robot behaviour in numerical simulations and conclude that fast and accurate feedback motion prediction is key for fast, reactive, and safe robot navigation around obstacles.</td>
                <td>Robot motion, Navigation, Service robots, Prediction methods, Kinematics, Robot sensing systems, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341787&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341787&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learned Parameter Selection for Robotic Information Gathering</td>
                <td>C. E. Denniston, G. Salhotra, A. Kangaslahti, D. A. Caron and G. S. Sukhatme</td>
                <td>2023</td>
                <td>When robots are deployed in the field for environmental monitoring they typically execute pre-programmed motions, such as lawnmower paths, instead of adaptive methods, such as informative path planning. One reason for this is that adaptive methods are dependent on parameter choices that are both critical to set correctly and difficult for the non-specialist to choose. Here, we show how to automatically configure a planner for informative path planning by training a reinforcement learning agent to select planner parameters at each iteration of informative path planning. We demonstrate our method with 37 instances of 3 distinct environments, and compare it against pure (end-to-end) reinforcement learning techniques, as well as approaches that do not use a learned model to change the planner parameters. Our method shows a 9.53% mean improvement in the cumulative reward across diverse environments when compared to end-to-end learning based methods; we also demonstrate via a field experiment how it can be readily used to facilitate high performance deployment of an information gathering robot.</td>
                <td>Training, Learning systems, Reinforcement learning, Trajectory, Environmental monitoring, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342080&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342080&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FISS+: Efficient and Focused Trajectory Generation and Refinement Using Fast Iterative Search and Sampling Strategy</td>
                <td>S. Sun et al.</td>
                <td>2023</td>
                <td>Trajectory planning plays a crucial role in autonomous driving systems, as it is tasked to generate feasible trajectories under highly dynamic scenarios within the time constraint. This paper proposes a novel two-stage coarse-to-fine framework for efficient sampling-based trajectory planning. The proposed method is designed to iteratively generate new trajectory samples focused on the low-cost regions in the sampling space. Two trajectory exploration algorithms are well-designed for efficient search in discretized coarse global space and continuous fine local space, respectively. Experimental results on the first-of-its-kind planning benchmark tool CommonRoad show that our method significantly outperforms the baseline methods both in optimality and computational efficiency. Overall, our approach offers a promising solution for efficient and effective trajectory planning in more autonomous vehicle applications.</td>
                <td>Trajectory planning, Benchmark testing, Trajectory, Computational efficiency, Space exploration, Iterative methods, Time factors, Motion and path planning, autonomous vehicle navigation, trajectory sampling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341498&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341498&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Failure-Adaptive Control for Dynamic Robots</td>
                <td>J. Hackett and C. Hubicki</td>
                <td>2023</td>
                <td>The human world is full of risks that threaten failure of robotic tasks. Dynamic robots, such as agile drones and walking bipeds, are particularly susceptible to failure because their time to make critical decisions is short. This work seeks a control algorithm which adapts to failures and reprioritizes robot behavior automatically, all at real-time speeds. Our failure-adaptive control framework learns failure probabilities from in situ experience and minimizes the risk of future failures using fast online planners (i.e. model predictive control). By reasoning about probabilities of failure, more imminent risks are automatically prioritized by the framework without manually tuning weighting factors. Further, our low-order probability model is learned using fast convex optimizations, allowing for immediate learning from triggered failures during operation. We demonstrate the framework's capability to learn and plan in real time (< 20 ms) in highly dynamic scenarios with micro-aerial vehicles (i.e. drones). We conduct two experiments: a chase-avoid task, and a chase-avoid - track task. In both scenarios, a single failure causes a categorical shift in robot behavior and the drone will adapt, plan, and execute a non-failing strategy within one second post-failure.</td>
                <td>Legged locomotion, Heuristic algorithms, Prediction algorithms, Real-time systems, Vehicle dynamics, Task analysis, Tuning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341946&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341946&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>KGNv2: Separating Scale and Pose Prediction for Keypoint-Based 6-DoF Grasp Synthesis on RGB-D Input</td>
                <td>Y. Chen, R. Xu, Y. Lin, H. Chen and P. A. Vela</td>
                <td>2023</td>
                <td>We propose an improved keypoint approach for 6-DoF grasp pose synthesis from RGB-D input. Keypoint-based grasp detection from image input demonstrated promising results in a previous study, where the visual information provided by color imagery compensates for noisy or imprecise depth measurements. However, it relies heavily on accurate keypoint prediction in image space. We devise a new grasp generation network that reduces the dependency on precise keypoint estimation. Given an RGB-D input, the network estimates both the grasp pose and the camera-grasp length scale. Re-design of the keypoint output space mitigates the impact of keypoint prediction noise on Perspective-n-Point (PnP) algorithm solutions. Experiments show that the proposed method outperforms the baseline by a large margin, validating its design. Though trained only on simple synthetic objects, our method demonstrates sim-to-real capacity through competitive results in real-world robot experiments.</td>
                <td>Visualization, Sensitivity, Shape, Numerical analysis, Image color analysis, Pose estimation, Prediction algorithms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342514&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342514&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning-Based Real-Time Torque Prediction for Grasping Unknown Objects with a Multi-Fingered Hand</td>
                <td>D. Winkelbauer, B. Bäuml and R. Triebel</td>
                <td>2023</td>
                <td>When grasping objects with a multi-finger hand, it is crucial for the grasp stability to apply the correct torques at each joint so that external forces are countered. Most current systems use simple heuristics instead of modeling the required torque correctly. Instead, we propose a learning-based approach that is able to predict torques for grasps on unknown objects in real-time. The neural network, trained end-to-end using supervised learning, is shown to predict torques that are more efficient, and the objects are held with less involuntary movement compared to all tested heuristic baselines. Specifically, for 90 % of the grasps the translational deviation of the object is below 2.9 mm and the rotational below 3.1°. To generate training data, we formulate the analytical computation of torques as an optimization problem and handle the indeterminacy of multi-contacts using an elastic model. We further show that the network generalizes to predict torques for unknown objects on the real robot system with an inference time of 1.5 ms. Website: dlr-alr.github.io/grasping/</td>
                <td>Analytical models, Torque, Computational modeling, Neural networks, Supervised learning, Training data, Grasping</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341970&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341970&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Grasp Pose is All You Need: Learning Multi-Fingered Grasping with Deep Reinforcement Learning from Vision and Touch</td>
                <td>F. Ceola, E. Maiettini, L. Rosasco and L. Natale</td>
                <td>2023</td>
                <td>Multi-fingered robotic hands have potential to enable robots to perform sophisticated manipulation tasks. However, teaching a robot to grasp objects with an anthropomorphic hand is an arduous problem due to the high dimensionality of state and action spaces. Deep Reinforcement Learning (DRL) offers techniques to design control policies for this kind of problems without explicit environment or hand modeling. However, state-of-the-art model-free algorithms have proven inefficient for learning such policies. The main problem is that the exploration of the environment is unfeasible for such high-dimensional problems, thus hampering the initial phases of policy optimization. One possibility to address this is to rely on off-line task demonstrations, but, oftentimes, this is too demanding in terms of time and computational resources. To address these problems, we propose the A Grasp Pose is All You Need (G-PAYN) method for the anthropomorphic hand of the iCub humanoid. We develop an approach to automatically collect task demonstrations to initialize the training of the policy. The proposed grasping pipeline starts from a grasp pose generated by an external algorithm, used to initiate the movement. Then a control policy (previously trained with the proposed G-PAYN) is used to reach and grab the object. We deployed the iCub into the MuJoCo simulator and use it to test our approach with objects from the YCB-Video dataset. Results show that G-PAYN outperforms current DRL techniques in the considered setting in terms of success rate and execution time with respect to the baselines. The code to reproduce the experiments is released together with the paper with an open source license11https://github.com/hsp-iit/rl-icub-dexterous-manipulation.</td>
                <td>Training, Deep learning, Visualization, Pipelines, Propioception, Grasping, Reinforcement learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341776&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341776&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Physics-Informed Learning to Enable Robotic Screw-Driving Under Hole Pose Uncertainties</td>
                <td>O. M. Manyar, S. V. Narayan, R. Lengade and S. K. Gupta</td>
                <td>2023</td>
                <td>Screw-driving is an important operation in numerous applications. In many situations, hole pose cannot be estimated very accurately. Autonomous screw-driving cannot be performed by traditional industrial manipulators in position control mode when the hole pose uncertainty is high. This paper presents a mobile manipulator system for performing autonomous screw-driving in the presence of uncertainties in the hole estimates. It utilizes active compliance in the form of impedance control of the robot and passive compliance in the screwing driving tool to deal with uncertainties. We present a physics-informed machine learning approach to automatically characterize the motion of the screw tip and explain how this motion leads to successful operation in the presence of uncertainty. We also present an approach for detecting failure modes and taking corrective actions. Code and video is available at: https://sites.google.com/usc.edu/physicsinformedscrewdriving</td>
                <td>Uncertainty, Service robots, Position control, Fasteners, Predictive models, Manipulators, Impedance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342151&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342151&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ADMNet: Anti-Drone Real-Time Detection and Monitoring</td>
                <td>X. Zhou et al.</td>
                <td>2023</td>
                <td>We propose a lightweight, effective, and efficient anti-drone network, namely ADMNet, for visually detecting and monitoring unfriendly drones with a constrained view field, flying against a complex environment. We merge an SPP module to the first head of YOLOv4 to improve accuracy and perform network compression to reduce inference latency and model size. To compensate for the accuracy loss caused by condensation, we propose an SPPS module and a ResNeck module for the neck of the network and implement an effective attention module for the backbone. Eventually, we present an accurate and compact ADMNet with barely 3.9 MB, ensuring low computational cost and real-time detection. Our method achieves state-of-the-art performance on three challenging real-world datasets (Average Precision @0.5IoU): Det-Fly 96.2%, NPS-Drones 92.0%, and TIBNet 89.7%. The throughput is higher than the prior work, in addition to its superior performance. The comparative testing in real-world scenarios proves that our method exhibits strong reliability and generalization ability. Deploying the network on drone onboard edge-computing devices enables real-time detection and monitoring of flying drones, highlighting the portability and viability of the ADMNet.</td>
                <td>Performance evaluation, Throughput, Real-time systems, Neck, Reliability, Monitoring, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341901&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341901&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-View Stereo with Learnable Cost Metric</td>
                <td>G. Yang et al.</td>
                <td>2023</td>
                <td>In this paper, we present LCM-MVSNet, a novel multi-view stereo (MVS) network with learnable cost metric (LCM) for more accurate and complete depth estimation and dense point cloud reconstruction. To adapt to the scene variation and improve the reconstruction quality in non-Lambertian low-textured scenes, we propose LCM to adaptively aggregate multi-view matching similarity into the 3D cost volume by leveraging sparse points hints. The proposed LCM benefits the MVS approaches in four folds, including depth estimation enhancement, reconstruction quality improvement, memory footprint reduction, and computational burden alleviation, allowing the depth inference for high-resolution images to achieve more accurate and complete reconstruction. Moreover, we improve the depth estimation by enhancing the propagation of shallow features via a bottom-up path and strengthen the end-to-end supervision by adapting the focal loss to reduce ambiguity caused by sample imbalance. Extensive experiments on two benchmark datasets show that our network achieves state-of-the-art performance on the DTU dataset and exhibits strong generalization ability with a competitive performance on the Tanks and Temples benchmark. Furthermore, we deploy our LCM-MVSNet into the real-world application for large-scale 3D reconstruction based on multi-view aerial images collected by self-developed UAV, demonstrating the robustness and scalability of our method. More detailed results are available in the Appendix11shorturl.at/rBG28</td>
                <td>Measurement, Point cloud compression, Costs, Three-dimensional displays, Scalability, Estimation, Benchmark testing, depth estimation, cost volume aggregation, multi-view stereo, 3D reconstruction, UAV</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341606&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341606&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Comparison Between Framed-Based and Event-Based Cameras for Flapping-Wing Robot Perception</td>
                <td>R. Tapia et al.</td>
                <td>2023</td>
                <td>Perception systems for ornithopters face severe challenges. The harsh vibrations and abrupt movements caused during flapping are prone to produce motion blur and strong lighting condition changes. Their strict restrictions in weight, size, and energy consumption also limit the type and number of sensors to mount onboard. Lightweight traditional cameras have become a standard off-the-shelf solution in many flapping-wing designs. However, bioinspired event cameras are a promising solution for ornithopter perception due to their microsecond temporal resolution, high dynamic range, and low power consumption. This paper presents an experimental comparison between frame-based and an event-based camera. Both technologies are analyzed considering the particular flapping-wing robot specifications and also experimentally analyzing the performance of well-known vision algorithms with data recorded onboard a flapping-wing robot. Our results suggest event cameras as the most suitable sensors for ornithopters. Nevertheless, they also evidence the open challenges for event-based vision on board flapping-wing robots.</td>
                <td>Vibrations, Power demand, Heuristic algorithms, Robot vision systems, Lighting, Cameras, High dynamic range, ornithopter UAV, flapping-wing aerial robot, event camera, computer vision</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342500&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342500&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Flexible Multi-DoF Aerial 3D Printing Supported with Automated Optimal Chunking</td>
                <td>N. Stamatopoulos, A. Banerjee and G. Nikolakopoulos</td>
                <td>2023</td>
                <td>The future of 3D printing utilizing unmanned aerial vehicles (UAVs) presents a promising capability to revolutionize manufacturing and to enable the creation of large-scale structures in remote and hard-to-reach areas e.g. in other planetary systems. Nevertheless, the limited payload capacity of UAVs and the complexity in the 3D printing of large objects pose significant challenges. In this article we propose a novel chunk-based framework for distributed 3D printing using UAVs that sets the basis for a fully collaborative aerial 3D printing of challenging structures. The presented framework, through a novel proposed optimisation process, is able to divide the 3D model to be printed into small, manageable chunks and to assign them to a UAV for partial printing of the assigned chunk, in a fully autonomous approach. Thus, we establish the algorithms for chunk division, allocation, and printing, and we also introduce a novel algorithm that efficiently partitions the mesh into planar chunks, while accounting for the inter-connectivity constraints of the chunks. The efficiency of the proposed framework is demonstrated through multiple physics based simulations in Gazebo, where a CAD construction mesh is printed via multiple UAVs carrying materials whose volume is proportionate to a fraction of the total mesh volume.</td>
                <td>Solid modeling, Three-dimensional displays, Shape, Three-dimensional printing, Autonomous aerial vehicles, Partitioning algorithms, Manufacturing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341882&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341882&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Memory Maps for Video Object Detection and Tracking on UAVs</td>
                <td>B. Kiefer, Y. Quan and A. Zell</td>
                <td>2023</td>
                <td>This paper introduces a novel approach to video object detection detection and tracking on Unmanned Aerial Vehicles (UAVs). By incorporating metadata, the proposed approach creates a memory map of object locations in actual world coordinates, providing a more robust and interpretable representation of object locations in both, image space and the real world. We use this representation to boost confidences, resulting in improved performance for several temporal computer vision tasks, such as video object detection, short and long-term single and multi-object tracking, and video anomaly detection. These findings confirm the benefits of metadata in enhancing the capabilities of UAVs in the field of temporal computer vision and pave the way for further advancements in this area.</td>
                <td>Computer vision, Object detection, Metadata, Autonomous aerial vehicles, Task analysis, Intelligent robots, Anomaly detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342453&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342453&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Localization of Aerial Vehicles via Active Control of Identical Ground Vehicles</td>
                <td>I. Spasojevic, X. Liu, A. Prabhu, A. Ribeiro, G. J. Pappas and V. Kumar</td>
                <td>2023</td>
                <td>This paper addresses the problem of active collaborative localization in heterogeneous robot teams with unknown data association. It involves positioning a small number of identical unmanned ground vehicles (UGVs) at desired positions so that an unmanned aerial vehicle (UAV) can, through unlabelled measurements of UGVs, uniquely determine its global pose. We model the problem as a sequential two player game, in which the first player positions the UGVs and the second identifies the two distinct hypothetical poses of the UAV at which the sets of measurements to the UGVs differ by as little as possible. We solve the underlying problem from the vantage point of the first player for a subclass of measurement models using a mixture of local optimization and exhaustive search procedures. Real-world experiments with a team of UAV and UGVs show that our method can achieve centimeter-level global localization accuracy. We also show that our method consistently outperforms random positioning of UGVs by a large margin, with as much as a 90% reduction in position and angular estimation error. Our method can tolerate a significant amount of random as well as non-stochastic measurement noise. This indicates its potential for reliable state estimation on board size, weight, and power (SWaP) constrained UAVs. This work enables robust localization in perceptually-challenged GPS-denied environments, thus paving the road for large-scale multi-robot navigation and mapping.</td>
                <td>Location awareness, Magnetometers, Autonomous aerial vehicles, Reliability, Odometry, Noise measurement, State estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341900&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341900&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Semantically-Enhanced Deep Collision Prediction for Autonomous Navigation Using Aerial Robots</td>
                <td>M. Kulkarni, H. Nguyen and K. Alexis</td>
                <td>2023</td>
                <td>This paper contributes a novel and modularized learning-based method for aerial robots navigating cluttered environments containing hard-to-perceive thin obstacles without assuming access to a map or the full pose estimation of the robot. The proposed solution builds upon a semantically-enhanced Variational Autoencoder that is trained with both real-world and simulated depth images to compress the input data, while preserving semantically-labeled thin obstacles and handling invalid pixels in the depth sensor's output. This compressed representation, in addition to the robot's partial state involving its linear/angular velocities and its attitude are then utilized to train an uncertainty-aware 3D Collision Prediction Network in simulation to predict collision scores for candidate action sequences in a predefined motion primitives library. A set of simulation and experimental studies in cluttered environments with various sizes and types of obstacles, including multiple hard-to-perceive thin objects, were conducted to evaluate the performance of the proposed method and compare against an end-to-end trained baseline. The results demonstrate the benefits of the proposed semantically-enhanced deep collision prediction for learning-based autonomous navigation.</td>
                <td>Image coding, Navigation, Robot sensing systems, Autonomous aerial vehicles, Surface texture, Trajectory, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342297&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342297&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Demonstrating Autonomous 3D Path Planning on a Novel Scalable UGV-UAV Morphing Robot</td>
                <td>E. Sihite et al.</td>
                <td>2023</td>
                <td>Some animals exhibit multi-modal locomotion capability to traverse a wide range of terrains and environments, such as amphibians that can swim and walk or birds that can fly and walk. This capability is extremely beneficial for expanding the animal's habitat range and they can choose the most energy efficient mode of locomotion in a given environment. The robotic biomimicry of this multi-modal locomotion capability can be very challenging but offer the same advantages. However, the expanded range of locomotion also increases the complexity of performing localization and path planning. In this work, we present our morphing multi-modal robot, which is capable of ground and aerial locomotion, and the implementation of readily available SLAM and path planning solutions to navigate a complex indoor environment.</td>
                <td>Location awareness, Three-dimensional displays, Simultaneous localization and mapping, Navigation, Habitats, Path planning, Energy efficiency</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342189&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342189&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Topology-Guided Perception-Aware Receding Horizon Trajectory Generation for UAVs</td>
                <td>G. Sun, X. Zhang, Y. Liu, H. Wang, X. Zhang and Y. Zhuang</td>
                <td>2023</td>
                <td>The perception-aware motion planning method based on the localization uncertainty has the potential to improve the localization accuracy for robot navigation. How-ever, most of the existing perception-aware methods pre-build a global feature map and can not generate the perception- aware trajectory in real time. This paper proposes a topology- guided perception-aware receding horizon trajectory generation method, which contains a topology-guided position trajectory generation and a perception-aware yaw angle trajectory generation. Specifically, a memorable active map is built by selectively storing the visual landmarks. After that, a library of candidate topological trajectories are generated, which are then evaluated in terms of the perception quality based on the active map, smoothness, collision possibility and feasibility. In addition, the yaw angle trajectory is obtained through a front-end multiple refined path search and a back-end path- guided trajectory optimization. Comparative simulation and real-world experiments are carried out to confirm that the proposed method can keep more visual features in view and reduce the localization error.</td>
                <td>Location awareness, Visualization, Uncertainty, Navigation, Robot sensing systems, Real-time systems, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342075&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342075&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Nonlinear Deterministic Observer for Inertial Navigation Using Ultra-Wideband and IMU Sensor Fusion</td>
                <td>H. A. Hashim, A. E. E. Eltoukhy, K. G. Vamvoudakis and M. I. Abouheaf</td>
                <td>2023</td>
                <td>Navigation in Global Positioning Systems (GPS)-denied environments requires robust estimators reliant on fusion of inertial sensors able to estimate rigid-body's orientation, position, and linear velocity. Ultra-wideband (UWB) and Inertial Measurement Unit (IMU) represent low-cost measurement technology that can be utilized for successful Inertial Navigation. This paper presents a nonlinear deterministic navigation observer in a continuous form that directly employs UWB and IMU measurements. The estimator is developed on the extended Special Euclidean Group $\mathbb{SE}_{2}$ (3) and ensures exponential convergence of the closed loop error signals starting from almost any initial condition. The discrete version of the proposed observer is tested using a publicly available real-world dataset of a drone flight.</td>
                <td>Uncertainty, Measurement units, Inertial sensors, Inertial navigation, Observers, Sensor fusion, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342083&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342083&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Model-Free Grasping with Multi-Suction Cup Grippers for Robotic Bin Picking</td>
                <td>P. Schillinger, M. Gabriel, A. Kuss, H. Ziesche and N. A. Vien</td>
                <td>2023</td>
                <td>This paper presents a novel method for model-free prediction of grasp poses for suction grippers with multiple suction cups. Our approach is agnostic to the design of the gripper and does not require gripper-specific training data. In particular, we propose a two-step approach, where first, a neural network predicts pixel-wise grasp quality for an input image to indicate areas that are generally graspable. Second, an optimization step determines the optimal gripper selection and corresponding grasp poses based on configured gripper layouts and activation schemes. In addition, we introduce a method for automated labeling for supervised training of the grasp quality network. Experimental evaluations on a real-world industrial application with bin picking scenes of varying difficulty demonstrate the effectiveness of our method.</td>
                <td>Training, Service robots, Training data, Predictive models, Labeling, Reliability, Grippers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341555&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341555&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Vision-Based State and Pose Estimation for Robotic Bin Picking of Cables</td>
                <td>A. Monguzzi, C. Cella, A. M. Zanchettin and P. Rocco</td>
                <td>2023</td>
                <td>This paper deals with the challenging task of picking semi-deformable linear objects (SDLOs) from a bin. SDLOs are deformable elements, such as cables, joined to a rigid part as a connector. We propose a vision-based strategy to detect, classify and estimate the pose and the state (free or occluded) of connectors belonging to an unspecified number of SDLOs, arranged in an unknown configuration in the bin. The connectors can then be grasped and manipulated by a dual-arm robot through a set of manipulation primitives. In this way, a single SDLO can be extracted from the bin and laid on the worktable. A subsequent association between the connectors and the extracted SDLOs is performed, allowing to firmly grasp a SDLO at its ends to further manipulate it. The procedure is tested in bin picking operations with several kinds of SDLOs and is applied to a use case involving a collaborative wire harnesses assembly task.</td>
                <td>Connectors, Wires, Pose estimation, Collaboration, Grasping, Topology, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342374&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342374&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Visuo-Haptic Object Shape Completion for Robot Manipulation</td>
                <td>L. Rustler, J. Matas and M. Hoffmann</td>
                <td>2023</td>
                <td>For robot manipulation, a complete and accurate object shape is desirable. Here, we present a method that combines visual and haptic reconstruction in a closed-loop pipeline. From an initial viewpoint, the object shape is reconstructed using an implicit surface deep neural network. The location with highest uncertainty is selected for haptic exploration, the object is touched, the new information from touch and a new point cloud from the camera are added, object position is re-estimated and the cycle is repeated. We extend Rustler et al. (2022) by using a new theoretically grounded method to determine the points with highest uncertainty, and we increase the yield of every haptic exploration by adding not only the contact points to the point cloud but also incorporating the empty space established through the robot movement to the object. Additionally, the solution is compact in that the jaws of a closed two-finger gripper are directly used for exploration. The object position is re-estimated after every robot action and multiple objects can be present simultaneously on the table. We achieve a steady improvement with every touch using three different metrics and demonstrate the utility of the better shape reconstruction in grasping experiments on the real robot. On average, grasp success rate increases from 63.3 % to 70.4 % after a single exploratory touch and to 82.7% after five touches. The collected data and code are publicly available (https://osf.io/j6rkd/, https://github.com/ctu-vras/vishac).</td>
                <td>Point cloud compression, Measurement, Surface reconstruction, Uncertainty, Codes, Shape, Pipelines</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342200&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342200&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Force Map: Learning to Predict Contact Force Distribution from Vision</td>
                <td>Alpizar, B. Leme and T. Ogata</td>
                <td>2023</td>
                <td>When humans see a scene, they can roughly imagine the forces applied to objects based on their expe-rience and use them to handle the objects properly. This paper considers transferring this “force-visualization” ability to robots. We hypothesize that a rough force distribution (named “force map”) can be utilized for object manipulation strategies even if accurate force estimation is impossible. Based on this hypothesis, we propose a training method to predict the force map from vision. To investigate this hypothesis, we generated scenes where objects were stacked in bulk through simulation and trained a model to predict the contact force from a single image. We further applied domain randomization to make the trained model function on real images. The experimental results showed that the model trained using only synthetic images could predict approximate patterns representing the contact areas of the objects even for real images. Then, we designed a simple algorithm to plan a lifting direction using the predicted force distribution. We confirmed that using the predicted force distribution contributes to finding natural lifting directions for typical real-world scenes. Furthermore, the evaluation through simulations showed that the disturbance caused to surrounding objects was reduced by 26 % (translation displacement) and by 39 % (angular displacement) for scenes where objects were overlapping.</td>
                <td>Training, Visualization, Solid modeling, Three-dimensional displays, Force, Training data, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342092&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342092&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Push to Know! - Visuo-Tactile Based Active Object Parameter Inference with Dual Differentiable Filtering</td>
                <td>A. Dutta, E. Burdet and M. Kaboli</td>
                <td>2023</td>
                <td>For robotic systems to interact with objects in dynamic environments, it is essential to perceive the physical properties of the objects such as shape, friction coefficient, mass, center of mass, and inertia. This not only eases selecting manipulation action but also ensures the task is performed as desired. However, estimating the physical properties of especially novel objects is a challenging problem, using either vision or tactile sensing. In this work, we propose a novel framework to estimate key object parameters using non-prehensile manipulation using vision and tactile sensing. Our proposed active dual differentiable filtering (ADDF) approach as part of our framework learns the object-robot interaction during non-prehensile object push to infer the object's parameters. Our proposed method enables the robotic system to employ vision and tactile information to interactively explore a novel object via non-prehensile object push. The novel proposed $N$-step active formulation within the differentiable filtering facilitates efficient learning of the object-robot interaction model and during inference by selecting the next best exploratory push actions (where to push? and how to push?). We extensively evaluated our framework in simulation and real-robotic scenarios, yielding superior performance to the state-of-the-art baseline.</td>
                <td>Parameter estimation, Filtering, Shape, Machine vision, Friction, Grasping, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341832&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341832&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>IOSG: Image-Driven Object Searching and Grasping</td>
                <td>H. Yu, X. Lou, Y. Yang and C. Choi</td>
                <td>2023</td>
                <td>When robots retrieve specific objects from cluttered scenes, such as home and warehouse environments, the target objects are often partially occluded or completely hidden. Robots are thus required to search, identify a target object, and successfully grasp it. Preceding works have relied on pre-trained object recognition or segmentation models to find the target object. However, such methods require laborious manual annotations to train the models and even fail to find novel target objects. In this paper, we propose an Image-driven Object Searching and Grasping (IOSG) approach where a robot is provided with the reference image of a novel target object and tasked to find and retrieve it. We design a Target Similarity Network that generates a probability map to infer the location of the novel target. IOSG learns a hierarchical policy; the high-level policy predicts the subtask type, whereas the low-level policies, explorer and coordinator, generate effective push and grasp actions. The explorer is responsible for searching the target object when it is hidden or occluded by other objects. Once the target object is found, the coordinator conducts target-oriented pushing and grasping to retrieve the target from the clutter. The proposed pipeline is trained with full self-supervision in simulation and applied to a real environment. Our model achieves a 96.0% and 94.5% task success rate on coordination and exploration tasks in simulation respectively, and 85.0% success rate on a real robot for the search-and-grasp task. Please refer to our project page for more information: https://z.umn.edu/iosg.</td>
                <td>Instance segmentation, Robot kinematics, Pipelines, Grasping, Manuals, Search problems, Object recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342009&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342009&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DexRepNet: Learning Dexterous Robotic Grasping Network with Geometric and Spatial Hand-Object Representations</td>
                <td>Q. Liu et al.</td>
                <td>2023</td>
                <td>Robotic dexterous grasping is a challenging problem due to the high degree of freedom (DoF) and complex contacts of multi-fingered robotic hands. Existing deep re-inforcement learning (DRL) based methods leverage human demonstrations to reduce sample complexity due to the high dimensional action space with dexterous grasping. However, less attention has been paid to hand-object interaction representations for high-level generalization. In this paper, we propose a novel geometric and spatial hand-object interaction representation, named DexRep, to capture object surface features and the spatial relations between hands and objects during grasping. DexRep comprises Occupancy Feature for rough shapes within sensing range by moving hands, Surface Feature for changing hand-object surface distances, and LocalGeo Feature for local geometric surface features most related to potential contacts. Based on the new representation, we propose a dexterous deep reinforcement learning method DexRepNet to learn a generalizable grasping policy. Experimental results show that our method outperforms baselines using existing representations for robotic grasping dramatically both in grasp success rate and convergence speed. It achieves a 93% grasping success rate on seen objects and higher than 80% grasping success rates on diverse objects of unseen categories in both simulation and real-world experiments.</td>
                <td>Deep learning, Shape, Grasping, Reinforcement learning, Robot sensing systems, Surface roughness, Rough surfaces</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342334&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342334&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Active Acoustic Sensing for Robot Manipulation</td>
                <td>S. Lu and H. Culbertson</td>
                <td>2023</td>
                <td>Perception in robot manipulation has been actively explored with the goal of advancing and integrating vision and touch for global and local feature extraction. However, it is difficult to perceive certain object internal states, and the integration of visual and haptic perception is not compact and is easily biased. We propose to address these limitations by developing an active acoustic sensing method for robot manipulation. Active acoustic sensing relies on the resonant properties of the object, which are related to its material, shape, internal structure, and contact interactions with the gripper and environment. The sensor consists of a vibration actuator paired with a piezo-electric microphone. The actuator generates a waveform, and the microphone tracks the waveform's propagation and distortion as it travels through the object. This paper presents the sensing principles, hardware design, simulation development, and evaluation of physical and simulated sensory data under different conditions as a proof-of-concept. This work aims to provide fundamentals on a useful tool for downstream robot manipulation tasks using active acoustic sensing, such as object recognition, grasping point estimation, object pose estimation, and external contact formation detection.</td>
                <td>Vibrations, Actuators, Visualization, Shape, Pose estimation, Robot sensing systems, Acoustics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342481&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342481&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Grasp Region Exploration for 7-DoF Robotic Grasping in Cluttered Scenes</td>
                <td>S. Zheng</td>
                <td>2023</td>
                <td>Robotic grasping is a fundamental skill for robots, but it is quite challenging in cluttered scenes. In cluttered scenes, the precise prediction of high-quality grasp configurations such as rotation and grasping width while avoiding collisions is essential. To accomplish this, the grasp detection models require the capabilities of stronger fine-grained information extracted around the grasp points. However, due to the computational resource restriction, point clouds are usually downsampled in existing networks, which inevitably make some potentially important points discarded. To overcome this problem, we propose a Grasp Region Exploration module to explore the area covered by high-quality grasps. Based on the grasp region, we enhance the point density around the grasp points to mitigate the loss of information caused by downsampling. Furthermore, we devise the Grasp Region Attention module to dynamically aggregate features of various points within the grasp region, such as the grasp point and contact points. The proposed method achieves state-of-the-art performance on the large-scale GraspNet-1Billion dataset. We also conduct real-world experiments on a Franka Emika Panda robot and show that the robot can grasp objects in cluttered scenes with a high success rate.</td>
                <td>Point cloud compression, Computational modeling, Pipelines, Grasping, Predictive models, Benchmark testing, Data mining</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341757&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341757&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bagging by Learning to Singulate Layers Using Interactive Perception</td>
                <td>L. Y. Chen et al.</td>
                <td>2023</td>
                <td>Many fabric handling and 2D deformable material tasks in homes and industries require singulating layers of material such as opening a bag or arranging garments for sewing. In contrast to methods requiring specialized sensing or end effectors, we use only visual observations with ordinary parallel jaw grippers. We propose SLIP: Singulating Layers using Interactive Perception, and apply SLIP to the task of autonomous bagging. We develop SLIP-Bagging, a bagging algorithm that manipulates a plastic or fabric bag from an unstructured state and uses SLIP to grasp the top layer of the bag to open it for object insertion. In physical experiments, a YuMi robot achieves a success rate of 67% to 81% across bags of a variety of materials, shapes, and sizes, significantly improving in success rate and generality over prior work. Experiments also suggest that SLIP can be applied to tasks such as singulating layers of folded cloth and garments. Supplementary material is available at https://sites.google.com/view/slip-bagging/.</td>
                <td>Visualization, Shape, Clothing, Robot sensing systems, Fabrics, Sensors, Plastics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341634&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341634&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction</td>
                <td>Dafle, I. Kasahara, S. Engin, J. Huh and V. Isler</td>
                <td>2023</td>
                <td>In this paper, we present a realtime method for simultaneous object-level scene understanding and grasp prediction. Specifically, given a single RGBD image of a scene, our method localizes all the objects in the scene and for each object, it generates the following: full 3D shape, scale, pose with respect to the camera frame, and a dense set of feasible grasps. The main advantage of our method is its computation speed as it avoids sequential perception and grasp planning. With detailed quantitative analysis of reconstruction quality and grasp accuracy, we show that our method delivers competitive performance compared to the state-of-the-art methods, while providing fast inference at 30 frames per second speed.</td>
                <td>Training, Three-dimensional displays, Shape, Statistical analysis, Semantics, Robot vision systems, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342307&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342307&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Flexible Handover with Real-Time Robust Dynamic Grasp Trajectory Generation</td>
                <td>S. Fang, H. Fang and C. Lu</td>
                <td>2023</td>
                <td>In recent years, there has been a significant effort dedicated to developing efficient, robust, and general human-to-robot handover systems. However, the area of flexible handover in the context of complex and continuous objects' motion remains relatively unexplored. In this work, we propose an approach for effective and robust flexible handover, which enables the robot to grasp moving objects with flexible motion trajectories with a high success rate. The key innovation of our approach is the generation of real-time robust grasp trajec-tories. We also design a future grasp prediction algorithm to enhance the system's adaptability to dynamic handover scenes. We conduct one-motion handover experiments and motion-continuous handover experiments on our novel benchmark that includes 31 diverse household objects. The system we have developed allows users to move and rotate objects in their hands within a relatively large range. The success rate of the robot grasping such moving objects is 78.15 % over the entire household object benchmark.</td>
                <td>Technological innovation, Tracking, Heuristic algorithms, Handover, Benchmark testing, Prediction algorithms, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341777&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341777&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>HyperTraj: Towards Simple and Fast Scene-Compliant Endpoint Conditioned Trajectory Prediction</td>
                <td>R. Huang, M. Pagnucco and Y. Song</td>
                <td>2023</td>
                <td>An important task in trajectory prediction is to model the uncertainty of agents' motions, which requires the system to propose multiple plausible future trajectories for agents based on their past movements. Recently, many approaches have been developed following an endpointconditioned deep learning framework by firstly predicting the distribution of endpoints, then sampling endpoints from it and finally completing their waypoints. However, this framework suffers a severe efficiency issue as it needs to repeatedly execute a separate decoder conditioned on multiple sampled endpoints. In this work, we propose a simple and fast endpoint conditioned fully convolutional trajectory prediction framework, called HyperTraj, by using dynamic convolutions to generate multiple trajectories, with the main benefits that (1) our prediction is conditioned on endpoint but takes almost constant time when the number of goals increases and (2) our model benefits from convolutional based predictions, such as the acceptance of various scene sizes and better modeling of agent-scene interactions. In our experiment, our model shows comparable or even better accuracy than our state-of-the-art baselines on SDD and VIRAT datasets with around 84% of acceleration and 90% model weight reduction for waypoint decoding.</td>
                <td>Heating systems, Uncertainty, Computational modeling, Memory management, Predictive models, Trajectory, Decoding</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341647&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341647&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PanelPose: A 6D Pose Estimation of Highly-Variable Panel Object for Robotic Robust Cockpit Panel Inspection</td>
                <td>H. Sun, P. Ni, Z. Li, Y. Wang, X. Zhu and Q. Cao</td>
                <td>2023</td>
                <td>In robotic cockpit inspection scenarios, the 6D pose of highly-variable panel objects is necessary. However, the buttons with different states on the panel cause the variable texture and point cloud, which confuses the traditional invariable object pose estimation method. The bottleneck is the variable texture and point cloud. To address this issue, we propose a simple yet effective method denoted as PanelPose that leverages synthetic data and edge-line features. Specifically, we extract edge and line features of RGB images and fuse these feature maps as a multi-feature fusion map (MFF Map) to focus on the shape features of panel objects. Moreover, we design an effective keypoint selection algorithm considering the shape information of panel objects, which simplifies keypoint localization for precise pose estimation. Finally, the panel object pose is estimated via PNP/RANSAC, refined by the multi-state template (MST) and multi-scale ICP. We experimentally show that state-of-the-art 6D pose estimation methods alone are not sufficient to solve the cockpit panel inspection task but that our method significantly improves the performance. In cockpit inspection scenarios, the panel localization error is less than 3mm using our method. Code and data are available at https://github.com/sunhan1997/PaneIPose.</td>
                <td>Point cloud compression, Location awareness, Shape, Image edge detection, Pose estimation, Inspection, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342304&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342304&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Image Restoration via UAVFormer for Under-Display Camera of UAV</td>
                <td>Z. Zheng and X. Jia</td>
                <td>2023</td>
                <td>The exposed cameras of UAVs can shake, shift, or even malfunction under the influence of harsh weather, while the add-on devices (Dupont lines) are very vulnerable to dam-age. Although we can place a low-cost transparent film overlay around the camera to protect it, this would also introduce image degradation issues (such as oversaturation, astigmatism, etc). To tackle the image degradation problem caused by overlaying transparent film, in this paper we propose a novel method to enhance the visual experience by adapting a deep network with UAV characteristics. Specifically, we propose a customized Transformer named UAVFormer to recover the image, which has a key module at each stage based on the Swin Transformer with local awareness (LAT). In the end, we use an evidential fusion algorithm to integrate the generated images at each stage to obtain a high-quality result. Furthermore, we create a high-resolution under-display camera dataset to support the training and testing of compared models. Our model can conduct high-quality recovery of images of 2K resolution on some embedded devices (Raspberry Pi 4b) in realtime. The URL for the code at https://github.com/zzr-idam/UAVFormer.</td>
                <td>Degradation, Uniform resource locators, Training, Visualization, Cameras, Transformers, Autonomous aerial vehicles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342454&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342454&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Semantic Scene Difference Detection in Daily Life Patroling by Mobile Robots Using Pre-Trained Large-Scale Vision-Language Model</td>
                <td>Y. Obinata et al.</td>
                <td>2023</td>
                <td>It is important for daily life support robots to detect changes in their environment and perform tasks. In the field of anomaly detection in computer vision, probabilistic and deep learning methods have been used to calculate the image distance. These methods calculate distances by focusing on image pixels. In contrast, this study aims to detect semantic changes in the daily life environment using the current development of large-scale vision-language models. Using its Visual Question Answering (VQA) model, we propose a method to detect semantic changes by applying multiple questions to a reference image and a current image and obtaining answers in the form of sentences. Unlike deep learning-based methods in anomaly detection, this method does not require any training or fine-tuning, is not affected by noise, and is sensitive to semantic state changes in the real world. In our experiments, we demonstrated the effectiveness of this method by applying it to a patrol task in a real-life environment using a mobile robot, Fetch Mobile Manipulator. In the future, it may be possible to add explanatory power to changes in the daily life environment through spoken language.</td>
                <td>Training, Visualization, Navigation, Semantics, Probabilistic logic, Question answering (information retrieval), Mobile robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342467&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342467&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Seeing the Fruit for the Leaves: Robotically Mapping Apple Fruitlets in a Commercial Orchard</td>
                <td>A. Qureshi et al.</td>
                <td>2023</td>
                <td>Aotearoa New Zealand has a strong and growing apple industry but struggles to access workers to complete skilled, seasonal tasks such as thinning. To ensure effective thinning and make informed decisions on a per-tree basis, it is crucial to accurately measure the crop load of individual apple trees. However, this task poses challenges due to the dense foliage that hides the fruitlets within the tree structure. In this paper, we introduce the vision system of an automated apple fruitlet thinning robot, developed to tackle the labor shortage issue. This paper presents the initial design, implementation, and evaluation specifics of the system. The platform straddles the 3.4 m tall 2D apple canopy structures to create an accurate map of the fruitlets on each tree. We show that this platform can measure the fruitlet load on an apple tree by scanning through both sides of the branch. The requirement of an overarching platform was justified since two-sided scans had a higher counting accuracy of 81.17% than one-sided scans at 73.7%. The system was also demonstrated to produce size estimates within 5.9% RMSE of their true size.</td>
                <td>Industries, Machine vision, Fitting, Estimation, Crops, Vegetation, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341502&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341502&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Cross-Domain Autonomous Driving Perception Using Contrastive Appearance Adaptation</td>
                <td>K. Yeung</td>
                <td>2023</td>
                <td>Addressing domain shifts for complex perception tasks in autonomous driving has long been a challenging problem. In this paper, we show that existing domain adaptation methods pay little attention to the content mismatch issue between source and target domains, thus weakening the domain adaptation per-formance and the decoupling of domain-invariant and domain-specific representations. To solve the aforementioned problems, we propose an image-level domain adaptation framework that aims at adapting source-domain images to the target domain with content-aligned source-target image pairs. Our framework consists of three mutually beneficial modules in a cycle: a cross-domain content alignment module to generate source-target pairs with consistent content representations in a self-supervised manner, a reference-guided image synthesis based on the generated content-aligned source-target image pairs, and a contrastive learning module to self-supervise domain-invariant feature extractor. Our contrastive appearance adaptation is task-agnostic and robust to complex perception tasks in autonomous driving. Our proposed method demonstrates state-of-the-art results in cross-domain object detection, semantic segmentation, and depth estimation as well as better image synthesis ability qualitatively and quantitatively.</td>
                <td>Image synthesis, Semantic segmentation, Estimation, Object detection, Benchmark testing, Feature extraction, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342103&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342103&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MENTOR: Multilingual Text Detection Toward Learning by Analogy</td>
                <td>C. Huang</td>
                <td>2023</td>
                <td>Text detection is frequently used in vision-based mobile robots when they need to interpret texts in their surroundings to perform a given task. For instance, delivery robots in multilingual cities need to be capable of doing multilingual text detection so that the robots can read traffic signs and road markings. Moreover, the target languages change from region to region, implying the need of efficiently re-training the models to recognize the novel/new languages. However, collecting and labeling training data for novel languages are cumbersome, and the efforts to re-train an existing/trained text detector are considerable. Even worse, such a routine would repeat whenever a novel language appears. This motivates us to propose a new problem setting for tackling the aforementioned challenges in a more efficient way: “We ask for a generalizable multilingual text detection framework to detect and identify both seen and unseen language regions inside scene images without the requirement of collecting supervised training data for unseen languages as well as model re-training”. To this end, we propose “MENTOR”, the first work to realize a learning strategy between zero-shot learning and few-shot learning for multilingual scene text detection. During the training phase, we leverage the “zero-cost” synthesized printed texts and the available training/seen languages to learn the meta-mapping from printed texts to language-specific kernel weights. Meanwhile, dynamic convolution networks guided by the language-specific kernel are trained to realize a detection-by-feature-matching scheme. In the inference phase, “zero-cost” printed texts are synthesized given a new target language. By utilizing the learned meta-mapping and the matching network, our “MENTOR” can freely identify the text regions of the new language. Experiments show our model can achieve comparable results with supervised methods for seen languages and outperform other methods in detecting unseen languages.</td>
                <td>Training, Zero-shot learning, Target recognition, Urban areas, Text detection, Training data, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342419&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342419&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards a Robust Adversarial Patch Attack Against Unmanned Aerial Vehicles Object Detection</td>
                <td>S. Shrestha, S. Pathak and E. K. Viegas</td>
                <td>2023</td>
                <td>Object detection techniques for autonomous Un-manned Aerial Vehicles (UAV) are built upon Deep Neural Networks (DNN), which are known to be vulnerable to adversarial patch perturbation attacks that lead to object detection evasion. Yet, current adversarial patch generation schemes are not designed for UAV imagery settings. This paper proposes a new robust adversarial patch generation attack against object detection with UAVs. We build adversarial patches considering UAV-specific settings such as the UAV camera perspective, viewing angle, distance, and brightness changes. As a result, built patches can also degrade the accuracy of object detector models implemented with different initializations and architectures. Experiments conducted on the VisDrone dataset have shown the proposal's feasibility, achieving an attack success rate of up to 80% in a white-box setting. In addition, we also transfer the patch against DNN models with different initializations and different architectures, reaching attack success rates of up to 75% and 78%, respectively, in a gray-box setting. GitHub: https://github.com/SamSamhuns/yolov5_adversarial</td>
                <td>Perturbation methods, Object detection, Detectors, Artificial neural networks, Autonomous aerial vehicles, Robustness, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342460&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342460&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Fast Point to Mesh Distance by Domain Voxelization</td>
                <td>G. Gutow and H. Choset</td>
                <td>2023</td>
                <td>Computing the distance from a point to a triangle mesh is a key computational step in robotics pipelines such as registration and collision detection, with applications to path planning, SLAM, and RGB-D vision. Numerous techniques to accelerate this computation have been developed, many of which use a cheap pre-processing step to construct a hierarchical decomposition of the mesh. If the mesh is fixed and known ahead of time, there is an opportunity to conduct more expensive pre-computations to accelerate the subsequent distance queries. This work presents a voxelization approach, implemented on both CPU and GPU, to compute point to mesh distance that constructs for each voxel a near-minimal set of triangles that is guaranteed to include every triangle that is closest to at least one point in the voxel. Theoretical and numerical comparisons with six alternative distance algorithms demonstrate the speed advantages of the proposed method.</td>
                <td>Simultaneous localization and mapping, Costs, Pipelines, Graphics processing units, Path planning, Complexity theory, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341468&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341468&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>AirLine: Efficient Learnable Line Detection with Local Edge Voting</td>
                <td>X. Lin and C. Wang</td>
                <td>2023</td>
                <td>Line detection is widely used in many robotic tasks such as scene recognition, 3D reconstruction, and simultaneous localization and mapping (SLAM). Compared to points, lines can provide both low-level and high-level geometrical information for downstream tasks. In this paper, we propose a novel learnable edge-based line detection algorithm, AirLine, which can be applied to various tasks. In contrast to existing learnable endpoint-based methods, which are sensitive to the geometrical condition of environments, AirLine can extract line segments directly from edges, resulting in a better generalization ability for unseen environments. To balance efficiency and accuracy, we introduce a region-grow algorithm and a local edge voting scheme for line parameterization. To the best of our knowledge, AirLine is one of the first learnable edge-based line detection methods. Our extensive experiments have shown that it retains state-of-the-art-Ievel precision, yet with a $3-80\times$ runtime acceleration compared to other learning-based methods, which is critical for low-power robots.</td>
                <td>Measurement, Learning systems, Simultaneous localization and mapping, Runtime, Three-dimensional displays, Image edge detection, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341655&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341655&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>3D Skeletonization of Complex Grapevines for Robotic Pruning</td>
                <td>E. Schneider, S. Jayanth, A. Silwal and G. Kantor</td>
                <td>2023</td>
                <td>Robotic pruning of dormant grapevines is an area of active research in order to promote vine balance and grape quality, but so far robotic efforts have largely focused on planar, simplified vines not representative of commercial vineyards. This paper aims to advance the robotic perception capabilities necessary for pruning in denser and more complex vine structures by extending plant skeletonization techniques. The proposed pipeline generates skeletal grapevine models that have lower reprojection error and higher connectivity than baseline algorithms. We also show how 3D and skeletal information enables prediction accuracy of pruning weight for dense vines surpassing prior work, where pruning weight is an important vine metric influencing pruning site selection.</td>
                <td>Learning systems, Three-dimensional displays, Pipelines, Training data, Data collection, Prediction algorithms, Noise measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341828&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341828&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>AdaptSeqVPR: An Adaptive Sequence-Based Visual Place Recognition Pipeline</td>
                <td>H. Li, G. Peng, J. Zhang, S. Vaikundam and D. Wang</td>
                <td>2023</td>
                <td>Visual Place Recognition (VPR) is essential for autonomous robots and unmanned vehicles, as an accurate identification of visited places can trigger a loop closure to optimize the built map. The most prevalent methods tackle VPR as a single-frame retrieval task, which uses a CNN-based encoder to describe and compare each individual frame. These methods, however, overlook the temporal information between frames. Other methods improve this by searching the database with consecutive frames, which can greatly reduce false positives. Nevertheless, current sequence-based methods typically assume the consecutive image frames to be captured at an approximately constant speed, which is not always the case in practice. Therefore, we propose an adaptive sequence search strategy (AdaptSeq), which can dynamically alter the step size of adjacent frames in the retrieved sequence trajectory. Furthermore, to address false positive retrieval of input frames, we propose a CNN-based discriminator named DDsNet. It can determine whether the top retrieved candidates are true positives based on the learned statistics rather than an artificial threshold. Overall, we construct a novel sequence-based VPR pipeline named AdaptSeqVPR. It utilizes a CNN-based encoder for frame descriptions, and encompasses AdaptSeq and DDsNet for sequence matching. The experimental results indicate that our AdaptSeqVPR exhibits superior performance compared to the baseline SeqSLAM and SeqVLAD. Notably, our method can robustly handle the sequence-based VPR for vehicles traveling at non-uniform speeds in changing environments.</td>
                <td>Visualization, Databases, Pipelines, Lighting, Search problems, Trajectory, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341533&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341533&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Automated Void Detection for Search and Rescue with 3D Perception</td>
                <td>A. Bal, A. Gupta, P. Goyal, D. Merrick, R. Murphy and H. Choset</td>
                <td>2023</td>
                <td>In a structural collapse, debris piles up in a chaotic and unstable manner, creating pockets and void spaces that are difficult to see or access. Often, these regions have the highest chances of concealing survivors and identifying such regions can increase the success of a search and rescue (SAR) operation while ensuring the safety of both survivors and rescue teams. In this paper, we present an approach for ex post facto void detection in rubble piles by using registered 3D point clouds reconstructed from aerial images captured at multiple times on the scene. We perform a temporal layering of these point clouds to capture the dynamic surface of the rubble pile from multiple days of the SAR operation and analyze this 3D structure to detect candidate regions corresponding to void spaces. The layering is achieved by a parallel 3D point cloud reconstruction of the scene using the COLMAP Structure from Motion pipeline. The void detection is achieved by applying multiple point filtering criteria in thin segments of the 3D point clouds of the rubble. We test our approach on aerial images collected from the Surfside Structural Collapse at Miami in June 2021. Our method achieves an improvement in registration compared to the use of standard point cloud registration methods on individual 3D reconstructions. Through our method, we see translation errors reduce by 82%. Additionally, our method detects 9 out of 10 void spaces that were observed by experts in the rubble.</td>
                <td>Point cloud compression, Surface reconstruction, Three-dimensional displays, Systematics, Structure from motion, Pose estimation, Excavation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341454&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341454&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Visual Localization Based on Multiple Maps</td>
                <td>Y. Lin, L. Liu, X. Liang and J. Li</td>
                <td>2023</td>
                <td>This paper proposes a multi-map based visual localization method for image sequences. Given multiple single-map based localization results, we combine them with SLAM to estimate robust and accurate camera poses under challenging conditions. Our method comprises three modules connected in a sequence. First, we reconstruct multiple reference maps using the Structure-from-Motion technique, one map for each reference sequence. A single-image-based localization pipeline is performed to estimate 6-DoF camera poses for each query image, one for each map. Second, a consensus set maximization module is proposed to select the best camera poses from multi-map poses, estimating one 6-DoF camera pose for each query image. Finally, a robust pose refinement module is proposed to optimize 6-DoF camera poses of query images, combining map-based localization and local SLAM information. Experiments show that the proposed pipeline achieves state-of-the-art performance on challenging map-based localization benchmarks. Demonstrating the broad applicability of our method, we obtained first place in the challenge of Map-Based Localization for Autonomous Driving at ECCV2022.</td>
                <td>Location awareness, Visualization, Simultaneous localization and mapping, Pipelines, Pose estimation, Lighting, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341812&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341812&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Interacting Multiple Model Approach Based on Maximum Correntropy Student's T Filter</td>
                <td>F. Candan, A. Beke and L. Mihaylova</td>
                <td>2023</td>
                <td>This paper presents a novel approach called the Interacting Multiple Model (IMM)-based Maximum Correntropy Student's T Filter (MCStF), which addresses the challenges posed by non-Gaussian measurement noises. The MCStF demonstrates superior performance compared to the IMM algorithm based on Kalman Filters (KFs) in both simulation environments and real-time systems. The Crazyflie 2.0 nano Unmanned Air Vehicle (UAV) model is used in the simulation validation, and results from 3000 independent Monte Carlo runs are shown. After getting the simulation results under monotonously changed non-Gaussian distribution, their performance results have been compared to each other. The same scenario has been applied in the real-time system using Crazyflie 2.0. Next, results from real-time tests are presented in which the position of Crazyflie 2.0 is estimated online.</td>
                <td>Monte Carlo methods, Atmospheric modeling, Simulation, Filtering algorithms, Autonomous aerial vehicles, Real-time systems, Noise measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341366&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341366&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Deep Robust Multi-Robot Re-Localisation in Natural Environments</td>
                <td>M. Ramezani, E. Griffiths, M. Haghighat, A. Pitt and P. Moghadam</td>
                <td>2023</td>
                <td>The success of re-localisation has crucial implications for the practical deployment of robots operating within a prior map or relative to one another in real-world scenarios. Using single-modality, place recognition and localisation can be compromised in challenging environments such as forests. To address this, we propose a strategy to prevent lidar-based re-localisation failure using lidar-image cross-modality. Our solution relies on self-supervised 2D-3D feature matching to predict alignment and misalignment. Leveraging a deep network for lidar feature extraction and relative pose estimation between point clouds, we train a model to evaluate the estimated transformation. A model predicting the presence of misalignment is learned by analysing image-lidar similarity in the embedding space and the geometric constraints available within the region seen in both modalities in Euclidean space. Experimental results using real datasets (offline and online modes) demonstrate the effectiveness of the proposed pipeline for robust re-localisation in unstructured, natural environments.</td>
                <td>Point cloud compression, Image segmentation, Laser radar, Simultaneous localization and mapping, Pipelines, Pose estimation, Forestry</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341798&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341798&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FVLoc-NeRF : Fast Vision-Only Localization within Neural Radiation Field</td>
                <td>G. Wenzhi, B. Haiyang, M. Yuanqu, L. Jia and C. Lijun</td>
                <td>2023</td>
                <td>In recent years, Neural Radiation Fields (NeRF) have shown tremendous potential in encoding highly-detailed 3D geometry and environmental appearance, thus making it a promising alternative to traditional explicit maps for robot localization. However, current NeRF localization methods suffer from significant computational overheads, primarily resulting from the large number of iterations or particle samples required, as well as the additional computational demands associated with the estimation of the initial pose through multimodal sensors. To overcome these challenges, we propose a novel and time-efficient NeRF localization pipeline, named FVLoc-NeRF. This pipeline solely employs RGB monocular images as input and leverages a retrieval method to obtain the initial pose. Subsequently, the pose update is derived using the Perspective-n-Point (PnP) algorithm, thereby considerably reducing the number of iterations and accelerating the localization process. Our extensive experimental results clearly demonstrate that FVLoc-NeRF is much faster than the state-of-the-art method.</td>
                <td>Location awareness, Geometry, Three-dimensional displays, Multimodal sensors, Pipelines, Pose estimation, Kinematics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342310&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342310&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RADA: Robust Adversarial Data Augmentation for Camera Localization in Challenging Conditions</td>
                <td>J. Wang, M. R. U. Saputra, C. Xiaoxuan Lu, N. Trigoni and A. Markham</td>
                <td>2023</td>
                <td>Camera localization is a fundamental problem for many applications in computer vision, robotics, and autonomy. Despite recent deep learning-based approaches, the lack of robustness in challenging conditions persists due to changes in appearance caused by texture-less planes, repeating structures, reflective surfaces, motion blur, and illumination changes. Data augmentation is an attractive solution, but standard image perturbation methods fail to improve localization robustness. To address this, we propose RADA, which concentrates on perturbing the most vulnerable pixels to generate relatively less image perturbations that perplex the network. Our method outperforms previous augmentation techniques, achieving up to twice the accuracy of state-of-the-art models even under ‘unseen’ challenging weather conditions. Videos of our results can be found at https://youtu.be/niOv7-fJeCA. The source code for RADA is publicly available at https://github.com/jialuwang123321/RADA.</td>
                <td>Location awareness, Training, Perturbation methods, Robot vision systems, Cameras, Data augmentation, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341653&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341653&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MagHT: A Magnetic Hough Transform for Fast Indoor Place Recognition</td>
                <td>Bellile, S. Bourgeois, C. Joly and A. Paljic</td>
                <td>2023</td>
                <td>This article proposes a novel indoor magnetic field-based place recognition algorithm that is accurate and fast to compute. For that, we modified the generalized “Hough Transform” to process magnetic data (MagHT). It takes as input a sequence of magnetic measures whose relative positions are recovered by an odometry system and recognizes the places in the magnetic map where they were acquired. It also returns the global transformation from the coordinate frame of the input magnetic data to the magnetic map reference frame. Experimental results on several real datasets in large indoor environments demonstrate that the obtained localization error, recall, and precision are similar to or are better than state-of-the-art methods while improving the runtime by several orders of magnitude. Moreover, unlike magnetic sequence matching-based solutions such as DTW, our approach is independent of the path taken during the magnetic map creation.</td>
                <td>Location awareness, Magnetic field measurement, Simultaneous localization and mapping, Three-dimensional displays, Runtime, Transforms, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342269&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342269&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>What to Learn: Features, Image Transformations, or Both?</td>
                <td>Y. Chen, B. Xu, F. Dümbgen and T. D. Barfoot</td>
                <td>2023</td>
                <td>Long-term visual localization is an essential problem in robotics and computer vision, but remains challenging due to the environmental appearance changes caused by lighting and seasons. While many existing works have attempted to solve it by directly learning invariant sparse keypoints and descriptors to match scenes, these approaches still struggle with adverse appearance changes. Recent developments in image transformations such as neural style transfer have emerged as an alternative to address such appearance gaps. In this work, we propose to combine an image transformation network and a feature-learning network to improve long-term localization performance. Given night-to-day image pairs, the image transformation network transforms the night images into day-like conditions prior to feature matching; the feature network learns to detect keypoint locations with their associated descriptor values, which can be passed to a classical pose estimator to compute the relative poses. We conducted various experiments to examine the effectiveness of combining style transfer and feature learning and its training strategy, showing that such a combination greatly improves long-term localization performance.</td>
                <td>Location awareness, Representation learning, Measurement, Training, Image transformation, Visualization, Pipelines</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342415&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342415&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Global Localization: Utilizing Relative Spatio-Temporal Geometric Constraints from Adjacent and Distant Cameras</td>
                <td>M. Altillawi, Z. Pataki, S. Li and Z. Liu</td>
                <td>2023</td>
                <td>Re-Iocalizing a camera from a single image in a previously mapped area is vital for many computer vision applications in robotics and augmented/virtual reality. In this work, we address the problem of estimating the 6 DoF camera pose relative to a global frame from a single image. We propose to leverage a novel network of relative spatial and temporal geometric constraints to guide the training of a Deep Network for Localization. We employ simultaneously spatial and temporal relative pose constraints that are obtained not only from adjacent camera frames but also from camera frames that are distant in the spatio-temporal space of the scene. We show that our method, through these constraints, is capable of learning to localize when little or very sparse ground-truth 3D coordinates are available. In our experiments, this is less than 1 % of available ground-truth data. We evaluate our method on 3 common visual localization datasets and show that it outperforms other direct pose estimation methods.</td>
                <td>Training, Location awareness, Visualization, Three-dimensional displays, Robot kinematics, Robot vision systems, Pose estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342050&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342050&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Uncertainty-Aware Lidar Place Recognition in Novel Environments</td>
                <td>K. Mason, J. Knights, M. Ramezani, P. Moghadam and D. Miller</td>
                <td>2023</td>
                <td>State-of-the-art lidar place recognition models exhibit unreliable performance when tested on environments different from their training dataset, which limits their use in complex and evolving environments. To address this issue, we investigate the task of uncertainty-aware lidar place recognition, where each predicted place must have an associated uncertainty that can be used to identify and reject incorrect predictions. We introduce a novel evaluation protocol and present the first comprehensive benchmark for this task, testing across five uncertainty estimation techniques and three large-scale datasets. Our results show that an Ensembles approach is the highest performing technique, consistently improving the performance of lidar place recognition and uncertainty estimation in novel environments, though it incurs a computational cost. Code is publicly available at https://github.com/csiro-robotics/Uncertainty-LPR.</td>
                <td>Training, Uncertainty, Laser radar, Protocols, Pose estimation, Benchmark testing, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341383&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341383&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Data-Driven Based Cascading Orientation and Translation Estimation for Inertial Navigation</td>
                <td>X. Deng et al.</td>
                <td>2023</td>
                <td>Recently, data-driven approaches have brought both opportunities and challenges for Inertial Navigation Systems. In this paper, we propose a novel data-driven method which is composed of cascading orientation and translation estimation with IMU-only measurements. For robust orientation estimation, we combine a CNN-based neural network with an EKF to eliminate orientation errors caused by sensor noises. We additionally propose a hybrid CNN-Transformer-based neural network which exploits both spatial and long-term temporal information to regress accurate translations. Specifically, we conduct detailed evaluations on datasets acquired by iPhone and Android devices. The result demonstrates that our method outperforms state-of-the-art methods in both orientation and translation errors.</td>
                <td>Neural networks, Estimation, Inertial navigation, Robot sensing systems, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341493&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341493&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Converting Depth Images and Point Clouds for Feature-Based Pose Estimation</td>
                <td>R. Lösch, M. Sastuba, J. Toth and B. Jung</td>
                <td>2023</td>
                <td>In recent years, depth sensors have become more and more affordable and have found their way into a growing amount of robotic systems. However, mono- or multi-modal sensor registration, often a necessary step for further pro-cessing, faces many challenges on raw depth images or point clouds. This paper presents a method of converting depth data into images capable of visualizing spatial details that are basically hidden in traditional depth images. After noise removal, a neighborhood of points forms two normal vectors whose difference is encoded into this new conversion. Compared to Bearing Angle images, our method yields brighter, higher-contrast images with more visible contours and more details. We tested feature-based pose estimation of both conversions in a visual odometry task and RGB-D SLAM. For all tested features, AKAZE, ORB, SIFT, and SURF, our new Flexion images yield better results than Bearing Angle images and show great potential to bridge the gap between depth data and classical computer vision. Source code is available here: https://rlsch.github.io/depth-flexion-conversion.</td>
                <td>Point cloud compression, Simultaneous localization and mapping, Source coding, Multimodal sensors, Pose estimation, Sensor systems, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341758&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341758&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>AirVO: An Illumination-Robust Point-Line Visual Odometry</td>
                <td>K. Xu, Y. Hao, S. Yuan, C. Wang and L. Xie</td>
                <td>2023</td>
                <td>This paper proposes an illumination-robust visual odometry (VO) system that incorporates both accelerated learning-based corner point algorithms and an extended line feature algorithm. To be robust to dynamic illumination, the proposed system employs the convolutional neural network (CNN) and graph neural network (GNN) to detect and match reliable and informative corner points. Then point feature matching results and the distribution of point and line features are utilized to match and triangulate lines. By accelerating CNN and GNN parts and optimizing the pipeline, the proposed system is able to run in real-time on low-power embedded platforms. The proposed VO was evaluated on several datasets with varying illumination conditions, and the results show that it outperforms other state-of-the-art VO systems in terms of accuracy and robustness. The open-source nature of the proposed system allows for easy implementation and customization by the research community, enabling further development and improvement of VO for various applications.</td>
                <td>Visualization, Simultaneous localization and mapping, Heuristic algorithms, Source coding, Pipelines, Lighting, Graph neural networks</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341914&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341914&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields</td>
                <td>A. Rosinol, J. J. Leonard and L. Carlone</td>
                <td>2023</td>
                <td>We propose a novel geometric and photometric 3D mapping pipeline for accurate and real-time scene reconstruction from casually taken monocular images. To achieve this, we leverage recent advances in dense monocular SLAM and real-time hierarchical volumetric neural radiance fields. Our insight is that dense monocular SLAM provides the right information to fit a neural radiance field of the scene in real-time, by providing accurate pose estimates and depth-maps with associated uncertainty. Our proposed pipeline achieves better geometric and photometric accuracy than competing approaches (up to 178% better PSNR and 75% better L1 depth), while working in real-time and using only monocular images.</td>
                <td>Simultaneous localization and mapping, Three-dimensional displays, Uncertainty, Buildings, Pipelines, Semantics, Streaming media</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341922&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341922&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Scale Jump-Aware Pose Graph Relaxation for Monocular SLAM with Re-Initializations</td>
                <td>R. Yuan, R. Cheng, L. Liu, T. Sun and L. Kneipl</td>
                <td>2023</td>
                <td>Pose graph relaxation has become an indispensable addition to SLAM enabling efficient global registration of sensor reference frames under the objective of satisfying pair-wise relative transformation constraints. The latter may be given by incremental motion estimation or global place recognition. While the latter case enables loop closures and drift compensation, care has to be taken in the monocular case in which local estimates of structure and displacements can differ from reality not just in terms of noise, but also in terms of a scale factor. Owing to the accumulation of scale propagation errors, this scale factor is drifting over time, hence scale-drift aware pose graph relaxation has been introduced. We extend this idea to cases in which the relative scale between subsequent sensor frames is unknown, a situation that can easily occur if monocular SLAM enters re-initialization and no reliable overlap between successive local maps can be identified. The approach is realized by a hybrid pose graph formulation that combines the regular similarity consistency terms with novel, scale-blind constraints. We apply the technique to the practically relevant case of small indoor service robots capable of effectuating purely rotational displacements, a condition that can easily cause tracking failures. We demonstrate that globally consistent trajectories can be recovered even if multiple re-initializations occur along the loop, and present an in-depth study of success and failure cases.</td>
                <td>Simultaneous localization and mapping, Service robots, Motion estimation, Optimization methods, Cameras, Land vehicles, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341995&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341995&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Optimizing the Extended Fourier Mellin Transformation Algorithm</td>
                <td>W. Jiang, C. Li, J. Cao and S. Schwertfeger</td>
                <td>2023</td>
                <td>With the increasing application of robots, stable and efficient Visual Odometry (VO) algorithms are becoming more and more important. Based on the Fourier Mellin Transformation (FMT) algorithm, the extended Fourier Mellin Transformation (eFMT) is an image registration approach that can be applied to downward-looking cameras, for example on aerial and underwater vehicles. eFMT extends FMT to multi-depth scenes and thus more application scenarios. It is a visual odometry method which estimates the pose transformation between three overlapping images. On this basis, we develop an optimized eFMT algorithm that improves certain aspects of the method and combines it with back-end optimization for the small loop of three consecutive frames. For this we investigate the extraction of uncertainty information from the eFMT registration, the related objective function and the graph-based optimization. Finally, we design a series of experiments to investigate the properties of this approach and compare it with other VO and SLAM (Simultaneous Localization and Mapping) algorithms. The results show the superior accuracy and speed of our o-eFMT approach, which is published as open source.</td>
                <td>Simultaneous localization and mapping, Uncertainty, Robot vision systems, Pose estimation, Linear programming, Robustness, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341356&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341356&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Marker-Based Visual SLAM Leveraging Hierarchical Representations</td>
                <td>Lopez, R. M. Salinas and H. Voos</td>
                <td>2023</td>
                <td>Fiducial markers can encode rich information about the environment and aid Visual SLAM (VSLAM) approaches in reconstructing maps with practical semantic information. Current marker-based VSLAM approaches mainly utilize markers for improving feature detections in low-feature environments and/or incorporating loop closure constraints, generating only low-level geometric maps of the environment prone to inaccuracies in complex environments. To bridge this gap, this paper presents a VSLAM approach utilizing a monocular camera along with fiducial markers to generate hierarchical representations of the environment while improving the camera pose estimate. The proposed approach detects semantic entities from the surroundings, including walls, corridors, and rooms encoded within markers, and appropriately adds topological constraints among them. Experimental results on a real-world dataset collected with a robot demonstrate that the proposed approach outperforms a marker-based VSLAM baseline in terms of accuracy, given the addition of new constraints while creating enhanced map representations. Furthermore, it shows satisfactory results when comparing the reconstructed map quality to the one rebuilt using a LiDAR SLAM approach.</td>
                <td>Legged locomotion, Visualization, Simultaneous localization and mapping, Semantic segmentation, Semantics, Robot vision systems, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341891&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341891&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RVWO: A Robust Visual-Wheel SLAM System for Mobile Robots in Dynamic Environments</td>
                <td>J. Mahmoud, A. Penkovskiy, H. T. Long Vuong, A. Burkov and S. Kolyubin</td>
                <td>2023</td>
                <td>This paper presents RVWO, a system designed to provide robust localization and mapping for wheeled mobile robots in challenging scenarios. The proposed approach leverages a probabilistic framework that incorporates semantic prior information about landmarks and visual re-projection error to create a landmark reliability model, which acts as an adaptive kernel for the visual residuals in optimization. Additionally, we fuse visual residuals with wheel odometry measurements, taking advantage of the planar motion assumption. The RVWO system is designed to be robust against wrong data association due to moving objects, poor visual texture, bad illumination, and wheel slippage. Evaluation results demonstrate that the proposed system shows competitive results in dynamic environments and outperforms existing approaches on both public benchmarks and our custom hardware setup. We also provide the code as an open-source contribution to the robotics community22https://github.com/be2rlab/rvwo.</td>
                <td>Visualization, Simultaneous localization and mapping, Semantics, Dynamics, Pose estimation, Wheels, Probabilistic logic</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342183&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342183&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Event Camera-Based Visual Odometry for Dynamic Motion Tracking of a Legged Robot Using Adaptive Time Surface</td>
                <td>Miller and D. Kim</td>
                <td>2023</td>
                <td>Our paper proposes a direct sparse visual odometry method that combines event and RGBD data to estimate the pose of agile-legged robots during dynamic locomotion and acrobatic behaviors. Event cameras offer high temporal resolution and dynamic range, which can eliminate the issue of blurred RGB images during fast movements. This unique strength holds a potential for accurate pose estimation of agile- legged robots, which has been a challenging problem to tackle. Our framework leverages the benefits of both RGBD and event cameras to achieve robust and accurate pose estimation, even during dynamic maneuvers such as jumping and landing a quadruped robot, the Mini-Cheetah. Our major contributions are threefold: Firstly, we introduce an adaptive time surface (ATS) method that addresses the whiteout and blackout issue in conventional time surfaces by formulating pixel-wise decay rates based on scene complexity and motion speed. Secondly, we develop an effective pixel selection method that directly samples from event data and applies sample filtering through ATS, enabling us to pick pixels on distinct features. Lastly, we propose a nonlinear pose optimization formula that simultaneously performs 3D-2D alignment on both RGB-based and event-based maps and images, allowing the algorithm to fully exploit the benefits of both data streams. We extensively evaluate the performance of our framework on both the public dataset and our own quadruped robot dataset, demonstrating its effectiveness in accurately estimating the pose of agile robots during dynamic movements. Supplemental video: https://youtu.be/-5ieQShOg3M</td>
                <td>Legged locomotion, Tracking, Dynamics, Robot vision systems, Pose estimation, Cameras, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342048&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342048&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhancing Robustness of Line Tracking Through Semi-Dense Epipolar Search in Line-Based SLAM</td>
                <td>U. Seo, H. Lim, E. M. Lee, H. Lim and H. Myung</td>
                <td>2023</td>
                <td>Line information from urban structures can be exploited as an additional geometrical feature to achieve robust vision-based simultaneous localization and mapping (SLAM) systems in textureless scenes. Sometimes, however, conventional line tracking methods fail to track caused by image blur or occlusion. Even though these lost line features are just a subset of plenty of features, the failure in feature tracking can potentially lead to performance degradation of the SLAM system, particularly in textureless environments. To tackle this problem, we propose a robust line-tracking method for line-based monocular visual-inertial odometry. The proposed method generates a semi-dense map composed of depth and sparsity mesh using estimated 3D features. By leveraging the semi-dense map, our method performs a range-adaptive epipo-lar search to match the lines, allowing for robust line tracking while simultaneously reducing false positives. Furthermore, an algorithm to avoid conflicts is proposed, which occurs when the tracked lines from consecutive matching do not accord with the lines matched by our method. This algorithm discriminately maintains line features while appropriately aggregating lines spread across multiple frames. As evaluated in the EuRoC dataset and a more challenging textureless corridor scene, our proposed method shows substantial performance increases compared with other line-based visual (-inertial) approaches.</td>
                <td>Visualization, Simultaneous localization and mapping, Uncertainty, Three-dimensional displays, Measurement uncertainty, Maintenance engineering, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342497&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342497&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching Using an Attention Graph Neural Network</td>
                <td>S. Kannapiran et al.</td>
                <td>2023</td>
                <td>Robust feature matching forms the backbone for most Visual Simultaneous Localization and Mapping (vSLAM), visual odometry, 3D reconstruction, and Structure from Motion (SfM) algorithms. However, recovering feature matches from texture-poor scenes is a major challenge and still remains an open area of research. In this paper, we present a Stereo Visual Odometry (StereoVO) technique based on point and line features which uses a novel feature-matching mechanism based on an Attention Graph Neural Network that is designed to perform well even under adverse weather conditions such as fog, haze, rain, and snow, and dynamic lighting conditions such as nighttime illumination and glare scenarios. We perform experiments on multiple real and synthetic datasets to validate our method's ability to perform StereoVO under low-visibility weather and lighting conditions through robust point and line matches. The results demonstrate that our method achieves more line feature matches than state-of-the-art line-matching algorithms, which when complemented with point feature matches perform consistently well in adverse weather and dynamic lighting conditions.</td>
                <td>Visualization, Heuristic algorithms, Lighting, Feature extraction, Graph neural networks, Robustness, Vehicle dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341872&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341872&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Selective Presentation of AI Object Detection Results While Maintaining Human Reliance</td>
                <td>Y. Fukuchi and S. Yamada</td>
                <td>2023</td>
                <td>Transparency in decision-making is an important factor for AI-driven autonomous systems to be trusted and relied on by users. Studies in the field of visual information processing typically attempt to make an AI system's behavior transparent by showing bounding boxes or heatmaps as explanations. However, it has also been found that an excessive amount of explanations sometimes causes information overload and brings negative results. This paper proposes SmartBBox, a method for reducing the number of bounding boxes to show while maintaining human reliance on an AI. It infers if each bounding box is worth showing by predicting its effect on human reliance. SmartBBox can autonomously learn to decide whether to show bounding boxes from humans' usage data. We implemented and tested SmartBBox in an autonomous driving scenario in which a human continuously decides whether to rely on an autonomous driving system while observing the dynamic results of object detection by the system. The results suggest that SmartBBox can reduce bounding boxes 64.8% on average from object recognition results while keeping human reliance at the same level as in the case where all the bounding boxes are presented.</td>
                <td>Heating systems, Visualization, Decision making, Object detection, Information processing, Behavioral sciences, Object recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341684&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341684&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Ego-Noise Reduction of a Mobile Robot Using Noise Spatial Covariance Matrix Learning and Minimum Variance Distortionless Response</td>
                <td>O. Lagacé, F. Ferland and F. Grondin</td>
                <td>2023</td>
                <td>The performance of speech and events recognition systems significantly improved recently thanks to deep learning methods. However, some of these tasks remain challenging when algorithms are deployed on robots due to the unseen mechanical noise and electrical interference generated by their actuators while training the neural networks. Ego-noise reduction as a preprocessing step therefore can help solve this issue when using pre-trained speech and event recognition algorithms on robots. In this paper, we propose a new method to reduce ego-noise using only a microphone array and less than two minute of noise recordings. Using Principal Component Analysis (PCA), the best covariance matrix candidate is selected from a dictionary created online during calibration and used with the Minimum Variance Distortionless Response (MVDR) beamformer. Results show that the proposed method runs in real-time, improves the signal-to-distortion ratio (SDR) by up to 10 dB, decreases the word error rate (WER) by 55% in some cases and increases the Average Precision (AP) of event detection by up to 0.2.</td>
                <td>Training, Dictionaries, Event detection, Error analysis, Speech recognition, Microphone arrays, Calibration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342193&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342193&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Extracting Dynamic Navigation Goal from Natural Language Dialogue</td>
                <td>L. Liang, G. Bian, H. Zhao, Y. Dong and H. Liu</td>
                <td>2023</td>
                <td>Effective access to relevant environmental changes in large human environments is critical for service robots to perform tasks. Since the position of a dynamic goal such as a human is variable, it will be difficult for the robot to locate him accurately. It is worth noting that humans can obtain information through social software, and deal with daily affairs. The current robots search for targets without considering some implicit information changes, which leads to not searching for the target objects in the end. Therefore, we propose to extract human implicit location change information from group chats dialogues, i.e., watching dialogues in group chats and extracting who, when, and where(3W), to assist robots in finding explicit character targets. Then we propose a dynamic spatiotemporal map(DSTM) to store the change information as knowledge for the robot. When the robot identifies a target person, it needs to follow the changing information in the scene to infer the possible location and probability of the target person, and then develop a search strategy. We deployed our framework on a custom mobile robot and performed instruction navigation tasks in a university building to evaluate our approach. We demonstrate the ability of our framework to collect and use information in a large human social environment.</td>
                <td>Navigation, Service robots, Natural languages, Search problems, Software, Spatiotemporal phenomena, Data mining</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342509&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342509&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TidyBot: Personalized Robot Assistance with Large Language Models</td>
                <td>J. Wu et al.</td>
                <td>2023</td>
                <td>For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.</td>
                <td>Training, Adaptation models, Benchmark testing, Data collection, Manipulators, Data models, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341577&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341577&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>L3MVN: Leveraging Large Language Models for Visual Target Navigation</td>
                <td>B. Yu, H. Kasaei and M. Cao</td>
                <td>2023</td>
                <td>Visual target navigation in unknown environments is a crucial problem in robotics. Despite extensive investigation of classical and learning-based approaches in the past, robots lack common-sense knowledge about household objects and layouts. Prior state-of-the-art approaches to this task rely on learning the priors during the training and typically require significant expensive resources and time for learning. To address this, we propose a new framework for visual target navigation that leverages Large Language Models (LLM) to impart common sense for object searching. Specifically, we introduce two paradigms: (i) zero-shot and (ii) feed-forward approaches that use language to find the relevant frontier from the semantic map as a long-term goal and explore the environment efficiently. Our analyse demonstrates the notable zero-shot generalization and transfer capabilities from the use of language. Experiments on Gibson and Habitat-Matterport 3D (HM3D) demonstrate that the proposed framework significantly outperforms existing map-based methods in terms of success rate and generalization. Ablation analyse also indicates that the common-sense knowledge from the language model leads to more efficient semantic exploration. Finally, we provide a real robot experiment to verify the applicability of our framework in real-world scenarios. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/l3mvn.</td>
                <td>Training, Visualization, Three-dimensional displays, Navigation, Semantics, Layout, Search problems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342512&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342512&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TopSpark: A Timestep Optimization Methodology for Energy-Efficient Spiking Neural Networks on Autonomous Mobile Agents</td>
                <td>R. V. W. Putra and M. Shafique</td>
                <td>2023</td>
                <td>Autonomous mobile agents (e.g., mobile ground robots and UAVs) typically require low-power/energy-efficient machine learning (ML) algorithms to complete their ML-based tasks (e.g., object recognition) while adapting to diverse environments, as mobile agents are usually powered by batteries. These requirements can be fulfilled by Spiking Neural Networks (SNNs) as they offer low power/energy processing due to their sparse computations and efficient online learning with bio-inspired learning mechanisms for adapting to different environments. Recent works studied that the energy consumption of SNNs can be optimized by reducing the computation time of each neuron for processing a sequence of spikes (i.e., timestep). However, state-of-the-art techniques rely on intensive design searches to determine fixed timestep settings for only the inference phase, thereby hindering the SNN systems from achieving further energy efficiency gains in both the training and inference phases. These techniques also restrict the SNN systems from performing efficient online learning at run time. Toward this, we propose TopSpark, a novel methodology that leverages adaptive timestep reduction to enable energy-efficient SNN processing in both the training and inference phases, while keeping its accuracy close to the accuracy of SNNs without timestep reduction. The key ideas of our TopSpark include: (1) analyzing the impact of different timestep settings on the accuracy; (2) identifying neuron parameters that have a significant impact on accuracy in different timesteps; (3) employing parameter enhancements that make SNNs effectively perform learning and inference using less spiking activity due to reduced timesteps; and (4) developing a strategy to tradeoff accuracy, latency, and energy to meet the design requirements. The experimental results show that, our TopSpark saves the SNN latency by 3.9x as well as energy consumption by 3.5x for training and 3.3x for inference on average, across different network sizes, learning rules, and workloads, while maintaining the accuracy within 2 % of that of SNNs without timestep reduction. In this manner, TopSpark enables low-power/energy-efficient SNN processing for autonomous mobile agents.</td>
                <td>Training, Energy consumption, Machine learning algorithms, Mobile agents, Neurons, Energy efficiency, Object recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342499&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342499&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Generating Executable Action Plans with Environmentally-Aware Language Models</td>
                <td>M. Gramopadhye and D. Szafir</td>
                <td>2023</td>
                <td>Large Language Models (LLMs) trained using massive text datasets have recently shown promise in generating action plans for robotic agents from high-level text queries. However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints. In this paper, we propose an approach to generate environmentally-aware action plans that agents are better able to execute. Our approach involves integrating environmental objects and object relations as additional inputs into LLM action plan generation to provide the system with an awareness of its surroundings, resulting in plans where each generated action is mapped to objects present in the scene. We also design a novel scoring function that, along with generating the action steps and associating them with objects, helps the system disambiguate among object instances and take into account their states. We evaluated our approach using the VirtualHome simulator and the ActivityPrograms knowledge base and found that action plans generated from our system had a 310% improvement in executability and a 147% improvement in correctness over prior work. The complete code and a demo of our method is publicly available at https://github.com/hri-ironlab/scene_aware_language_planner.</td>
                <td>Codes, Affordances, Knowledge based systems, Cognition, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341989&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341989&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Interaction-Aware and Hierarchically-Explainable Heterogeneous Graph-based Imitation Learning for Autonomous Driving Simulation</td>
                <td>M. Tabatabaie, S. He and K. G. Shin</td>
                <td>2023</td>
                <td>Understanding and learning the actor-to-X inter-actions (AXIs), such as those between the focal vehicles (actor) and other traffic participants (e.g., other vehicles, pedestrians) as well as traffic environments (e.g., city/road map), is essential for the development of a decision-making model and simulation of autonomous driving (AD). Existing practices on imitation learning (IL) for AD simulation, despite the advances in the model learnability, have not accounted for fusing and differentiating the heterogeneous AXIs in complex road environments. Furthermore, how to further explain the hierarchical structures within the complex AXIs remains largely under-explored. To overcome these challenges, we propose HGIL, an interaction- aware and hierarchically-explainable Heterogeneous _Graph- based Imitation Learning approach for AD simulation. We have designed a novel heterogeneous interaction graph (HIG) to provide local and global representation as well as awareness of the AXIs. Integrating the HIG as the state embeddings, we have designed a hierarchically-explainable generative adversarial imitation learning approach, with local sub-graph and global cross-graph attention, to capture the interaction behaviors and driving decision-making processes. Our data-driven simulation and explanation studies have corroborated the accuracy and explainability of HGIL in learning and capturing the complex AXIs.</td>
                <td>Pedestrians, Roads, Decision making, Behavioral sciences, Autonomous vehicles, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342051&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342051&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Zero-Shot Fault Detection for Manipulators Through Bayesian Inverse Reinforcement Learning</td>
                <td>H. Zhao, X. Liu and G. Dudek</td>
                <td>2023</td>
                <td>We consider the detection of faults in robotic manipulators, with particular emphasis on faults that have not been observed or identified in advance, which naturally includes those that occur very infrequently. Recent studies indicate that the reward function obtained through Inverse Reinforcement Learning (IRL) can help detect anomalies caused by faults in a control system (i.e. fault detection). Current IRL methods for fault detection, however, either use a linear reward representation or require extensive sampling from the environment to estimate the policy, rendering them inappropriate for safety-critical situations where sampling of failure observations via fault injection can be expensive and dangerous. To address this issue, this paper proposes a zero-shot and exogenous fault detector based on an approximate variational reward imitation learning (AVRIL) structure. The fault detector recovers a reward signal as a function of externally observable information to describe the normal operation, which can then be used to detect anomalies caused by faults. Our method incorporates expert knowledge through a customizable reward prior distribution, allowing the fault detector to learn the reward solely from normal operation samples, without the need for a simulator or costly interactions with the environment. We evaluate our approach for exogenous partial fault detection in multi-stage robotic manipulator tasks, comparing it with several baseline methods. The results demonstrate that our method more effectively identifies unseen faults even when they occur within just three controller time steps.</td>
                <td>Fault diagnosis, Support vector machines, Fault detection, Robot vision systems, Reinforcement learning, Manipulators, Rendering (computer graphics)</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342143&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342143&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Chat with the Environment: Interactive Multimodal Perception Using Large Language Models</td>
                <td>X. Zhao, M. Li, C. Weber, M. B. Hafez and S. Wermter</td>
                <td>2023</td>
                <td>Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. An interactive perception framework is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-model.github.io/.</td>
                <td>Training, Computational modeling, Memory management, Process control, Programming, Robot sensing systems, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342363&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342363&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Reinforcement Learning for Robot Navigation with Adaptive Forward Simulation Time (AFST) in a Semi-Markov Model</td>
                <td>Y. Chen et al.</td>
                <td>2023</td>
                <td>Deep reinforcement learning (DRL) algorithms have proven effective in robot navigation, especially in unknown environments, by directly mapping perception inputs into robot control commands. However, most existing methods ignore the local minimum problem in navigation and thereby cannot handle complex unknown environments. In this paper, we propose the first DRL-based navigation method modeled by a semi-Markov decision process (SMDP) with continuous action space, named Adaptive Forward Simulation Time (AFST), to overcome this problem. Specifically, we reduce the dimensions of the action space and improve the distributed proximal policy optimization (DPPO) algorithm for the specified SMDP problem by modifying its GAE to better estimate the policy gradient in SMDPs. Experiments in various unknown environments demonstrate the effectiveness of AFST.</td>
                <td>Deep learning, Adaptation models, Navigation, Robot control, Reinforcement learning, Aerospace electronics, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341985&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341985&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-Training</td>
                <td>R. Bonatti, S. Vemprala, S. Ma, F. Frujeri, S. Chen and A. Kapoor</td>
                <td>2023</td>
                <td>Robotics has long been a field riddled with complex systems architectures whose modules and connections, whether traditional or learning-based, require significant human expertise and prior knowledge. Inspired by large pre-trained language models, this work introduces a paradigm for pretraining a general purpose representation that can serve as a starting point for multiple tasks on a given robot. We present the Perception-Action Causal Transformer (PACT), a generative transformer-based architecture that aims to build representations directly from robot data in a self-supervised fashion. Through autoregressive prediction of states and actions over time, our model implicitly encodes dynamics and behaviors for a particular robot. Our experimental evaluation focuses on the domain of mobile agents, where we show that this robot-specific representation can function as a single starting point to achieve distinct tasks such as safe navigation, localization and mapping. We evaluate two form factors: a wheeled robot that uses a LiDAR sensor as perception input (MuSHR), and a simulated agent that uses first-person RGB images (Habitat). We show that finetuning small task-specific networks on top of the larger pretrained model results in significantly better performance compared to training a single model from scratch for all tasks simultaneously, and comparable performance to training a separate large model for each task independently. By sharing a common good-quality representation across tasks we can lower overall model capacity and speed up the real-time deployment of such systems.</td>
                <td>Training, Navigation, Mobile agents, Predictive models, Robot sensing systems, Transformers, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342381&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342381&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Neural Field Movement Primitives for Joint Modelling of Scenes and Motions</td>
                <td>A. Tekden, M. P. Deisenroth and Y. Bekiroglu</td>
                <td>2023</td>
                <td>This paper presents a novel Learning from Demonstration (LfD) method that uses neural fields to learn new skills efficiently and accurately. It achieves this by utilizing a shared embedding to learn both scene and motion representations in a generative way. Our method smoothly maps each expert demonstration to a scene-motion embedding and learns to model them without requiring hand-crafted task parameters or large datasets. It achieves data efficiency by enforcing scene and motion generation to be smooth with respect to changes in the embedding space. At inference time, our method can retrieve scene-motion embeddings using test time optimization, and generate precise motion trajectories for novel scenes. The proposed method is versatile and can employ images, 3D shapes, and any other scene representations that can be modeled using neural fields. Additionally, it can generate both end-effector positions and joint angle-based trajectories. Our method is evaluated on tasks that require accurate motion trajectory generation, where the underlying task parametrization is based on object positions and geometric scene changes. Experimental results demonstrate that the proposed method outperforms the baseline approaches and generalizes to novel scenes. Furthermore, in real-world experiments, we show that our method can successfully model multi-valued trajectories, it is robust to the distractor objects introduced at inference time, and it can generate 6D motions.</td>
                <td>Solid modeling, Three-dimensional displays, Shape, End effectors, Trajectory, Task analysis, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342170&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342170&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Augmentation Enables One-Shot Generalization in Learning from Demonstration for Contact-Rich Manipulation</td>
                <td>X. Li, M. Baum and O. Brock</td>
                <td>2023</td>
                <td>We introduce a Learning from Demonstration (LID) approach for contact-rich manipulation tasks, i.e., tasks in which the manipulandum's motion is constrained by contact with the environment. Our approach is motivated by the insight that even a large number of demonstrations will often not contain sufficient information to obtain a general policy for the task. To obtain general policies, our approach augments the information contained in a single demonstration. This autonomous augmentation is based on the insight that environmental constraints play a central role in generalization. We validate our approach in real-world experiments with mechanisms with multiple, interdependent articulations, including latch locks, chain locks, and drawers with handles. The extracted policies, obtained from a single augmented human demonstration, generalize to different mechanisms of the same type and in varying environmental settings.</td>
                <td>Latches, Data mining, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341625&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341625&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Using Single Demonstrations to Define Autonomous Manipulation Contact Tasks in Unstructured Environments via Object Affordances</td>
                <td>F. Regal et al.</td>
                <td>2023</td>
                <td>Performing a manipulation contact task in an unknown and unstructured environment is still a challenge. Learning from Demonstration (LfD) techniques provide an intuitive means to define difficult-to-model contact tasks, but have attributes that make them undesirable for novice users in uncertain environments. We present a novel end-to-end system that captures a single manipulation task demonstration from an augmented reality (AR) head-mounted display (HMD), computes an affordance primitive (AP) representation of the task, and sends the task parameters to a mobile manipulator for execution in realtime. Using an AR HMD for task demon-stration and APs for task representation has several distinct advantages. AR task demonstration is intuitive, practical, and can be accomplished without requiring sensor installment in the task environment. APs provide a compact and legible task representation, enabling scalability, generalization, and modification of the task without significant data processing overhead. In this effort, we demonstrate system generalization with 10 object manipulation tasks, confirming the computed parameters from all tasks fit within AP tolerances. Secondly, we evaluate a mobile manipulator robot's ability to perform human-demonstrated tasks using AP representation. To increase robustness, we devised and tested four methods to correct for inherent, irreducible position errors in the system. A final study shows the system has a manipulation success rate of 96 % from a single manipulation demonstration on an industrial wheel valve.</td>
                <td>Three-dimensional displays, Service robots, Affordances, Robot kinematics, Wheels, Resists, Manipulators</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342493&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342493&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Constrained Dynamic Movement Primitives for Collision Avoidance in Novel Environments</td>
                <td>S. Shaw et al.</td>
                <td>2023</td>
                <td>Dynamic movement primitives are widely used for learning skills that can be demonstrated to a robot by a skilled human or controller. While their generalization capabilities and simple formulation make them very appealing to use, they possess no strong guarantees to satisfy operational safety constraints for a task. We present constrained dynamic movement primitives (CDMPs), which can allow for positional constraint satisfaction in the robot workspace. Our method solves a non-linear optimization to perturb an existing DMP's forcing weights to admit a Zeroing Barrier Function (ZBF), which certifies positional workspace constraint satisfaction. We demonstrate our approach under different positional constraints on the end-effector movement on multiple physical robots, such as obstacle avoidance and workspace limitations.</td>
                <td>End effectors, Safety, Collision avoidance, Task analysis, Optimization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341839&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341839&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Constraints on Autonomous Behavior from Proactive Feedback</td>
                <td>C. Basich, S. Mahmud and S. Zilberstein</td>
                <td>2023</td>
                <td>Learning from feedback is a common paradigm to acquire information that is hard to specify a priori. In this work, we consider an agent with a known nominal reward model that captures its high-level task objective. Furthermore, the agent operates subject to constraints that are unknown a priori and must be inferred from human interventions. Unlike existing methods, our approach does not rely on full or partial demonstration trajectories or assume a fully reactive human. Instead, we assume access only to sparse interventions, which may in fact be generated proactively by the human, and we only make minimal assumptions about the human. We provide both theoretical bounds on performance and empirical validations of our method. We show that our method enables an agent to learn a constraint set with high accuracy that generalizes well to new environments within a domain, whereas methods that only consider reactive feedback learn an incorrect constraint set that does not generalize well, making constraint violations more likely in new environments.</td>
                <td>Behavioral sciences, Trajectory, Calibration, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341801&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341801&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Models of Adversarial Agent Behavior Under Partial Observability</td>
                <td>S. Ye, M. Natarajan, Z. Wu, R. Paleja, L. Chen and M. C. Gombolay</td>
                <td>2023</td>
                <td>The need for opponent modeling and tracking arises in several real-world scenarios, such as professional sports, video game design, and drug-trafficking interdiction. In this work, we present Graph based Adversarial Modeling with Mutual Information (GrAMMI) for modeling the behavior of an adversarial opponent agent. GrAMMI is a novel graph neural network (GNN) based approach that uses mutual information maximization as an auxiliary objective to predict the current and future states of an adversarial opponent with partial observability. To evaluate GrAMMI, we design two large-scale, pursuit-evasion domains inspired by real-world scenarios, where a team of heterogeneous agents is tasked with tracking and interdicting a single adversarial agent, and the adversarial agent must evade detection while achieving its own objectives. With the mutual information formulation, GrAMMI outperforms all baselines in both domains and achieves 31.68% higher log-likelihood on average for future adversarial state predictions across both domains.</td>
                <td>Video games, Predictive models, Graph neural networks, Behavioral sciences, Trajectory, Observability, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341378&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341378&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Real-Time Motion Retargeting via Neural Latent Prediction</td>
                <td>T. Wang, H. Zhang, L. Chen, D. Wang, Y. Wang and R. Xiong</td>
                <td>2023</td>
                <td>Human-robot motion retargeting is a crucial approach for fast learning motion skills. Achieving real-time retargeting demands high levels of synchronization and accuracy. Even though existing retargeting methods have swift calculation, they still cause time-delay effect on the synchronous retargeting. To mitigate this issue, this paper proposes a motion retargeting method guided by prediction, which effectively reduces the adverse impact of time-delay. The proposed pipeline contains motion retargeting in spatial-temporal graph-based structure and motion prediction in the latent space. The motion sequence retargeting builds mapping and paired data from human poses to corresponding robot configurations for training prediction model, and generated robot motion satisfies limit and self-collision constrains. The controller guided by prediction imports future robot joint motion to achieve advanced trajectory tracking, thereby compensating for delay time spent on calculation and tracking. Experimental results show that our method outperforms other methods in terms of synchronization and similarity. Furthermore, our method exhibits fault-tolerant capability in scenarios involving the loss of human information input.</td>
                <td>Wrist, Training, Fault tolerance, Trajectory tracking, Fault tolerant systems, Aerospace electronics, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342022&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342022&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Deep Probabilistic Movement Primitives with a Bayesian Aggregator</td>
                <td>M. Przystupa, F. Haghverd, M. Jagersand and S. Tosatto</td>
                <td>2023</td>
                <td>Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations. Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of move-ments (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep movement primitive's model proposed that is capable of all previous operations, limiting neural movement primitive's potential applications. This paper proposes a deep movement primitive architecture that encodes all the operations above and uses a Bayesian context aggregator that allows a more sound context conditioning and blending. Our results demonstrate our approach can scale to reproduce complex motions on a larger variety of input choices compared to baselines while maintaining operations of linear movement primitives provide.</td>
                <td>Limiting, Merging, Modulation, Gaussian distribution, Probabilistic logic, Bayes methods, Parametric statistics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342441&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342441&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Self-Supervised Visual Motor Skills via Neural Radiance Fields</td>
                <td>P. Gesel, N. Sojib and M. Begum</td>
                <td>2023</td>
                <td>In this paper, we propose a novel network architecture for visual imitation learning that exploits neural radiance fields (NeRFs) and key-point correspondence for self-supervised visual motor policy learning. The proposed network architecture incorporates a dynamic system output layer for policy learning. Combining the stability and goal adaption properties of dynamic systems with the robustness of keypoint-based correspondence yields a policy that is invariant to significant clutter, occlusions, lighting conditions changes, and spatial variations in goal configurations. Experiments on multiple manipulation tasks show that our method outperforms comparable visual motor policy learning methods on both in-distribution and out-of-distribution scenarios when using a small number of training samples.</td>
                <td>Training, Learning systems, Visualization, Lighting, Network architecture, Robustness, Dynamical systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341682&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341682&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Autonomous Ultrasound Scanning Towards Standard Plane Using Interval Interaction Probabilistic Movement Primitives</td>
                <td>Y. Hu and M. Tavakoli</td>
                <td>2023</td>
                <td>Learning from demonstrations is the paradigm where robots acquire new skills demonstrated by an expert and alleviate the physical burden on experts to perform repetitive tasks. Ultrasound scanning is one of the ways to view the anatomical structures of soft tissues, but it is repetitive for some tissue scanning tasks. In this study, an autonomous ultrasound scanning towards a standard plane framework is proposed. Interaction probabilistic movement primitives (iProMP) was proposed for the collaborative tasks for human and robot movement. Inspired by the interval type-2 fuzzy system, an interval iProMP is proposed to learn the ultrasound scanning navigation strategy from scanning demonstrations and the collaborative agents are the robot movement and ultrasound image information. The proposed interval iProMP improves the capacity of dealing with uncertainties due to insufficient observations during reproduction. U-Net is applied to recognize the desired ultrasound image shown during demonstrations and a confidence map is used to evaluate the ultrasound image quality. Breast seroma scanning is chosen as the ultrasound scanning task to validate the performance of the proposed autonomous ultrasound scanning framework. Ultrasound navigation is to realize autonomous ultrasound scanning for localizing the breast seroma. The simulation comparison result shows the better performance of the proposed interval iProMP under insufficient observation, compared to traditional iProMP. The experiment result validates the feasibility and generality of the proposed autonomous ultrasound scanning framework using interval iProMP with a higher success rate than that with traditional iProMP.</td>
                <td>Ultrasonic imaging, Uncertainty, Image recognition, Navigation, Collaboration, Breast, Probabilistic logic</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341685&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341685&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Automated Key Action Detection for Closed Reduction of Pelvic Fractures by Expert Surgeons in Robot-Assisted Surgery</td>
                <td>B. Bian</td>
                <td>2023</td>
                <td>Pelvic fractures are one of the most serious traumas in orthopedics, and the technical proficiency and expertise of the surgical team strongly influence the quality of reduction results. With the advancement of information technology and robotics, robot-assisted pelvic fracture reduction surgery is expected to reduce the impact caused by inexperienced doctors and improve the accuracy and stability of pelvic reduction. However, this requires the robot to detect key surgeon actions from time-series data, enabling the robot to independently perceive the surgical status, predict the surgeon's intentions, assess the demonstrated level of professional competence, and assess the progress of the surgery. Therefore, a multi-task deep learning neural network architecture is proposed, which incorporates Convolutional Neural Network-Bidirectional Long Short-Term Memory (CNN-BiLSTM) along with tri-modality fusion and feature extraction techniques. The proposed framework aims to achieve key action detection in closed reduction operations for pelvic fractures. Subsequently, a trimodal fine-grained dataset was constructed, wherein 29, 32, and 14 labels were marked on flexion, position, and pressure data for 14 key closed reduction actions. The experimental results show that the correct detection rate of closed reduction actions is 92.3 %, significantly higher than the commonly used recognition algorithms. This work provides a method for the robot to learn the surgeon's professional knowledge, provides the basis for the operation's motion perception, and contributes to the autonomy of the robot-assisted closed reduction surgery of pelvic fractures.</td>
                <td>Deep learning, Neural networks, Surgery, Medical services, Feature extraction, Robot sensing systems, Multitasking</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342019&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342019&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LAMP: Leveraging Language Prompts for Multi-Person Pose Estimation</td>
                <td>S. Hu, C. Zheng, Z. Zhou, C. Chen and G. Sukthankar</td>
                <td>2023</td>
                <td>Human-centric visual understanding is an important desideratum for effective human-robot interaction. In order to navigate crowded public places, social robots must be able to interpret the activity of the surrounding humans. This paper addresses one key aspect of human-centric visual understanding, multi-person pose estimation. Achieving good performance on multi-person pose estimation in crowded scenes is difficult due to the challenges of occluded joints and instance separation. In order to tackle these challenges and overcome the limitations of image features in representing invisible body parts, we propose a novel prompt-based pose inference strategy called LAMP (Language Assisted Multi-person Pose estimation). By utilizing the text representations generated by a well-trained language model (CLIP), LAMP can facilitate the understanding of poses on the instance and joint levels, and learn more robust visual representations that are less susceptible to occlusion. This paper demonstrates that language-supervised training boosts the performance of single-stage multi-person pose estimation, and both instance-level and joint-level prompts are valuable for training. The code is available at https://github.com/shengnanh20/LAMP.</td>
                <td>Training, Visualization, Codes, Navigation, Pose estimation, Social robots, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341430&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341430&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Detecting Changes in Functional State: A Comparative Analysis Using Wearable Sensors and a Sensorized Tip</td>
                <td>J. Otamendi and A. Zubizarreta</td>
                <td>2023</td>
                <td>Gait analysis can provide relevant information about the physical and neurological conditions of individuals. For this reason, several studies have recently been carried out in an attempt to monitor people's gait and automatically detect gait anomalies. Among the various monitoring systems available for gait analysis, wearable sensors are considered the gold standard due to their wide capture range and low cost. However, in the case of people that require assistive devices for walking, some studies have proposed the use of sensorized devices in order to minimize invasiveness. Nevertheless, there is still a lack of comparative works that evaluate the performance of sensorized assistive devices for walking with widely used wearable sensors. Hence, this paper presents a comparison between the performance of accelerometer-based wearable sensors and a sensorized tip developed by the authors to detect gait anomalies. The comparative study has been carried out in a controlled environment with five healthy subjects, in which three different physical states have been simulated. A machine-learning based anomaly detection approach has been implemented based on the data captured by a set of wearable sensors and the sensorized tip, and the overall performance of both monitoring systems has been evaluated. Results show that even if both devices can provide an average accuracy of more than 80% in gait anomaly detection, the sensorized tip provides better performance.</td>
                <td>Performance evaluation, Legged locomotion, Three-dimensional displays, Robot sensing systems, Reflection, Assistive devices, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341723&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341723&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DiffuPose: Monocular 3D Human Pose Estimation via Denoising Diffusion Probabilistic Model</td>
                <td>J. Choi, D. Shim and H. J. Kim</td>
                <td>2023</td>
                <td>Thanks to the development of 2D keypoint detectors, monocular 3D human pose estimation (HPE) via 2D-to-3D uplifting approaches have achieved remarkable improvements. Still, monocular 3D HPE is a challenging problem due to the inherent depth ambiguities and occlusions. To handle this problem, many previous works exploit temporal information to mitigate such difficulties. However, there are many real-world applications where frame sequences are not accessible. This paper focuses on reconstructing a 3D pose from a single 2D keypoint detection. Rather than exploiting temporal information, we alleviate the depth ambiguity by generating multiple 3D pose candidates which can be mapped to an identical 2D keypoint. We build a novel diffusion-based framework to effectively sample diverse 3D poses from an off-the-shelf 2D detector. By considering the correlation between human joints by replacing the conventional denoising U-Net with graph convolutional network, our approach accomplishes further performance improvements. We evaluate our method on the widely adopted Human3.6M and HumanEva-I datasets. Comprehensive experiments are conducted to prove the efficacy of the proposed method, and they confirm that our model outperforms state-of-the-art multi-hypothesis 3D HPE methods.</td>
                <td>Solid modeling, Three-dimensional displays, Correlation, Gaussian noise, Pose estimation, Noise reduction, Detectors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342204&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342204&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>BodySLAM++: Fast and Tightly-Coupled Visual-Inertial Camera and Human Motion Tracking</td>
                <td>D. F. Henning, C. Choi, S. Schaefer and S. Leutenegger</td>
                <td>2023</td>
                <td>Robust, fast, and accurate human state - 6D pose and posture - estimation remains a challenging problem. For real-world applications, the ability to estimate the human state in realtime is highly desirable. In this paper, we present BodySLAM++, a fast, efficient, and accurate human and camera state estimation framework relying on visual-inertial data. BodySLAM++ extends an existing visual-inertial state estimation framework, OKVIS2, to solve the dual task of estimating camera and human states simultaneously. Our system improves the accuracy of both human and camera state estimation with respect to baseline methods by 26 % and 12%, respectively, and achieves realtime performance at 15+ frames per second on an Intel i7-model CPU. Experiments were conducted on a custom dataset containing both ground truth human and camera poses collected with an indoor motion tracking system.</td>
                <td>Three-dimensional displays, Tracking, Shape, Dynamics, Cameras, Robot sensing systems, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342291&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342291&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Characterizing the Onset and Offset of Motor Imagery During Passive Arm Movements Induced by an Upper-Body Exoskeleton</td>
                <td>K. Mitra, F. S. Racz, S. Kumar, A. D. Deshpande and J. Del R. Millán</td>
                <td>2023</td>
                <td>Two distinct technologies have gained attention lately due to their prospects for motor rehabilitation: robotics and brain-machine interfaces (BMIs). Harnessing their combined efforts is a largely uncharted and promising direction that has immense clinical potential. However, a significant challenge is whether motor intentions from the user can be accurately detected using non-invasive BMIs in the presence of instrumental noise and passive movements induced by the rehabilitation exoskeleton. As an alternative to the straight-forward continuous control approach, this study instead aims to characterize the onset and offset of motor imagery during passive arm movements induced by an upper-body exoskeleton to allow for the natural control (initiation and termination) of functional movements. Ten participants were recruited to perform kinesthetic motor imagery (MI) of the right arm while attached to the robot, simultaneously cued with LEDs indicating the initiation and termination of a goal-oriented reaching task. Using electroencephalogram signals, we built a decoder to detect the transition between i) rest and beginning MI and ii) maintaining and ending MI. Offline decoder evaluation achieved group average onset accuracy of 60.7% and 66.6% for offset accuracy, revealing that the start and stop of MI could be identified while attached to the robot. Furthermore, pseudo-online evaluation could replicate this performance, forecasting reliable online exoskeleton control in the future. Our approach showed that participants could produce quality and reliable sensorimotor rhythms regardless of noise or passive arm movements induced by wearing the exoskeleton, which opens new possibilities for BMI control of assistive devices.</td>
                <td>Performance evaluation, Exoskeletons, Robot sensing systems, Manipulators, Light emitting diodes, Neurorehabilitation, Decoding</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342492&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342492&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CLiFF-LHMP: Using Spatial Dynamics Patterns for Long- Term Human Motion Prediction</td>
                <td>Y. Zhu et al.</td>
                <td>2023</td>
                <td>Human motion prediction is important for mobile service robots and intelligent vehicles to operate safely and smoothly around people. The more accurate predictions are, particularly over extended periods of time, the better a system can, e.g., assess collision risks and plan ahead. In this paper, we propose to exploit maps of dynamics (MoDs, a class of general representations of place-dependent spatial motion patterns, learned from prior observations) for long-term human motion prediction (LHMP). We present a new MoD-informed human motion prediction approach, named CLiFF-LHMP, which is data efficient, explainable, and insensitive to errors from an upstream tracking system. Our approach uses CLiFF -map, a specific MoD trained with human motion data recorded in the same environment. We bias a constant velocity prediction with samples from the CLiFF-map to generate multi-modal trajectory predictions. In two public datasets we show that this algorithm outperforms the state of the art for predictions over very extended periods of time, achieving 45 % more accurate prediction performance at 50s compared to the baseline.</td>
                <td>Tracking, Service robots, Dynamics, Green products, Prediction methods, Stairs, Probabilistic logic</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342031&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342031&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>GloPro: Globally-Consistent Uncertainty-Aware 3D Human Pose Estimation & Tracking in the Wild</td>
                <td>S. Schaefer, D. F. Henning and S. Leutenegger</td>
                <td>2023</td>
                <td>An accurate and uncertainty-aware 3D human body pose estimation is key to enabling truly safe but efficient human-robot interactions. Current uncertainty-aware methods in 3D human pose estimation are limited to predicting the uncertainty of the body posture, while effectively neglecting the body shape and root pose. In this work, we present GloPro, which to the best of our knowledge the first framework to predict an uncertainty distribution of a 3D body mesh including its shape, pose, and root pose, by efficiently fusing visual clues with a learned motion model. We demonstrate that it vastly outperforms state-of-the-art methods in terms of human trajectory accuracy in a world coordinate system (even in the presence of severe occlusions), yields consistent uncertainty distributions, and can run in real-time. Our code will be released upon acceptance at https: $I$ Igithub. coml smartroboticslab/GloPro.</td>
                <td>Visualization, Uncertainty, Three-dimensional displays, Shape, Pose estimation, Human-robot interaction, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342032&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342032&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Anytime, Anywhere: Human Arm Pose from Smartwatch Data for Ubiquitous Robot Control and Teleoperation</td>
                <td>F. C. Weigend, S. Sonawani, M. Drolet and H. B. Amor</td>
                <td>2023</td>
                <td>This work devises an optimized machine learning approach for human arm pose estimation from a single smart-watch. Our approach results in a distribution of possible wrist and elbow positions, which allows for a measure of uncertainty and the detection of multiple possible arm posture solutions, i.e., multimodal pose distributions. Combining estimated arm postures with speech recognition, we turn the smartwatch into a ubiquitous, low-cost and versatile robot control interface. We demonstrate in two use-cases that this intuitive control interface enables users to swiftly intervene in robot behavior, to temporarily adjust their goal, or to train completely new control policies by imitation. Extensive experiments show that the approach results in a 40% reduction in prediction error over the current state-of-the-art and achieves a mean error of 2.56 cm for wrist and elbow positions.</td>
                <td>Wrist, Wearable Health Monitoring Systems, Uncertainty, Robot control, Measurement uncertainty, Speech recognition, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341624&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341624&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Recognizing Real-World Intentions using A Multimodal Deep Learning Approach with Spatial-Temporal Graph Convolutional Networks</td>
                <td>J. Shi, C. Liu, C. T. Ishi, B. Wu and H. Ishiguro</td>
                <td>2023</td>
                <td>Identifying intentions is a critical task for comprehending the actions of others, anticipating their future behavior, and making informed decisions. However, it is challenging to recognize intentions due to the uncertainty of future human activities and the complex influence factors. In this work, we explore the method of recognizing intentions alluded under human behaviors in the real world, aiming to boost intelligent systems' ability to recognize potential intentions and understand human behaviors. We collect data containing real-world human behaviors before using a hand dispenser and a temperature scanner at the building entrance. These data are processed and labeled into intention categories. A questionnaire is conducted to survey the human ability in inferring the intentions of others. Skeleton data and image features are extracted inspired by the answer to the questionnaire. For skeleton-based intention recognition, we propose a spatial-temporal graph convolutional network that performs graph convolutions on both part-based graphs and adaptive graphs, which achieves the best performance compared with baseline models in the same task. A deep-learning-based method using multimodal features is proposed to automatically infer intentions, which is demonstrated to accurately predict intentions based on past behaviors in the experiment, significantly outperforming humans.</td>
                <td>Deep learning, Temperature measurement, Surveys, Temperature distribution, Uncertainty, Feature extraction, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341981&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341981&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>VADER: Vector-Quantized Generative Adversarial Network for Motion Prediction</td>
                <td>M. S. Yasar and T. Iqbal</td>
                <td>2023</td>
                <td>Human motion prediction is an essential component for enabling close-proximity human-robot collaboration. The task of accurately predicting human motion is non-trivial and is compounded by the variability of human motion and the presence of multiple humans in proximity. To address some of the open challenges in motion prediction, in this work, we propose VADER, a novel sequence learning algorithm that models past observed poses using a flexible discrete latent space. VADER introduces the concept of Vector Quantization for human motion prediction, enabling the learning of a discrete latent space without being restricted by any static prior. In addition, we propose a new objective function that uses the discriminator objective to penalize deviation of predicted motion from the ground-truth. Finally, to explicitly model interaction in multiple humans, we introduce a lightweight attention mechanism to condition per-agent prediction on the previous hidden states of all the agents. Our evaluation across three scenarios: single-agent, multi-agent, and human-robot collaboration shows that VADER outperformed all the state-of-the-art approaches, resulting in more feasible human poses that align better with the ground-truth. Finally, we conducted extensive ablation studies to emphasize the importance of the proposed modules.</td>
                <td>Vector quantization, Collaboration, Predictive models, Benchmark testing, Generative adversarial networks, Prediction algorithms, Linear programming</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342324&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342324&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SG-LSTM: Social Group LSTM for Robot Navigation Through Dense Crowds</td>
                <td>R. Bhaskara, M. Chiu and A. Bera</td>
                <td>2023</td>
                <td>As personal robots become increasingly accessible and affordable, their applications extend beyond large corporate warehouses and factories to operate in diverse, less controlled environments, where they interact with larger groups of people. In such contexts, ensuring not only safety and efficiency but also mitigating potential adverse psychological impacts on humans and adhering to unwritten social norms become paramount. In this research, we aim to address these challenges by developing a cutting-edge model capable of predicting pedestrian movements and interactions in crowded environments. To this end, we propose a novel approach called the Social Group Long Short-term Memory (SG-LSTM) model, which effectively captures the complexities of human group behavior and interactions within dense surroundings. By integrating social awareness into the LSTM architecture, our model achieves significantly enhanced trajectory predictions. The implementation of our SG-LSTM model empowers navigation algorithms to compute collision-free paths faster and with higher accuracy, particularly in complex and crowded scenarios. To foster further advancements in social navigation research, we contribute a substantial video dataset comprising labeled pedestrian groups, which we release to the broader research community. To thoroughly evaluate the performance of our approach, we conduct extensive experiments on multiple datasets, including ETH, Hotel, and MOT15. We compare various prediction approaches, such as LIN, LSTM, O-LSTM, and S-LSTM, and rigorously assess runtime performance.</td>
                <td>Pedestrians, Runtime, Navigation, Social groups, Computational modeling, Psychology, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341954&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341954&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Online Continual Learning for Robust Indoor Object Recognition</td>
                <td>U. Michieli and M. Ozay</td>
                <td>2023</td>
                <td>Vision systems mounted on home robots need to interact with unseen classes in changing environments. Robots have limited computational resources, labelled data and storage capability. These requirements pose some unique challenges: models should adapt without forgetting past knowledge in a data- and parameter-efficient way. We characterize the problem as few-shot (FS) online continual learning (OCL), where robotic agents learn from a non-repeated stream of few-shot data updating only a few model parameters. Additionally, such models experience variable conditions at test time, where objects may appear in different poses (e.g., horizontal or vertical) and environments (e.g., day or night). To improve robustness of CL agents, we propose RobOCLe, which; 1) constructs an enriched feature space computing high order statistical moments from the embedded features of samples; and 2) computes similarity between high order statistics of the samples on the enriched feature space, and predicts their class labels. We evaluate robustness of CL models to train/test augmentations in various cases. We show that different moments allow RobOCLe to capture different properties of deformations, providing higher robustness with no decrease of inference speed.</td>
                <td>Adaptation models, Deformation, Computational modeling, Robustness, Data models, Object recognition, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341474&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341474&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PaintNet: Unstructured Multi-Path Learning from 3D Point Clouds for Robotic Spray Painting</td>
                <td>G. Tiboni, R. Camoriano and T. Tommasi</td>
                <td>2023</td>
                <td>Popular industrial robotic problems such as spray painting and welding require (i) conditioning on free-shape 3D objects and (ii) planning of multiple trajectories to solve the task. Yet, existing solutions make strong assumptions on the form of input surfaces and the nature of output paths, resulting in limited approaches unable to cope with real-data variability. By leveraging on recent advances in 3D deep learning, we introduce a novel framework capable of dealing with arbitrary 3D surfaces, and handling a variable number of unordered output paths (i.e. unstructured). Our approach predicts local path segments, which can be later concatenated to reconstruct long-horizon paths. We extensively validate the proposed method in the context of robotic spray painting by releasing PaintNet, the first public dataset of expert demonstrations on free-shape 3D objects collected in a real industrial scenario. A thorough experimental analysis demonstrates the capabilities of our model to promptly predict smooth output paths that cover up to 95% of previously unseen object surfaces, even without explicitly optimizing for paint coverage.</td>
                <td>Surface reconstruction, Three-dimensional displays, Service robots, Welding, Trajectory, Task analysis, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341480&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341480&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks</td>
                <td>R. Korekata et al.</td>
                <td>2023</td>
                <td>This paper describes a domestic service robot (DSR) that fetches everyday objects and carries them to specified destinations according to free-form natural language instructions. Given an instruction such as “Move the bottle on the left side of the plate to the empty chair,” the DSR is expected to identify the bottle and the chair from multiple candidates in the environment and carry the target object to the destination. Most of the existing multimodal language understanding methods are impractical in terms of computational complexity because they require inferences for all combinations of target object candidates and destination candidates. We propose Switching Head-Tail Funnel UNITER, which solves the task by predicting the target object and the destination individually using a single model. Our method is validated on a dataset based on a standard dataset for Vision-and-Language Navigation with object manipulation tasks. The results show that our method outperforms the baseline method in terms of language comprehension accuracy. Furthermore, we conduct physical experiments in which a DSR delivers standardized everyday objects in a standardized domestic environment as requested by instructions with referring expressions. The experimental results show that the object grasping and placing actions are achieved with success rates of more than 90 %.</td>
                <td>Service robots, Navigation, Computational modeling, Natural languages, Switches, Predictive models, Object recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342165&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342165&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FeatDANet: Feature-level Domain Adaptation Network for Semantic Segmentation</td>
                <td>J. Li, W. Shi, D. Zhu, G. Zhang, X. Zhang and J. Li</td>
                <td>2023</td>
                <td>Unsupervised domain adaptation (UDA) is proposed to better adapt the network trained on labeled synthetic data to unlabeled real-world data for addressing the annotation cost. However, most of these methods pay more attention to domain distributions in input and output stages while ignoring the important differences in semantic expressions and local details in middle feature stages. Therefore, a novel UDA network named FeatDANet is presented to align feature-level domain distributions at each encoder layer. Specifically, two attention-based modules abbreviated as IFAM and DFLM are designed and implemented by mixing queries and keys between domains for advisable domain adaptation. The former realizes Inter-domain Features Alignment by transferring feature style, and the latter achieves Domain-invariant Features Learning robustly for the domain shift. Furthermore, FeatDANet is constructed as a self-training network with three weight-sharing branches, and an improved pseudo-labels learning strategy is suggested by identifying more confident pseudolabels and maximizing the use of pseudo-labels. It increases the participation of unlabeled data and also ensures stability in training. Extensive experiments show that FeatDANet achieves state-of-the-art performances on the tasks of GTA→Cityscapes and Synthia→Cityscapes.</td>
                <td>Training, Representation learning, Costs, Annotations, Semantic segmentation, Semantics, Data augmentation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341639&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341639&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>BlinkFlow: A Dataset to Push the Limits of Event-Based Optical Flow Estimation</td>
                <td>Y. Li et al.</td>
                <td>2023</td>
                <td>Event cameras provide high temporal precision, low data rates, and high dynamic range visual perception, which are well-suited for optical flow estimation. While data-driven optical flow estimation has obtained great success in RGB cameras, its generalization performance is seriously hindered in event cameras mainly due to the limited and biased training data. In this paper, we present a novel simulator, BlinkSim, for the fast generation of large-scale data for event-based optical flow. BlinkSim incorporates a configurable rendering engine alongside an event simulation suite. By leveraging the wealth of current 3D assets, the rendering engine enables us to automatically build up thousands of scenes with different objects, textures, and motion patterns and render very high-frequency images for realistic event data simulation. Based on BlinkSim, we construct a large training dataset and evaluation benchmark BlinkFlow that contains sufficient, diversiform, and challenging event data with optical flow ground truth. Experiments show that BlinkFlow improves the generalization performance of state-of-the-art methods by more than 40% on average and up to 90%. Moreover, we further propose an Event-based optical Flow transFormer (E-FlowFormer) architecture. Powered by our BlinkFlow, E-FlowFormer outperforms the SOTA methods by up to 91% on the MVSEC dataset and 14% on the DSEC dataset and presents the best generalization performance. The source code and data are available at https://zju3dv.github.io/blinkflow/.</td>
                <td>Training, Estimation, Training data, Benchmark testing, Cameras, Transformers, Rendering (computer graphics)</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341802&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341802&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Discovering Adaptable Symbolic Algorithms from Scratch</td>
                <td>S. Kelly et al.</td>
                <td>2023</td>
                <td>Autonomous robots deployed in the real world will need control policies that rapidly adapt to environmental changes. To this end, we propose AutoRobotics-Zero (ARZ), a method based on AutoML-Zero that discovers zero-shot adaptable policies from scratch. In contrast to neural network adaption policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine. We evolve modular policies that tune their model parameters and alter their inference algorithm on-the-fly to adapt to sudden environmental changes. We demonstrate our method on a realistic simulated quadruped robot, for which we evolve safe control policies that avoid falling when individual limbs suddenly break. This is a challenging task in which two popular neural network baselines fail. Finally, we conduct a detailed analysis of our method on a novel and challenging non-stationary control task dubbed Cataclysmic Cartpole. Results confirm our findings that ARZ is significantly more robust to sudden environmental changes and can build simple, interpretable control policies.</td>
                <td>Adaptation models, Neural networks, Inference algorithms, Registers, Quadrupedal robots, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341979&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341979&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Visual Pre-Training for Navigation: What Can We Learn from Noise?</td>
                <td>Y. Ko and P. Agrawal</td>
                <td>2023</td>
                <td>One powerful paradigm in visual navigation is to predict actions from observations directly. Training such an end-to-end system allows representations useful for downstream tasks to emerge automatically. However, the lack of inductive bias makes this system data inefficient. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on synthetic noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. The code is available at https://yanweiw.github.io/noise2ptz/</td>
                <td>Training, Visualization, Codes, Navigation, Crops, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342521&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342521&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Risk-Aware Stochastic Ship Routing Using Conditional Value-at-Risk</td>
                <td>Cantos and R. Fitch</td>
                <td>2023</td>
                <td>Improving the safety and efficiency of maritime shipping has the potential to reduce carbon emissions and improve profitability. Stochastic ship routing, the problem of finding a safe, efficient, and timely route for a ship is difficult in large part because of uncertainty in weather forecasts, which come as an ensemble, a collection of many possible future weather conditions. Previous safety-aware ship routing methods have used either conservative interpretations of the ensemble by assuming the worst-case weather conditions, leading to excessive fuel consumption, or by using the average weather conditions, leading to potentially unsafe routes. In this paper, we investigate the use of the well-known Conditional Value-at-risk (CVaR) in the objective and constraint functions for ship routing problems, which allows a range of risk tolerances between the average and the worst case. We illustrate the advantages of using CVaR for the problem of ship routing in several simulation examples using real weather forecasts.</td>
                <td>Uncertainty, Sensitivity analysis, Profitability, Weather forecasting, Tail, Routing, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341431&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341431&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RAMP: Hierarchical Reactive Motion Planning for Manipulation Tasks Using Implicit Signed Distance Functions</td>
                <td>V. Vasilopoulos, S. Garg, P. Piacenza, J. Huh and V. Isler</td>
                <td>2023</td>
                <td>We introduce Reactive Action and Motion Planner (RAMP), which combines the strengths of sampling-based and reactive approaches for motion planning. In essence, RAMP is a hierarchical approach where a novel variant of a Model Predictive Path Integral (MPPI) controller is used to generate trajectories which are then followed asynchronously by a local vector field controller. We demonstrate, in the context of a table clearing application, that RAMP can rapidly find paths in the robot's configuration space, satisfy task and robot-specific constraints, and provide safety by reacting to static or dynamically moving obstacles. RAMP achieves superior performance through a number of key innovations: we use Signed Distance Function (SDF) representations directly from the robot configuration space, both for collision checking and reactive control. The use of SDFs allows for a smoother definition of collision cost when planning for a trajectory, and is critical in ensuring safety while following trajectories. In addition, we introduce a novel variant of MPPI which, combined with the safety guarantees of the vector field trajectory follower, performs incremental real-time global trajectory planning. Simulation results establish that our method can generate paths that are comparable to traditional and state-of-the-art approaches in terms of total trajectory length while being up to 30 times faster. Real-world experiments demonstrate the safety and effectiveness of our approach in challenging table clearing scenarios. Videos and code are available at: https://samsunglabs.github.io/RAMP-project-page/</td>
                <td>Technological innovation, Trajectory planning, Aerospace electronics, Real-time systems, Trajectory, Safety, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342397&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342397&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Safe and Aggressive Motion Generation for Dynamic Targets Pick-and-Place</td>
                <td>J. Shao et al.</td>
                <td>2023</td>
                <td>In this paper, we present a framework to generate time-optimal trajectories for dynamic target pick-and-place tasks. We develop an optimization-based trajectory generation method for manipulators, which can conduct spatial-temporal deformation under user-defined requirements. We formulate the problem of dynamic target pick-and-place, in which the trajectory duration and jerk are optimized and terminal states are adjusted instead of being fixed. The motions are constrained within the mechanical limits and to avoid collisions. Constraints transcription is adopted to convert constraints to weighted penalties. Then the problem can be solved based on the trajectory generation method with a high-level optimizer. We integrate the proposed method with online perception into a robot arm platform, in which a conveyor belt is used to transport the objects. Simulations and real-world experiments are conducted under a range of object speeds. Results show that the proposed method achieves online grasping under the object velocity up to 0.5m/s with an average computing time of 190ms.</td>
                <td>Computational modeling, Dynamics, Pose estimation, Minimization, Robustness, Trajectory, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341580&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341580&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SPONGE: Sequence Planning with Deformable-ON-Rigid Contact Prediction from Geometric Features</td>
                <td>Dakka and V. Kyrki</td>
                <td>2023</td>
                <td>Planning robotic manipulation tasks, especially those that involve interaction between deformable and rigid objects, is challenging due to the complexity in predicting such interactions. We introduce SPONGE, a sequence planning pipeline powered by a deep learning-based contact prediction model for contacts between deformable and rigid bodies under interactions. The contact prediction model is trained on synthetic data generated by a developed simulation environ-ment to learn the mapping from point-cloud observation of a rigid target object and the pose of a deformable tool, to 3D representation of the contact points between the two bodies. We experimentally evaluated the proposed approach for a dish cleaning task both in simulation and on a real Franka Emika Panda with real-world objects. The experimental results demonstrate that in both scenarios the proposed planning pipeline is capable of generating high-quality trajectories that can accomplish the task by achieving more than 90% area coverage on different objects of varying sizes and curvatures while minimizing travel distance. Code and video are available at: https://irobotics.aalto.fi/sponge/.</td>
                <td>Deformable models, Pipelines, Predictive models, Real-time systems, Data models, Planning, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341704&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341704&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Contact-Aware Non-Prehensile Manipulation for Object Retrieval in Cluttered Environments</td>
                <td>Y. Jiang, Y. Jia and X. Li</td>
                <td>2023</td>
                <td>Non-prehensile manipulation methods usually use a simple end effector, e.g., a single rod, to manipulate the object. Compared to the grasping method, such an end effector is compact and flexible, and hence it can perform tasks in a constrained workspace; As a trade-off, it has relatively few degrees of freedom (DoFs), resulting in an under-actuation problem with complex constraints for planning and control. This paper proposes a new non-prehensile manipulation method for the task of object retrieval in cluttered environments, using a rod-like pusher. Specifically, a candidate trajectory in a cluttered environment is first generated with an improved Rapidly-Exploring Random Tree (RRT) planner; Then, a Model Predictive Control (MPC) scheme is applied to stabilize the slider's poses through necessary contact with obstacles. Different from existing methods, the proposed approach is with the contact-aware feature, which enables the synthesized effect of active removal of obstacles, avoidance behavior, and switching contact face for improved dexterity. Hence both the feasibility and efficiency of the task are greatly promoted. The performance of the proposed method is validated in a planar object retrieval task, where the target object, surrounded by many fixed or movable obstacles, is manipulated and isolated. Both simulation and experimental results are presented.</td>
                <td>Robust control, Contacts, Dynamics, Switches, End effectors, Planning, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341476&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341476&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SDF-Pack: Towards Compact Bin Packing with Signed-Distance-Field Minimization</td>
                <td>H. Pan et al.</td>
                <td>2023</td>
                <td>Robotic bin packing is very challenging, especially when considering practical needs such as object variety and packing compactness. This paper presents SDF-Pack, a new approach based on signed distance field (SDF) to model the geometric condition of objects in a container and compute the object placement locations and packing orders for achieving a more compact bin packing. Our method adopts a truncated SDF representation to localize the computation, and based on it, we formulate the SDF -minimization heuristic to find optimized placements to compactly pack objects with the existing ones. To further improve space utilization, if the packing sequence is controllable, our method can suggest which object to be packed next. Experimental results on a large variety of everyday objects show that our method can consistently achieve higher packing compactness over 1,000 packing cases, enabling us to pack more objects into the container, compared with the existing heuristics under various packing settings. The code is publicly available at: https://github.com/kwpoon/SDF-Pack.</td>
                <td>Codes, Computational modeling, Reinforcement learning, Containers, Aerospace electronics, Minimization, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341940&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341940&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Modal Planning on Regrasping for Stable Manipulation</td>
                <td>J. Hu, Z. Tang and H. I. Christensen</td>
                <td>2023</td>
                <td>Nowadays, a number of grasping algorithms [1], [2] have been proposed, that can predict a candidate of grasp poses, even for unseen objects. This enables a robotic manipulator to pick-and-place such objects. However, some of the predicted grasp poses to stably lift a target object may not be directly approachable due to workspace limitations. In such cases, the robot will need to re-grasp the desired object to enable successful grasping on it. This involves planning a sequence of continuous actions such as sliding, re-grasping, and transferring. To address this multi-modal problem, we propose a Markov-Decision Process-based multi-modal planner that can rearrange the object into a position suitable for stable manipulation. We demonstrate improved performance in both simulation and the real world for pick-and-place tasks.</td>
                <td>Manifolds, Sequential analysis, Trajectory planning, Grasping, Manipulators, Stability analysis, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341842&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341842&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Object Manipulation Planning with Monte Carlo Tree Search</td>
                <td>H. Zhu, A. Meduri and L. Righetti</td>
                <td>2023</td>
                <td>This paper presents an efficient approach to object manipulation planning using Monte Carlo Tree Search (MCTS) to find contact sequences and an efficient ADMM-based trajectory optimization algorithm to evaluate the dynamic feasibility of candidate contact sequences. To accelerate MCTS, we propose a methodology to learn a goal-conditioned policy-value network and a feasibility classifier to direct the search towards promising nodes. Further, manipulation-specific heuristics enable to drastically reduce the search space. Systematic object manipulation experiments in a physics simulator and on real hardware demonstrate the efficiency of our approach. In particular, our approach scales favorably for long manipulation sequences thanks to the learned policy-value network, significantly improving planning success rate. All source code including the baseline can be found at https://hzhu.io/contact-mcts.</td>
                <td>Monte Carlo methods, Systematics, Heuristic algorithms, Source coding, Scalability, Search problems, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341813&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341813&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A New Method Combining Single-Point Push and Double-Point Complete Push for Partially Observable Scenarios</td>
                <td>J. Zhang, C. Bai and J. Guo</td>
                <td>2023</td>
                <td>Pushing objects into a target configuration is an important skill for robots. When there are obstacles in the scenario, the movement range of the objects will be limited, and objects are easily lost due to obscured by obstacles, which complicates the pushing task. This paper proposes a push path planning algorithm SDP-RRT (Single and Double Push - Rapidly-exploring Random Tree) to combine single-point push and double-point complete push in a partially observable environment with obstacles. The single-double push combines the flexibility of a single-point push and the stability of a double-point complete push. To find the reliable pushing point and pushing direction of the double-point complete push, we improve the judgment conditions of the double-point complete push and estimate the object's center of mass based on the LSTM (Long Short-Term Memory) neural network. Finally, the success rate and push steps of single-double and single-point push are compared in both the simulation and the natural environments, and the feasibility of the SDP-RRT algorithm is verified.</td>
                <td>Neural networks, Stability analysis, Path planning, Reliability, Task analysis, Manipulator dynamics, Long short term memory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342422&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342422&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Pre-and Post-Contact Policy Decomposition for Non-Prehensile Manipulation with Zero-Shot Sim-To-Real Transfer</td>
                <td>M. Kim, J. Han, J. Kim and B. Kim</td>
                <td>2023</td>
                <td>We present a system for non-prehensile manipulation that require a significant number of contact mode transitions and the use of environmental contacts to successfully manipulate an object to a target location. Our method is based on deep reinforcement learning which, unlike state-of-the-art planning algorithms, does not require apriori knowledge of the physical parameters of the object or environment such as friction coefficients or centers of mass. The planning time is reduced to the simple feed-forward prediction time on a neural network. We propose a computational structure, action space design, and curriculum learning scheme that facilitates efficient exploration and sim-to-real transfer. In challenging real-world non-prehensile manipulation tasks, we show that our method can generalize over different objects, and succeed even for novel objects not seen during training. Project website: https://sites.google.com/view/nonprenehsile-decomposition</td>
                <td>Training, Deep learning, Friction, Neural networks, Reinforcement learning, Prediction algorithms, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341657&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341657&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Material-Agnostic Shaping of Granular Materials with Optimal Transport</td>
                <td>N. Alatur, O. Andersson, R. Siegwart and L. Ott</td>
                <td>2023</td>
                <td>From construction materials, such as sand or asphalt, to kitchen ingredients, like rice, sugar, or salt; the world is full of granular materials. Despite impressive progress in robotic manipulation of single objects, granular materials remain a challenge due to difficulties in modelling these highly deformable and inhomogeneous materials, which are governed by dynamics that are hard to capture analytically. We argue that despite the high degrees of freedom and the complex underlying dynamics of granular materials, many practical problems that require manipulating them can be solved by leveraging simple models, informative motion priors, and a fast feedback loop. In this work, we show that computational Optimal Transport (OT) can be leveraged to derive informative, robot-agnostic motion priors for transforming a pile of granular materials from a source into a target distribution and generate robot motion plans with a next-best sweep planner that uses a simple material-agnostic sweep model. We plan sweeps directly on a height map representation of the material distribution and hence avoid a costly particle-level treatment of the problem. We validate our approach with a large set of simulation and hardware experiments that demonstrate several complex shaping tasks, including gathering, separating, and writing letters with different types of granular materials.</td>
                <td>Robot motion, Adaptation models, Shape, Computational modeling, Dynamics, Writing, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342505&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342505&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning to Efficiently Plan Robust Frictional Multi-Object Grasps</td>
                <td>W. C. Agboh et al.</td>
                <td>2023</td>
                <td>We consider a decluttering problem where multiple rigid convex polygonal objects rest in randomly placed positions and orientations on a planar surface and must be efficiently transported to a packing box using both single and multi-object grasps. Prior work considered frictionless multi-object grasping. In this paper, we introduce friction to increase the number of potential grasps for a given group of objects, and thus increase picks per hour. We train a neural network using real examples to plan robust multi-object grasps. In physical experiments, we find a 13.7% increase in success rate, a 1.6x increase in picks per hour, and a 6.3x decrease in grasp planning time compared to prior work on multi-object grasping. Compared to single-object grasping, we find a 3.1x increase in picks per hour.</td>
                <td>Friction, Neural networks, Grasping, Planning, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341895&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341895&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>POMDP-Guided Active Force-Based Search for Robotic Insertion</td>
                <td>C. Wang, H. Luo, K. Zhang, H. Chen, J. Pan and W. Zhang</td>
                <td>2023</td>
                <td>In robotic insertion tasks where the uncertainty exceeds the allowable tolerance, a good search strategy is essential for successful insertion and significantly influences efficiency. The commonly used blind search method is time-consuming and does not exploit the rich contact information. In this paper, we propose a novel search strategy that actively utilizes the information contained in the contact configuration and shows high efficiency. In particular, we formulate this problem as a Partially Observable Markov Decision Process (POMDP) with carefully designed primitives based on an in-depth analysis of the contact configuration's static stability. From the formulated POMDP, we can derive a novel search strategy. Thanks to its simplicity, this search strategy can be incorporated into a Finite-State-Machine (FSM) controller. The behaviors of the FSM controller are realized through a low-level Cartesian Impedance Controller. Our method is based purely on the robot's proprioceptive sensing and does not need visual or tactile sensors. To evaluate the effectiveness of our proposed strategy and control framework, we conduct extensive comparison experiments in simulation, where we compare our method with the baseline approach. The results demonstrate that our proposed method achieves a higher success rate with a shorter search time and search trajectory length compared to the baseline method. Additionally, we show that our method is robust to various initial displacement errors.</td>
                <td>Visualization, Propioception, Search problems, Robot sensing systems, Stability analysis, Trajectory, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342421&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342421&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Shared Autonomy Control for Slosh-Free Teleoperation</td>
                <td>R. I. C. Muchacho, S. Bien, R. Laha, A. Naceri, L. F. C. Figueredo and S. Haddadin</td>
                <td>2023</td>
                <td>Shared-autonomy control strategies in teleoperation combine human decision-making and robot precision to solve complex tasks. In other words, advanced autonomous control algorithms can compensate for imprecise human commands, reduce the mental workload of the user, and enable the execution of tasks that otherwise wouldn't be feasible. This paper addresses one of these previously challenging scenarios. Herein, we present a novel control framework and motion generator that allows for real-time non-prehensile slosh-free teleoperation of liquids. The proposed approach is able to generate robust trajectories on the follower side which ensures task-space, joint-space, and manipulability constraint satisfaction. Our findings were evaluated through user studies and real-world scenarios. Participants were even explicitly challenged to try to spill liquid through teleoperation, reaching speeds up to 0.6 m/s.</td>
                <td>Liquids, Robot kinematics, Transportation, Kinematics, Robot sensing systems, Real-time systems, Mathematical models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342234&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342234&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Leader-Follower Formation Control of a Large-Scale Swarm of Satellite System Using the State-Dependent Riccati Equation: Orbit-to-Orbit and In-Same-Orbit Regulation</td>
                <td>S. R. Nekoo, J. Yao, A. Suarez, R. Tapia and A. Ollero</td>
                <td>2023</td>
                <td>The state-dependent Riccati equation (SDRE) is a nonlinear optimal controller with a flexible structure which is one of the main advantages of this method. Here in this work, this flexibility is used to present a novel design for handling a soft constraint for state variables (trajectories). The concept is applied to a large-scale swarm control system, with more than 1000 agents. The control of the swarm satellite system is devoted to two modes of orbit-to-orbit and in-same-orbit cases. Keeping the satellites in one orbit in regulation (point-to-point motion) requires additional constraints while they are moving in Cartesian coordinates. For a small number of agents trajectory design could be done for each satellite individually, though, for a swarm with many agents, that is not practical. The constraint has been incorporated into the cost function of optimal control and resulted in a modified SDRE control law. The proposed method successfully controlled a swarm case of 1024 agents in leader-follower mode for orbit-to-orbit and in-same-orbit simulations. The soft constraint presented a percentage of 0.05 in the error of the satellites with respect to travel distance, in in-same-orbit regulation. The presented approach is systematic and could be performed for larger swarm systems with different agents and dynamics.</td>
                <td>Visualization, Satellites, Systematics, Shape, Riccati equations, Orbits, Regulation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342383&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342383&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lunar Excavator Mission Operations Using Dynamic Movement Primitives</td>
                <td>J. M. Cloud, M. Q. Tram, W. J. Beksi and M. A. DuPuis</td>
                <td>2023</td>
                <td>To support sustainable infrastructure on the Moon, NASA must leverage robots to extract lunar resources for in-situ processing and construction. As part of this effort, NASA is launching the in-situ resource utilization (ISRU) Pilot Excavator later this decade to validate a robotic regolith excavator based on the Regolith Advanced Surface Systems Operations Robot (RASSOR). RASSOR is designed to extract and transport regolith to meet the needs of ISRU architectures. During its mission, Pilot Excavator will be tasked with driving in test patterns to demonstrate the operational concept. One possible test pattern is a circular trajectory around the lander while avoiding surface hazards such as lunar rocks. To this end, we utilize dynamic movement primitives to represent navigation sequences as primitive trajectories. We introduce a novel obstacle avoidance parameter, which is configured to avoid rocks throughout testing exercises. We demonstrate the effectiveness our method in a newly developed simulation tool called the Simulated Excavation Environment for Lunar Operations (SEELO) using models based on the NASA RASSOR 2.0 excavator. Our results show that the robot is able to safety and robustly navigate the lunar surface with densely populated rock obstacles while retaining the desired circle pattern behavior.</td>
                <td>Navigation, Moon, NASA, Excavation, Rocks, Trajectory, Behavioral sciences, Space Robotics and Automation, Learning from Demonstration, Mining Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342005&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342005&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Fast Bi-Monocular Visual Odometry Using Factor Graph Sparsification</td>
                <td>C. Debeunne, J. Vallvé, A. Torres and D. Vivet</td>
                <td>2023</td>
                <td>Visual navigation has become a standard in robotic applications with the emergence of robust and versatile algorithms. In particular, Visual Odometry (VO) has proven to be the most reliable navigation solution for space missions to estimate an unmanned vehicle's motion and state. Lava Tubes exploration is one of the recent challenges in this field of applied robotics. VO in this scenario requires more robustness to poor lighting conditions while keeping a low computational cost. We propose investigating an indirect bi-monocular VO based on sliding-window optimization in such a context. It focuses on maintaining the sparsity of the problem while keeping the information of the marginalized frames to reduce the computational burden. Different sparse graph topologies are studied to encode information from the past and are evaluated on accuracy and computation load. The best method retained is then compared to state-of-the-art systems on real data under extreme illumination conditions and reaches similar accuracy results at a lower computational cost.</td>
                <td>Visualization, Three-dimensional displays, Navigation, Lava, Lighting, Topology, Electron tubes</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341644&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341644&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Assisting Spectral Mapping Using Cameras</td>
                <td>S. Vijayarangan and D. Wettergreen</td>
                <td>2023</td>
                <td>Spectral mapping, typically performed at the orbital scale, is hard at the rover scale as it necessitates larger coverage and field operable spectrometers. In this work, we propose using RGB cameras to assist spectral mapping. RGB cameras placed on a wide range of robotic platforms, including aerial vehicles, which can explore large regions compared to ground rovers. Our method uses a spectral model which learns the spatial relationship of spectra in a compressed feature space. We show that RGB data can contribute to this feature space and thereby enhance spectral reconstruction accuracy.</td>
                <td>Space vehicles, Robot vision systems, NASA, Cameras, Solar system, Planetary orbits, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342072&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342072&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Autonomous Multi-Robot Servicing for Spacecraft Operation Extension</td>
                <td>L. Gao, G. Cordova, C. Danielson and R. Fierro</td>
                <td>2023</td>
                <td>This paper considers the robotic on-orbit servicing of a client satellite. We present an adaptive robotic system to perform two on-orbit tasks: client manipulation and jammed component dislodging (e.g., a solar panel). We use adaptive control due to the uncertainty of the client dynamics and the stiffness of the jammed component. We present two methods for dislodging: a decentralized approach with multiple free-fiying collaborating agents and then transferring the same decentralized approach to a multi-robot arms system. We present simulation studies for these on-orbit servicing tasks. These studies validate the effectiveness of the presented controller despite uncertainty and unmodeled dynamics, such as nonlinear friction.</td>
                <td>Space vehicles, Uncertainty, Satellites, Simulation, Manipulators, Nonlinear dynamical systems, Solar panels</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341875&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341875&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SOLL-E: A Module Transport and Placement Robot for Autonomous Assembly of Discrete Lattice Structures</td>
                <td>W. Park et al.</td>
                <td>2023</td>
                <td>This paper presents the design and development of a transport and placement robot that demonstrates autonomous assembly of structural building blocks. The robots are intended to serve as a critical component of automated structural assembly and maintenance systems. The Scaling Omni-directional Lattice Locomoting Explorer (SOLL-E) uses a 5-DoF bipedal inchworm locomotion architecture with locking foot and cargo grippers. The locomotion system employs large magnet gap diameter BLDC motors with moderate timing belt gearing for primary joints, and DC planetary gearmotors for turning. Foot and cargo grippers are identical, with servo-actuated locking mechanisms. Three modular controller boards are used to control these actuators in real-time, with command and telemetry data transferred between the server and each controller board via WiFi. Functionality and performance were evaluated in a ground demonstration.</td>
                <td>Scalability, Lattices, Turning, Timing, Telemetry, Servers, Grippers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341479&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341479&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bi-Level Image-Guided Ergodic Exploration with Applications to Planetary Rovers</td>
                <td>E. Wittemyer and I. Abraham</td>
                <td>2023</td>
                <td>We present a method for image-guided exploration for mobile robotic systems. Our approach extends ergodic exploration methods, a recent exploration approach that prioritizes complete coverage of a space, with the use of a learned image classifier that automatically detects objects and updates an information map to guide further exploration and localization of objects. Additionally, to improve outcomes of the information collected by our robot's visual sensor, we present a decomposition of the ergodic optimization problem as bi-level coarse and fine solvers, which act respectively on the robot's body and the robot's visual sensor. Our approach is applied to geological survey and localization of rock formations for Mars rovers, with real images from Mars rovers used to train the image classifier. Results demonstrate 1) improved localization of rock formations compared to naive approaches while 2) minimizing the path length of the exploration through the bi-level exploration.</td>
                <td>Location awareness, Space vehicles, Surveys, Visualization, Mars, Navigation, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341437&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341437&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>6D Object Pose Estimation from Approximate 3D Models for Orbital Robotics</td>
                <td>M. Ulmer, M. Durner, M. Sundermeyer, M. Stoiber and R. Triebel</td>
                <td>2023</td>
                <td>We present a novel technique to estimate the 6D pose of objects from single images where the 3D geometry of the object is only given approximately and not as a precise 3D model. To achieve this, we employ a dense 2D-to-3D correspondence predictor that regresses 3D model coordinates for every pixel. In addition to the 3D coordinates, our model also estimates the pixel-wise coordinate error to discard correspondences that are likely wrong. This allows us to generate multiple 6D pose hypotheses of the object, which we then refine iteratively using a highly efficient region-based approach. We also introduce a novel pixel-wise posterior formulation by which we can estimate the probability for each hypothesis and select the most likely one. As we show in experiments, our approach is capable of dealing with extreme visual conditions including overexposure, high contrast, or low signal-to-noise ratio. This makes it a powerful technique for the particularly challenging task of estimating the pose of tumbling satellites for in-orbit robotic applications. Our method achieves state-of-the-art performance on the SPEED+ dataset and has won the SPEC2021 post-mortem competition. Code, trained models, and the used satellite model will be made publicly available.</td>
                <td>Solid modeling, Visualization, Three-dimensional displays, Satellites, Robot kinematics, Pose estimation, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341511&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341511&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Origami-Inspired Deployable Space Debris Collector</td>
                <td>Y. Tanaka, A. A. Anibha, L. Jung, R. Gul, J. Sun and R. Dai</td>
                <td>2023</td>
                <td>This paper develops a novel approach to design, actuate, and manufacture a space debris collector based on the conical Kresling origami pattern. The deployable nature of origami structures and the radial closability of the conical Kresling pattern are leveraged to form an enclosure volume for collecting space debris at different sizes. We first introduce the geometric, volume, and energy models of the conical Kresling pattern. Based on these models, the debris collector design problem is formulated as a parameter optimization problem to minimize the actuation energy for the folding process while satisfying the minimum volume constraint and geometric/functional constraints. To automatically capture debris in space, an actuation system is designed, which is compatible with space environments. Moreover, the multi-material three-dimensional printing technology is applied to build the designed debris collector, which makes it feasible for manufacturing the product in orbit. The proposed design, actuation, and manufacturing approaches are verified with experimental tests using a designed prototype.</td>
                <td>Space vehicles, Solid modeling, Three-dimensional displays, Space debris, Prototypes, Process control, Three-dimensional printing, Space Robots, Origami-Inspired Deployable System, Debris Connector, Optimal Design</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341589&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341589&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Principled ICP Covariance Modelling in Perceptually Degraded Environments for the EELS Mission Concept</td>
                <td>W. Talbot et al.</td>
                <td>2023</td>
                <td>The Exobiology Extant Life Surveyor (EELS) is a snake-like mobile instruments platform under development at Jet Propulsion Laboratory (JPL) for a mission concept to find evidence of life on Saturn's sixth largest moon, Enceladus. To conduct a life surveying mission there, the EELS platform must first traverse an unknown icy surface terrain before undertaking a controlled descent into a cryovolcanic vent. The remoteness of Enceladus and the icy nature of its terrain demands a level of autonomy in navigation significantly higher than previous rover missions. The perception system onboard EELS must be highly resilient to perceptually-degraded environments such as flat, open ice fields, icy plumes, and repeating geometries in vents. EELS' perception system is implemented as a multi-sensor Simultaneous Localisation And Mapping (SLAM) solution called SERPENT. State Estimation through Robust Perception in Extreme and Novel Terrains (SERPENT) estimates the robot trajectory and maintains a map database, from which dense global or local maps can be obtained on demand for downstream planning algorithms. This system opts to incorporate measurements from many sensor modalities (laser scans, images, IMU, altimeter, etc.), solving the SLAM problem through joint optimisation, and thus requires that the contribution of each sensor be balanced through careful modelling of their uncertainties. With a specific focus on Light Detection And Ranging (LiDAR) in this context, this paper proposes a principled approach to model the covariances of point-to-plane Iterative Closest Point (ICP). It performs a rigorous comparative analysis of new and existing covariance models, and is the first time some of these have been tested within a complete SLAM pipeline. These models are evaluated on perceptually challenging datasets collected in glacial environments by the EELS sensor suite (see Figures 1, 2). SERPENT is open-sourced at https://github.com/jpl-eels/serpent.</td>
                <td>Simultaneous localization and mapping, Uncertainty, Saturn, Vents, Pipelines, Optimized production technology, Propulsion</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341455&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341455&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Deep Functional Predictive Control (deep-FPC): Robot Pushing 3-D Cluster Using Tactile Prediction</td>
                <td>K. Nazari et al.</td>
                <td>2023</td>
                <td>This paper introduces a novel approach to address the problem of Physical Robot Interaction (PRI) during robot pushing tasks. The approach uses a data-driven forward model based on tactile predictions to inform the controller about potential future movements of the object being pushed, such as a strawberry stem, using a robot tactile finger. The model is integrated into a Deep Functional Predictive Control (d-FPC) system to control the displacement of the stem on the tactile finger during pushes. Pushing an object with a robot finger along a desired trajectory in 3D is a highly nonlinear and complex physical robot interaction, especially when the object is not stably grasped. The proposed approach controls the stem movements on the tactile finger in a prediction horizon. The effectiveness of the proposed FPC is demonstrated in a series of tests involving a real robot pushing a strawberry in a cluster. The results indicate that the d-FPC controller can successfully control PRI in robotic manipulation tasks beyond the handling of strawberries. The proposed approach offers a promising direction for addressing the challenging PRI problem in robotic manipulation tasks.</td>
                <td>Three-dimensional displays, Predictive models, Robot sensing systems, Trajectory, Sensors, Convolutional neural networks, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342410&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342410&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Wireless Capacitive Tactile Sensor Arrays for Sensitive/Delicate Robot Grasping</td>
                <td>S. Ergun et al.</td>
                <td>2023</td>
                <td>Uncertainties in grasp prediction for unknown, arbitrarily shaped objects in cluttered environments and un- certainty of the kinematics (e.g., series elastic robots or soft robots) can lead to poor grasps. For delicate objects, such poor grasps may damage the objects when they are dropped or when high local pressure is introduced in the grasping process. We propose a tactile sensor concept that allows predicting the quality of a grasp such that the object can be safely moved without being dropped. This prediction is done using an initial low force grasp and the force is only increased when the contact area is sufficiently large. The proposed customizable wireless Capacitive Tactile Sensor Array (CTSA) uses the deformation of a polymer to assess the contact area and the force distribution. A common homogeneous deformable electrode is embedded in the polymer. This electrode does not require any patterning nor any electrical connection but to ground. We present the manufacturing process which allows for robust yet cost effective realizations with a variety of electrode materials including conductive inks, conductive textiles, metal meshes and metal sheets. With the different approaches, parameters such as sensitivity and recovery time can be adjusted. Furthermore, the robustness of the sensor towards strong forces and objects with sharp edges and corners is shown. Finally, we demonstrate the benefits of the proposed sensor for grasping in a series of scenarios with rigid and soft 3D printed objects of various shapes. Allowing a reasonable false positive rate, 100 % of unsuccessful grasps in our evaluation experiments could be detected from the initial low force grasp.</td>
                <td>Electrodes, Wireless communication, Wireless sensor networks, Contacts, Force, Tactile sensors, Metals</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342163&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342163&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Active Planar Mass Distribution Estimation with Robotic Manipulation</td>
                <td>J. Yuan, C. Choi, E. B. Tadmor and V. Isler</td>
                <td>2023</td>
                <td>In this work, we present a method to estimate the planar mass distribution of a rigid object through robotic interactions and force/torque feedback. This is a challenging problem because of the complexity of modeling physical dynamics and the action dependencies across the model parameters. We propose a sequential estimation strategy combined with a set of robot action selection rules based on the analytical formulation of a discrete-time dynamics model. To evaluate the performance of our approach, we also manufactured re-configurable block objects that allow us to modify the object mass distribution while having access to the ground truth values. We compare our approach against multiple baselines and show that it can estimate the mass distribution with around 10% error, while the baselines have errors ranging from 18% to 68%.</td>
                <td>Analytical models, Shape, Dynamics, Estimation, Distance measurement, Complexity theory, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342506&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342506&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Touch if it's Transparent! ACTOR: Active Tactile-Based Category-Level Transparent Object Reconstruction</td>
                <td>P. K. Murali, B. Porr and M. Kaboli</td>
                <td>2023</td>
                <td>Accurate shape reconstruction of transparent ob-jects is a challenging task due to their non-Lambertian surfaces and yet necessary for robots for accurate pose perception and safe manipulation. As vision-based sensing can produce erroneous measurements for transparent objects, the tactile modality is not sensitive to object transparency and can be used for reconstructing the object's shape. We propose AC-TOR, a novel framework for ACtive tactile-based category-level Transparent Object Reconstruction. ACTOR leverages large datasets of synthetic object with our proposed self-supervised learning approach for object shape reconstruction as the collection of real-world tactile data is prohibitively expensive. ACTOR can be used during inference with tactile data from category-level unknown transparent objects for reconstruction. Furthermore, we propose an active-tactile object exploration strategy as probing every part of the object surface can be sample inefficient. We also demonstrate tactile-based category-level object pose estimation task using ACTOR. We perform an extensive evaluation of our proposed methodology with real-world robotic experiments with comprehensive comparison studies with state-of-the-art approaches. Our proposed method outperforms these approaches in terms of tactile-based object reconstruction and object pose estimation.</td>
                <td>Surface reconstruction, Shape, Shape measurement, Pose estimation, Self-supervised learning, Robot sensing systems, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341680&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341680&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learn from Incomplete Tactile Data: Tactile Representation Learning with Masked Autoencoders</td>
                <td>G. Cao, J. Jiang, D. Bollegala and S. Luo</td>
                <td>2023</td>
                <td>The missing signal caused by the objects being occluded or an unstable sensor is a common challenge during data collection. Such missing signals will adversely affect the results obtained from the data, and this issue is observed more frequently in robotic tactile perception. In tactile perception, due to the limited working space and the dynamic environment, the contact between the tactile sensor and the object is frequently insufficient and unstable, which causes the partial loss of signals, thus leading to incomplete tactile data. The tactile data will therefore contain fewer tactile cues with low information density. In this paper, we propose a tactile representation learning method, named TacMAE, based on Masked Autoencoder to address the problem of incomplete tactile data in tactile perception. In our framework, a portion of the tactile image is masked out to simulate the missing contact regions. By reconstructing the missing signals in the tactile image, the trained model can achieve a high-level understanding of surface geometry and tactile properties from limited tactile cues. The experimental results of tactile texture recognition show that TacMAE can achieve a high recognition accuracy of 71.4% in the zero-shot transfer and 85.8% after fine-tuning, which are 15.2% and 8.2% higher than the results without using masked modeling. The extensive experiments on YCB objects demonstrate the knowledge transferability of our proposed method and the potential to improve efficiency in tactile exploration.</td>
                <td>Representation learning, Geometry, Training, Surface reconstruction, Zero-shot learning, Shape, Tactile sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341788&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341788&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Attention for Robot Touch: Tactile Saliency Prediction for Robust Sim-to-Real Tactile Control</td>
                <td>Y. Lin, M. Comi, A. Church, D. Zhang and N. F. Lepora</td>
                <td>2023</td>
                <td>High-resolution tactile sensing can provide accurate information about local contact in contact-rich robotic tasks. However, the deployment of such tasks in unstructured environments remains under-investigated. To improve the robustness of tactile robot control in unstructured environments, we propose and study a new concept: tactile saliency for robot touch, inspired by the human touch attention mechanism from neuroscience and the visual saliency prediction problem from computer vision. In analogy to visual saliency, this concept involves identifying key information in tactile images captured by a tactile sensor. While visual saliency datasets are commonly annotated by humans, manually labelling tactile images is challenging due to their counterintuitive patterns. To address this challenge, we propose a novel approach comprised of three interrelated networks: 1) a Contact Depth Network (ConDepNet), which generates a contact depth map to localize deformation in a real tactile image that contains target and noise features; 2) a Tactile Saliency Network (TacSalNet), which predicts a tactile saliency map to describe the target areas for an input contact depth map; 3) and a Tactile Noise Generator (TacNGen), which generates noise features to train the TacSalNet. Experimental results in contact pose estimation and edge-following in the presence of distractors showcase the accurate prediction of target features from real tactile images. Overall, our tactile saliency prediction approach gives robust sim-to-real tactile control in environments with unknown distractors. Project page: https://sites.google.com/view/tactile-saliency/.</td>
                <td>Visualization, Neuroscience, Robot control, Pose estimation, Tactile sensors, Robustness, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341888&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341888&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FingerTac - An Interchangeable and Wearable Tactile Sensor for the Fingertips of Human and Robot Hands</td>
                <td>P. Sathe, A. Schmitz, T. P. Tomo, S. Somlor, S. Funabashi and S. Shigeki</td>
                <td>2023</td>
                <td>Skill transfer from humans to robots is challenging. Presently, many researchers focus on capturing only position or joint angle data from humans to teach the robots. Even though this approach has yielded impressive results for grasping applications, reconstructing motion for object handling or fine manipulation from a human hand to a robot hand has been sparsely explored. Humans use tactile feedback to adjust their motion to various objects, but capturing and reproducing the applied forces is an open research question. In this paper we introduce a wearable fingertip tactile sensor, which captures the distributed 3-axis force vectors on the fingertip. The fingertip tactile sensor is interchangeable between the human hand and the robot hand, meaning that it can also be assembled to fit on a robot hand such as the Allegro hand. This paper presents the structural aspects of the sensor as well as the methodology and approach used to design, manufacture, and calibrate the sensor. The sensor is able to measure forces accurately with a mean absolute error of 0.21, 0.16, and 0.44 Newtons in X, Y, and Z directions, respectively.</td>
                <td>Force measurement, Measurement uncertainty, Force, Tactile sensors, Grasping, Robots, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342285&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342285&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>AcouSkin: Full Surface Contact localization Using Acoustic Waves</td>
                <td>A. K. Kosta et al.</td>
                <td>2023</td>
                <td>Contact sensing and localization capabilities that mimic human skin are highly desirable for robots. In this paper, we introduce AcouSkin, an acoustic wave based full surface contact localization system. Acoustic waves produced by piezoelectric transceivers using a monotone are coupled to surfaces turning them into an active sensor. Our system leverages information from four piezoelectric transceivers mounted on the surface of an acrylic sheet and vacuum cleaner robot bumper to localize contacts to 18 unique segments. We first characterize acoustic wave propagation based on signal and material properties and then propose hardware and software methods to realize full surface contact localization. Our results show that AcouSkin can reliably localize contact on a flat acrylic sheet with 18 uniformly spaced locations across a 54cm length with mean absolute error (MAE) of ≤ 1 locations using maximum likelihood estimator (MLE) and multilayer perceptron (MLP) models. On the vacuum cleaner robot bumper AcouSkin shows a zero MAE. Further, the system is also able to localize contacts made using forces as low as 2N (Newtons) and as high as 20N. Overall, AcouSkin provides full surface contact localization while requiring minimal instrumentation with easy deployment on real-world robots.</td>
                <td>Location awareness, Maximum likelihood estimation, Vacuum systems, Surface acoustic waves, Robot sensing systems, Turning, Transceivers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342359&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342359&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Unsupervised OmniMVS: Efficient Omnidirectional Depth Inference via Establishing Pseudo-Stereo Supervision</td>
                <td>Z. Chen, C. Lin, L. Nie, K. Liao and Y. Zhao</td>
                <td>2023</td>
                <td>Omnidirectional multi-view stereo (MVS) vision is attractive for its ultra-wide field-of-view (FoV), enabling machines to perceive 360°3D surroundings. However, the existing solutions require expensive dense depth labels for supervision, making them impractical in real-world applications. In this paper, we propose the first unsupervised omnidirectional MVS framework based on multiple fisheye images. To this end, we project all images to a virtual view center and composite two panoramic images with spherical geometry from two pairs of back-to-back fisheye images. The two 360° images formulate a stereo pair with a special pose, and the photometric consistency is leveraged to establish the unsupervised constraint, which we term “Pseudo-Stereo Supervision”. In addition, we propose Un-OmniMVS, an efficient unsupervised omnidirectional MVS network, to facilitate the inference speed with two efficient components. First, a novel feature extractor with frequency attention is proposed to simultaneously capture the non-local Fourier features and local spatial features, explicitly facilitating the feature representation. Then, a variance-based light cost volume is put forward to reduce the computational complexity. Experiments exhibit that the performance of our unsupervised solution is competitive to that of the state-of-the-art (SoTA) supervised methods with better generalization in real-world data. The code will be available at https://github.com/Chen-z-s/Un-OmniMVS.</td>
                <td>Geometry, Costs, Codes, Feature extraction, Cameras, Computational complexity, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342332&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342332&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-IMU Proprioceptive State Estimator for Humanoid Robots</td>
                <td>E. Deschaud and F. Goulette</td>
                <td>2023</td>
                <td>Algorithms for state estimation of humanoid robots usually assume that the feet remain flat and in a constant position while in contact with the ground. However, this hypothesis is easily violated while walking, especially for human-like gaits with heel-toe motion. This reduces the time during which the contact assumption can be used, or requires higher variances to account for errors. In this paper, we present a novel state estimator based on the extended Kalman filter that can properly handle any contact configuration. We consider multiple inertial measurement units (IMUs) distributed throughout the robot's structure, including on both feet, which are used to track multiple bodies of the robot. This multi-IMU instrumentation setup also has the advantage of allowing the deformations in the robot's structure to be estimated, improving the kinematic model used in the filter. The proposed approach is validated experimentally on the exoskeleton Atalante and is shown to present low drift, performing better than similar single-IMU filters. The obtained trajectory estimates are accurate enough to construct elevation maps that have little distortion with respect to the ground truth.</td>
                <td>Legged locomotion, Deformation, Exoskeletons, Humanoid robots, Propioception, Kinematics, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341849&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341849&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing</td>
                <td>Walter, A. Nappi, T. Harbaum and J. Becker</td>
                <td>2023</td>
                <td>Human Pose Estimation (HPE) to assess human motion in sports, rehabilitation or work safety requires accurate sensing without compromising the sensitive underlying personal data. Therefore, local processing is necessary and the limited energy budget in such systems can be addressed by Inertial Measurement Units (IMU) instead of common camera sensing. The central trade-off between accuracy and efficient use of hardware resources is rarely discussed in research. We address this trade-off by a simulative Design Space Exploration (DSE) of a varying quantity and positioning of IMU -sensors. First, we generate IMU-data from a publicly available body model dataset for different sensor configurations and train a deep learning model with this data. Additionally, we propose a combined metric to assess the accuracy-resource trade-off. We used the DSE as a tool to evaluate sensor configurations and identify beneficial ones for a specific use case. Exemplary, for a system with equal importance of accuracy and resources, we identify an optimal sensor configuration of 4 sensors with a mesh error of 6.03 cm, increasing the accuracy by 32.7 % and reducing the hardware effort by two sensors compared to state of the art. Our work can be used to design health applications with well-suited sensor positioning and attention to data privacy and resource-awareness.</td>
                <td>Deep learning, Data privacy, Tracking, Sternum, Pose estimation, Robot sensing systems, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341256&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341256&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Language-Conditioned Observation Models for Visual Object Search</td>
                <td>T. Nguyen, V. Hrosinkov, E. Rosen and S. Tellex</td>
                <td>2023</td>
                <td>Object search is a challenging task because when given complex language descriptions (e.g., “find the white cup on the table”), the robot must move its camera through the environment and recognize the described object. Previous works map language descriptions to a set of fixed object detectors with predetermined noise models, but these approaches are challenging to scale because new detectors need to be made for each object. In this work, we bridge the gap in realistic object search by posing the search problem as a partially observable Markov decision process (POMDP) where the object detector and visual sensor noise in the observation model is determined by a single Deep Neural Network conditioned on complex language descriptions. We incorporate the neural network's outputs into our language-conditioned observation model (LCOM) to represent dynamically changing sensor noise. With an LCOM, any language description of an object can be used to generate an appropriate object detector and noise model, and training an LCOM only requires readily available supervised image-caption datasets. We empirically evaluate our method by comparing against a state-of-the-art object search algorithm in simulation, and demonstrate that planning with our observation model yields a significantly higher average task completion rate (from 0.46 to 0.66) and more efficient and quicker object search than with a fixed-noise model. We demonstrate our method on a Boston Dynamics Spot robot, enabling it to handle complex natural language object descriptions and efficiently find objects in a room-scale environment.</td>
                <td>Adaptation models, Solid modeling, Visualization, Three-dimensional displays, Natural languages, Detectors, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341492&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341492&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TransCAR: Transformer-Based Camera-and-Radar Fusion for 3D Object Detection</td>
                <td>S. Pang, D. Morris and H. Radha</td>
                <td>2023</td>
                <td>Despite radar's popularity in the automotive industry, for fusion-based 3D object detection, most existing works focus on LiDAR and camera fusion. In this paper, we propose TransCAR, a Transformer-based Camera-And-Radar fusion solution for 3D object detection. Our TransCAR consists of two modules. The first module learns 2D features from surround-view camera images and then uses a sparse set of 3D object queries to index into these 2D features. The vision-updated queries then interact with each other via transformer self-attention layer. The second module learns radar features from multiple radar scans and then applies transformer decoder to learn the interactions between radar features and vision-updated queries. The cross-attention layer within the transformer decoder can adaptively learn the soft-association between the radar features and vision-updated queries instead of hard-association based on sensor calibration only. Finally, our model estimates a bounding box per query using set-to-set Hungarian loss, which enables the method to avoid non-maximum suppression. TransCAR improves the velocity estimation using the radar scans without temporal information. The superior experimental results of our TransCAR on the challenging nuScenes datasets illustrate that our TransCAR outperforms state-of-the-art Camera-Radar fusion-based 3D object detection approaches.</td>
                <td>Three-dimensional displays, Estimation, Radar, Object detection, Radar imaging, Transformers, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341793&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341793&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception</td>
                <td>X. Wang</td>
                <td>2023</td>
                <td>Closing the domain gap between training and deployment and incorporating multiple sensor modalities are two challenging yet critical topics for self-driving. Existing work only focuses on single one of the above topics, overlooking the simultaneous domain and modality shift which pervasively exists in real-world scenarios. A model trained with multi-sensor data collected in Europe may need to run in Asia with a subset of input sensors available. In this work, we propose DualCross, a cross-modality cross-domain adaptation framework to facilitate the learning of a more robust monocular bird's-eye-view (BEV) perception model, which transfers the point cloud knowledge from a LiDAR sensor in one domain during the training phase to the camera-only testing scenario in a different domain. This work results in the first open analysis of cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild. We benchmark our approach on large-scale datasets under a wide range of domain shifts and show state-of-the-art results against various baselines. Our project webpage is at https://yunzeman.github.io/DualCross.</td>
                <td>Training, Point cloud compression, Adaptation models, Three-dimensional displays, Laser radar, Europe, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341473&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341473&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>INF: Implicit Neural Fusion for LiDAR and Camera</td>
                <td>S. Zhou, S. Xie, R. Ishikawa, K. Sakurada, M. Onishi and T. Oishi</td>
                <td>2023</td>
                <td>Sensor fusion has become a popular topic in robotics. However, conventional fusion methods encounter many difficulties, such as data representation differences, sensor variations, and extrinsic calibration. For example, the calibration methods used for LiDAR-camera fusion often require manual operation and auxiliary calibration targets. Implicit neural representations (INRs) have been developed for 3D scenes, and the volume density distribution involved in an INR unifies the scene information obtained by different types of sensors. Therefore, we propose implicit neural fusion (INF) for LiDAR and camera. INF first trains a neural density field of the target scene using LiDAR frames. Then, a separate neural color field is trained using camera images and the trained neural density field. Along with the training process, INF both estimates LiDAR poses and optimizes extrinsic parameters. Our experiments demonstrate the high accuracy and stable performance of the proposed method.</td>
                <td>Training, Temperature sensors, Laser radar, Three-dimensional displays, Robot vision systems, Sensor fusion, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341648&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341648&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Modal Upper Limbs Human Motion Estimation from a Reduced Set of Affordable Sensors</td>
                <td>M. Adjel et al.</td>
                <td>2023</td>
                <td>This study aims at developing a new affordable motion capture system for human upper limbs' joint kinematics estimation based on a reduced set of visual inertial measurement units coupled with a markerless skeleton tracking algorithm. The markerless skeleton tracking algorithm allows to alleviate the kinematic redundancy that is observed if only a single visual inertial measurement unit is used at the hand level but it introduces undesired outliers. A Sliding Window Inverse Kinematics Algoritm based on a biomechanical model is proposed to filter out outliers. It has the advantage to constrain the evolution of joint kinematics while being able to handle multi- modalities. The proposed system was validated with five healthy volunteers performing a popular rehabilitation pick and place task. Joint angles estimated using our method were compared with the ones obtained using a reference stereophotogrammetric system. The results showed an average root mean square error of 9.7deg along with an average correlation of 0.8. These results compare favorably with literature results obtained with more numerous and relatively costly sensors or more elaborated and expensive markerless systems.</td>
                <td>Wrist, Visualization, Measurement units, Tracking, Shoulder, Inertial navigation, Calibration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342040&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342040&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Cognition Difference-Based Dynamic Trust Network for Distributed Bayesian Data Fusion</td>
                <td>Y. Li, Z. Zhang, J. Wang, H. Zhang, E. Zhou and F. Zhang</td>
                <td>2023</td>
                <td>Distributed Data Fusion (DDF), as a prevalent technique that empowers scalable, flexible, and robust information fusing, has been employed in various multi-sensor networks operating in uncertain and dynamic environments. This paper proposes a cognition difference-based mechanism to construct a dynamic trust network for real-time DDF, where the cognition difference is defined as the statistical difference between the sensors' estimated probability distributions. Distinguished by the mutual correlation between trust and cognition difference, two principles of determining trust are investigated, and their performances are analyzed by conducting simulations in the scenarios of source seeking. Our simulation and experiment results show that the proposed approach is effective in providing comprehensive and robust performance in general and unstructured environments.</td>
                <td>Analytical models, Correlation, Data integration, Cognition, Real-time systems, Probability distribution, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341770&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341770&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sparse Dense Fusion for 3D Object Detection</td>
                <td>Y. Gao, C. Sima, S. Shi, S. Di, S. Liu and H. Li</td>
                <td>2023</td>
                <td>With the prevalence of multimodal learning, camera-LiDAR fusion has gained popularity in 3D object detection. Many fusion approaches have been proposed, falling into two main categories: sparse-only or dense-only, differentiated by their feature representation within the fusion module. We analyze these approaches within a shared taxonomy, identifying two key challenges: (1) Sparse-only methodologies maintain 3D geometric prior but fail to capture the semantic richness from camera data, and (2) Dense-only strategies preserve semantic continuity at the expense of precise geometric information derived from LiDAR. Upon analysis, we deduce that due to their respective architectural designs, some degree of information loss is inevitable. To counteract this loss, we introduce Sparse Dense Fusion (SD-Fusion), an innovative framework combining both sparse and dense fusion modules via the Transformer architecture. The simple yet effective fusion strategy enhances semantic texture and simultaneously leverages spatial structure data. Employing our SD-Fusion strategy, we assemble two popular methods with moderate performance, achieving a 4.3% increase in mAP and a 2.5% rise in NDS, thus ranking first in the nuScenes benchmark. Comprehensive ablation studies validate the effectiveness of our approach and empirically support our findings.</td>
                <td>Three-dimensional displays, Laser radar, Semantics, Taxonomy, Object detection, Benchmark testing, Transformers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341620&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341620&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Visual Contact Pressure Estimation for Grippers in the Wild</td>
                <td>J. A. Collins, C. Houff, P. Grady and C. C. Kemp</td>
                <td>2023</td>
                <td>Sensing contact pressure applied by a gripper can benefit autonomous and teleoperated robotic manipulation, but adding tactile sensors to a gripper's surface can be difficult or impractical. If a gripper visibly deforms, contact pressure can be visually estimated using images from an external camera that observes the gripper. While researchers have demonstrated this capability in controlled laboratory settings, prior work has not addressed challenges associated with visual pressure estimation in the wild, where lighting, surfaces, and other factors vary widely. We present a deep learning model and associated methods that enable visual pressure estimation under widely varying conditions. Our model, Visual Pressure Estimation for Robots (ViPER), takes an image from an eye-in-hand camera as input and outputs an image representing the pressure applied by a soft gripper. Our key insight is that force/torque sensing can be used as a weak label to efficiently collect training data in settings where pressure measurements would be difficult to ob-tain. When trained on this weakly labeled data combined with fully labeled data that includes pressure measurements, ViPER outperforms prior methods, enables precision manipulation in cluttered settings, and provides accurate estimates for unseen conditions relevant to in-home use.</td>
                <td>Visualization, Force measurement, Robot vision systems, Estimation, Training data, Tactile sensors, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342124&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342124&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hydra-Multi: Collaborative Online Construction of 3D Scene Graphs with Multi-Robot Teams</td>
                <td>Y. Chang, N. Hughes, A. Ray and L. Carlone</td>
                <td>2023</td>
                <td>3D scene graphs have recently emerged as an expressive high-level map representation that describes a 3D environment as a layered graph where nodes represent spatial concepts at multiple levels of abstraction (e.g., objects, rooms, buildings) and edges represent relations between concepts (e.g., inclusion, adjacency). This paper describes Hydra-Multi, the first multi-robot spatial perception system capable of constructing a multi-robot 3D scene graph online from sensor data collected by robots in a team. In particular, we develop a centralized system capable of constructing a joint 3D scene graph by taking incremental inputs from multiple robots, effectively finding the relative transforms between the robots' frames, and incorporating loop closure detections to correctly reconcile the scene graph nodes from different robots. We evaluate Hydra-Multi on simulated and real scenarios and show it is able to reconstruct accurate 3D scene graphs online. We also demonstrate Hydra-Multi's capability of supporting heterogeneous teams by fusing different map representations built by robots with different sensor suites.</td>
                <td>Three-dimensional displays, Buildings, Collaboration, Transforms, Robot sensing systems, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341838&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341838&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TWO: A Simple Method of Directly Closing the Loop for LiDAR Odometry</td>
                <td>Z. Zhang, Z. Yao and M. Lu</td>
                <td>2023</td>
                <td>In this paper, we propose a simple method, termed TWO, of directly closing the loop for LiDAR odometry. TWO suggests assigning high weights to the LIDAR observations corresponding to the old parts of the map; since these parts are built with the low-drift poses from the early odometry and can help drag the drifted odometry back to the correct global position when the LiDAR scans the points of these parts again. Also, we present the method of checking the consistency of the plane normal to address the two-side problem that may cause damage when using TWO. Moreover, we show that the proposed method is lightweight and needs little extra computation and storage space compared to the original odometry. The proposed TWO is integrated into the state-of-the-art LiDAR odometry A-LOAM and LiDAR-inertial odometry FAST-LIO2, and it is tested thoroughly on five public datasets and our private handheld dataset. The experiments show that the TWO can effectively help these two methods directly close most loops and produce localization results with apparently lower drifts.</td>
                <td>Location awareness, Laser radar, Pose estimation, Odometry, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341569&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341569&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MEM: Multi-Modal Elevation Mapping for Robotics and Learning</td>
                <td>G. Erni, J. Frey, T. Miki, M. Mattamala and M. Hutter</td>
                <td>2023</td>
                <td>Elevation maps are commonly used to represent the environment of mobile robots and are instrumental for locomotion and navigation tasks. However, pure geometric information is insufficient for many field applications that require appearance or semantic information, which limits their applicability to other platforms or domains. In this work, we extend a 2.5D robot-centric elevation mapping framework by fusing multi-modal information from multiple sources into a popular map representation. The framework allows inputting data contained in point clouds or images in a unified manner. To manage the different nature of the data, we also present a set of fusion algorithms that can be selected based on the information type and user requirements. Our system is designed to run on the GPU, making it real-time capable for various robotic and learning tasks. We demonstrate the capabilities of our framework by deploying it on multiple robots with varying sensor configurations and showcasing a range of applications that utilize multi-modal layers, including line detection, human detection, and colorization.</td>
                <td>Micromechanical devices, Point cloud compression, Navigation, Semantic segmentation, Semantics, Graphics processing units, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342108&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342108&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Cooperative LiDAR Localization and Mapping for V2X Connected Autonomous Vehicles</td>
                <td>N. Ritter, K. Alomari and D. Goehring</td>
                <td>2023</td>
                <td>Cooperative Simultaneous Localization and Mapping (C-SLAM) is an active research topic in mobile robotics. However, its application in the field of autonomous driving is rare. While the advent of Vehicle-to-Everything (V2X) communication has empowered Connected Autonomous Vehicles (CAV) to exchange data with each other, recent research on CAV cooperation tasks has primarily focused on cooperative perception and global positioning improvement. Techniques for organizing multiple CAV to work together to achieve localization and mapping in unknown environments have not been actively explored. We propose a C-SLAM system for CAVs that employs sparse LiDAR feature representations to enable vehicles to exchange data using standard V2X messages. The system was tested in real environments using two connected vehicles. The results show that the proposed V2X-based C-SLAM system can operate in both centralized and decentralized manners and output accurate pose estimates and global maps, showing promising application possibilities.</td>
                <td>Location awareness, Simultaneous localization and mapping, Laser radar, Connected vehicles, Trajectory, Task analysis, Autonomous vehicles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341513&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341513&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Resilient and Distributed Multi-Robot Visual SLAM: Datasets, Experiments, and Lessons Learned</td>
                <td>Y. Tian et al.</td>
                <td>2023</td>
                <td>This paper revisits Kimera-Multi, a distributed multi-robot Simultaneous Localization and Mapping (SLAM) system, towards the goal of deployment in the real world. In particular, this paper has three main contributions. First, we describe improvements to Kimera-Multi to make it resilient to large-scale real-world deployments, with particular emphasis on handling intermittent and unreliable communication. Second, we collect and release challenging multi-robot benchmarking datasets obtained during live experiments conducted on the MIT campus, with accurate reference trajectories and maps for evaluation. The datasets include up to 8 robots traversing long distances (up to 8 km) and feature many challenging elements such as severe visual ambiguities (e.g., in underground tunnels and hallways), mixed indoor and outdoor trajectories with different lighting conditions, and dynamic entities (e.g., pedestrians and cars). Lastly, we evaluate the resilience of Kimera-Multi under different communication scenarios, and provide a quantitative comparison with a centralized baseline system. Based on the results from both live experiments and subsequent analysis, we discuss the strengths and weaknesses of Kimera-Multi, and suggest future directions for both algorithm and system design. We release the source code of Kimera-Multi and all datasets to facilitate further research towards the reliable real-world deployment of multi-robot SLAM systems.</td>
                <td>Visualization, Simultaneous localization and mapping, Pedestrians, Statistical analysis, Source coding, Urban areas, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342377&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342377&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Assignment Algorithms for Multi-Robot Multi-Target Tracking with Sufficient and Limited Sensing Capability</td>
                <td>P. Li and L. Zhou</td>
                <td>2023</td>
                <td>We study the problem of assigning robots with actions to track targets. The objective is to optimize the robot team's tracking quality which can be defined as the reduction in the uncertainty of the targets' states. Specifically, we consider two assignment problems given the different sensing capabilities of the robots. In the first assignment problem, a single robot is sufficient to track a target. To this end, we present a greedy algorithm (Algorithm 1) that assigns a robot with its action to each target. We prove that the greedy algorithm has a 1/2-approximation bound and runs in polynomial time. Then, we study the second assignment problem where two robots are necessary to track a target. We design another greedy algorithm (Algorithm 2) that assigns a pair of robots with their actions to each target. We prove that the greedy algorithm achieves a 1/3-approximation bound and has a polynomial running time. Moreover, we illustrate the performance of the two greedy algorithms in the ROS-Gazebo environment where the tracking patterns of one robot following one target using Algorithm 1 and two robots following one target using Algorithm 2 are clearly observed. Further, we conduct extensive comparisons to demonstrate that the two greedy algorithms perform close to their optimal counterparts and much better than their respective (1/2 and 1/3) approximation bounds.</td>
                <td>Greedy algorithms, Target tracking, Uncertainty, Computational modeling, Robot sensing systems, Approximation algorithms, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341514&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341514&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-View Robust Collaborative Localization in High Outlier Ratio Scenes Based on Semantic Features</td>
                <td>Y. Tang, M. Wang, Y. Deng, Y. Yang, Z. Lan and Y. Yue</td>
                <td>2023</td>
                <td>Filtering out outlier data associations between local maps can improve the robustness and accuracy of multi-robot localization. When the overlap is low and the field of view difference is large, it is likely to produce outlier data associations between local maps, which will reduce the matching accuracy and even lead to the failure of collaborative localization. To solve this problem, this paper proposes a novel outdoor robust collaborative localization algorithm (HORCL) capable for high outlier ratio scenes. The Mixture Probability Model (MPM) and the Hierarchical EM (Expectation Maximization) algorithm in HORCL are applied to screen two levels of outliers (loop closure constraints and point pairs) and improve localization performance. Specifically, the inlier probabilities of data associations are calculated in MPM to identify outliers by considering geometric distances, semantic consistency, and spatial consistency. Then, outlier loop closures and outlier point pairs in inlier constraints are filtered by applying the Hierarchical EM algorithm, thereby relieving the adverse effect of outliers on localization accuracy. The proposed algorithm is validated on public datasets and compared with the latest methods, demonstrating the improvement in localization accuracy and robustness. The code is available at https://github.com/BIT-TYJ/HORCL.</td>
                <td>Location awareness, Semantics, Collaboration, Probability, Filtering algorithms, Robustness, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342524&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342524&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>KD-EKF: A Consistent Cooperative Localization Estimator Based on Kalman Decomposition</td>
                <td>N. Hao, F. He, C. Tian, Y. Yao and W. Xia</td>
                <td>2023</td>
                <td>In this paper, we revisit the inconsistency problem of EKF-based cooperative localization (CL) from the perspective of system decomposition. By transforming the linearized system used by the standard EKF into its Kalman observable canonical form, the observable and unobservable components of the system are separated. Consequently, the factors causing the dimension reduction of the unobservable subspace are explicitly isolated in the state propagation and measurement Jacobians of the Kalman observable canonical form. Motivated by these insights, we propose a new CL algorithm called KD-EKF which aims to enhance consistency. The key idea behind the KD-EKF algorithm involves perform state estimation in the transformed coordinates so as to eliminate the influencing factors of observability in the Kalman observable canonical form. As a result, the KD-EKF algorithm ensures correct observability properties and consistency. We extensively verify the effectiveness of the KD-EKF algorithm through both Monte Carlo simulations and real-world experiments. The results demonstrate that the KD-EKF outperforms state-of-the-art algorithms in terms of accuracy and consistency.</td>
                <td>Location awareness, Jacobian matrices, Simultaneous localization and mapping, Monte Carlo methods, Filtering algorithms, Kalman filters, Observability, Cooperative localization, Kalman decomposition, nonlinear estimation, extended Kalman filter, consistency</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341604&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341604&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Distributed Scheduling Method for Networked UAV Swarm based on Computing for Communication</td>
                <td>R. Chen, J. Li, Y. Chen and Y. Huang</td>
                <td>2023</td>
                <td>UAV swarms have attracted much attention for post-disaster search and rescue, pollution monitoring and trace-ability, etc., where distributed scheduling is required to arrange careful tasks and time quickly. The market-based methods are widely favored but they rely on the environmentally influenced communication network to complete negotiation, while the on-board computing of UAV is robust and redundant. This paper proposes a distributed scheduling method for networked UAV swarm based on computing for communication, which trades a modest increase in computing for a significant decrease in communication. First, by analyzing the task removal strategies of two representative methods, the consensus-based bundle algorithm (CBBA) and performance impact (PI) algorithm, a new removal strategy is proposed, which expands the explo-ration of the bundle and can potentially reduce communication rounds. Second, the proposed task-related optimization method can extract task conflict nodes from the native communication protocol, and use the sampling and estimation strategies to resolve task conflicts in advance. Third, historical bids are cleverly used to infer others' locations, which is necessary for task-related optimization. Fourth, to verify the algorithm in real communication, a hardware-in-the-loop (HIL) ad-hoc network simulation system is constructed, which uses real network protocols and simulated channel transmissions. Finally, the HIL Monte Carlo simulation results show that, compared with CBBA and PI, the proposed method can significantly reduce the number of communication rounds and the total scheduling time, without increasing the communication protocol overhead and loss of optimization.</td>
                <td>Protocols, Pollution, Monte Carlo methods, Processor scheduling, Optimization methods, Autonomous aerial vehicles, Prediction algorithms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342228&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342228&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Visual, Spatial, Geometric-Preserved Place Recognition for Cross-View and Cross-Modal Collaborative Perception</td>
                <td>P. Gao, J. Liang, Y. Shen, S. Son and M. C. Lin</td>
                <td>2023</td>
                <td>Place recognition plays an important role in multi-robot collaborative perception, such as aerial-ground search and rescue, in order to identify the same place they have visited. Recently, approaches based on semantics showed the promising performance to address cross-view and cross-modal challenges in place recognition, which can be further categorized as graph-based and geometric-based methods. However, both methods have shortcomings, including ignoring geometric cues and affecting by large non-overlapped regions between observations. In this paper, we introduce a novel approach that integrates semantic graph matching and distance fields (DF) matching for cross-view and cross-modal place recognition. Our method uses a graph representation to encode visual-spatial cues of semantics and uses a set of class-wise DFs to encode geometric cues of a scene. Then, we formulate place recognition as a two-step matching problem. We first perform semantic graph matching to identify the correspondence of semantic objects. Then, we estimate the overlapped regions based on the identified correspondences and further align these regions to compute their geometric-based DF similarity. Finally, we integrate graph-based similarity and geometry-based DF similarity to match places. We evaluate our approach over two public benchmark datasets, including KITTI and AirSim. Compared with the previous methods, our approach achieves around 10% improvement in ground-ground place recognition in KITTI and 35% improvement in aerial-ground place recognition in AirSim.</td>
                <td>Point cloud compression, Visualization, Laser radar, Runtime, Semantics, Robot vision systems, Collaboration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341898&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341898&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Co-Simulation Framework for Communication and Control in Autonomous Multi-Robot Systems</td>
                <td>S. Acharya, M. Bharatheesha, Y. Simmhan and B. Amrutur</td>
                <td>2023</td>
                <td>Multi-Robot Systems (MRS) are transforming diverse domains like logistics, cargo management, and agriculture. However, ensuring that the behavior is correct under various network conditions within such complex environments is challenging, and meeting the desired automation goals is difficult. We propose the CORNET 2.0 co-simulation framework to jointly and accurately simulate multi-agent robotic systems within physical environments and the communication network models within such environments. Our modular framework allows diverse robot and network models to seamlessly integrate to simulate the robot's autonomy, physical space, and network features, such as latency, throughput, and loss intrinsic to the network topology and communication technology. A key novelty of CORNET 2.0 is its accurate synchronizing of mobility and time, which ensures that the physical location of a robot at a point in time, and the network properties and packets that flow from that location, are aligned. This is vital to model and validate MRS coordination algorithms that rely on network interactions. We provide a detailed evaluation of CORNET 2.0 in modeling real-world MRS use cases, such as leader-follower and warehouse environments, that help highlights the benefits.</td>
                <td>Automation, Network topology, Robot kinematics, Throughput, Safety, Multi-robot systems, Communication networks</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342407&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342407&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FPECMV: Learning-Based Fault-Tolerant Collaborative Localization Under Limited Connectivity</td>
                <td>R. Ou, G. Liang and T. L. Lam</td>
                <td>2023</td>
                <td>Collaborative localization (CL) has garnered substantial attention in the field of robotics in recent years. Nonetheless, conventional CL algorithms have faced challenges when dealing with practical issues such as spurious sensor data and limited or discontinued observation and communication in real-world settings. This paper proposes a fault-tolerant practical estimated cross-covariance minimum variance update method (FPECMV) designed to tackle these challenges under limited connectivity. The proposed algorithm uses a CNN-based method to evaluate confidence, along with a fault isolation module to identify faults and manage spurious data in real time. The proposed fault isolation module utilizes relative measurement information that randomly occurs, without requiring high observation and communication prerequisites. Notably, the algorithm takes into account correlations among agents to maintain consistency in localization filters and attain accurate localization despite constraints posed by limited connectivity. To evaluate the performance of the proposed algorithm, experiments were conducted in a collaborative multi-robot environment with spurious sensor data and limited connectivity, using both the BULLET simulation and physical mobile robots. The experimental results indicate that the overall localization performance of the proposed algorithm is improved by 21.0% compared to the state of the art. The experiment results demonstrate the effectiveness of our algorithm in localizing group agents in challenging and intricate scenarios with limited connectivity and spurious sensor data.</td>
                <td>Location awareness, Fault tolerance, Visualization, Correlation, Fault tolerant systems, Collaboration, Distributed databases</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342426&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342426&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dynamic Multi-Target Tracking Using Heterogeneous Coverage Control</td>
                <td>R. Lin and M. Egerstedt</td>
                <td>2023</td>
                <td>A coverage-based collaborative control strategy is developed in this paper for a multi-robot system with heterogeneous effective sensing ranges and safe operation zones to simultaneously estimate the states of and follow multiple targets governed by stochastic dynamics. Multiplicatively weighted Voronoi diagrams are exploited to define each robot's dominant region considering its limited sensing radius. The asymptotically stable system dynamics enabling the heterogeneous multi-robot system to (locally) optimally cover the time-varying probability density distributions that characterize the uncertainties of the targets' positions is derived, and minimally perturbed by control barrier functions designed to ensure that each robot moves within its safe operation zone in a collision-free manner. Specific target dynamics and measurement models are chosen in the experiment, whose results demonstrate the effectiveness of the proposed dynamic multi-target tracking approach.</td>
                <td>Target tracking, Uncertainty, System dynamics, Robot sensing systems, Sensors, Noise measurement, Multi-robot systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342237&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342237&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Decentralized Connectivity Maintenance for Quadrotor UAVs with Field of View Constraints</td>
                <td>M. Bernard, C. Pacchierotti and P. R. Giordano</td>
                <td>2023</td>
                <td>We present a decentralized connectivity-maintenance algorithm for controlling a group of quadrotor UAVs with limited field of view (FOV) and not sharing a common reference frame for collectively expressing measurements and commands. This is in contrast to the vast majority of previous works on this topic which, instead, make the (simplifying) assumptions of omnidirectional sensing and presence of a common shared frame. For achieving this goal, we design a gradient-based connectivity-maintenance controller able to take into account the presence of a limited FOV. We also propose a novel (to our knowledge) decentralized estimator of the relative orientation among neighboring robots, which is a necessary quantity for correctly implementing the connectivity-maintenance action. We validate the framework in realistic simulations that show the effectiveness of our approach.</td>
                <td>Maintenance engineering, Robot sensing systems, Sensors, Intelligent robots, Quadrotors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342003&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342003&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FogROS2-SGC: A ROS2 Cloud Robotics Platform for Secure Global Connectivity</td>
                <td>K. Chen et al.</td>
                <td>2023</td>
                <td>The Robot Operating System (ROS2) is the most widely used software platform for building robotics applications. FogROS2 extends ROS2 to allow robots to access cloud computing on demand. We introduce FogROS2-SGC, an extension of FogROS2 that can effectively connect robot systems across different physical locations, networks, and Data Distribution Services (DDS). With globally unique and location-independent identifiers, FogROS2-SGC can securely and efficiently route data between robotics components around the globe. FogROS2-SGC is agnostic to the ROS2 distribution and configuration, is compatible with non-ROS2 software, and seamlessly extends existing ROS2 applications without any code modification. We evaluate FogROS2-SGC with 4 robots and compute nodes that are 3600 km apart. Experiments suggest FogROS2-SGC is 19x faster than rosbridge (a ROS2 package with comparable features, but lacking security). Videos and code are available on the website https://sites.google.com/view/fogros2-sgc.</td>
                <td>Cloud computing, Codes, Operating systems, Buildings, Software, Security, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341719&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341719&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Covering Dynamic Demand with Multi-Resource Heterogeneous Teams</td>
                <td>M. Coffey and A. Pierson</td>
                <td>2023</td>
                <td>In this work, we consider a team of heterogeneous robots equipped with various types and quantities of resources, and tasked with supplying these resources to multiple dynamic demand locations. We present an adaptive control policy that enables robots to serve a dynamic demand: we allow demand to deplete as robots supply resources, and we allow demand injection and movement of demand locations. We show that the demand is input-to-state stable (ISS) under our proposed resource dynamics, and thus the robots can drive the demand to a steady state. Finally, we present simulations and hardware experiments to demonstrate our approach, and demonstrate the benefits of coverage over a persistent monitoring approach.</td>
                <td>Dynamic scheduling, Hardware, Steady-state, Task analysis, Adaptive control, Robots, Monitoring</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342119&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342119&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Decision-Oriented Learning with Differentiable Submodular Maximization for Vehicle Routing Problem</td>
                <td>G. Shi and P. Tokekar</td>
                <td>2023</td>
                <td>We study the problem of learning a function that maps context observations (input) to parameters of a submodular function (output). Our motivating case study is a specific type of vehicle routing problem, in which a team of Unmanned Ground Vehicles (UGVs) can serve as mobile charging stations to recharge a team of Unmanned Ground Vehicles (UAVs) that execute persistent monitoring tasks. We want to learn the mapping from observations of UAV task routes and wind field to the parameters of a submodular objective function, which describes the distribution of landing positions of the UAVs. Traditionally, such a learning problem is solved independently as a prediction phase without considering the downstream task optimization phase. However, the loss function used in prediction may be misaligned with our final goal, i.e., a good routing decision. Good performance in the isolated prediction phase does not necessarily lead to good decisions in the downstream routing task. In this paper, we propose a framework that incorporates task optimization as a differentiable layer in the prediction phase. Our framework allows end-to-end training of the prediction model without using engineered intermediate loss that is targeted only at the prediction performance. In the proposed framework, task optimization (submodular maximization) is made differentiable by introducing stochastic perturbations into deterministic algorithms (i.e., stochastic smoothing). We demonstrate the efficacy of the proposed framework using synthetic data. Experimental results of the mobile charging station routing problem show that the proposed framework can result in better routing decisions, e.g. the average number of UAVs recharged increases, compared to the prediction-optimization separate approach.</td>
                <td>Training, Smoothing methods, Vehicle routing, Stochastic processes, Charging stations, Routing, Land vehicles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342311&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342311&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Privacy-Preserving and Uncertainty-Aware Federated Trajectory Prediction for Connected Autonomous Vehicles</td>
                <td>M. Peng, J. Wang, D. Song, F. Miao and L. Su</td>
                <td>2023</td>
                <td>Deep learning is the method of choice for trajectory prediction for autonomous vehicles. Unfortunately, its data-hungry nature implicitly requires the availability of sufficiently rich and high-quality centralized datasets, which easily leads to privacy leakage. Besides, uncertainty-awareness becomes increasingly important for safety-crucial cyber physical systems whose prediction module heavily relies on machine learning tools. In this paper, we relax the data collection requirement and enhance uncertainty-awareness by using Federated Learning on Connected Autonomous Vehicles with an uncertainty-aware global objective. We name our algorithm as FLTP. We further introduce ALFLTP which boosts FLTP via using active learning techniques in adaptatively selecting participating clients. We consider two different metrics negative log-likelihood (NLL) and aleatoric uncertainty (AU) for client selection. Experiments on Argoverse dataset show that FLTP significantly outperforms the model trained on local data. In addition, ALFLTP-AU converges faster in training regression loss and performs better in terms of Miss Rate (MR) than FLTP in most rounds, and has more stable round-wise performance than ALFLTP-NLL.</td>
                <td>Training, Measurement, Gold, Data privacy, Uncertainty, Federated learning, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341638&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341638&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>On Collaborative Robot Teams for Environmental Monitoring: A Macroscopic Ensemble Approach</td>
                <td>V. Edwards, T. C. Silva, B. Mehta, J. Dhanoa and M. A. Hsieh</td>
                <td>2023</td>
                <td>With the rapidly changing climate and an increase in extreme weather events, it is necessary to have better methods to monitor and study the impacts of these phenomena on urban river environments. Multi-robot environmental monitoring has long focused on strategies that assign individual robots to distinct regions or task objectives. While these methods have seen success for Autonomous Surface Vehicles (ASVs), the spatial expanse and temporal variability of rivers impose an increased burden on existing techniques, necessitating computationally intensive replanning. Alternative methods aim to model and control teams of robots by prescribing global constraints on the system, using the insight that robots' transitions between tasks are stochastic and time-based. These methods do not require replanning because robots will perform different tasks achieving the overall desired system state, focusing on temporal switching alone limits their overall descriptive power. In this paper, we present a method that considers collaborations between robots to inform task switching based on spatial proximity. Our results suggest that in unknown environments macroscopic models provide increased flexibility for individual robot task execution as compared to coverage control methods.</td>
                <td>Collaboration, Stochastic processes, Switches, Predictive models, Rivers, Environmental monitoring, Task analysis, Macroscopic models, Robotic Teams, Environmental Monitoring</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342495&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342495&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Part-level Scene Reconstruction Affords Robot Interaction</td>
                <td>Z. Zhang et al.</td>
                <td>2023</td>
                <td>Existing methods for reconstructing interactive scenes primarily focus on replacing reconstructed objects with CAD models retrieved from a limited database, resulting in significant discrepancies between the reconstructed and observed scenes. To address this issue, our work introduces a part-level reconstruction approach that reassembles objects using primitive shapes. This enables us to precisely replicate the observed physical scenes and simulate robot interactions with both rigid and articulated objects. By segmenting reconstructed objects into semantic parts and aligning primitive shapes to these parts, we assemble them as CAD models while estimating kinematic relations, including parent-child contact relations, joint types, and parameters. Specifically, we derive the optimal primitive alignment by solving a series of optimization problems, and estimate kinematic relations based on part semantics and geometry. Our experiments demonstrate that part-level scene reconstruction outperforms object-level reconstruction by accurately capturing finer details and improving precision. These reconstructed part-level interactive scenes provide valuable kinematic information for various robotic applications; we showcase the feasibility of certifying mobile manipulation planning in these interactive scenes before executing tasks in the physical world.</td>
                <td>Geometry, Solid modeling, Shape, Databases, Semantics, Kinematics, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342208&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342208&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Online Adaptive Disparity Estimation for Dynamic Scenes in Structured Light Systems</td>
                <td>R. Qiao, H. Kawasaki and H. Zha</td>
                <td>2023</td>
                <td>In recent years, deep neural networks have shown remarkable progress in dense disparity estimation from dynamic scenes in monocular structured light systems. However, their performance significantly drops when applied in unseen environments. To address this issue, self-supervised online adaptation has been proposed as a solution to bridge this performance gap. Unlike traditional fine-tuning processes, online adaptation performs test-time optimization to adapt networks to new domains. Therefore, achieving fast convergence during the adaptation process is critical for attaining satisfactory accuracy. In this paper, we propose an unsupervised loss function based on long sequential inputs. It ensures better gradient directions and faster convergence. Our loss function is designed using a multi-frame pattern flow, which comprises a set of sparse trajectories of the projected pattern along the sequence. We estimate the sparse pseudo ground truth with a confidence mask using a filter-based method, which guides the online adaptation process. Our proposed framework significantly improves the online adaptation speed and achieves superior performance on unseen data. The code is available on https://github.com/CodePointer/TIDENet.</td>
                <td>Bridges, Codes, Estimation, Computer architecture, Artificial neural networks, Trajectory, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342000&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342000&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RaPlace: Place Recognition for Imaging Radar using Radon Transform and Mutable Threshold</td>
                <td>H. Jang, M. Jung and A. Kim</td>
                <td>2023</td>
                <td>Due to the robustness in sensing, radar has been highlighted, overcoming harsh weather conditions such as fog and heavy snow. In this paper, we present a novel radar-only place recognition that measures the similarity score by utilizing Radon-transformed sinogram images and cross-correlation in frequency domain. Doing so achieves rigid transform invariance during place recognition, while ignoring the effects of radar multipath and ring noises. In addition, we compute the radar similarity distance using mutable threshold to mitigate variability of the similarity score, and reduce the time complexity of processing a copious radar data with hierarchical retrieval. We demonstrate the matching performance for both intra-session loop-closure detection and global place recognition using a publicly available imaging radar datasets. We verify reliable performance compared to existing stable radar place recognition method. Furthermore, codes for the proposed imaging radar place recognition is released for community https://github.com/hyesu-jang/RaPlace.</td>
                <td>Image recognition, Simultaneous localization and mapping, Snow, Imaging, Radar, Transforms, Radar imaging</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341883&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341883&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Pyramid Semantic Graph-Based Global Point Cloud Registration with Low Overlap</td>
                <td>Z. Qiao, Z. Yu, H. Yin and S. Shen</td>
                <td>2023</td>
                <td>Global point cloud registration is essential in many robotics tasks like loop closing and relocalization. Unfortunately, the registration often suffers from the low overlap between point clouds, a frequent occurrence in practical applications due to occlusion and viewpoint change. In this paper, we propose a graph-theoretic framework to address the problem of global point cloud registration with low overlap. To this end, we construct a consistency graph to facilitate robust data association and employ graduated non-convexity (GNC) for reliable pose estimation, following the state-of-the-art (SoTA) methods. Unlike previous approaches, we use semantic cues to scale down the dense point clouds, thus reducing the problem size. Moreover, we address the ambiguity arising from the consistency threshold by constructing a pyramid graph with multi-level consistency thresholds. Then we propose a cascaded gradient ascend method to solve the resulting densest clique problem and obtain multiple pose candidates for every consistency threshold. Finally, fast geometric verification is employed to select the optimal estimation from multiple pose candidates. Our experiments, conducted on a self-collected indoor dataset and the public KITTI dataset, demonstrate that our method achieves the highest success rate despite the low overlap of point clouds and low semantic quality. We have open-sourced our code1 for this project.</td>
                <td>Point cloud compression, Location awareness, Monte Carlo methods, Semantics, Buildings, Pose estimation, Merging</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341394&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341394&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Verifiable Goal Recognition for Autonomous Driving with Occlusions</td>
                <td>C. Brewitt, M. Tamborski, C. Wang and S. V. Albrecht</td>
                <td>2023</td>
                <td>Goal recognition (GR) involves inferring the goals of other vehicles, such as a certain junction exit, which can enable more accurate prediction of their future behaviour. In autonomous driving, vehicles can encounter many different scenarios and the environment may be partially observable due to occlusions. We present a novel GR method named Goal Recognition with Interpretable Trees under Occlusion (OGRIT). OGRIT uses decision trees learned from vehicle trajectory data to infer the probabilities of a set of generated goals. We demonstrate that OGRIT can handle missing data due to occlusions and make inferences across multiple scenarios using the same learned decision trees, while being computationally fast, accurate, interpretable and verifiable. We also release the inDO, rounDO and OpenDDO datasets of occluded regions used to evaluate OGRIT.</td>
                <td>Data models, Trajectory, Decision trees, Junctions, Autonomous vehicles, Intelligent robots, Anomaly detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342386&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342386&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Semantically Informed MPC for Context-Aware Robot Exploration</td>
                <td>Y. Goel, N. Vaskevicius, L. Palmieri, N. Chebrolu, K. O. Arras and C. Stachniss</td>
                <td>2023</td>
                <td>We investigate the task of object goal navigation in unknown environments where a target object is given as a semantic label (e.g. find a couch). This task is challenging as it requires the robot to consider the semantic context in diverse settings (e.g. TVs are often nearby couches). Most of the prior work tackles this problem under the assumption of a discrete action policy whereas we present an approach with continuous control which brings it closer to real world applications. In this paper, we use information-theoretic model predictive control on dense cost maps to bring object goal navigation closer to real robots with kinodynamic constraints. We propose a deep neural network framework to learn cost maps that encode semantic context and guide the robot towards the target object. We also present a novel way of fusing mid-level visual representations in our architecture to provide additional semantic cues for cost map prediction. The experiments show that our method leads to more efficient and accurate goal navigation with higher quality paths than the reported baselines. The results also indicate the importance of mid-level representations for navigation by improving the success rate by 8 percentage points.</td>
                <td>Visualization, Costs, Uncertainty, TV, Navigation, Semantics, Network architecture</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341564&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341564&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Automotive Radar Missing Dimension Reconstruction from Motion</td>
                <td>C. Lin</td>
                <td>2023</td>
                <td>Automotive radars have been reliably used in most assisted and autonomous driving systems due to their robustness to extreme weather conditions. With radial velocity measurements from automotive radars, moving targets such as cars, trucks, and buses can be tracked robustly. However, due to the lack of elevation angles in measurements from automotive radars, stationary targets at different heights, such as maintenance holes and bridges, cannot be distinguished. Most autonomous systems rely on sensor fusion or ignore stationary targets to tackle the problem of missing the elevation angle dimension, which derives safety issues. We propose a simple yet effective approach to estimate the elevation angles of stationary targets from relative velocity and radial velocity measurements from an automotive radar. In contrast to structure from motion in computer vision, we utilize the instantaneous velocity generated from the motion of the ego vehicle. The radial velocity of each target is the projection of relative velocity onto the radial direction from radar to target. The radial velocity of each target can be inferred given the target's azimuth, elevation angle, and relative velocity. Accordingly, the elevation angle of each stationary target can be uniquely calculated given the velocity of radar and the target's azimuth and radial velocity measurements. The radar's velocity is estimated with the existing radar odometry algorithm and IMU. The proposed method is verified with real-world data. We evaluate the system's performance with a pre-built point cloud map and a good localization module in a real-world scenario. The proposed elevation angle reconstruction can reach a 1.41-degree mean error and standard deviation of 0.6 degrees in elevation angle.</td>
                <td>Target tracking, Structure from motion, Radar measurements, Azimuth, System performance, Radar, Radar tracking, Automotive radar, Autonomous driving, 3D radar perception</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342167&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342167&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>VERN: Vegetation-Aware Robot Navigation in Dense Unstructured Outdoor Environments</td>
                <td>A. J. Sathyamoorthy et al.</td>
                <td>2023</td>
                <td>We propose a novel method for autonomous legged robot navigation in densely vegetated environments with a variety of pliable/traversable and non-pliable/untraversable vegetation. We present a novel few-shot learning classifier that can be trained on a few hundred RGB images to differentiate flora that can be navigated through, from the ones that must be circumvented. Using the vegetation classification and 2D lidar scans, our method constructs a vegetation-aware traversability cost map that accurately represents the pliable and non-pliable obstacles with lower, and higher traversability costs, respectively. Our cost map construction accounts for misclassifications of the vegetation and further lowers the risk of collisions, freezing and entrapment in vegetation during navigation. Furthermore, we propose holonomic recovery behaviors for the robot for scenarios where it freezes, or gets physically entrapped in dense, pliable vegetation. We demonstrate our method on a Boston Dynamics Spot robot in real-world unstructured environments with sparse and dense tall grass, bushes, trees, etc. We observe an increase of 25-90% in success rates, 10-90% decrease in freezing rate, and up to 65% decrease in the false positive rate compared to existing methods.</td>
                <td>Legged locomotion, Costs, Laser radar, Navigation, Vegetation mapping, Vegetation, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342393&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342393&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sequential Neural Barriers for Scalable Dynamic Obstacle Avoidance</td>
                <td>H. Yu, C. Hirayama, C. Yu, S. Herbert and S. Gao</td>
                <td>2023</td>
                <td>There are two major challenges for scaling up robot navigation around dynamic obstacles: the complex interaction dynamics of the obstacles can be hard to model analytically, and the complexity of planning and control grows exponentially in the number of obstacles. Data-driven and learning-based methods are thus particularly valuable in this context. However, data-driven methods are sensitive to distribution drift, making it hard to train and generalize learned models across different obstacle densities. We propose a novel method for compositional learning of Sequential Neural Control Barrier models (SN-CBFs) to achieve scalability. Our approach exploits an important observation: the spatial interaction patterns of multiple dynamic obstacles can be decomposed and predicted through temporal sequences of states for each obstacle. Through decomposition, we can generalize control policies trained only with a small number of obstacles, to environments where the obstacle density can be 100x higher. We demonstrate the benefits of the proposed methods in improving dynamic collision avoidance in comparison with existing methods including potential fields, end-to-end reinforcement learning, and model-predictive control. We also perform hardware experiments and show the practical effectiveness of the approach in the supplementary video.</td>
                <td>Training, Scalability, Robot control, Reinforcement learning, Predictive models, Robot sensing systems, Probabilistic logic</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341605&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341605&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Collision Prevention Strategy Using Sparse 2D Spatial Information for Indoor Mobile Robots</td>
                <td>Y. Koh, E. Chang and M. Choi</td>
                <td>2023</td>
                <td>This paper proposes a collision prevention strategy for indoor mobile robots that utilizes a low-cost two-dimensional (2D) range scanner. The use of indoor mobile robots for versatile tasks, such as serving, cleaning, and delivery, has been increasing. As these robots navigate in environments with dynamic obstacles of arbitrary sizes and shapes that move at irregular speeds and directions, sophisticated motion planning is essential to ensure safe and successful task completion. Previous works have relied on object-wise detection and tracking methods that demand significant computing resources and rich information. However, these approaches are challenging to apply for commercial robots that require light computation and guaranteed safety. In this paper, we propose a novel and straightforward strategy that predicts the behavior of dynamic obstacles using agglomerated dynamic points-based trajectory envelope estimation (ADTE), instead of the conventional object-wise approach. We also introduce the stop trigger as a practical and safe strategy that uses the predicted information. The stop trigger operates independently of the primary motion planning module to isolate prediction uncertainty. The proposed strategy was verified through robot testing and simulation with standardized test scenarios defined in ISO 18646–2. The robot testing was officially conducted by an authorized organization. The results demonstrate the effectiveness of the proposed strategy in preventing collisions with moving obstacles.</td>
                <td>Uncertainty, Tracking, Dynamics, Trajectory, Planning, Mobile robots, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341443&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341443&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Arena-Rosnav 2.0: A Development and Benchmarking Platform for Robot Navigation in Highly Dynamic Environments</td>
                <td>L. Kästner et al.</td>
                <td>2023</td>
                <td>Following up on our previous works, in this paper, we present Arena-Rosnav 2.0 an extension to our previous works Arena-Bench [1] and Arena-Rosnav [2], which adds a variety of additional modules for developing and benchmarking robotic navigation approaches. The platform is fundamentally restructured and provides unified APIs to add additional functionalities such as planning algorithms, simulators, or evaluation functionalities. We have included more realistic simulation and pedestrian behavior and provide a profound documentation to lower the entry barrier. We evaluated our system by first, conducting a user study in which we asked experienced researchers as well as new practitioners and students to test our system. The feedback was mostly positive and a high number of participants are utilizing our system for other research endeavors. Finally, we demonstrate the feasibility of our system by integrating two new simulators and a variety of state of the art navigation approaches and benchmark them against one another. The platform is openly available at https://github.com/Arena-Rosnav.</td>
                <td>Training, Navigation, Documentation, Tutorials, Benchmark testing, User interfaces, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342152&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342152&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Data-Efficient Policy Selection for Navigation in Partial Maps via Subgoal-Based Abstraction</td>
                <td>A. Paudel and G. J. Stein</td>
                <td>2023</td>
                <td>We present a novel approach for fast and reliable policy selection for navigation in partial maps. Leveraging the recent learning-augmented model-based Learning over Subgoals Planning (LSP) abstraction to plan, our robot reuses data collected during navigation to evaluate how well other alternative policies could have performed via a procedure we call offline all-policy replay. Costs from offline alt-policy replay constrain policy selection among the LSP-based policies during deployment, allowing for improvements in convergence speed, cumulative regret and average navigation cost. With only lim-ited prior knowledge about the nature of unseen environments, we achieve at least 67% and as much as 96% improvements on cumulative regret over the baseline bandit approach in our experiments in simulated maze and office-like environments.</td>
                <td>Costs, Navigation, Data models, Planning, Reliability, Intelligent robots, Convergence</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342047&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342047&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Accelerating Reinforcement Learning for Autonomous Driving Using Task-Agnostic and Ego-Centric Motion Skills</td>
                <td>T. Zhou, L. Wang, R. Chen, W. Wang and Y. Liu</td>
                <td>2023</td>
                <td>Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/suboptimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with task-specific skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-specific and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efficiently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts significantly in learning efficiency and task performance.</td>
                <td>Visualization, Roads, Reinforcement learning, Color, Libraries, Space exploration, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341449&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341449&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Probabilistic Traversability Model for Risk-Aware Motion Planning in Off-Road Environments</td>
                <td>X. Cai, M. Everett, L. Sharma, P. R. Osteen and J. P. How</td>
                <td>2023</td>
                <td>A key challenge in off-road navigation is that even visually similar terrains or ones from the same semantic class may have substantially different traction properties. Existing work typically assumes no wheel slip or uses the expected traction for motion planning, where the predicted trajectories provide a poor indication of the actual performance if the terrain traction has high uncertainty. In contrast, this work proposes to analyze terrain traversability with the empirical distribution of traction parameters in unicycle dynamics, which can be learned by a neural network in a self-supervised fashion. The probabilistic traction model leads to two risk-aware cost formulations that account for the worst-case expected cost and traction. To help the learned model generalize to unseen environment, terrains with features that lead to unreliable predictions are detected via a density estimator fit to the trained network's latent space and avoided via auxiliary penalties during planning. Simulation results demonstrate that the proposed approach outperforms existing work that assumes no slip or uses the expected traction in both navigation success rate and completion time. Furthermore, avoiding terrains with low density-based confidence score achieves up to 30% improvement in success rate when the learned traction model is used in a novel environment.</td>
                <td>Costs, Uncertainty, Navigation, Simulation, Semantics, Wheels, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341350&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341350&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>NeU-NBV: Next Best View Planning Using Uncertainty Estimation in Image-Based Neural Rendering</td>
                <td>L. Jin, X. Chen, J. Rückin and M. Popović</td>
                <td>2023</td>
                <td>Autonomous robotic tasks require actively perceiving the environment to achieve application-specific goals. In this paper, we address the problem of positioning an RGB camera to collect the most informative images to represent an unknown scene, given a limited measurement budget. We propose a novel mapless planning framework to iteratively plan the next best camera view based on collected image measurements. A key aspect of our approach is a new technique for uncertainty estimation in image-based neural rendering, which guides measurement acquisition at the most uncertain view among view candidates, thus maximising the information value during data collection. By incrementally adding new measurements into our image collection, our approach efficiently explores an unknown scene in a mapless manner. We show that our uncertainty estimation is generalisable and valuable for view planning in unknown scenes. Our planning experiments using synthetic and real-world data verify that our uncertainty-guided approach finds informative images leading to more accurate scene representations when compared against baselines.</td>
                <td>Uncertainty, Robot vision systems, Measurement uncertainty, Estimation, Position measurement, Rendering (computer graphics), Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342226&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342226&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ITIRRT: A Decoupled Framework for the Integration of Machine Learning Into Path Planning</td>
                <td>T. Barbie and S. Mukai</td>
                <td>2023</td>
                <td>Machine learning models have been successfully applied in many separate domains in the recent years. Yet, there is still a large disconnect with the path planning community mainly due to implementation difficulty. In this paper, we identify the issues and present a path planning framework that aims to ease the integration of machine learning models. The framework is divided into two phases: prediction and planning. During the prediction phase machine learning models are used to predict a path solution which, while not perfect, is enough to jump-start our proposed planner. The planning phase begins with the initialization of multiple independent tree data structures and, during execution, our planner merges the different trees until a path is found. Due to the architecture of our framework, the prediction and the planning phases are totally decoupled, resulting in a planner that treats the information given by the prediction model as a guess and not as a path to be corrected. This also allows our planner to maintain its probabilistic completeness. Our planner was evaluated on simulated environments and on a small robotic arm. We show that our planner can successfully leverage predicted paths to significantly improve the planning performances. We also demonstrate the flexibility of our framework by integrating multiple machine learning models of varying complexity.</td>
                <td>Tree data structures, Computational modeling, Machine learning, Computer architecture, Predictive models, Probabilistic logic, Manipulators</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341747&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341747&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Navigation Among Movable Obstacles Using Machine Learning Based Total Time Cost Optimization</td>
                <td>K. Zhang, E. Lucet, J. A. dit Sandretto and D. Filliat</td>
                <td>2023</td>
                <td>Most navigation approaches treat obstacles as static objects and choose to bypass them. However, the detour could be costly or could lead to failures in indoor environments. The recently developed navigation among movable obstacles (NAMO) methods prefer to remove all the movable obstacles blocking the way, which might be not the best choice when planning and moving obstacles takes a long time. We propose a pipeline where the robot solves the NAMO problems by optimizing the total time to reach the goal. This is achieved by a supervised learning approach that can predict the time of planning and performing obstacle motion before actually doing it if this leads to faster goal reaching. Besides, a pose generator based on reinforcement learning is proposed to decide where the robot can move the obstacle. The method is evaluated in two kinds of simulation environments and the results demonstrate its advantages compared to the classical bypass and obstacle removal strategies.</td>
                <td>Costs, Navigation, Supervised learning, Pipelines, Reinforcement learning, Generators, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341355&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341355&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Spatial Reasoning via Deep Vision Models for Robotic Sequential Manipulation</td>
                <td>H. Zhou, I. Schubert, M. Toussaint and O. S. Oguz</td>
                <td>2023</td>
                <td>In this paper, we propose using deep neural architectures (i.e., vision transformers and ResNet) as heuristics for sequential decision-making in robotic manipulation problems. This formulation enables predicting the subset of objects that are relevant for completing a task. Such problems are often addressed by task and motion planning (TAMP) formulations combining symbolic reasoning and continuous motion planning. In essence, the action-object relationships are resolved for discrete, symbolic decisions that are used to solve manipulation motions (e.g., via nonlinear trajectory optimization). However, solving long-horizon tasks requires consideration of all possible action-object combinations which limits the scalability of TAMP approaches. To overcome this combinatorial complexity, we introduce a visual perception module integrated with a TAMP-solver. Given a task and an initial image of the scene, the learned model outputs the relevancy of objects to accomplish the task. By incorporating the predictions of the model into a TAMP formulation as a heuristic, the size of the search space is significantly reduced. Results show that our framework finds feasible solutions more efficiently when compared to a state-of-the-art TAMP solver.</td>
                <td>Visualization, Image color analysis, Scalability, Vision sensors, Transformers, Cognition, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342010&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342010&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SpeedFormer: Learning Speed Profiles with Upper and Lower Boundary Constraints Based on Transformer</td>
                <td>G. Jeong</td>
                <td>2023</td>
                <td>This paper presents a new method for generating speed profiles for autonomous vehicles using a Transformer-based network that predicts the coefficients of quintic polynomials. To train and validate the network, we curate a dataset of 500K simulated urban driving scenarios, where the ground truths are obtained by running offline model predictive control (MPC) optimization. We also present tailored loss functions to emulate MPC behavior and constrain upper-and-lower boundary conditions to provide feasible speed profiles. Extensive experimental results demonstrate the efficacy of the proposed method in providing high-quality speed profiles for a large number of path candidates and long planning horizons. The proposed method is capable of generating efficient speed profiles for 1024 path candidates within 30ms.</td>
                <td>Scalability, Network architecture, Transformers, Boundary conditions, Linear programming, Real-time systems, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341689&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341689&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Stackelberg Meta-Learning for Strategic Guidance in Multi-Robot Trajectory Planning</td>
                <td>Y. Zhao and Q. Zhu</td>
                <td>2023</td>
                <td>Trajectory guidance requires a leader robotic agent to assist a follower robotic agent to cooperatively reach the target destination. However, planning cooperation becomes difficult when the leader serves a family of different followers and has incomplete information about the followers. There is a need for learning and fast adaptation of different cooperation plans. We develop a Stackelberg meta-learning approach to address this challenge. We first formulate the guided trajectory planning problem as a dynamic Stackelberg game to capture the leader-follower interactions. Then, we leverage meta-learning to develop cooperative strategies for different followers. The leader learns a meta-best-response model from a prescribed set of followers. When a specific follower initiates a guidance query, the leader quickly adapts to the follower-specific model with a small amount of learning data and uses it to perform trajectory guidance. We use simulations to elaborate that our method provides a better generalization and adaptation per-formance on learning followers' behavior than other learning approaches. The value and the effectiveness of guidance are also demonstrated by the comparison with zero guidance scenarios11The simulation codes are available at https://github.com/yuhan16/Stackelberg-Meta-Learning..</td>
                <td>Metalearning, Adaptation models, Trajectory planning, Human-robot interaction, Games, Trajectory, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342202&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342202&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>NaviSTAR: Socially Aware Robot Navigation with Hybrid Spatio-Temporal Graph Transformer and Preference Learning</td>
                <td>C. Min</td>
                <td>2023</td>
                <td>Developing robotic technologies for use in human society requires ensuring the safety of robots' navigation behaviors while adhering to pedestrians' expectations and social norms. However, understanding complex human-robot interactions (HRI) to infer potential cooperation and response among robots and pedestrians for cooperative collision avoid-ance is challenging. To address these challenges, we propose a novel socially-aware navigation benchmark called NaviS Tar, which utilizes a hybrid Spatio- Temporal grAph tRansformer to understand interactions in human-rich environments fusing crowd multi-modal dynamic features. We leverage an off-policy reinforcement learning algorithm with preference learning to train a policy and a reward function network with supervi-sor guidance. Additionally, we design a social score function to evaluate the overall performance of social navigation. To compare, we train and test our algorithm with other state-of-the-art methods in both simulator and real-world scenarios independently. Our results show that NaviSTAR outperforms previous methods with outstanding performance11The source code and experiment videos of this work are available at: https://sites.google.com/view/san-navistar</td>
                <td>Training, Pedestrians, Navigation, Heuristic algorithms, Source coding, Human-robot interaction, Transformers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341395&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341395&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PTDRL: Parameter Tuning Using Deep Reinforcement Learning</td>
                <td>E. Goldsztejn, T. Feiner and R. Brafman</td>
                <td>2023</td>
                <td>A variety of autonomous navigation algorithms exist that allow robots to move around in a safe and fast manner. Many of these algorithms require parameter re-tuning when facing new environments. In this paper, we propose PTDRL, a parameter-tuning strategy that adaptively selects from a fixed set of parameters those that maximize the expected reward for a given navigation system. Our learning strategy can be used for different environments, different platforms, and different user preferences. Specifically, we attend to the problem of social navigation in indoor spaces, using a classical motion planning algorithm as our navigation system and training its parameters to optimize its behavior. Experimental results show that PTDRL can outperform other online parameter-tuning strategies.</td>
                <td>Training, Deep learning, Navigation, Reinforcement learning, Planning, Behavioral sciences, Tuning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342140&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342140&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hierarchical Transformer for Visual Affordance Understanding using a Large-scale Dataset</td>
                <td>S. A. A. Shah and Z. Khalifa</td>
                <td>2023</td>
                <td>Recognition, detection, and segmentation tasks in machine vision have focused on studying the physical and textural attributes of objects. However, robots and intelligent machines require the ability to understand visual cues, such as the visual affordances that objects offer, to interact intelligently with novel objects. In this paper, we present a large-scale multi-view RGBD visual affordance learning dataset a benchmark of 47,210 RGBD images from 37 object categories, annotated with 15 visual affordance categories and 35 cluttered/complex scenes. We deploy a Vision Transformer (ViT), called Visual Affordance Transformer (VAT), for the affordance segmentation task. Due to its hierarchical architecture, VAT can learn multiple affordances at various scales, making it suitable for objects of varying sizes. Our experimental results show the superior performance of VAT compared to state-of-the-art deep learning networks. In addition, the challenging nature of the proposed dataset highlights the potential for new and robust affordance learning algorithms. Our dataset is publicly available at https://sites.google.com/view/afaqshah/dataset.</td>
                <td>Deep learning, Training, Visualization, Image segmentation, Affordances, Object segmentation, Benchmark testing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341976&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341976&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Traffic Incident Database with Multiple Labels Including Various Perspective Environmental Information</td>
                <td>S. Nishiyama, T. Saito, R. Nakamura, G. Ohtani, H. Kataoka and K. Hara</td>
                <td>2023</td>
                <td>Traffic accident recognition is essential in developing automated driving and Advanced Driving Assistant System technologies. A large dataset of annotated traffic accidents is necessary to improve the accuracy of traffic accident recognition using deep learning models. Conventional traffic accident datasets provide annotations on the presence or absence of traffic accidents and other teacher labels, improving traffic accident recognition performance. However, the labels annotated in conventional datasets need to be more comprehensive to de-scribe traffic accidents in detail. Therefore, we propose V-TIDB, a large-scale traffic accident recognition dataset annotated with various environmental information as multi-labels. Our proposed dataset aims to improve the performance of traffic accident recognition by annotating ten types of environmental information as teacher labels in addition to the presence or absence of traffic accidents. V-TIDB is constructed by collecting many videos from the Internet and annotating them with appropriate environmental information. In our experiments, we compare the performance of traffic accident recognition when only labels related to the presence or absence of traffic accidents are trained and when environmental information is added as a multi-label. In the second experiment, we compare the performance of the training with only “contact level,” which represents the severity of the traffic accident, and the performance with environmental information added as a multi-label. The results showed that 6 out of 10 environmental information labels improved the performance of recognizing the presence or absence of traffic accidents. In the experiment on the degree of recognition of traffic accidents, the performance of recognition of car wrecks and contacts was improved for all environmental information. These experiments show that V-TIDB can be used to learn traffic accident recognition models that take environmental information into account in detail and can be used for appropriate traffic accident analysis.</td>
                <td>Training, Deep learning, Analytical models, Databases, Annotations, Internet, Automobiles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342176&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342176&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark Under Multiple Environments</td>
                <td>H. Hu et al.</td>
                <td>2023</td>
                <td>Different environments pose a great challenge to the outdoor robust visual perception for long-term autonomous driving, and the generalization of learning-based algorithms on different environments is still an open problem. Although monocular depth prediction has been well studied recently, few works focus on the robustness of learning-based depth prediction across different environments, e.g. changing illumination and seasons, owing to the lack of such a multi-environment real-world dataset and benchmark. To this end, the cross-season monocular depth prediction dataset and benchmark, SeasonDepth, is introduced to benchmark the depth estimation performance under different environments. We investigate several state-of-the-art representative open-source supervised and self-supervised depth prediction methods using newly-formulated metrics. Through extensive experimental evaluation on the proposed dataset and cross-dataset evaluation with current autonomous driving datasets, the performance and robustness against the influence of multiple environments are analyzed qualitatively and quantitatively. We show that long-term monocular depth prediction is still challenging and believe our work can boost further research on the long-term robustness and generalization for outdoor visual perception. The dataset is available on https://seasondepth.github.io.</td>
                <td>Measurement, Lighting, Estimation, Prediction methods, Benchmark testing, Prediction algorithms, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341917&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341917&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods</td>
                <td>Y. Jing et al.</td>
                <td>2023</td>
                <td>Visual pre-training with large-scale real-world data has made great progress in recent years, showing great potential in robot learning with pixel observations. However, the recipes of visual pre-training for robot manipulation tasks are yet to be built. In this paper, we thoroughly investigate the effects of visual pre-training strategies on robot manipulation tasks from three fundamental perspectives: pre-training datasets, model architectures and training methods. Several significant experimental findings are provided that are beneficial for robot learning. Further, we propose a visual pre-training scheme for robot manipulation termed Vi-PRoM, which combines self-supervised learning and supervised learning. Concretely, the former employs contrastive learning to acquire underlying patterns from large-scale unlabeled data, while the latter aims learning visual semantics and temporal dynamics. Extensive experiments on robot manipulations in various simulation environments and the real robot demonstrate the superiority of the proposed scheme. Videos and more details can be found on https://explore-pretrain-robot.github.io.</td>
                <td>Training, Visualization, Supervised learning, Semantics, Self-supervised learning, Robot learning, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342201&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342201&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DORMADL - Dataset of Human-Operated Robot Arm Motion in Activities of Daily Living</td>
                <td>F. Goldau, Y. Shivashankar, A. Baumeister, L. Drescher, P. Tolle and U. Frese</td>
                <td>2023</td>
                <td>This work presents a dataset of human-operated robot motion to be used within the context of assistive robotics and assorted fields, such as learning from demonstrations, machine-learning based robot control, and activity recognition. The data consists of individual sequences of intentional robot motion performing a task in an environment of daily living. There are 2973 sequences generated in a high-resolution simulation and 986 sequences performed in reality, totaling to 1.16 M datapoints. The data includes labels for the robot's pose, motion and activity. This paper also provides data augmentation methods and a detailed dataset analysis as well as simple models trained on the dataset as a baseline for future research. The dataset can be downloaded free-of-charge at https://www.kaggle.com/f371xx/dormadl.</td>
                <td>Robot motion, Measurement, Machine learning, Activity recognition, User interfaces, Predictive models, Manipulators</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341459&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341459&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>IPA-3D1K: A Large Retail 3D Model Dataset for Robot Picking</td>
                <td>J. Lindermayr et al.</td>
                <td>2023</td>
                <td>Robotic applications like automated order picking in warehouses or retail stores, or fetch and carry tasks in hospitals, care homes, or households rely on the capability of service robots to find and handle a specific type of object. These applications are challenging as the set of objects is very large and varies over time. Despite its significance, there is no suitable universal large-scale dataset available from the retail domain, which allows for a principled analysis of all relevant robotics research aspects in that field. Hence, this paper introduces a novel dataset of more than 1,000 retail objects, including color images, 3D scans, and high-resolution textured 3D models of individual objects, synthetic scenes and real settings, which covers the specifics of the retail domain. The dataset was designed to serve researchers in all relevant robotics tasks in retail like 3D reconstruction and object modeling, large-scale object classification and instance detection including incremental learning and fine-grained detection, text reading, logo detection, semantic grounding and affordance detection, grasp analysis and manipulation planning, as well as digital twinning and virtual environments. Based on synthetic RGB images of scenes created from the 3D models, two exemplary use cases are examined in this paper to demonstrate the benefits of the dataset: we evaluate the state-of-the-art incremental object detection method InstanceNet and a few-shot fine-grained object classification method. The results prove the suitability of InstanceNet for incremental object detection on large datasets and are promising for the few-shot object classification system.</td>
                <td>Solid modeling, Analytical models, Three-dimensional displays, Service robots, Semantics, Virtual environments, Object detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342260&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342260&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>EVOLIN Benchmark: Evaluation of Line Detection and Association</td>
                <td>K. Ivanov, G. Ferrer and A. Kornilova</td>
                <td>2023</td>
                <td>Lines are interesting geometrical features commonly seen in indoor and urban environments. There is missing a complete benchmark where one can evaluate lines from a sequential stream of images in all its stages: Line detection, Line Association and Pose error. To do so, we present a complete and exhaustive benchmark for visual lines in a SLAM front-end, both for RGB and RGBD, by providing a plethora of complementary metrics. We have also labeled data from well-known SLAM datasets in order to have all in one poses and accurately annotated lines. In particular, we have evaluated 17 line detection algorithms, 5 line associations methods and the resultant pose error for aligning a pair of frames with several combinations of detector-association. We have packaged all methods and evaluations metrics and made them publicly available on web-page33https://prime-slam.github.io/evolin/.</td>
                <td>Measurement, Visualization, Simultaneous localization and mapping, Urban areas, Benchmark testing, Streaming media, Solids</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342185&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342185&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Generating Scenarios from High-Level Specifications for Object Rearrangement Tasks</td>
                <td>S. van Waveren, C. Pek, I. Leite, J. Tumova and D. Kragic</td>
                <td>2023</td>
                <td>Rearranging objects is an essential skill for robots. To quickly teach robots new rearrangements tasks, we would like to generate training scenarios from high-level specifications that define the relative placement of objects for the task at hand. Ideally, to guide the robot's learning we also want to be able to rank these scenarios according to their difficulty. Prior work has shown how generating diverse scenario from specifications and providing the robot with easy-to-difficult samples can improve the learning. Yet, existing scenario generation methods typically cannot generate diverse scenarios while controlling their difficulty. We address this challenge by conditioning generative models on spatial logic specifications to generate spatially-structured scenarios that meet the specification and desired difficulty level. Our experiments showed that generative models are more effective and data-efficient than rejection sam-pling and that the spatially-structured scenarios can drastically improve training of downstream tasks by orders of magnitude.</td>
                <td>Training, Computational modeling, Semantics, Reinforcement learning, Robot learning, Scenario generation, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341369&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341369&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations, Affordances, and Reconstructions</td>
                <td>A. Guo et al.</td>
                <td>2023</td>
                <td>We present the HANDAL dataset for category-level object pose estimation and affordance prediction. Unlike previous datasets, ours is focused on robotics-ready manipulable objects that are of the proper size and shape for functional grasping by robot manipulators, such as pliers, utensils, and screwdrivers. Our annotation process is streamlined, requiring only a single off-the-shelf camera and semi-automated processing, allowing us to produce high-quality 3D annotations without crowd-sourcing. The dataset consists of 308k annotated image frames from 2.2k videos of 212 real-world objects in 17 categories. We focus on hardware and kitchen tool objects to facilitate research in practical scenarios in which a robot manipulator needs to interact with the environment beyond simple pushing or indiscriminate grasping. We outline the usefulness of our dataset for 6-DoF category-level pose+scale estimation and related tasks. We also provide 3D reconstructed meshes of all objects, and we outline some of the bottlenecks to be addressed for democratizing the collection of datasets like this one. Project website: https://nvlabs.github.io/HANDAL/</td>
                <td>Three-dimensional displays, Annotations, Shape, Affordances, Grasping, Streaming media, 6-DOF</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341672&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341672&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Understanding the Impact of Image Quality and Distance of Objects to Object Detection Performance</td>
                <td>Y. Hao et al.</td>
                <td>2023</td>
                <td>Object detection is a fundamental task for autonomous driving, which aim to identify and localize objects within an image. Deep learning has made great strides for object detection, with popular models including Faster R-CNN, YOLO, and SSD. The detection accuracy and computational cost of object detection depend on the spatial resolution of an image, which may be constrained by both the camera and storage considerations. Furthermore, original images are often compressed and uploaded to a remote server for object detection. Compression is often achieved by reducing either spatial or amplitude resolution or, at times, both, both of which have well-known effects on performance. Detection accuracy also depends on the distance of the object of interest from the camera. Our work examines the impact of spatial and amplitude resolution, as well as object distance, on object detection accuracy and computational cost. As existing models are optimized for uncompressed (or lightly compressed) images over a narrow range of spatial resolution, we develop a resolution-adaptive variant of YOLOv5 (RA-YOLO), which varies the number of scales in the feature pyramid and detection head based on the spatial resolution of the input image. To train and evaluate this new method, we created a dataset of images with diverse spatial and amplitude resolutions by combining images from the TJU and Eurocity datasets and generating different resolutions by applying spatial resizing and compression. We first show that RA-YOLO achieves a good trade-off between detection accuracy and inference time over a large range of spatial resolutions. We then evaluate the impact of spatial and amplitude resolutions on object detection accuracy using the proposed RA-YOLO model. We demonstrate that the optimal spatial resolution that leads to the highest detection accuracy depends on the ‘tolerated’ image size (constrained by the available bandwidth or storage). We further assess the impact of the distance of an object to the camera on the detection accuracy and show that higher spatial resolution enables a greater detection range. These results provide important guidelines for choosing the image spatial resolution and compression settings predicated on available bandwidth, storage, desired inference time, and/or desired detection range, in practical applications.</td>
                <td>YOLO, Image quality, Adaptation models, Image coding, Computational modeling, Bandwidth, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342139&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342139&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>The Bystander Affect Detection (BAD) Dataset for Failure Detection in HRI</td>
                <td>A. Bremers et al.</td>
                <td>2023</td>
                <td>For a robot to repair its own error, it must first know it has made a mistake. One way that people detect errors is from the implicit reactions from bystanders - their confusion, smirks, or giggles clue us in that something unexpected occurred. To enable robots to detect and act on bystander responses to task failures, we developed a novel method to elicit bystander responses to human and robot errors. Using 46 different stimulus videos featuring a variety of human and machine task failures, we collected a total of 2,452 webcam videos of human reactions from 54 participants. To test the viability of the collected data, we used the bystander reaction dataset as input to a deep-learning model, BADNet, to predict failure occurrence. We tested different data labeling methods and learned how they affect model performance, achieving precisions above 90%. We discuss strategies (manual labelling, failure-vs-control, and failure-time) used to model bystander reactions and predict failure, and how this approach can be used in real-world robotic deployments to detect errors and improve robot performance. As part of this work, we also contribute with the “Bystander Affect Detection” (BAD) dataset of bystander reactions, supporting the development of better prediction models.</td>
                <td>Webcams, Human-robot interaction, Manuals, Predictive models, Maintenance engineering, Data models, Labeling, affective computing, robot error, human response dataset</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342442&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342442&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Few-Shot Segmentation and Semantic Segmentation for Underwater Imagery</td>
                <td>I. Kabir et al.</td>
                <td>2023</td>
                <td>This paper tackles image segmentation problems for underwater environments. First, we introduce a novel under-water animal-centric dataset with dense pixel-level annotations containing diverse fine-grained animal categories to mitigate the lack of diverse categories in the existing benchmarks. Then, we solve two image segmentation tasks using underwater images in this dataset: (i) few-shot segmentation, and (ii) semantic segmentation. For the segmentation task in a few-shot learning framework, we propose a novel attention-guided deep neural network architecture by infusing attention modules in various stages of our proposed network. We systematically explore how the learned attention maps can improve few-shot segmentation performance for underwater imagery. Finally, we assess the semantic segmentation problem on our proposed dataset by benchmarking it with two state-of-the-art semantic segmentation methods. We believe our new problem setup, i.e., few-shot segmentation for underwater environments, will be a valuable addition to the existing underwater semantic segmentation task. We believe our novel dataset will pave the way for developing better algorithms and exploring new research directions for marine robotics and underwater image understanding. We publicly release our dataset and the code to advance image understanding research in underwater environments: https://github.com/Imran220S/uwsnet.</td>
                <td>Codes, Annotations, Animals, Semantic segmentation, Artificial neural networks, Benchmark testing, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342227&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342227&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RACECAR - The Dataset for High-Speed Autonomous Racing</td>
                <td>A. Kulkarni et al.</td>
                <td>2023</td>
                <td>This paper describes RACECAR, the first open dataset for full-scale and high-speed autonomous racing. Multi-modal sensor data was collected from fully autonomous Indy race cars operating at speeds of up to 170 mph (273 kph). Six teams who raced in the Indy Autonomous Challenge (2021–2022) have contributed to this dataset. The dataset spans 11 racing scenarios across two race tracks which include solo laps, multi-agent laps, overtaking situations, high-accelerations, banked tracks, obstacle avoidance, pit entry and exit at different speeds. The dataset contains 27 racing sessions across 11 scenarios with over 6.5 hours of autonomous racing. The data has been released in both ROS 2 and nuScenes format. We have also developed the ROSbag2nuScenes conversion library to achieve this. The RACECAR data is unique because of the high-speed environment of autonomous racing. We present several benchmark problems on localization, object detection and tracking (LiDAR, Radar, and Camera), and mapping to explore issues that arise at the limits of operation of the vehicle. RACECAR data can be accessed at https://github.com/linklab-uva/RACECAR_DATA.</td>
                <td>Location awareness, Laser radar, Multimodal sensors, Radar detection, Object detection, Benchmark testing, Radar tracking</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342053&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342053&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>WIT-UAS: A Wildland-Fire Infrared Thermal Dataset to Detect Crew Assets from Aerial Views</td>
                <td>A. Jong et al.</td>
                <td>2023</td>
                <td>We present the Wildland-fire Infrared Thermal (WIT-UAS) dataset for long-wave infrared sensing of crew and vehicle assets amidst prescribed wildland fire environments. While such a dataset is crucial for safety monitoring in wildland fire applications, to the authors' awareness, no such dataset focusing on assets near fire is publicly available. Presumably, this is due to the barrier to entry of collaborating with fire management personnel. We present two related data subsets: WIT-UAS-ROS consists of full ROS bag files containing sensor and robot data of UAS flight over the fire, and WIT-UAS-Image contains hand-labeled long-wave infrared (LWIR) images extracted from WIT-UAS-ROS. Our dataset is the first to focus on asset detection in a wildland fire environment. We show that thermal detection models trained without fire data frequently detect false positives by classifying fire as people. By adding our dataset to training, we show that the false positive rate is reduced significantly. Yet asset detection in wildland fire environments is still significantly more challenging than detection in urban environments, due to dense obscuring trees, greater heat variation, and overbearing thermal signal of the fire. We publicize this dataset to encourage the community to study more advanced models to tackle this challenging environment. The dataset, code and pretrained models are available at https://github.com/castacks/WIT-UAS-Dataset.</td>
                <td>Training, Analytical models, Wildfires, Urban areas, Training data, Robot sensing systems, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341683&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341683&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Infrastructure to support robots: a practical, scalable model for comparative evaluation of design choices</td>
                <td>G. McFassel and D. A. Shell</td>
                <td>2023</td>
                <td>It is easier to program effective robots when they inhabit highly structured environments. The growing literature on methods to aid robot design has given comparatively little consideration to elements external to the robot itself, yet such elements can encode or enhance information (to improve perception), can alter the effects or costs of actions (to help control), and can provide regularity by imposing constraints. External elements have the potential to be shared, to scale elastically, and to spread both benefits and installation/operating costs. These are traits of infrastructure in support of robots. We introduce a basic but flexible mathematical model –via the MDP framework– for rational evaluation of proposed additions and changes to environments, including where infrastructure may improve precision or performance of either perception or actuation. Through it, one can assess the numbers of agents needed for infrastructure investment to be economical, determine when installation costs would be recouped, and evaluate the effect of behavior changes as responses to environmental modifications. To demonstrate how the model can be instantiated, four simple but practical case studies are presented.</td>
                <td>Costs, Mathematical models, Behavioral sciences, Intelligent robots, Investment</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10421774&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10421774&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Prototypical Contrastive Transfer Learning for Multimodal Language Understanding</td>
                <td>S. Otsuki, S. Ishikawa and K. Sugiura</td>
                <td>2023</td>
                <td>Although domestic service robots are expected to assist individuals who require support, they cannot currently interact smoothly with people through natural language. For example, given the instruction “Bring me a bottle from the kitchen,” it is difficult for such robots to specify the bottle in an indoor environment. Most conventional models have been trained on real-world datasets that are labor-intensive to collect, and they have not fully leveraged simulation data through a transfer learning framework. In this study, we propose a novel transfer learning approach for multimodal language understanding called Prototypical Contrastive Transfer Learning (PCTL), which uses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task of identifying target objects in domestic environments according to free-form natural language instructions. To validate PCTL, we built new real-world and simulation datasets. Our experiment demonstrated that PCTL outperformed existing methods. Specifically, PCTL achieved an accuracy of 78.1 %, whereas simple fine-tuning achieved an accuracy of 73.4 %.</td>
                <td>Visualization, Service robots, Transfer learning, Natural languages, Data models, Indoor environment, Object recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341388&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341388&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Re-Thinking Classification Confidence with Model Quality Quantification</td>
                <td>Y. Pan and H. Zhao</td>
                <td>2023</td>
                <td>Deep neural networks using for real-world classification task require high reliability and robustness. However, the Softmax output by the last layer of network is often over-confident. We propose a novel confidence estimation method by considering model quality for deep classification models. Two metrics, MQ-Repres and MQ-Discri are developed accordingly to evaluate the model quality, and also provide a new confidence estimation called MQ-Conf for online inference. We demonstrate the capability of the proposed method by the $3D$ semantic segmentation tasks using three different deep networks. Through confusion analysis and feature visualization we show the rationality and reliability of the model quality quantification method.11This work is supported by the National Natural Science Foundation of China under Grant U22A2061 and High-performance Computing Platform of Peking University.</td>
                <td>Measurement, Visualization, Three-dimensional displays, Computational modeling, Semantic segmentation, High performance computing, Estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341548&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341548&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Self-Supervised Drivable Area Segmentation Using LiDAR's Depth Information for Autonomous Driving</td>
                <td>F. Ma, Y. Liu, S. Wang, J. Wu, W. Qi and M. Liu</td>
                <td>2023</td>
                <td>Drivable area segmentation is an essential component of the visual perception system for autonomous driving vehicles. Recent efforts in deep neural networks have sig-nificantly improved semantic segmentation performance for autonomous driving. However, most DNN-based methods need a large amount of data to train the models, and collecting large-scale datasets with manually labeled ground truth is costly, tedious, time consuming and requires the availability of experts, making DNN-based methods often difficult to implement in real world applications. Hence, in this paper, we introduce a novel module named automatic data labeler (ADL), which leverages a deterministic LiDAR-based method for ground plane segmentation and road boundary detection to create large datasets suitable for training DNNs. Furthermore, since the data generated by our ADL module is not as accurate as the manually annotated data, we introduce uncertainty estimation to compensate for the gap between the human labeler and our ADL. Finally, we train the semantic segmentation neural networks using our automatically generated labels on the KITTI dataset [10] and KITTI-CARLA dataset [7]. The experimental results demonstrate that our proposed ADL method not only achieves impressive performance compared to manual labeling but also exhibits more robust and accurate results than both traditional methods and state-of-the-art self-supervised methods.</td>
                <td>Point cloud compression, Training, Laser radar, Uncertainty, Semantic segmentation, Roads, Training data</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341687&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341687&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Vehicle Motion Forecasting Using Prior Information and Semantic-Assisted Occupancy Grid Maps</td>
                <td>Zapata, L. Rummelhard, A. Spalanzani and C. Laugier</td>
                <td>2023</td>
                <td>Motion prediction is a challenging task for autonomous vehicles due to uncertainty in the sensor data, the non-deterministic nature of future, and complex behavior of agents. In this paper, we tackle this problem by representing the scene as dynamic occupancy grid maps (DOGMs), associating semantic labels to the occupied cells and incorporating map information. We propose a novel framework that combines deep-learning-based spatio-temporal and probabilistic approaches to predict vehicle behaviors. Contrary to the conventional OGM prediction methods, evaluation of our work is conducted against the ground truth annotations. We experiment and validate our results on real-world NuScenes dataset and show that our model shows superior ability to predict both static and dynamic vehicles compared to OGM predictions. Furthermore, we perform an ablation study and assess the role of semantic labels and map in the architecture.</td>
                <td>Uncertainty, Pedestrians, Semantics, Dynamics, Predictive models, Probabilistic logic, Behavioral sciences, Scene Prediction, Motion Forecasting, Deep Learning, Autonomous Vehicles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342507&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342507&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhance Local Feature Consistency with Structure Similarity Loss for 3D Semantic Segmentation</td>
                <td>W. Chen</td>
                <td>2023</td>
                <td>Recently, many research studies have been carried out on using deep learning methods for 3D point cloud understanding. However, there is still no remarkable result on 3D point cloud semantic segmentation compared to those of 2D research. One important reason is that 3D data has higher dimensionality but lacks large datasets, which means that the deep learning model is difficult to optimize and easy to overfit. To overcome this, an essential method is to provide more priors to the learning of deep models. In this paper, we focus on semantic segmentation for point clouds in the real world. To provide priors to the model, we propose a novel loss function called Linearity and Planarity to enhance local feature consistency in the regions with similar local structure. Experiments show that the proposed method improves baseline performance on both indoor and outdoor datasets e.g. S3DIS and Semantic3D.</td>
                <td>Point cloud compression, Semiconductor device modeling, Deep learning, Training, Solid modeling, Three-dimensional displays, Semantic segmentation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342338&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342338&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lightweight Semantic Segmentation Network for Semantic Scene Understanding on Low-Compute Devices</td>
                <td>H. Son and J. Weiland</td>
                <td>2023</td>
                <td>Semantic scene understanding is beneficial for mobile robots. Semantic information obtained through onboard cameras can improve robots' navigation performance. However, obtaining semantic information on small mobile robots with constrained power and computation resources is challenging. We propose a new lightweight convolution neural network comparable to previous semantic segmentation algorithms for mobile applications. Our network achieved 73.06% on the Cityscapes validation set and 71.8% on the Cityscapes test set. Our model runs at 116 fps with $\mathbf{1024\mathrm{x}2048}$, 172 fps with $1024\mathrm{x}1024$, and 175 fps with $720\mathrm{x}960$ on NVIDIA GTX 1080. We analyze a model size, which is defined as the summation of the number of floating operations and the number of parameters. The smaller model size enables tiny mobile robot systems that should operate multiple tasks simultaneously to work efficiently. Our model has the smallest model size compared to the real-time semantic segmentation convolution neural networks ranked on Cityscapes real-time benchmark and other high performing, lightweight convolution neural networks. On the Camvid test set, our model achieved a mIoU of 73.29% with Cityscapes pre-training, which outperformed the accuracy of other lightweight convolution neural networks. For mobile applicability, we measured frame-per-second on different low-compute devices. Our model operates 35 fps on Jetson Xavier AGX, 21 FPS on Jetson Xavier NX, and 14 FPS on a ROS ASUS gaming phone. $1024\mathrm{x}2048$ resolution is used for the Jetson devices, and $512\mathrm{x}512$ size is utilized for the measurement on the phone. Our network did not use extra datasets such as ImageNet, Coarse Cityscapes, and Mapillary. Additionally, we did not use TensorRT to achieve fast inference speed. Compared to other real-time and lightweight CNNs, our model achieved significantly more efficiency while balancing accuracy, inference speed, and model size.</td>
                <td>Performance evaluation, Convolution, Semantic segmentation, Semantics, Neural networks, Robot vision systems, Size measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342110&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342110&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LiDAR-SGMOS: Semantics-Guided Moving Object Segmentation with 3D LiDAR</td>
                <td>S. Gu, S. Yao, J. Yang, C. Xu and H. Kong</td>
                <td>2023</td>
                <td>Most of the existing moving object segmentation (MOS) methods regard MOS as an independent task, in this paper, we associate the MOS task with semantic segmentation, and propose a semantics-guided network for moving object segmentation (LiDAR-SGMOS). We first transform the range image and semantic features of the past scan into the range view of current scan based on the relative pose between scans. The residual image is obtained by calculating the normalized absolute difference between the current and transformed range images. Then, we apply a Meta-Kernel based cross scan fusion (CSF) module to adaptively fuse the range images and semantic features of current scan, the residual image and transformed features. Finally, the fused features with rich motion and semantic information are processed to obtain reliable MOS results. We also introduce a residual image augmentation method to further improve the MOS performance. Our method outperforms most LiDAR-MOS methods with only two sequential LiDAR scans as inputs on the SemanticKITTI MOS dataset.</td>
                <td>Training, Laser radar, Three-dimensional displays, Semantic segmentation, Semantics, Object segmentation, Transforms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341426&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341426&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Fusion for Bayesian Semantic Mapping</td>
                <td>Cantin and E. Montijano</td>
                <td>2023</td>
                <td>The integration of semantic information in a map allows robots to understand better their environment and make high-level decisions. In the last few years, neural networks have shown enormous progress in their perception capabilities. However, when fusing multiple observations from a neural network in a semantic map, its inherent overconfidence with unknown data gives too much weight to the outliers and decreases the robustness. To mitigate this issue we propose a novel robust fusion method to combine multiple Bayesian semantic predictions. Our method uses the uncertainty estimation provided by a Bayesian neural network to calibrate the way in which the measurements are fused. This is done by regularizing the observations to mitigate the problem of overconfident outlier predictions and using the epistemic uncertainty to weigh their influence in the fusion, resulting in a different formulation of the probability distributions. We validate our robust fusion strategy by performing experiments on photo-realistic simulated environments and real scenes. In both cases, we use a network trained on different data to expose the model to varying data distributions. The results show that considering the model's uncertainty and regularizing the probability distribution of the observations distribution results in a better semantic segmentation performance and more robustness to outliers, compared with other methods. Video - https://youtu.be/5xVGm7z9c-0</td>
                <td>Uncertainty, Semantic segmentation, Semantics, Neural networks, Measurement uncertainty, Robot sensing systems, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342253&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342253&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ConSOR: A Context-Aware Semantic Object Rearrangement Framework for Partially Arranged Scenes</td>
                <td>K. Ramachandruni, M. Zuo and S. Chernova</td>
                <td>2023</td>
                <td>Object rearrangement is the problem of enabling a robot to identify the correct object placement in a complex environment. Prior work on object rearrangement has explored a diverse set of techniques for following user instructions to achieve some desired goal state. Logical predicates, images of the goal scene, and natural language descriptions have all been used to instruct a robot in how to arrange objects. In this work, we argue that burdening the user with specifying goal scenes is not necessary in partially-arranged environments, such as common household settings. Instead, we show that contextual cues from partially arranged scenes (i.e., the placement of some number of pre-arranged objects in the environment) provide sufficient context to enable robots to perform object rearrangement without any explicit user goal specification. We introduce ConSOR, a Context-aware Semantic Object Rearrangement framework that utilizes contextual cues from a partially arranged initial state of the environment to complete the arrangement of new objects, without explicit goal specification from the user. We demonstrate that ConSOR strongly outperforms two baselines in generalizing to novel object arrangements and unseen object categories. The code and data are available at https://github.com/kartikvrama/consor.</td>
                <td>Codes, Semantics, Natural languages, Object recognition, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341873&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341873&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>IDA: Informed Domain Adaptive Semantic Segmentation</td>
                <td>Z. Chen, Z. Ding, J. M. Gregory and L. Liu</td>
                <td>2023</td>
                <td>Mixup-based data augmentation has been validated to be a critical stage in the self-training framework for unsupervised domain adaptive semantic segmentation (UDASS), which aims to transfer knowledge from a well-annotated (source) domain to an unlabeled (target) domain. Existing self-training methods usually adopt the popular region-based mixup techniques with a random sampling strategy, which unfortunately ignores the dynamic evolution of different semantics across various domains as training proceeds. To improve the UDA-SS performance, we propose an Informed Domain Adaptation (IDA) model, a self-training framework that mixes the data based on class-level segmentation performance, which aims to emphasize small-region semantics during mixup. In our IDA model, the class-level performance is tracked by an expected confidence score (ECS). We then use a dynamic schedule to determine the mixing ratio for data in different domains. Extensive experimental results reveal that our proposed method is able to outperform the state-of-the-art UDA-SS method by a margin of 1.1 mIoU in the adaptation of GTA-V to Cityscapes and of 0.9 mIoU in the adaptation of SYNTHIA to Cityscapes. Code link: https://github.com/ArlenCHEN/IDA.git</td>
                <td>Training, Adaptation models, Schedules, Codes, Semantic segmentation, Semantics, Dynamic scheduling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342254&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342254&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Handle Robot for Providing Bodily Support to Elderly Persons</td>
                <td>R. Bolli, P. Bonato and H. Harry Asada</td>
                <td>2023</td>
                <td>Age-related loss of mobility and an increased risk of falling remain major obstacles for older adults to live independently. Many elderly people lack the coordination and strength necessary to perform activities of daily living, such as getting out of bed or stepping into a bathtub. A traditional solution is to install grab bars around the home. For assisting in bathtub transitions, grab bars are fixed to a bathroom wall. However, they are often too far to reach and stably support the user; the installation locations of grab bars are constrained by the room layout and are often suboptimal. In this paper, we present a mobile robot that provides an older adult with a handlebar located anywhere in space - “Handle Anywhere”. The robot consists of an omnidirectional mobile base attached to a reposition able handlebar. We further develop a methodology to optimally place the handle to provide the maximum support for the elderly user while performing common postural changes. A cost function with a trade-off between mechanical advantage and manipulability of the user's arm was optimized in terms of the location of the handlebar relative to the user. The methodology requires only a sagittal plane video of the elderly user performing the postural change, and thus is rapid, scalable, and uniquely customizable to each user. A proof-of-concept prototype was built, and the optimization algorithm for handle location was validated experimentally.</td>
                <td>Protocols, Robot kinematics, Sociology, Prototypes, Muscles, Mobile robots, Older adults</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341348&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341348&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Hybrid FNS Generator for Human Trunk Posture Control with Incomplete Knowledge of Neuromusculoskeletal Dynamics</td>
                <td>X. Bao, A. R. Friederich, R. J. Triolo and M. L. Audu</td>
                <td>2023</td>
                <td>The trunk movements of an individual paralyzed by spinal cord injury (SCI) can be restored by Functional Neuromuscular Stimulation (FNS), a technique that applies low-level current to motor nerves to activate the muscles generating torques, and thus, produce trunk motions. FNS can be modulated to control trunk movements. However, a stabilizing modulation policy (i.e., control law) is difficult to derive due to the complexity of neuromusculoskeletal dynamics, which consist of skeletal dynamics (i.e., multi-joint rigid body dynamics) and neuromuscular dynamics (i.e., a highly nonlinear, non-autonomous, and input redundant dynamics). Therefore, an FNS-based control method that can stabilize the trunk without knowing the accurate skeletal and neuromuscular dynamics is desired. This work proposed an FNS generator, which consists of a robust nonlinear controller (RNC) that provides stabilizing torque command and an artificial neural network (ANN)-based torque-to-activation (T-A) map to ensure that the muscle generates the stabilizing torque to the skeleton. Due to the robustness and learning capability of this control framework, full knowledge of the trunk neuromusculoskeletal dynamics is not required. The proposed control framework has been tested in a simulation environment where an anatomically realistic 3D musculoskeletal model of the human trunk was manipulated to follow a time-varying reference that moves in the anterior-posterior and medial-lateral directions. From the results, it can be seen that the trunk motion converges to a satisfactory trajectory while the ANN is being updated. The results suggest the potential of this control framework for trunk tracking tasks in a clinical application.</td>
                <td>Torque, Three-dimensional displays, Neuromuscular, Tracking, Dynamics, Artificial neural networks, Generators</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342462&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342462&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Insole-Type Walking Assist Device Capable of Inducing Inversion-Eversion of the Ankle Angle to the Neutral Position</td>
                <td>T. Itami, K. Date, Y. Ishii, J. Yoneyama and T. Aoki</td>
                <td>2023</td>
                <td>In recent years, the aging of society has become a serious problem, especially in developed countries. Walking is an important element in extending healthy life expectancy in old age. In particular, induction of proper ankle joint alignment at heel contact is important during the gait cycle from the perspective of smooth weight transfer and reduction of burden on the knees and hip. In this study, we focus on the behavior of the ankle joint at heel contact and propose an insole-type assist device that can induce the ankle angle inversion/eversion rotation. The proposed device has tilting of the heel part from left to right in response to the rotation of a stepping motor, and an inertial sensor mounted inside controls the heel part to always maintain a horizontal position. The effectiveness of the proposed device is verified by evaluating the amount of lateral thrust of the knee joint of six healthy male subjects during a foot-stepping motion using motion capture system. The results showed that the amount of lateral thrust is significantly reduced by wearing the device with control.</td>
                <td>Legged locomotion, Knee, Pistons, Induction motors, Inertial sensors, Scholarships, Motion capture</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342205&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342205&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design for Hip Abduction Assistive Device Based on Relationship Between Hip Joint Motion and Torque During Running</td>
                <td>M. Lee, M. B. Hong, G. T. Kim and S. Kim</td>
                <td>2023</td>
                <td>Numerous attempts have been made to reduce metabolic energy while running with the help of assistive devices. A majority of studies on the assistive devices have focused on the assisting torque in the sagittal plane. In the case of running, however, the abduction torque in the frontal plane at the hip joint is greater than the flexion/extension torque in the sagittal plane. During running, as does an elastic body, the abduction torque and the motion of the hip joint have a linear relationship, but are opposite in direction. It is expected that the hip abduction torque can be assisted with a simple passive method by using an elastic body that reflects the movement characteristics of the hip joint. In this study, therefore, a system to assist hip abduction torque using a leaf spring was proposed with a prototype testing. While running with the assist system proposed, the leaf spring aids the abduction torque on the stance phase, and the torque is not generated due to the passive revolute joint on the swing phase. The joint angle is changed with respective to the rotation in the flexion/extension direction to prevent discomfort torque during swing phase and to increase the duration of the torque action during stance phase. A preliminary test was conducted on one subject using the prototype of the hip joint abduction torque assistive device. The participant with the assistive device reduced metabolic energy by 5% compared to the case without abduction torque assist while running at 2.5m/s. In order to increase the amount of metabolic reduction, the device shall be supplemented by system mass reduction and hip joint position optimization.</td>
                <td>Torque, Government, Prototypes, Assistive devices, Springs, Hip, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341611&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341611&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dynamic Hand Proprioception via a Wearable Glove with Fabric Sensors</td>
                <td>Bottiglio</td>
                <td>2023</td>
                <td>Continuous enhancement in wearable technologies has led to several innovations in the healthcare, virtual reality, and robotics sectors. One form of wearable technology is wear-able sensors for kinematic measurements of human motion. However, measuring the kinematics of human movement is a challenging problem as wearable sensors need to conform to complex curvatures and deform without limiting the user's natural range of motion. In fine motor activities, such challenges are further exacerbated by the dense packing of several joints, coupled joint motions, and relatively small deformations. This work presents the design, fabrication, and characterization of a thin, breathable sensing glove capable of reconstructing fine motor kinematics. The fabric glove features capacitive sensors made from layers of conductive and dielectric fabrics, culminating in a non-bulky and discrete glove design. This study demonstrates that the glove can reconstruct the joint angles of the wearer with a root mean square error of 7.2 degrees, indicating promising applicability to dynamic pose reconstruction for wearable technology and robot teleoperation.</td>
                <td>Thumb, Kinematics, Sensor phenomena and characterization, Robot sensing systems, Fabrics, Capacitive sensors, Root mean square</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342129&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342129&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Wearable Robotic Rehabilitation System for Neuro-Rehabilitation Aimed at Enhancing Mediolateral Balance</td>
                <td>Z. Yu, V. Nalam, A. Alili and H. H. Huang</td>
                <td>2023</td>
                <td>There is increasing evidence of the role of compromised mediolateral balance in falls and the need for rehabilitation specifically focused on mediolateral direction for various populations with motor deficits. To address this need, we have developed a neurorehabilitation platform by integrating a wearable robotic hip abduction-adduction exoskeleton with a visual interface. The platform is expected to influence and rehabilitate the underlying visuomotor mechanisms in individuals by having users perform motion tasks based on visual feedback while the robot applies various controlled resistances governed by the admittance controller implemented in the robot. A preliminary study was performed on 3 non disabled individuals to analyze the performance of the system and observe any adaptation in hip joint kinematics and kinetics as a result of the visuomotor training under 4 different admittance conditions. All three subjects exhibited increased consistency of motion during training and interlimb coordination to achieve motion tasks, demonstrating the utility of the system. Further analysis of observed human-robot torque interactions and electromyography (EMG) signals, and its implication in neurorehabilitation aimed at populations suffering from chronic stroke are discussed.</td>
                <td>Training, Visualization, Robot kinematics, Sociology, Neurorehabilitation, Electromyography, Admittance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341735&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341735&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Analysis of Lower Extremity Shape Characteristics in Various Walking Situations for the Development of Wearable Robot</td>
                <td>J. Park, H. S. Choi and H. In</td>
                <td>2023</td>
                <td>A strap is a frequently utilized component for securing wearable robots to their users in order to facilitate force transmission between humans and the devices. For the appropriate function of the wearable robot, the pressure between the strap and the skin should be maintained appropriately. Due to muscle contraction, the cross-section area of the human limb changes according to the movement of the muscle. The cross-section area change causes the change in the pressure applied by the strap. Therefore, for a new strap design to resolve this, it is necessary to understand the shape change characteristics of the muscle where the strap is applied. In this paper, the change in the circumference of the thigh and the calf during walking was measured and analyzed by multiple string pot sensors. With a treadmill and string pot sensors using potentiometers, torsion springs, and leg circumference changes were measured for different walking speeds and slopes. And, gait cycles were divided according to a signal from the FSR sensor inserted in the right shoe. From the experimental results, there were changes in the circumference of about 8.5mm and 3mm for the thigh and the calf, respectively. And we found tendencies in various walking circumstances such as walking speed and degree of the slope. It is confirmed that they can be used for estimation algorithms of gait cycles or gait circumstances.</td>
                <td>Legged locomotion, Shape, Force, Thigh, Estimation, Wearable robots, Muscles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341496&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341496&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Finding Biomechanically Safe Trajectories for Robot Manipulation of the Human Body in a Search and Rescue Scenario</td>
                <td>Y. Chiu, Y. Zhi, N. Shinde and M. C. Yip</td>
                <td>2023</td>
                <td>There has been increasing awareness of the difficulties in reaching and extracting people from mass casualty scenarios, such as those arising from natural disasters. While platforms have been designed to consider reaching casualties and even carrying them out of harm's way, the challenge of repositioning a casualty from its found configuration to one suitable for extraction has not been explicitly explored. Furthermore, this planning problem needs to incorporate biomechanical safety considerations for the casualty. Thus, we present a first solution to biomechanically safe trajectory generation for repositioning limbs of unconscious human casualties. We describe biomechanical safety as mathematical constraints, mechanical descriptions of the dynamics for the robot-human coupled system, and the planning and trajectory optimization process that considers this coupled and constrained system. We finally evaluate our approach over several variations of the problem and demonstrate it on a real robot and human subject. This work provides a crucial part of search and rescue that can be used in conjunction with past and present works involving robots and vision systems designed for search and rescue.</td>
                <td>Biomechanics, Terrorism, Machine vision, Human factors, Threat assessment, Explosions, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342353&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342353&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dynamic Multi-Query Motion Planning with Differential Constraints and Moving Goals</td>
                <td>M. Gentner, F. Zillenbiller, A. Kraft and E. Steinbach</td>
                <td>2023</td>
                <td>Planning robot motions in complex environments is a fundamental research challenge and central to the autonomy, efficiency, and ultimately adoption of robots. While often the environment is assumed to be static, real-world settings, such as assembly lines, contain complex shaped, moving obstacles and changing target states. Therein robots must perform safe and efficient motions to achieve their tasks. In repetitive environments and multi-goal settings, reusable roadmaps can substantially reduce the overall query time. Most dynamic roadmap-based planners operate in state-time-space, which is computationally demanding. Interval-based methods store availabilities as node attributes and thereby circumvent the dimensionality increase. However, current approaches do not consider higher-order constraints, which can ultimately lead to collisions during execution. Furthermore, current approaches must replan when the goal changes. To this end, we propose a novel roadmap-based planner for systems with third-order differential constraints operating in dynamic environments with moving goals. We construct a roadmap with availabilities as node attributes. During the query phase, we use a Double-Integrator Minimum Time (DIMT) solver to recursively build feasible trajectories and accurately estimate arrival times. An exit node set in combination with a moving goal heuristic is used to efficiently find the fastest path through the roadmap to the moving goal. We evaluate our method with a simulated UAV operating in dynamic 2D environments and show that it also transfers to a 6-DoF manipulator. We show higher success rates than other state-of-the-art methods both in collision avoidance and reaching a moving goal.</td>
                <td>Dynamics, Autonomous aerial vehicles, 6-DOF, Planning, Trajectory, Task analysis, Complex systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341670&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341670&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Reactive and Safe Co-Navigation with Haptic Guidance</td>
                <td>M. Coffey, D. Zhang, R. Tron and A. Pierson</td>
                <td>2023</td>
                <td>We propose a co-navigation algorithm that enables a human and a robot to work together to navigate to a common goal. In this system, the human is responsible for making high-level steering decisions, and the robot, in turn, provides haptic feedback for collision avoidance and path suggestions while reacting to changes in the environment. Our algorithm uses optimized Rapidly-exploring Random Trees (RRT*) to generate paths to lead the user to the goal, via an attractive force feedback computed using a Control Lyapunov Function (CLF). We simultaneously ensure collision avoidance where necessary using a Control Barrier Function (CBF). We demonstrate our approach using simulations with a virtual pilot, and hardware experiments with a human pilot. Our results show that combining RRT* and CBFs is a promising tool for enabling collaborative human-robot navigation.</td>
                <td>Navigation, Heuristic algorithms, Computational modeling, Force feedback, Dynamics, Hardware, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342042&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342042&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An MCTS-DRL Based Obstacle and Occlusion Avoidance Methodology in Robotic Follow-Ahead Applications</td>
                <td>S. Leisiazar, E. J. Park, A. Lim and M. Chen</td>
                <td>2023</td>
                <td>We propose a novel methodology for robotic follow-ahead applications that address the critical challenge of obstacle and occlusion avoidance. Our approach effectively navigates the robot while ensuring avoidance of collisions and occlusions caused by surrounding objects. To achieve this, we developed a high-level decision-making algorithm that generates short-term navigational goals for the mobile robot. Monte Carlo Tree Search is integrated with a Deep Reinforcement Learning method to enhance the performance of the decision-making process and generate more reliable navigational goals. Through extensive experimentation and analysis, we demonstrate the effectiveness and superiority of our proposed approach in comparison to the existing follow-ahead human-following robotic methods. Our code is available at https://github.com/saharLeisiazar/follow-ahead-ros.</td>
                <td>Deep learning, Monte Carlo methods, Navigation, Decision making, Reinforcement learning, Reliability, Mobile robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342150&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342150&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Proactive Model Predictive Control with Multi-Modal Human Motion Prediction in Cluttered Dynamic Environments</td>
                <td>L. Heuer, L. Palmieri, A. Rudenko, A. Mannucci, M. Magnusson and K. O. Arras</td>
                <td>2023</td>
                <td>For robots navigating in dynamic environments, exploiting and understanding uncertain human motion prediction is key to generate efficient, safe and legible actions. The robot may perform poorly and cause hindrances if it does not reason over possible, multi-modal future social interactions. With the goal of enhancing autonomous navigation in cluttered environments, we propose a novel formulation for nonlinear model predictive control including multi-modal predictions of human motion. As a result, our approach leads to less conservative, smooth and intuitive human-aware navigation with reduced risk of collisions, and shows a good balance between task efficiency, collision avoidance and human comfort. To show its effectiveness, we compare our approach against the state of the art in crowded simulated environments, and with real-world human motion data from the THOR dataset. This comparison shows that we are able to improve task efficiency, keep a larger distance to humans and significantly reduce the collision time, when navigating in cluttered dynamic environ-ments. Furthermore, the method is shown to work robustly with different state-of-the-art human motion predictors.</td>
                <td>Navigation, Trajectory planning, Robot kinematics, Dynamics, Stochastic processes, Prediction methods, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341702&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341702&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Novel Obstacle-Avoidance Solution With Non-Iterative Neural Controller for Joint-Constrained Redundant Manipulators</td>
                <td>W. Li, Z. Yi, Y. Zou, H. Wu, Y. Yang and Y. Pan</td>
                <td>2023</td>
                <td>Obstacle avoidance (OA) and joint-limit avoidance (JLA) are essential for redundant manipulators to ensure safe and reliable robotic operations. One solution to OA and JLA is to incorporate the involved constraints into a quadratic programming (QP), by solving which OA and JLA can be achieved. There exist a few non-iterative solvers such as zeroing neural networks (ZNNs), which can solve each sampled QP problem using only one iteration, yet no solution is suitable for OA and JLA due to the absence of some derivative information. To tackle these issues, this paper proposes a novel solution with a non-iterative neural controller termed NCP-ZNN for joint-constrained redundant manipulators. Unlike iterative methods, the neural controller involving derivative information proposed in this paper possesses some positive features including non-iterative computing and convergence with time. In this paper, the reestablished OA-JLA scheme is first introduced. Then, the design details of the neural controller are presented. After that, some comparative simulations based on a PA10 robot and an experiment based on a Franka Emika Panda robot are conducted, demonstrating that the proposed neural controller is more competent in OA and JLA.</td>
                <td>Computational modeling, Neural networks, Manipulators, Mathematical models, Iterative methods, Reliability, Quadratic programming</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342293&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342293&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TTC4MCP: Monocular Collision Prediction Based on Self-Supervised TTC Estimation</td>
                <td>C. Li, Y. Qian, C. Sun, W. Yan, C. Wang and M. Yang</td>
                <td>2023</td>
                <td>Vision-based collision prediction for autonomous driving is a challenging task due to the dynamic movement of vehicles and diverse types of obstacles. Most existing methods rely on object detection algorithms, which only predict predefined collision targets, such as vehicles and pedestrians, and cannot anticipate emergencies caused by unknown obstacles. To address this limitation, we propose a novel approach using pixel-wise time-to-collision (TTC) estimation for monocular collision prediction (TTC4MCP). Our approach predicts TTC and optical flow from monocular images and identifies potential collision areas using feature clustering and motion analysis. To overcome the challenge of training TTC estimation models without ground truth data in new scenes, we propose a self-supervised TTC training method, enabling collision prediction in a wider range of scenarios. TTC4MCP is evaluated on multiple road conditions and demonstrates promising results in terms of accuracy and robustness.</td>
                <td>Training, Roads, Estimation, Self-supervised learning, Object detection, Predictive models, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341966&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341966&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DAMON: Dynamic Amorphous Obstacle Navigation using Topological Manifold Learning and Variational Autoencoding</td>
                <td>A. Dastider and M. Lin</td>
                <td>2023</td>
                <td>DAMON leverages manifold learning and variational autoencoding to achieve obstacle avoidance, allowing for motion planning through adaptive graph traversal in a pre-learned low-dimensional hierarchically-structured manifold graph that captures intricate motion dynamics between a robotic arm and its obstacles. This versatile and reusable approach is applicable to various collaboration scenarios. The primary advantage of DAMON is its ability to embed information in a low-dimensional graph, eliminating the need for repeated computation required by current sampling-based methods. As a result, it offers faster and more efficient motion planning with significantly lower computational overhead and memory footprint. In summary, DAMON is a breakthrough methodology that addresses the challenge of dynamic obstacle avoidance in robotic systems and offers a promising solution for safe and efficient human-robot collaboration. Our approach has been experimentally validated on a 7-DoF robotic manipulator in both simulation and physical settings. DAMON enables the robot to learn and generate skills for avoiding previously-unseen obstacles while achieving predefined objectives. We also optimize DAMON's design parameters and performance using an analytical framework. Our approach outperforms mainstream methodologies, including RRT, RRT*, Dynamic RRT*, L2RRT, and MpNet, with 40% more trajectory smoothness and over 65% improved latency performance, on average.</td>
                <td>Three-dimensional displays, Heuristic algorithms, Dynamics, Collaboration, Manifold learning, Trajectory, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342035&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342035&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>gatekeeper: Online Safety Verification and Control for Nonlinear Systems in Dynamic Environments</td>
                <td>D. Agrawal, R. Chen and D. Panagou</td>
                <td>2023</td>
                <td>This paper presents the gatekeeper algorithm, a real-time and computationally-lightweight method to ensure that nonlinear systems can operate safely in dynamic environments despite limited perception. gatekeeper integrates with existing path planners and feedback controllers by introducing an additional verification step that ensures that proposed trajectories can be executed safely, despite nonlinear dynamics subject to bounded disturbances, input constraints and partial knowledge of the environment. Our key contribution is that (A) we propose an algorithm to recursively construct committed trajectories, and (B) we prove that tracking the committed trajectory ensures the system is safe for all time into the future. The method is demonstrated on a complicated firefighting mission in a dynamic environment, and compares against the state-of-the-art techniques for similar problems.</td>
                <td>Heuristic algorithms, Merging, Logic gates, Control systems, Real-time systems, Trajectory, Nonlinear dynamical systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341790&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341790&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Combinatorial Disjunctive Constraints for Obstacle Avoidance in Path Planning</td>
                <td>R. Garcia, I. V. Hicks and J. Huchette</td>
                <td>2023</td>
                <td>We present a new approach for modeling avoidance constraints in 2D environments, in which waypoints are assigned to obstacle-free polyhedral regions. Constraints of this form are often formulated as mixed-integer programming (MIP) problems employing big-M techniques-however, these are generally not the strongest formulations possible with respect to the MIP's convex relaxation (so called ideal formulations), potentially resulting in larger computational burden. We instead model obstacle avoidance as combinatorial disjunctive constraints and leverage the independent branching scheme to construct small, ideal formulations. As our approach requires a biclique cover for an associated graph, we exploit the structure of this class of graphs to develop a fast subroutine for obtaining biclique covers in polynomial time. We also contribute an open-source Julia library named ClutteredEnvPathOpt to facilitate computational experiments of MIP formulations for obstacle avoidance. Experiments have shown our formulation is more compact and remains competitive on a number of instances compared with standard big-M techniques, for which solvers possess highly optimized procedures.</td>
                <td>Algorithms, Computational modeling, Programming, Path planning, Libraries, Collision avoidance, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342117&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342117&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Reachability-Aware Collision Avoidance for Tractor-Trailer System with Non-Linear MPC and Control Barrier Function</td>
                <td>Y. Tang, I. Mamaev, J. Qin, C. Wurll and B. Hein</td>
                <td>2023</td>
                <td>This paper proposes a reachability-aware model predictive control with a discrete control barrier function for backward obstacle avoidance for a tractor-trailer system. The framework incorporates the state-variant reachable set obtained through sampling-based reachability analysis and symbolic regression into the objective function of model predictive control. By optimizing the intersection of the reachable set and iterative non-safe region generated by the control barrier function, the system demonstrates better performance in terms of safety with a constant decay rate, while enhancing the feasibility of the optimization problem. The proposed algorithm improves real-time performance due to a shorter horizon and outperforms the state-of-the-art algorithms in the simulation environment and on a real robot.</td>
                <td>Prediction algorithms, Linear programming, Real-time systems, Safety, Iterative methods, Collision avoidance, Reachability analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341919&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341919&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Continuous Implicit SDF Based Any-Shape Robot Trajectory Optimization</td>
                <td>T. Zhang, J. Wang, C. Xu, A. Gao and F. Gao</td>
                <td>2023</td>
                <td>Optimization-based trajectory generation methods are widely used in whole-body planning for robots. However, existing work either oversimplifies the robot's geometry and environment representation, resulting in a conservative trajectory or suffers from a huge overhead in maintaining additional information such as the Signed Distance Field (SDF). To bridge the gap, we consider the robot as an implicit function, with its surface boundary represented by the zero-level set of its SDF. We further employ another implicit function to lazily compute the signed distance to the swept volume generated by the robot and its trajectory. The computation is efficient by exploiting continuity in space-time, and the implicit function guarantees continuous collision evaluation even for nonconvex robots with complex surfaces. We also propose a trajectory optimization pipeline applicable to the implicit SDF. Simulation and real-world experiments validate the high performance of our approach for arbitrarily shaped robot trajectory optimization.</td>
                <td>Geometry, Pipelines, Planning, Computational efficiency, Collision avoidance, Robots, Trajectory optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342104&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342104&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robo-Centric ESDF: A Fast and Accurate Whole-Body Collision Evaluation Tool for Any-Shape Robotic Planning</td>
                <td>S. Geng, Q. Wang, L. Xie, C. Xu, Y. Cao and F. Gao</td>
                <td>2023</td>
                <td>For letting mobile robots travel flexibly through complicated environments, increasing attention has been paid to the whole-body collision evaluation. Most existing works either opt for the conservative corridor-based methods that impose strict requirements on the corridor generation, or ESDF-based methods that suffer from high computational overhead. It is still a great challenge to achieve fast and accurate whole-body collision evaluation. In this paper, we propose a Robo-centric ESDF (RC-ESDF) that is pre-built in the robot body frame and is capable of seamlessly applied to any-shape mobile robots, even for those with non-convex shapes. RC-ESDF enjoys lazy collision evaluation, which retains only the minimum information sufficient for whole-body safety constraint and significantly speeds up trajectory optimization. Based on the analytical gradients provided by RC-ESDF, we optimize the position and rotation of robot jointly, with whole-body safety, smoothness, and dynamical feasibility taken into account. Extensive simulation and real-world experiments verified the reliability and generalizability of our method.</td>
                <td>Shape, Trajectory planning, Real-time systems, Safety, Planning, Mobile robots, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342074&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342074&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Global Map Assisted Multi-Agent Collision Avoidance via Deep Reinforcement Learning around Complex Obstacles</td>
                <td>Y. Du, J. Zhang, J. Xu, X. Cheng and S. Cui</td>
                <td>2023</td>
                <td>State-of-the-art multi-agent collision avoidance algorithms face limitations when applied to cluttered public environments, where obstacles may have a variety of shapes and structures. The issue arises because most of these algorithms are agent-level methods. They concentrate solely on preventing collisions between the agents while the obstacles are handled merely out-of-policy. Obstacle-aware policies output an action considering both agents and obstacles. Current obstacle-aware algorithms, mainly based on Lidar sensor data, struggle to handle collision avoidance around complex obstacles. To resolve this issue, this paper investigates how to find a better way to travel around diverse obstacles. In particular, we present a global map assisted collision avoidance algorithm which, following the lead of a high-level goal guide and using an obstacle representation called distance map, considers other agents and obstacles simultaneously. Moreover, our model can be loaded into each agent individually, making it applicable to large maps or more agents. Simulation results indicate that our model outperforms the state-of-the-art algorithms, showing in scenarios with complex obstacles. We present a notion for incorporating global information in decentralized decision-making, along with a method for extending agent-level algorithms to cluttered environments in real-world scenarios.</td>
                <td>Deep learning, Laser radar, Shape, Navigation, Simulation, Decision making, Reinforcement learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341762&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341762&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Aggregating Single-Wheeled Mobile Robots for Omnidirectional Movements</td>
                <td>M. Wang, Y. Su, H. Li, J. Li, J. Liang and H. Liu</td>
                <td>2023</td>
                <td>This paper presents a novel modular robot system that can self-reconfigure to achieve omnidirectional movements for collaborative object transportation. Each robotic module is equipped with a steerable omni-wheel for navigation and is shaped as a regular icositetragon with a permanent magnet installed on each corner for stable docking. After aggregating multiple modules and forming a structure that can cage a target object, we have developed an optimization-based method to compute the distribution of all wheels' heading directions, which enables efficient omnidirectional movements of the structure. By implementing a hierarchical controller on our prototyped system in both simulation and experiment, we validated the trajectory tracking performance of an individual module and a team of six modules in multiple navigation and collaborative object transportation settings. The results demonstrate that the proposed system can maintain a stable caging formation and achieve smooth transportation, indicating the effectiveness of our hardware and locomotion designs.</td>
                <td>Navigation, Trajectory tracking, Scalability, Warehousing, Transportation, Collaboration, Wheels</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341772&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341772&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Smooth Stride Length Change of Rat Robot with a Compliant Actuated Spine Based on CPG Controller</td>
                <td>Y. Huang, Z. Bing, Z. Zhang, K. Huang, F. O. Morin and A. Knoll</td>
                <td>2023</td>
                <td>The aim of this research is to investigate the relationship between spinal flexion and quadruped locomotion in a rat robot equipped with a compliant spine, controlled by a central pattern generator (CPG). The study reveals that spinal flexion can enhance limb stride length, but it may also cause significant and unexpected motion disturbances during stride length variations. To address this issue, this paper proposes a CPG model driven by spinal flexion and a novel oscillator that incorporates a circular limit cycle and accounts for the anticipated stride length transition process. This approach effectively matches the torque change with the dynamics of stride length changes, leading to lower energy consumption. Extensive simulations are conducted to evaluate the efficacy of the proposed oscillator and compare it with the original kinetic model and other CPG models. The results demonstrate that the designed CPG model with the proposed oscillator yields smoother gait transitions during stride length variations and reduces energy consumption.</td>
                <td>Legged locomotion, Adaptation models, Energy consumption, Robot kinematics, Spine, Limit-cycles, Generators</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341791&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341791&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Terrain-Adaptive Locomotion with Agile Behaviors by Imitating Animals</td>
                <td>T. Li et al.</td>
                <td>2023</td>
                <td>In this paper, we present a general learning framework for controlling a quadruped robot that can mimic the behavior of real animals and traverse challenging terrains. Our method consists of two steps: an imitation learning step to learn from motions of real animals, and a terrain adaptation step to enable generalization to unseen terrains. We capture motions from a Labrador on various terrains to facilitate terrain adaptive locomotion. Our experiments demonstrate that our policy can traverse various terrains and produce a natural-looking behavior. We deployed our method on the real quadruped robot $\boldsymbol{Max}$ [1] via zero-shot simulation-to-reality transfer, achieving a speed of 1.1 m/s on stairs climbing.</td>
                <td>Laser radar, Animals, Robot vision systems, Stairs, Cameras, Motion capture, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342271&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342271&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Stable Adaptive Extended Kalman Filter for Estimating Robot Manipulators Link Velocity and Acceleration</td>
                <td>S. A. Baradaran Birjandi, H. Khurana, A. Billard and S. Haddadin</td>
                <td>2023</td>
                <td>One can estimate the velocity and acceleration of robot manipulators by utilizing nonlinear observers. This involves combining inertial measurement units (IMUs) with the motor encoders of the robot through a model-based sensor fusion technique. This approach is lightweight, versatile (suitable for a wide range of trajectories and applications), and straightforward to implement. In order to further improve the estimation accuracy while running the system, we propose to adapt the noise information in this paper. This would automatically reduce the system vulnerability to imperfect modelings and sensor changes. Moreover, viable strategies to maintain the system stability are introduced. Finally, we thoroughly evaluate the overall framework with a seven DoF robot manipulator whose links are equipped with IMUs.</td>
                <td>Adaptation models, Estimation error, Measurement units, Sensor fusion, Observers, Robot sensing systems, Stability analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342476&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342476&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Provably Correct Sensor-Driven Path-Following for Unicycles Using Monotonic Score Functions</td>
                <td>B. Clark, V. Hariprasad and H. A. Poonawala</td>
                <td>2023</td>
                <td>This paper develops a provably stable sensor-driven controller for path-following applications of robots with unicycle kinematics, one specific class of which is the wheeled mobile robot (WMR). The sensor measurement is converted to a scalar value (the score) through some mapping (the score function); the latter may be designed or learned. The score is then mapped to forward and angular velocities using a simple rule with three parameters. The key contribution is that the correctness of this controller only relies on the score function satisfying monotonicity conditions with respect to the underlying state - local path coordinates - instead of achieving specific values at all states. The monotonicity conditions may be checked online by moving the WMR, without state estimation, or offline using a generative model of measurements such as in a simulator. Our approach provides both the practicality of a purely measurement-based control and the correctness of state-based guarantees. We demonstrate the effectiveness of this path-following approach on both a simulated and a physical WMR that use a learned score function derived from a binary classifier trained on real depth images.</td>
                <td>Robot kinematics, Kinematics, Robot sensing systems, Angular velocity, Mobile robots, State estimation, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342233&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342233&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Contact Reduction with Bounded Stiffness for Robust Sim-to-Real Transfer of Robot Assembly</td>
                <td>C. Pham</td>
                <td>2023</td>
                <td>In sim-to-real Reinforcement Learning (RL), a policy is trained in a simulated environment and then deployed on the physical system. The main challenge of sim-to-real RL is to overcome the reality gap - the discrepancies between the real world and its simulated counterpart. Using generic geometric representations, such as convex decomposition, triangular mesh, signed distance field can improve simulation fidelity, and thus potentially narrow the reality gap. Common to these approaches is that many contact points are generated for geometrically-complex objects, which slows down simulation and may cause numerical instability. Contact reduction methods address these issues by limiting the number of contact points, but the validity of these methods for sim-to-real RL has not been confirmed. In this paper, we present a contact reduction method with bounded stiffness to improve the simulation accuracy. Our experiments show that the proposed method critically enables training RL policy for a tight-clearance double pin insertion task and successfully deploying the policy on a rigid, position-controlled physical robot.</td>
                <td>Training, Limiting, Reinforcement learning, Pins, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341866&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341866&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Trajectory Tracking via Multiscale Continuous Attractor Networks</td>
                <td>T. Joseph, T. Fischer and M. Milford</td>
                <td>2023</td>
                <td>Animals and insects showcase remarkably robust and adept navigational abilities, up to literally circumnavigating the globe. Primary progress in robotics inspired by these natural systems has occurred in two areas: highly theoretical computational neuroscience models, and handcrafted systems like RatSLAM and NeuroSLAM. In this research, we present work bridging the gap between the two, in the form of Multiscale Continuous Attractor Networks (MCAN), that combine the multiscale parallel spatial neural networks of the previous theoretical models with the real-world robustness of the robot-targeted systems, to enable trajectory tracking over large velocity ranges. To overcome the limitations of the reliance of previous systems on hand-tuned parameters, we present a genetic algorithm-based approach for automated tuning of these networks, substantially improving their usability. To provide challenging navigational scale ranges, we open source a flexible city-scale navigation simulator that adapts to any street network, enabling high throughput experimentation11https://github.com/theresejoseph/Trajectory_ Tracking_via_MCAN/ • In extensive experiments using the city-scale navigation environment and Kitti, we show that the system is capable of stable dead reckoning over a wide range of velocities and environmental scales, where a single-scale approach fails.</td>
                <td>Dead reckoning, Trajectory tracking, Computational modeling, Insects, Throughput, Robustness, Usability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341938&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341938&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Control of a Ballbot Drivetrain with High Agility, Minimal Footprint, and High Payload</td>
                <td>Wecksler</td>
                <td>2023</td>
                <td>This paper presents the design and control of a ballbot drivetrain that aims to achieve high agility, minimal footprint, and high payload capacity while maintaining dynamic stability. Two hardware platforms and analytical models were developed to test design and control methodologies. The full-scale ballbot prototype (MiaPURE) was constructed using off-the-shelf components and designed to have agility, footprint, and balance similar to that of a walking human. The planar inverted pendulum testbed (PIPTB) was developed as a reduced-order testbed for quick validation of system performance. We then proposed a simple yet robust cascaded LQR-PI controller to balance and maneuver the ballbot drivetrain with a heavy payload. This is crucial because the drivetrain is often subject to high stiction due to elastomeric components in the torque transmission system. This controller was first tested in the PIPTB to compare with traditional LQR and cascaded PI-PD controllers, and then implemented in the ballbot drivetrain. The MiaPURE drivetrain was able to carry a payload of 60 kg, achieve a maximum speed of 2.3 m/s, and come to a stop from a speed of 1.4 m/s in 2 seconds in a selected translation direction. Finally, we demonstrated the omnidirectional movement of the ballbot drivetrain in an indoor environment as a payload-carrying robot and a human-riding mobility device. Our experiments demonstrated the feasibility of using the ballbot drivetrain as a universal mobility platform with agile movements, minimal footprint, and high payload capacity using our proposed design and control methodologies.</td>
                <td>Torso, Torque, Friction, System performance, Prototypes, Control systems, Stability analysis, Body Balancing, Wheeled Robots, Underactuated Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342007&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342007&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Bayesian Reinforcement Learning Method for Periodic Robotic Control Under Significant Uncertainty</td>
                <td>Y. Jia, P. M. Uriguen Eljuri and T. Taniguchi</td>
                <td>2023</td>
                <td>This paper addresses the lack of research on periodic reinforcement learning for physical robot control by presenting a 3-phase periodic Bayesian reinforcement learning method for uncertain environments. Drawing on cognition theory, the proposed approach achieves effective convergence with fewer training episodes. The coach-based demonstration phase narrows the search space and establishes a foundation for a coarse-to-fine control strategy. The reconnaissance phase enhances adaptability by discovering a valuable global repre-sentation, and the operation phase produces accurate robotic control by applying the learned representation and periodically updating local information. Comparative analysis with state-of-the-art methods validates the efficacy of our approach on exemplar control tasks in simulation and a biomedical project involving a simulated cranial window task.</td>
                <td>Training, Uncertainty, Robot control, Reinforcement learning, Reconnaissance, Bayes methods, Cranial</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342166&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342166&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots</td>
                <td>N. Sontakke, H. Chae, S. Lee, T. Huang, D. W. Hong and S. Hal</td>
                <td>2023</td>
                <td>The light and soft characteristics of Buoyancy Assisted Lightweight Legged Unit (BALLU) robots have a great potential to provide intrinsically safe interactions in environments involving humans, unlike many heavy and rigid robots. However, their unique and sensitive dynamics impose challenges to obtaining robust control policies in the real world. In this work, we demonstrate robust sim-to-real transfer of control policies on the BALLU robots via system identification and our novel residual physics learning method, Environment Mimic (EnvMimic). First, we model the nonlinear dynamics of the actuators by collecting hardware data and optimizing the simulation parameters. Rather than relying on standard supervised learning formulations, we utilize deep reinforcement learning to train an external force policy to match real-world trajectories, which enables us to model residual physics with greater fidelity. We analyze the improved simulation fidelity by comparing the simulation trajectories against the real-world ones. We finally demonstrate that the improved simulator allows us to learn better walking and turning policies that can be successfully deployed on the hardware of BALLU.</td>
                <td>Legged locomotion, Supervised learning, Buoyancy, Robot sensing systems, Turning, Hardware, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342062&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342062&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DiffClothAI: Differentiable Cloth Simulation with Intersection-free Frictional Contact and Differentiable Two-Way Coupling with Articulated Rigid Bodies</td>
                <td>X. Yu, S. Zhao, S. Luo, G. Yang and L. Shao</td>
                <td>2023</td>
                <td>Differentiable Simulations have recently proven useful for various robotic manipulation tasks, including cloth manipulation. In robotic cloth simulation, it is crucial to maintain intersection-free properties. We present DiffClothAI, a differentiable cloth simulation with intersection-free friction contact and two-way coupling with articulated rigid bodies. DiffClothAI integrates the Project Dynamics and Incremental Potential Contact coherently and proposes an effective method to derive gradients in the Cloth Simulation. It also establishes the differentiable coupling mechanism between articulated rigid bodies and cloth. We conduct a comprehensive evaluation of DiffClothAI's effectiveness and accuracy and perform a variety of experiments in downstream robotic manipulation tasks. Supplemental materials and videos are available on our project webpage at https://sites.google.com/view/diffsimcloth.</td>
                <td>Couplings, Friction, Computational modeling, Task analysis, Intelligent robots, Videos</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341573&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341573&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Timor Python: A Toolbox for Industrial Modular Robotics</td>
                <td>J. Külz, M. Mayer and M. Althoff</td>
                <td>2023</td>
                <td>Modular Reconfigurable Robots (MRRs) represent an exciting path forward for industrial robotics, opening up new possibilities for robot design. Compared to monolithic manipulators, they promise greater flexibility, improved maintainability, and cost-efficiency. However, there is no tool or standardized way to model and simulate assemblies of modules in the same way it has been done for robotic manipulators for decades. We introduce the Toolbox for Industrial Modular Robotics (Timor), a Python toolbox to bridge this gap and integrate modular robotics into existing simulation and optimization pipelines. Our open-source library offers model generation and task-based configuration optimization for MRRs. It can easily be integrated with existing simulation tools - not least by offering URDF export of arbitrary modular robot assemblies. Moreover, our experimental study demonstrates the effectiveness of Timor as a tool for designing modular robots optimized for specific use cases.</td>
                <td>Service robots, Source coding, Pipelines, Tutorials, Manipulators, Libraries, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341935&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341935&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Ultra-Low Inertia 6-DOF Manipulator Arm for Touching the World</td>
                <td>K. Nishii, A. Hatano and Y. Okumatsu</td>
                <td>2023</td>
                <td>As robotic intelligence increases, so does the im-portance of agents that collect data from real-world environments. When learning in contact with the environment, one must consider how to minimize the impact on the environment and maintain reproducibility. To achieve this, the contact force with the environment must be reduced. One way to achieve this is to reduce the inertia of the arm. In this study, we present an arm we have developed with 6 degrees of freedom and low inertia. The inertia of our arm has been significantly reduced compared to previous research, and experiments have confirmed that it also has low joint friction torque and good contact sensitivity.</td>
                <td>Wrist, Torque, Sensitivity, Friction, Robot sensing systems, Manipulators, 6-DOF</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342445&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342445&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Determination of the Characteristics of Gears of Robot-Like Systems by Analytical Description of their Structure</td>
                <td>Heuser, M. Zimmermann and K. Stahl</td>
                <td>2023</td>
                <td>The axes of robots and robot-like systems (RLS) usually include e-motor-gearbox-arrangements for optimal connection of the elements. The characteristics of the drive system and thus also of the robot depend strongly on the gears. Different gearbox designs are available which differ in stiffness, efficiency and further properties. For an application-optimal design of RLS a uniform documentation and a comparability of gearbox concepts is a decisive factor. The application-optimal design is supported by an interdisciplinary approach between mechanical engineering and software design, guided by adequate product development methodology. The quite heterogeneous characterization of gearboxes for RLS which is currently the state of the art is a relevant obstacle in the flexible and optimal design of RLS. The paper shows the analysis of the gear structure with unified symbols for specific machine elements and contact types. The introduced method gives insight into the mechanical structure of the gearboxes. Similarities between gear types can thus be revealed. This also enables the classification of new developments in the state of the art. Moreover, the developed method for analyzing the gear structure can be used to determine the characteristics of gears. Examples for these characteristics are backlash, efficiency or stiffness. Specifically, the stiffness of gears can be synthesized by the force action of individual contacts and the individual phenomena that occur with them. The representation by individual phenomena also makes it possible to extend the calculation to include influencing parameters such as temperature that have not been sufficiently taken into account so far.</td>
                <td>Temperature, Software design, Gears, Force, Symbols, Documentation, Product development</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342105&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342105&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Tension Jamming for Deployable Structures</td>
                <td>D. Hasegawa, B. Aktaş and R. D. Howe</td>
                <td>2023</td>
                <td>Deployable structures provide adaptability and versatility for applications such as temporary architectures, space structures, and biomedical devices. Jamming is a mechanical phenomenon with which dramatic changes in stiffness can be achieved by increasing the frictional and kinematic coupling between constituents in a structure by applying an external pressure. This study applies jamming, which has been primarily used in medium-scale soft robotics applications to large-scale deployable structures with components that are soft and compact during transport, but rigid upon deployment. It proposes a new jamming structure with a novel built-in actuation mechanism which enables high-performance at large scales: a composite beam made of rectangular segments along a cable which can be pre-tensioned and thus jammed. Two theoretical models are developed to provide insights into the mechanical behavior of the composite beams and predict their performance under loading. A scale model of a deployable bridge is built using the tension-based composite beams, and the bridge is deployed and assembled by air with a drone demonstrating the versatility and viability of the proposed approach for robotics applications.</td>
                <td>Bridges, Atmospheric modeling, Loading, Kinematics, Soft robotics, Predictive models, Behavioral sciences, Mechanism Design, Compliant Joints and Mechanisms, Soft Robot Materials and Design</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342131&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342131&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Task2Morph: Differentiable Task-Inspired Framework for Contact-Aware Robot Design</td>
                <td>Y. Cai et al.</td>
                <td>2023</td>
                <td>Optimizing the morphologies and the controllers that adapt to various tasks is a critical issue in the field of robot design, aka. embodied intelligence. Previous works typically model it as a joint optimization problem and use search-based methods to find the optimal solution in the morphology space. However, they ignore the implicit knowledge of task-to-morphology mapping which can directly inspire robot design. For example, flipping heavier boxes tends to require more muscular robot arms. This paper proposes a novel and general differentiable task-inspired framework for contact-aware robot design called Task2Morph. We abstract task features highly related to task performance and use them to build a task-to-morphology mapping. Further, we embed the mapping into a differentiable robot design process, where the gradient information is leveraged for both the mapping learning and the whole optimization. The experiments are conducted on three scenarios, and the results validate that Task2Morph outperforms DiffHand, which lacks a task-inspired morphology module, in terms of efficiency and effectiveness.</td>
                <td>Adaptation models, Morphology, Optimization methods, Aerospace electronics, Search problems, Manipulators, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341360&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341360&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Constraint Programming for Component-Level Robot Design</td>
                <td>A. Wilhelm and N. Napp</td>
                <td>2023</td>
                <td>Effective design automation for building robots would make development faster and easier while also less prone to design errors. However, complex multi-domain constraints make creating such tools difficult. One persistent challenge in achieving this goal of design automation is the fundamental problem of component selection, an optimization problem where, given a general robot model, components must be selected from a possibly large set of catalogs to minimize design objectives while meeting target specifications. Different approaches to this problem have used Monotone Co-Design Problems (MCDPs) or linear and quadratic programming, but these require judicious system approximations that affect the accuracy of the solution. We take an alternative approach formulating the component selection problem as a combinatorial optimization problem, which does not require any system approximations, and using constraint programming (CP) to solve this problem with a depth-first branch-and-bound algorithm. As the efficacy of CP critically depends upon the orderings of variables and their domain values, we present two heuristics specific to the problem of component selection that significantly improve solve time compared to traditional constraint satisfaction programming heuristics. We also add redundant constraints to the optimization problem to further improve run time by evaluating certain global constraints before all relevant variables are assigned. We demonstrate that our CP approach can find optimal solutions from over 20 trillion candidate solutions in only seconds, up to 48 times faster than an MCDP approach solving the same problem. Finally, for three different robot designs we build the corresponding robots to physically validate that the selected components meet the target design specifications.</td>
                <td>Constraint handling, Design automation, Computational modeling, Morphology, Programming, Approximation algorithms, Quadratic programming</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341679&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341679&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Implementation of a Two-Limbed 3T1R Haptic Device</td>
                <td>J. Yi</td>
                <td>2023</td>
                <td>This paper presents a haptic device with a simple architecture of only two limbs that can provide translational motion in three degrees of freedom (DOF) and one-DOF rotational motion. Actuation redundancy eliminates all forward-kinematic singularities and improves the motion-force transmission property. Thanks to the special structure of the kinematic chains, all actuators are close to the base and full gravity compensation is achieved passively by using springs. Force producibility analysis shows that this haptic device is able to produce long-term continuous force feedback of 15–30 N in each direction. By developing a prototype of the haptic device and a virtual three-dimensional simulator, a preliminary performance evaluation of the haptic device was conducted. In addition, a torque distribution algorithm considering a relaxed form of actuator-torque saturation was experimentally evaluated, and a comparison with other algorithms reveals that this algorithm offers several advantages.</td>
                <td>Performance evaluation, Actuators, Torque, Redundancy, Force feedback, Prototypes, Kinematics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342273&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342273&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Combining Measurement Uncertainties with the Probabilistic Robustness for Safety Evaluation of Robot Systems</td>
                <td>J. Baek, C. Ledermann, T. Asfour and T. Kröger</td>
                <td>2023</td>
                <td>In this paper, we present a method to engage measurement uncertainties with the probabilistic robustness to one system uncertainty measure. Providing a metric indicating the potential occurrence of dangerous situations is highly essential for safety-critical robot applications. Due to the difficulty of finding a quantifiable, unambiguous representation however, such a metric has not been derived to date. In case of sensory devices, measurement uncertainties are usually provided by manufacturer specifications. Apart from that, several contributions demonstrate that the accuracy of neural networks is verifiable via the robustness. However, state-of-the-art literature is mainly concerned with theoretical investigations such that scarce attention has been devoted to the transfer of the robustness to real-world applications. To fill this gap, we show how the probabilistic robustness can be made useful for evaluating quantitative safety limits. Our key idea is to exploit the analogy between measurement uncertainties and the probabilistic robustness: While measurement uncertainties reflect possible shifts due to technical limitations, the robustness refers to the tolerated amount of distortions in the input data for an unaltered output. Inspired by this analogy, we combine both measures to quantify the system uncertainty online. We validate our method in different settings under real-world conditions. Our findings exemplify that incorporating the novel uncertainty metric effectively prevents the rate of dangerous situations in Human-Robot Collaboration.</td>
                <td>Uncertainty, Measurement uncertainty, Neural networks, Probabilistic logic, Robot sensing systems, Distortion, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342112&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342112&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Computational Design of Closed-Chain Linkages: Respawn Algorithm for Generative Design</td>
                <td>D. V. Ivolga, I. I. Borisov, K. V. Nasonov and S. A. Kolyubin</td>
                <td>2023</td>
                <td>Designing robots is a multiphase process aimed at solving a multi-criteria optimization problem to find the best possible detailed design. Generative design (GD) aims to accelerate the design process compared to manual design, since GD allows exploring and exploiting the vast design space more efficiently. In the field of robotics, however, relevant research focuses mostly on the generation of fully-actuated open chain kinematics, which is trivial in mechanical engineering perspective. Within this paper, we address the problem of generative design of closed-chain linkage mechanisms. A GD algorithm has to be able to generate meaningful mechanisms which satisfy conditions of existence. We propose an optimization-driven algorithm for generation of planar closed-chain linkages to follow a predefined trajectory. The algorithm creates an unlimited range of physically reproducible design alternatives that can be further tested in simulation. These tests could be done in order to find solutions that satisfy extra criteria, e.g., desired dynamic behavior or low energy consumption. The proposed algorithm is called “respawn” since it builds a new linkage after the ancestor has been tested in a virtual environment in pursuit for the optimal solution. To show that the algorithm is general enough, we show a set of generated linkages that can be used for a wide class of robots.</td>
                <td>Couplings, Heuristic algorithms, Software algorithms, Virtual environments, Transforms, Software, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341425&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341425&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>On Designing a Learning Robot: Improving Morphology for Enhanced Task Performance and Learning</td>
                <td>M. Sorokin et al.</td>
                <td>2023</td>
                <td>As robots become more prevalent, optimizing their design for better performance and efficiency is becoming increasingly important. However, current robot design practices overlook the impact of perception and design choices on a robot's learning capabilities. To address this gap, we propose a comprehensive methodology that accounts for the interplay between the robot's perception, hardware characteristics, and task requirements. Our approach optimizes the robot's morphology holistically, leading to improved learning and task execution proficiency. To achieve this, we introduce a Morphology-AGnostIc Controller (MAGIC), which helps with the rapid assessment of different robot designs. The MAGIC policy is efficiently trained through a novel PRIvileged Single-stage learning via latent alignMent (PRISM) framework, which also encourages behaviors that are typical of robot onboard observation. Our simulation-based results demonstrate that morphologies optimized holistically improve the robot performance by 15-20% on various manipulation tasks, and require 25x less data to match human-expert made morphology performance. In summary, our work contributes to the growing trend of learning-based approaches in robotics and emphasizes the potential in designing robots that facilitate better learning. The project's website can be found at learning-robot.github.io</td>
                <td>Training, Morphology, Market research, Hardware, Robot learning, Behavioral sciences, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341905&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341905&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Development of A Dynamic Quadruped with Tunable, Compliant Legs</td>
                <td>F. Chen, W. Tao and D. M. Aukes</td>
                <td>2023</td>
                <td>To facilitate the study of how passive leg stiffness influences locomotion dynamics and performance, we have developed an affordable and accessible 400 g quadruped robot driven by tunable compliant laminate legs, whose series and parallel stiffness can be easily adjusted; fabrication only takes 2.5 hours for all four legs. The robot can trot at 0.52 m/s or 4.4 body lengths per second with a 3.2 cost of transport (COT). Through locomotion experiments in both the real world and simulation we demonstrate that legs with different stiffness have an obvious impact on the robot's average speed, COT, and pronking height. When the robot is trotting at 4 Hz in the real world, changing the leg stiffness yields a maximum improvement of 37.1% in speed and 62.0% in COT, showing its great potential for future research on locomotion controller designs and leg stiffness optimizations.</td>
                <td>Legged locomotion, Measurement, Force, Space exploration, Quadrupedal robots, Laminates, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342283&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342283&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Soft Robot Shape Estimation: A Load-Agnostic Geometric Method</td>
                <td>C. Sorensen and M. D. Killpack</td>
                <td>2023</td>
                <td>In this paper we present a novel kinematic representation of a soft continuum robot to enable full shape estimation using a purely geometric solution. The kinematic representation involves using length varying piecewise constant curvature segments to describe the deformed shape of the robot. Based on this kinematic representation, we can use overlapping length sensors to estimate the shape of continuously deformable bodies without prior knowledge of the current loading conditions. We show an implementation that assumes one change in curvature along the length of a joint, using string potentiometers as an arc length sensor, and an orientation measurement from the tip of the continuum joint. For 56 randomized joint configurations, we estimate the shape of a 250 mm long continually deformable robot with less then 2.5 mm of average error. The average error is reported for each of the 10 different equally spaced points along the length, demonstrating the ability to accurately represent the full shape of the soft robot.</td>
                <td>Shape, Loading, Estimation, Kinematics, Soft robotics, Potentiometers, Position measurement, Robot sensing systems, Sensors, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342225&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342225&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Generalized Proportional Integral Control for Trajectory Tracking of Soft Actuators in a Pediatric Wearable Assistive Device</td>
                <td>C. Mucchiani, Z. Liu, I. Sahin, E. Kokkoni and K. Karydis</td>
                <td>2023</td>
                <td>Soft robotics hold promise in the development of safe yet powered assistive wearable devices for infants. Key to this is the development of closed-loop controllers that can help regulate pneumatic pressure in the device's actuators in an effort to induce controlled motion at the user's limbs and be able to track different types of trajectories. This work develops a controller for soft pneumatic actuators aimed to power a pediatric soft wearable robotic device prototype for upper extremity motion assistance. The controller tracks desired trajectories for a system of soft pneumatic actuators supporting two-degree-of-freedom shoulder joint motion on an infant-sized engineered mannequin. The degrees of freedom assisted by the actuators are equivalent to shoulder motion (abduction/adduction and flexion/extension). Embedded inertial measurement unit sensors provide real-time joint feedback. Experimental data from performing reaching tasks using the engineered mannequin are obtained and compared against ground truth to evaluate the performance of the developed controller. Results reveal the proposed controller leads to accurate trajectory tracking performance across a variety of shoulder joint motions.</td>
                <td>Pneumatic actuators, Performance evaluation, Pediatrics, Trajectory tracking, Wearable computers, Shoulder, Soft robotics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342528&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342528&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Data-Efficient Online Learning of Ball Placement in Robot Table Tennis</td>
                <td>P. Tobuschat, H. Ma, D. Büchler, B. Schölkopf and M. Muehlebach</td>
                <td>2023</td>
                <td>We present an implementation of an online op-timization algorithm for hitting a predefined target when returning ping-pong balls with a table tennis robot. The online algorithm optimizes over so-called interception policies, which define the manner in which the robot arm intercepts the ball. In our case, these are composed of the state of the robot arm (position and velocity) at interception time. Gradient information is provided to the optimization algorithm via the mapping from the interception policy to the landing point of the ball on the table, which is approximated with a black-box and a grey-box approach. Our algorithm is applied to a robotic arm with four degrees of freedom that is driven by pneumatic artificial muscles. As a result, the robot arm is able to return the ball onto any predefined target on the table after about 2–5 iterations. We highlight the robustness of our approach by showing rapid convergence with both the black-box and the grey-box gradients. In addition, the small number of iterations required to reach close proximity to the target also underlines the sample efficiency. A demonstration video can be found here: https://youtu.be/VC3KJoCss0k.</td>
                <td>Closed box, Pneumatic systems, Manipulators, Approximation algorithms, Prediction algorithms, Robustness, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342132&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342132&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Reduced-Order Soft Robot Controller</td>
                <td>C. Liang, X. Gao, K. Wu and Z. Pan</td>
                <td>2023</td>
                <td>Deformable robots are notoriously difficult to model or control due to its high-dimensional configuration spaces. Direct trajectory optimization suffers from the curse-of-dimensionality and incurs a high computational cost, while learning-based controller optimization methods are sensitive to hyper-parameter tuning. To overcome these limitations, we hypothesize that high fidelity soft robots can be both simulated and controlled by restricting to low-dimensional spaces. Under such assumption, we propose a two-stage algorithm to identify such simulation- and control-spaces. Our method first identifies the so-called simulation-space that captures the salient deformation modes, to which the robot's governing equation is restricted. We then identify the control-space, to which control signals are restricted. We propose a multi-fidelity Riemannian Bayesian bilevel optimization to identify task-specific control spaces. We show that the dimension of control-space can be less than 10 for a high-DOF soft robot to accomplish walking and swimming tasks, allowing low-dimensional MPC controllers to be applied to soft robots with tractable computational complexity.</td>
                <td>Computational modeling, Soft robotics, Aerospace electronics, Robot sensing systems, Bayes methods, Task analysis, Read only memory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341432&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341432&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Single-Parameter Model for Soft Bellows Actuators under Axial Deformation and Loading</td>
                <td>E. Treadway, M. Brei, A. Sedal and R. B. Gillespie</td>
                <td>2023</td>
                <td>Soft fluidic actuators are becoming popular for their backdrivability, potential for high power density, and their support for power supply through flexible tubes. Control and design of such actuators requires serviceable models that describe how they relate fluid pressure and flow to mechanical force and motion. We present a simple 2-port model of a bellows actuator that accounts for the relationships among fluid and mechanical variables imposed by the kinematics of the deforming bellows structure and accounts for elastic energy stored in the actuator's thermoplastic material structure. Elastic energy storage due to axial deformation is captured by revolving a differential strip whose linear elastic behavior is a nonlinear function of the actuator length. The model is evaluated through experiments in which either actuator length and pressure or force and pressure are imposed. The model has an error of 9.8% of the force range explored and yields insight into the effects of geometry changes. The resulting model can be used for model-based control or actuator design across the full operating range and can be exercised under either imposed force or imposed actuator length.</td>
                <td>Actuators, Bellows, Solid modeling, Fluids, Computational modeling, Force, Loading</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341619&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341619&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Task and Configuration Space Compliance of Continuum Robots via Lie Group and Modal Shape Formulations</td>
                <td>A. L. Orekhov, G. L. H. Johnston and N. Simaan</td>
                <td>2023</td>
                <td>Continuum robots suffer large deflections due to internal and external forces. Accurate modeling of their passive compliance is necessary for accurate environmental interaction, especially in scenarios where direct force sensing is not practical. This paper focuses on deriving analytic formulations for the compliance of continuum robots that can be modeled as Kirchhoff rods. Compared to prior works, the approach presented herein is not subject to the constant-curvature assumptions to derive the configuration space compliance, and we do not rely on computationally-expensive finite difference approximations to obtain the task space compliance. Using modal approximations over curvature space and Lie group integration, we obtain closed-form expressions for the task and configuration space compliance matrices of continuum robots, thereby bridging the gap between constant-curvature analytic formulations of configuration space compliance and variable curvature task space compliance. We first present an analytic expression for the compliance of a single Kirchhoff rod. We then extend this formulation for computing both the task space and configuration space compliance of a tendon-actuated continuum robot. We then use our formulation to study the tradeoffs between computation cost and modeling accuracy as well as the loss in accuracy from neglecting the Jacobian derivative term in the compliance model. Finally, we experimentally validate the model on a tendon-actuated continuum segment, demonstrating the model's ability to predict passive deflections with error below 11.5% percent of total arc length.</td>
                <td>Jacobian matrices, Shape, Computational modeling, Motion segmentation, Soft robotics, Predictive models, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341594&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341594&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Localization Framework for Boundary Constrained Soft Robots</td>
                <td>K. Tanaka, Q. Zhou, A. Srivastava and M. Spenko</td>
                <td>2023</td>
                <td>Soft robots possess unique capabilities for adapting to the environment and interacting with it safely. However, their deformable nature also poses challenges for controlling their movement. In particular, the large deformations of a soft robot make it difficult to localize its individual body parts, which in turn impedes effective control. This paper introduces a novel localization framework designed for soft robots that are constrained by boundaries and benefit from unique hardware architecture. To this end, we propose a method that exploits the flexible boundaries of the robot to create an onboard sensor capable of measuring the relative distances between its sub-robots. This measurement data is incorporated into a linear Kalman filter for accurate localization. We evaluate the framework's performance in benchmark and dynamic cases and demonstrate its effectiveness in improving localization accuracy compared to an IMU-based approach. The results also show that the proposed method achieves sufficient localization accuracy for contact-based mapping, enabling the robot to sense the location of obstacles in the environment. Finally, we validate the proposed framework using a physical prototype of a boundary-constrained soft robot and demonstrate its ability to accurately estimate the robot's shape. This framework has the potential to enable soft robots to autonomously navigate and map unknown environments, which could be beneficial for a variety of exploration tasks.</td>
                <td>Location awareness, Shape, Navigation, Prototypes, Soft robotics, Robot sensing systems, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341817&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341817&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>eViper: A Scalable Platform for Untethered Modular Soft Robots</td>
                <td>H. Cheng et al.</td>
                <td>2023</td>
                <td>Soft robots present unique capabilities, but have been limited by the lack of scalable technologies for construction and the complexity of algorithms for efficient control and motion. These depend on soft-body dynamics, high-dimensional actuation patterns, and external/onboard forces. This paper presents scalable methods and platforms to study the impact of weight distribution and actuation patterns on fully untethered modular soft robots. An extendable Vibrating Intelligent Piezo-Electric Robot (eViper), together with an open-source Simulation Framework for Electroactive Robotic Sheet (SFERS) implemented in PyBullet, was developed as a platform to analyze the complex weight-locomotion interaction. By integrating power electronics, sensors, actuators, and batteries onboard, the eViper platform enables rapid design iteration and evaluation of different weight distribution and control strategies for the actuator arrays. The design supports both physics-based modeling and data-driven modeling via onboard automatic data-acquisition capabilities. We show that SFERS can provide useful guidelines for optimizing the weight distribution and actuation patterns of the eViper, thereby achieving maximum speed or minimum cost of transport (COT).</td>
                <td>Actuators, Heuristic algorithms, Soft robotics, Robot sensing systems, Software, Power electronics, Intelligent sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342402&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342402&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Domain Randomization for Robust, Affordable and Effective Closed-Loop Control of Soft Robots</td>
                <td>G. Tiboni, A. Protopapa, T. Tommasi and G. Averta</td>
                <td>2023</td>
                <td>Soft robots are gaining popularity thanks to their intrinsic safety to contacts and adaptability. However, the potentially infinite number of Degrees of Freedom makes their modeling a daunting task, and in many cases only an approximated description is available. This challenge makes reinforcement learning (RL) based approaches inefficient when deployed on a realistic scenario, due to the large domain gap between models and the real platform. In this work, we demonstrate, for the first time, how Domain Randomization (DR) can solve this problem by enhancing RL policies for soft robots with: i) robustness w.r.t. unknown dynamics parameters; ii) reduced training times by exploiting drastically simpler dynamic models for learning; iii) better environment exploration, which can lead to exploitation of environmental constraints for optimal performance. Moreover, we introduce a novel algorithmic extension to previous adaptive domain randomization methods for the automatic inference of dynamics parameters for deformable objects. We provide an extensive evaluation in simulation on four different tasks and two soft robot designs, opening interesting perspectives for future research on Reinforcement Learning for closed-loop soft robot control.</td>
                <td>Training, Adaptation models, Heuristic algorithms, Reinforcement learning, Soft robotics, Robustness, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342537&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342537&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Implementation of a Cosserat Rod-Based Configuration Tracking Controller on a Multi-Segment Soft Robotic Arm</td>
                <td>A. Doroudchi, Z. Qiao, W. Zhang and S. Berman</td>
                <td>2023</td>
                <td>Controlling soft continuum robotic arms is challenging due to their hyper-redundancy and dexterity. In this paper we experimentally demonstrate, for the first time, closed-loop control of the configuration space variables of a soft robotic arm, composed of independently controllable segments, using a Cosserat rod model of the robot and the distributed sensing and actuation capabilities of the segments. Our controller solves the inverse dynamic problem by simulating the Cosserat rod model in MATLAB using a computationally efficient numerical solution scheme, and it applies the computed control output to the actual robot in real time. The position and orientation of the tip of each segment are measured in real time, while the remaining unknown variables that are needed to solve the inverse dynamics are estimated simultaneously in the simulation. We implement the controller on a multi-segment silicone robotic arm with pneumatic actuation, using a motion capture system to measure the segments' positions and orientations. The controller is used to reshape the arm into configurations that are achieved through combinations of bending and extension deformations in 3D space. Although the possible deformations are limited for this robot platform, our study demonstrates the potential for implementing the control approach on a wide range of continuum robots in practice. The resulting tracking performance indicates the effectiveness of the controller and the accuracy of the simulated Cosserat rod model.</td>
                <td>Motion segmentation, Pneumatic systems, Soft robotics, Position measurement, Robot sensing systems, Manipulators, Mathematical models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342160&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342160&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>IF-Based Trajectory Planning and Cooperative Control for Transportation System of Cable Suspended Payload With Multi UAVs</td>
                <td>Y. Zhang, J. Xu, C. Zhao and J. Dong</td>
                <td>2023</td>
                <td>In this paper, we tackle the control and trajectory planning problems for the cooperative transportation system of cable-suspended payload with multi Unmanned Aerial Vehicles (UAVs). Firstly, a payload controller is presented considering the dynamic coupling between the UAV and the payload to accomplish the active suppression of payload swing and the complex payload trajectory tracking. Secondly, different from the simplification of obstacles in most approaches, we propose three Insetting Formation (IF) algorithms for the complete obstacle shape to generate collision-free waypoints for the cooperative transportation system. An IF strategy is proposed by integrating three IF algorithms to improve the success rate of obstacle avoidance and reduce the algorithm complexity for performing the aggressive flight. Finally, we verify the robustness and high performance of the proposed algorithm through benchmark comparison and real-world experiments. Moreover, our source code is released as an open-source ros package.</td>
                <td>Trajectory planning, Trajectory tracking, Shape, Source coding, Transportation, Autonomous aerial vehicles, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341351&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341351&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Cooperative Dual-Arm Control for Heavy Object Manipulation Based on Hierarchical Quadratic Programming</td>
                <td>M. Dio, A. Völz and K. Graichen</td>
                <td>2023</td>
                <td>This paper presents a new control scheme for cooperative dual-arm robots manipulating heavy objects. The proposed method uses the full dynamical model of the kinematically coupled robot system and builds on a hierarchical quadratic programming (HQP) formulation to enforce dynamical inequality constraints such as joint torques or internal loads. This ensures optimal tracking of an object trajectory, while additional objectives with lower priority are optimized on the prior solution space. Therefore, the redundancy of the inherent load distribution problem between the two arms can be eliminated. With this approach, higher object loads can be manipulated compared to non-optimized methods. Simulations with a 14 degree of freedom (dof) dual-arm robotic system demonstrate the effectiveness of the proposed control method. The real-time feasibility is guaranteed with an average computation time of less than 0.35 milliseconds at a control rate of 1 kilohertz.</td>
                <td>Time-frequency analysis, Torque, Computational modeling, Dynamics, Trajectory, Quadratic programming, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341854&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341854&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-UAV Adaptive Path Planning Using Deep Reinforcement Learning</td>
                <td>J. Westheider, J. Rückin and M. Popović</td>
                <td>2023</td>
                <td>Efficient aerial data collection is important in many remote sensing applications. In large-scale monitoring scenarios, deploying a team of unmanned aerial vehicles (UAVs) offers improved spatial coverage and robustness against individual failures. However, a key challenge is cooperative path planning for the UAVs to efficiently achieve a joint mission goal. We propose a novel multi-agent informative path planning approach based on deep reinforcement learning for adaptive terrain monitoring scenarios using UAV teams. We introduce new network feature representations to effectively learn path planning in a 3D workspace. By leveraging a counterfactual baseline, our approach explicitly addresses credit assignment to learn cooperative behaviour. Our experimental evaluation shows improved planning performance, i.e. maps regions of interest more quickly, with respect to non-counterfactual variants. Results on synthetic and real-world data show that our approach has superior performance compared to state-of-the-art non-learning-based methods, while being transferable to varying team sizes and communication constraints.</td>
                <td>Deep learning, Three-dimensional displays, Reinforcement learning, Autonomous aerial vehicles, Path planning, Robustness, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342516&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342516&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Emergent Cooperative Behavior in Distributed Target Tracking with Unknown Occlusions</td>
                <td>T. Li, L. W. Krakow and S. Gopalswamy</td>
                <td>2023</td>
                <td>Tracking multiple moving objects of interest (OOI) with multiple robot systems (MRS) has been addressed by active sensing that maintains a shared belief of OOIs and plans the motion of robots to maximize the information quality. Mobility of robots enables the behavior of pursuing better visibility, which is constrained by sensor field of view (FoV) and occlusion objects. We first extend prior work to detect, maintain and share occlusion information explicitly, allowing us to generate occlusion-aware planning even if à priori semantic occlusion information is unavailable. The efficacy of active sensing approaches is often evaluated according to estimation error and information gain metrics. However, these metrics do not directly explain the level of cooperative behavior engendered by the active sensing algorithms. Next, we extract different emergent cooperative behaviors that stem from the same underlying algorithms but manifest differently under differing scenarios. In particular, we highlight and demonstrate three emergent behavior patterns in active sensing MRS: (i) Change of tracking responsibility between agents when tracking trajectories with divergent directions or due to a re-allocation of the resource among heterogeneous agents; (ii) Awareness of occlusions to a trajectory and temporal leave-and-return of the sensing agent; (iii) Sharing of local occlusion objects in MRS that subsequently improves the awareness of occlusion.</td>
                <td>Measurement, Target tracking, Heuristic algorithms, Semantics, Robot sensing systems, Behavioral sciences, Sensors, Active Sensing, Visibility, Occlusion-Aware, Cooperative Emergent Behavior, Dynamic Occlusion Map</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342357&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342357&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Objective Sparse Sensing with Ergodic Optimization</td>
                <td>A. Rao and H. Choset</td>
                <td>2023</td>
                <td>We consider a search problem where a robot has one or more types of sensors, each suited to detecting different types of targets or target information. Often, information in the form of a distribution of possible target locations, or locations of interest, may be available to guide the search. When multiple types of information exist, then a distribution for each type of information must also exist, thereby making the search problem that uses these distributions to guide the search a multi-objective one. In this paper, we consider a multi-objective search problem when the “cost” to use a sensor is limited. To this end, we leverage the ergodic metric, which drives agents to spend time in regions proportional to the expected amount of information there. We define the multi-objective sparse sensing ergodic (MO-SS-E) metric in order to optimize when and where each sensor measurement should be taken while planning trajectories that balance the multiple objectives. We observe that our approach maintains coverage performance as the number of samples taken considerably degrades. Further empirical results on different multi-agent problem setups demonstrate the applicability of our approach for both homogeneous and heterogeneous multi-agent teams.</td>
                <td>Measurement, Sensor fusion, Robot sensing systems, Search problems, Sensor systems, Sensors, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341535&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341535&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Team Coordination on Graphs with State-Dependent Edge Costs</td>
                <td>M. Limbu, Z. Hu, S. Oughourli, X. Wang, X. Xiao and D. Shishika</td>
                <td>2023</td>
                <td>This paper studies a team coordination problem in a graph environment. Specifically, we incorporate “support” action which an agent can take to reduce the cost for its teammate to traverse some high cost edges. Due to this added feature, the graph traversal is no longer a standard multi-agent path planning problem. To solve this new problem, we propose a novel formulation that poses it as a planning problem in a joint state space: the joint state graph (JSG). Since the edges of JSG implicitly incorporate the support actions taken by the agents, we are able to now optimize the joint actions by solving a standard single-agent path planning problem in JSG. One main drawback of this approach is the curse of dimensionality in both the number of agents and the size of the graph. To improve scalability in graph size, we further propose a hierarchical decomposition method to perform path planning in two levels. We provide both theoretical and empirical complexity analyses to demonstrate the efficiency of our two algorithms.</td>
                <td>Costs, Statistical analysis, Scalability, Path planning, Planning, Complexity theory, Game theory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341820&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341820&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Incorporating Stochastic Human Driving States in Cooperative Driving Between a Human-Driven Vehicle and an Autonomous Vehicle</td>
                <td>S. Hossain, J. Lu, H. Bai and W. Sheng</td>
                <td>2023</td>
                <td>Modeling a human-driven vehicle is a difficult subject since human drivers have a variety of stochastic behavioral components that influence their driving styles. We develop a cooperative driving framework to incorporate dif-ferent human behavior aspects, including the attentiveness of a driver and the tendency of the driver following advising commands. To demonstrate the framework, we consider the merging coordination between a human-driven vehicle and an autonomous vehicle (AV) in a connected environment. We propose a stochastic model predictive controller (sMPC) to address the stochasticity in human driving behavior and design coordinated merging actions to optimize the AV input and influence human driving behavior through advising commands. Simulation and human-in-the-loop (HITL) experimental results show that our formulation is capable of accommodating a distracted driver and optimizing AV inputs based on human driving behavior recognition.</td>
                <td>Merging, Stochastic processes, Predictive models, Human in the loop, Behavioral sciences, Autonomous vehicles, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341565&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341565&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Epistemic Planning for Heterogeneous Robotic Systems</td>
                <td>L. Bramblett and N. Bezzo</td>
                <td>2023</td>
                <td>In applications such as search and rescue or disaster relief, heterogeneous multi-robot systems (MRS) can provide significant advantages for complex objectives that require a suite of capabilities. However, within these application spaces, communication is often unreliable, causing inefficiencies or outright failures to arise in most MRS algorithms. Many researchers tackle this problem by requiring all robots to either maintain communication using proximity constraints or assuming that all robots will execute a predetermined plan over long periods of disconnection. The latter method allows for higher levels of efficiency in a MRS, but failures and environmental uncertainties can have cascading effects across the system, especially when a mission objective is complex or time-sensitive. To solve this, we propose an epistemic planning framework that allows robots to reason about the system state, leverage heterogeneous system makeups, and optimize information dissemination to disconnected neighbors. Dynamic epistemic logic formalizes the propagation of belief states, and epistemic task allocation and gossip is accomplished via a mixed integer program using the belief states for utility predictions and planning. The proposed framework is validated using simulations and experiments with heterogeneous vehicles.</td>
                <td>Uncertainty, Space communications, Power system protection, Search problems, Planning, Resource management, Multi-robot systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341352&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341352&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Reinforced Potential Field for Multi-Robot Motion Planning in Cluttered Environments</td>
                <td>D. Zhang, X. Zhang, Z. Zhang, B. Zhu and Q. Zhang</td>
                <td>2023</td>
                <td>Motion planning is challenging for multiple robots in cluttered environments without communication, especially in view of real-time efficiency, motion safety, distributed computation, and trajectory optimality, etc. In this paper, a reinforced potential field method is developed for distributed multi-robot motion planning, which is a synthesized design of reinforcement learning and artificial potential fields. An observation embedding with a self-attention mechanism is presented to model the robot-robot and robot-environment interactions. A soft wall-following rule is developed to improve the trajectory smoothness. Our method belongs to reactive planning, but environment properties are implicitly encoded. The total amount of robots in our method can be scaled up to any number. The performance improvement over a vanilla APF and RL method has been demonstrated via numerical simulations. Experiments are also performed using quadrotors to further illustrate the competence of our method.</td>
                <td>Heuristic algorithms, Dynamics, Reinforcement learning, Numerical simulation, Robot sensing systems, Real-time systems, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342416&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342416&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robot Team Data Collection with Anywhere Communication</td>
                <td>M. A. Schack, J. G. Rogers, Q. Han and N. T. Dantam</td>
                <td>2023</td>
                <td>Using robots to collect data is an effective way to obtain information from the environment and communicate it to a static base station. Furthermore, robots have the capability to communicate with one another, potentially decreasing the time for data to reach the base station. We present a Mixed Integer Linear Program that reasons about discrete routing choices, continuous robot paths, and their effect on the latency of the data collection task. We analyze our formulation, discuss optimization challenges inherent to the data collection problem, and propose a factored formulation that finds optimal answers more efficiently. Our work is able to find paths that reduce latency by up to 101% compared to treating all robots independently in our tested scenarios.</td>
                <td>Base stations, Data collection, Traveling salesman problems, Routing, Task analysis, Robots, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342349&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342349&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Coordination of Multiple Mobile Manipulators for Ordered Sorting of Cluttered Objects</td>
                <td>J. Ahn, S. Lee and C. Nam</td>
                <td>2023</td>
                <td>We present a coordination method for multiple mobile manipulators to sort objects in clutter. We consider the object rearrangement problem in which the objects must be sorted into different groups in a particular order. In clutter, the order constraints could not be easily satisfied since some objects occlude other objects so the occluded ones are not directly accessible to the robots. Those objects occluding others need to be moved more than once to make the occluded objects accessible. Such rearrangement problems fall into the class of nonmonotone rearrangement problems which are computation-ally intractable. While the nonmonotone problems with order constraints are harder, involving with multiple robots requires another computation for task allocation. In this work, we aim to develop a fast method, albeit sub-optimally, for the multi-robot coordination for ordered sorting in clutter. The proposed method finds a sequence of objects to be sorted using a search such that the order constraint in each group is satisfied. The search can solve nonmonotone instances that require temporal relocation of some objects to access the next object to be sorted. Once a complete sorting sequence is found, the objects in the sequence are assigned to multiple mobile manipulators using a greedy task allocation method. We develop four versions of the method with different search strategies. In the experiments, we show that our method can find a sorting sequence quickly (e.g., 4.6 sec with 20 objects sorted into five groups) even though the solved instances include hard nonmonotone ones. The extensive tests and the experiments in simulation show the ability of the method to solve the real-world sorting problem using multiple mobile manipulators.</td>
                <td>Geometry, Robot kinematics, Search problems, Manipulators, Planning, Resource management, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342179&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342179&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MOTLEE: Distributed Mobile Multi-Object Tracking with Localization Error Elimination</td>
                <td>M. B. Peterson, P. C. Lusk and J. P. How</td>
                <td>2023</td>
                <td>We present MOTLEE, a distributed mobile multi-object tracking algorithm that enables a team of robots to collaboratively track moving objects in the presence of localization error. Existing approaches to distributed tracking make limiting assumptions regarding the relative spatial relationship of sensors, including assuming a static sensor network or that perfect localization is available. Instead, we develop an algorithm based on the Kalman-Consensus filter for distributed tracking that properly leverages localization uncertainty in collaborative tracking. Further, our method allows the team to maintain an accurate understanding of dynamic objects in the environment by realigning robot frames and incorporating frame alignment uncertainty into our object tracking formulation. We evaluate our method in hardware on a team of three mobile ground robots tracking four people. Compared to previous works that do not account for localization error, we show that MOTLEE is resilient to localization uncertainties, enabling accurate tracking in distributed, dynamic settings with mobile tracking sensors.</td>
                <td>Location awareness, Uncertainty, Simultaneous localization and mapping, Limiting, Collaboration, Object detection, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342388&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342388&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dynamic Object Tracking for Quadruped Manipulator with Spherical Image-Based Approach</td>
                <td>T. Zhang, S. Guo, X. Xiong, W. Li, Z. Qi and Y. Lou</td>
                <td>2023</td>
                <td>Exactly estimating and tracking the motion of surrounding dynamic objects is one of important tasks for the autonomy of a quadruped manipulator. However, with only an onboard RGB camera, it is still a challenging work for a quadruped manipulator to track the motion of a dynamic object moving with unknown and changing velocities. To address this problem, this manuscript proposes a novel image-based visual servoing (IBVS) approach consisting of three elements: a spherical projection model, a robust super-twisting observer, and a model predictive controller (MPC). The spherical projection model decouples the visual error of the dynamic target into linear and angular ones. Then, with the presence of the visual error, the robustness of the observer is exploited to estimate the unknown and changing velocities of the dynamic target without depth estimation. Finally, the estimated velocity is fed into the model predictive controller (MPC) to generate joint torques for the quadruped manipulator to track the motion of the dynamical target. The proposed approach is validated through hardware experiments and the experimental results illustrate the approach's effectiveness in improving the autonomy of the quadruped manipulator.</td>
                <td>Visualization, Target tracking, Dynamics, Estimation, Predictive models, Observers, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341608&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341608&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Proprioception and Tail Control Enable Extreme Terrain Traversal by Quadruped Robots</td>
                <td>Y. Yang, J. Norby, J. K. Yim and A. M. Johnson</td>
                <td>2023</td>
                <td>Legged robots leverage ground contacts and the reaction forces they provide to achieve agile locomotion. However, uncertainty coupled with contact discontinuities can lead to failure, especially in real-world environments with unexpected height variations such as rocky hills or curbs. To enable dynamic traversal of extreme terrain, this work introduces 1) a proprioception-based gait planner for estimating unknown hybrid events due to elevation changes and responding by modifying contact schedules and planned footholds online, and 2) a two-degree-of-freedom tail for improving contact-independent control and a corresponding decoupled control scheme for better versatility and efficiency. Simulation results show that the gait planner significantly improves stability under unforeseen terrain height changes compared to methods that assume fixed contact schedules and footholds. Further, tests have shown that the tail is particularly effective at maintaining stability when encountering a terrain change with an initial angular disturbance. The results show that these approaches work synergistically to stabilize locomotion with elevation changes up to 1.5 times the leg length and tilted initial states.</td>
                <td>Legged locomotion, Schedules, Uncertainty, Simulation, Dynamics, Tail, Dynamic scheduling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342384&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342384&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Run and Catch: Dynamic Object-Catching of Quadrupedal Robots</td>
                <td>Y. You et al.</td>
                <td>2023</td>
                <td>Quadrupedal robots are performing increasingly more real-world capabilities, but are primarily limited to locomotion tasks. To expand their task-level abilities of object acquisition, i.e., run-to-catch as frisbee catching for dogs, this paper developed a control pipeline using stereo vision for legged robots which allows for dynamic catching balls while the robot is in motion. To achieve high-frame-rate tracking, we designed a ball that can actively emit homogeneous infrared (IR) light and then located the flying ball based on binocular vision positioning using the onboard RealSense D450 camera with an additional IR bandpass filter. The camera was mounted on top of a 2-DoF head to gain a full view of the target ball. A state estimation module was developed to fuse the vision positioning, camera motor readings, localization result of RealSense T265 equipped on the back, and the legged odometry output altogether. With the use of a ballistic model, we achieved a robust estimation of both the ball and robot positions in an inertial coordinate. Additionally, we developed a close-loop catching strategy and employed trajectory prediction so that tracking and run-to-catch were performed simultaneously, which is critical for such drastically dynamic and precise tasks. The proposed approach was validated through both static testing and dynamic catch experiments conducted on the CyberDog robot with a high success rate.</td>
                <td>Legged locomotion, Robot kinematics, Robot vision systems, Cameras, Stereo vision, Quadrupedal robots, Odometry</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341977&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341977&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Composite Control Strategy for Quadruped Robot by Integrating Reinforcement Learning and Model-Based Control</td>
                <td>S. Lyu, H. Zhao and D. Wang</td>
                <td>2023</td>
                <td>Locomotion in the wild requires the quadruped robot to have strong capabilities in adaptation and robustness. The deep reinforcement learning (DRL) exhibits the huge potential in environmental adaptability, while its stability issues remain open. On the other hand, the quadruped robot dynamic model contains a lot of useful information that is beneficial to the robust control. The combination of DRL with model-based control may take both strengths and hold promises in better robustness. In this paper, the DRL and the proposed model-based controller are firmly integrated in a novel manner such that the proposed model-based controller is able to rectify the gait commands generated by DRL based on the system dynamic model so as to enhance the robustness of the quadruped robot against the external disturbances. Besides, a potential energy function is introduced to achieve the compliant contact. The stability of the proposed method is ensured in terms of passivity analysis. Several physical experiments are carried out to verify the performance of the proposed method.</td>
                <td>Potential energy, Robust control, Legged locomotion, Adaptation models, System dynamics, Reinforcement learning, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341908&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341908&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Load Awareness: Sensorless Body Payload Sensing and Localization for Heavy Quadruped Robot</td>
                <td>S. Liu, S. Zhou, Z. Pan, Z. Niu and R. Wang</td>
                <td>2023</td>
                <td>Heavy quadrupedal drives have great potential for overcoming obstacles, showing great possibilities for transportation industries in complex environments. Ground reaction force (GRF) is a crucial state variable for quadrupedal control. Most GRF observations are implemented in lightweight quadrupeds, with little consideration of the loading being static or slippery on the body. However, the load information is vital to the heavy-duty quadruped applied in transportation tasks. In this paper, we disassembled the whole-body dynamics into the body dynamics combined with the individual floating single-leg dynamics and completed observing the virtual coupling effects between the body and legs. Based on the observed coupling force and centroidal dynamics (CD), the GRF of a stance leg is obtained without the awareness of body weight, movement, and load information. Furthermore, we utilized the body dynamics and the observed virtual force to obtain the body's unknown payload. By reconstructing the moment balance equation, we obtained the payload's position concerning the body coordinate. Compared to conventional quadrupedal GRF observation methods, this framework achieves higher observation accuracy in heavy quadrupeds without load and body information. Additionally, it enables real-time calculation of load magnitude and position.</td>
                <td>Legged locomotion, Robot kinematics, Friction, Force, Dynamics, Robot sensing systems, Mathematical models, Quadrupedal robots, Force sensors, Payloads</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342158&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342158&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Evolutionary-Based Online Motion Planning Framework for Quadruped Robot Jumping</td>
                <td>H. Liu</td>
                <td>2023</td>
                <td>Offline evolutionary-based methodologies have supplied a successful motion planning framework for the quadrupedal jump. However, the time-consuming computation caused by massive population evolution in offline evolutionary-based jumping framework significantly limits the popularity in the quadrupedal field. This paper presents a time-friendly online motion planning framework based on meta-heuristic Differential evolution (DE), Latin hypercube sampling, and Configuration space (DLC). The DLC framework establishes a multidimensional optimization problem leveraging centroidal dynamics to determine the ideal trajectory of the center of mass (CoM) and ground reaction forces (GRFs). The configuration space is introduced to the evolutionary optimization in order to condense the searching region. Latin hypercube sampling offers more uniform initial populations of DE under limited sampling points, accelerating away from a local minimum. This research also constructs a collection of pre-motion trajectories as a warm start when the objective state is in the neighborhood of the pre-motion state to drastically reduce the solving time. The proposed methodology is successfully validated via real robot experiments for online jumping trajectory optimization with different jumping motions (e.g., ordinary jumping, flipping, and spinning).</td>
                <td>Sociology, Hypercubes, Libraries, Planning, Quadrupedal robots, Spinning, Statistics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342082&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342082&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-IMU Proprioceptive Odometry for Legged Robots</td>
                <td>S. Yang, Z. Zhang, B. Bokser and Z. Manchester</td>
                <td>2023</td>
                <td>This paper presents a novel, low-cost proprioceptive sensing solution for legged robots with point feet to achieve accurate low-drift long-term position and velocity estimation. In addition to conventional sensors, including one body Inertial Measurement Unit (IMU) and joint encoders, we attach an additional IMU to each calf link of the robot just above the foot. An extended Kalman filter is used to fuse data from all sensors to estimate the robot's body and foot positions in the world frame. Using the additional IMUs, the filter is able to reliably determine foot contact modes and detect foot slips without tactile or pressure-based foot contact sensors. This sensing solution is validated in various hardware experiments, which confirm that it can reduce position drift by nearly an order of magnitude compared to conventional approaches with only a very modest increase in hardware and computational costs.</td>
                <td>Legged locomotion, Measurement units, Propioception, Sensor fusion, Hardware, Reliability, Odometry</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342061&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342061&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Motion Guidelines for Quadrupedal Locomotion of Maximum Speed or Efficiency with Serial and Parallel Legs</td>
                <td>K. Machairas and E. Papadopoulos</td>
                <td>2023</td>
                <td>Analytical expressions are derived for actuator demands in quadrupedal locomotion of constant speed and height by using a reduction from a trot/ pace 6-bar model to a single-legged model and employing two widely used two-segmented leg architectures, the serial and the parallel. A method is developed that outputs optimal gait characteristics and leg designs for a robot to move with maximum efficiency or speed. Also, generic guidelines are presented, which answer questions such as: which speed should be selected for maximum efficiency, or which is the optimal leg architecture (serial/ parallel) and leg length for maximum efficiency or speed.</td>
                <td>Legged locomotion, Procurement, Analytical models, Actuators, Technological innovation, Motion segmentation, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341886&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341886&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Legged Locomotion on Steep Planetary Terrain</td>
                <td>G. Valsecchi, C. Weibel, H. Kolvenbach and M. Hutter</td>
                <td>2023</td>
                <td>Scientific exploration of planetary bodies is an activity well-suited for robots. Unfortunately, the regions that are richer in potential discoveries, such as impact craters, caves, and volcanic terraces, are hard to access with wheeled robots. Recent advances in legged-based approaches have shown the potential of the technology to overcome difficult terrains such as slopes and slippery surfaces. In this work, we focus on locomotion for sandy slopes, comparing standard walking policies with a novel crawling-based gait for quadrupedal robots. We fine-tuned a state-of-the-art locomotion framework and introduced hardware modifications to the robot ANYmal, which enables walking on its knees. Moreover, we integrated a novel metric for stability, the stability margin, in the training process to increase robustness in such conditions. We benchmarked the locomotion policies in simulation and in real-world experiments on a martian soil simulant. Our results show a significant improvement in terms of robustness and stability, especially at higher slope angles beyond 15 degrees.</td>
                <td>Legged locomotion, Knee, Training, Surface morphology, Robot sensing systems, Stability analysis, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341665&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341665&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dynamic Hybrid Locomotion and Jumping for Wheeled-Legged Quadrupeds</td>
                <td>M. Hosseini, D. Rodriguez and S. Behnke</td>
                <td>2023</td>
                <td>Hybrid wheeled-legged quadrupeds have the potential to navigate challenging terrain with agility and speed and over long distances. However, obstacles can impede their progress by requiring the robots to either slow down to step over obstacles or modify their path to circumvent the obstacles. We propose a motion optimization framework for quadruped robots that incorporates non-steerable wheels and dynamic jumps, enabling them to perform hybrid wheeled-legged locomotion while overcoming obstacles without slowing down. Our approach involves a model predictive controller that uses a time-varying rigid body dynamics model of the robot, including legs and wheels, to track dynamic motions such as jumping. We also introduce a method for driving with minimal leg swings to reduce energy consumption by sparing the effort involved in lifting the wheels. Our method was tested successfully on the wheeled Mini Cheetah and the Unitree AlienGo robots. Further videos and results are available at https://www.ais.uni-bonn.de/∼hosseini/iros2023</td>
                <td>Legged locomotion, Tracking, Dynamics, Wheels, Predictive models, Trajectory, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341824&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341824&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Quadrupedal Footstep Planning Using Learned Motion Models of a Black-Box Controller</td>
                <td>I. Taouil, G. Turrisi, D. Schleich, V. Barasuol, C. Semini and S. Behnke</td>
                <td>2023</td>
                <td>Legged robots are increasingly entering new domains and applications, including search and rescue, inspection, and logistics. However, for such a systems to be valuable in real-world scenarios, they must be able to autonomously and robustly navigate irregular terrains. In many cases, robots that are sold on the market do not provide such abilities, being able to perform only blind locomotion. Furthermore, their controller cannot be easily modified by the end-user, requiring a new and time-consuming control synthesis. In this work, we present a fast local motion planning pipeline that extends the capabilities of a black-box walking controller that is only able to track high-level reference velocities. More precisely, we learn a set of motion models for such a controller that maps high-level velocity commands to Center of Mass (CoM) and footstep motions. We then integrate these models with a variant of the $A$ * algorithm to plan the CoM trajectory, footstep sequences, and corresponding high-level velocity commands based on visual information, allowing the quadruped to safely traverse irregular terrains at demand.</td>
                <td>Legged locomotion, Visualization, Uncertainty, Tracking, Pipelines, Closed box, Stairs</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342440&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342440&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Locomotion Planning of a Truss Robot on Irregular Terrain</td>
                <td>J. Bae, I. Park, M. Yim and T. Seo</td>
                <td>2023</td>
                <td>This paper proposes a new locomotion algorithm for truss robots on irregular terrain, in particular, for the Variable Topology Truss (VTT) system. The previous Polygon-based Random Tree (PRT) search algorithm for support polygon generation is extended to irregular terrain while considering friction and internal force limitations. By characterizing terrain, unreachable areas are excluded from search to increase efficiency. A one-step rolling motion primitive is generated based on the kinematics, statics, and constraints of VTT. The locomotion planning is completed by transforming and connecting multiple motion primitives with respect to the desired support polygons. The algorithm's performance is verified by conducting simulations in multiple types of environments.</td>
                <td>Three-dimensional displays, Friction, Force, Static analysis, Kinematics, Planning, Topology</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341447&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341447&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Model Predictive Path Integral Method for Fast, Proactive, and Uncertainty-Aware UAV Planning in Cluttered Environments</td>
                <td>J. Higgins, N. Mohammad and N. Bezzo</td>
                <td>2023</td>
                <td>Current motion planning approaches for autonomous mobile robots often assume that the low level controller of the system is able to track the planned motion with very high accuracy. In practice, however, tracking error can be affected by many factors, and could lead to potential collisions when the robot must traverse a cluttered environment. To address this problem, this paper proposes a novel receding-horizon motion planning approach based on Model Predictive Path Integral (MPPI) control theory – a flexible sampling-based control technique that requires minimal assumptions on vehicle dynamics and cost functions. This flexibility is leveraged to propose a motion planning framework that also considers a data-informed risk function. Using the MPPI algorithm as a motion planner also reduces the number of samples required by the algorithm, relaxing the hardware requirements for implementation. The proposed approach is validated through trajectory generation for a quadrotor unmanned aerial vehicle (UAV), where fast motion increases trajectory tracking error and can lead to collisions with nearby obstacles. Simulations and hardware experiments demonstrate that the MPPI motion planner proactively adapts to the obstacles that the UAV must negotiate, slowing down when near obstacles and moving quickly when away from obstacles, resulting in a complete reduction of collisions while still producing lively motion.</td>
                <td>Adaptation models, Trajectory tracking, Heuristic algorithms, Predictive models, Autonomous aerial vehicles, Hardware, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341501&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341501&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Energy-Efficient Team Orienteering Problem in the Presence of Time-Varying Ocean Currents</td>
                <td>A. Mansfield, D. G. Macharet and M. A. Hsieh</td>
                <td>2023</td>
                <td>Autonomous Marine Vehicles (AMVs) have gained interest for scientific and commercial applications, including pipeline and algae bloom monitoring, contaminant tracking, and ocean debris removal. The Team Orienteering Problem (TOP) is relevant in this context as Multi-Robot Systems (MRSs) allow for better coverage of the area of interest, simultaneous data collection at different locations, and an increase in the overall robustness and efficiency of the mission. However, route planning for AMVs in dynamic ocean environments is challenging due to the coupling of environmental and vehicle dynamics. We propose a multi-objective formulation that accounts for the trade-offs between visiting multiple task locations and energy consumption by the vehicles subject to a time budget. This work focuses on vehicles that can maintain a constant net speed but can be adapted to vehicles with constant thrust. Different from existing approaches, our method is able to leverage time-varying ocean currents to improve the energy efficiency of resulting routes. We validate our approach experimentally by superimposing ocean flow models with benchmark instances of the TOP.</td>
                <td>Oceans, Pipelines, Energy efficiency, Robustness, Planning, Multi-robot systems, Vehicle dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341645&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341645&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Agent Multi-Objective Ergodic Search Using Branch and Bound</td>
                <td>A. K. Srinivasan, G. Gutow, Z. Ren, I. Abraham, B. Vundurthy and H. Choset</td>
                <td>2023</td>
                <td>Search and rescue applications often need multiple agents to complete a set of conflicting tasks. This paper studies a Multi-Agent Multi-Objective Ergodic Search (MA-MO-ES) approach to this problem where each objective or task is to cover a domain subject to an information map. The goal is to allocate coverage tasks to agents so that all maps are explored ergodically. The combinatorial nature of task allocation makes it computationally expensive to solve for optimal allocation using brute force. Apart from a large number of possible allocations, computing the cost of a task allocation is itself an expensive planning problem. To mitigate the computational challenge, we present a branch and bound-based algorithm with pruning techniques that reduce the number of allocations to be searched to find optimal coverage task allocation. We also present an approach to leverage the similarity between information maps to further reduce computation. Extensive testing on 147 randomly generated test cases shows an order of magnitude improvement in runtime compared to an exhaustive brute force approach.</td>
                <td>Runtime, Costs, Force, Search problems, Planning, Resource management, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341353&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341353&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Leveraging Single-Goal Predictions to Improve the Efficiency of Multi-Goal Motion Planning with Dynamics</td>
                <td>Y. Lu and E. Plaku</td>
                <td>2023</td>
                <td>Multi-goal motion planning requires a robot to plan collision-free and dynamically-feasible motions to reach multiple goals, often in unstructured, obstacle-rich environments. This is challenging due to the complex dependencies between navigation and high-level reasoning, requiring the robot to explore a vast space of feasible motions and goal sequences. Our approach combines machine learning and Traveling Salesman Problem (TSP) solvers with sampling-based motion planning. Machine learning predicts distances and directions between locations, considering obstacles and robot dynamics, which the TSP solver uses to compute promising tours. Sampling-based motion planning expands a motion tree to follow the tours along the predicted directions. We demonstrate the effectiveness of our approach through experiments with vehicle and snake-like robot models operating in unstructured environments with multiple goals.</td>
                <td>Navigation, Computational modeling, Dynamics, Machine learning, Traveling salesman problems, Planning, Space exploration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341945&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341945&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DynGMP: Graph Neural Network-Based Motion Planning in Unpredictable Dynamic Environments</td>
                <td>W. Zhang et al.</td>
                <td>2023</td>
                <td>Neural networks have already demonstrated attractive performance for solving motion planning problems, especially in static and predictable environments. However, efficient neural planners that can adapt to unpredictable dynamic environments, a highly demanded scenario in many practical applications, are still under-explored. To fill this research gap and enrich the existing motion planning approaches, in this pa-per, we propose DynGMP, a graph neural network (GNN)-based planner that provides high-performance planning solutions in unpredictable dynamic environments. By fully leveraging the prior exploration experience and minimizing the replanning cost incurred by environmental change, DynGMP achieves high planning performance and efficiency simultaneously. Empirical evaluations across different environments show that DynGMP can achieve close to 100% success rate with fast planning speed and short path cost. Compared with existing non-learning and learning-based counterparts, DynGMP shows very significant planning performance improvement, e.g., at least 2.7×, 2.2×, $2.4\times$ and $2\times$ faster planning speed with low path distance in four environments, respectively.</td>
                <td>Learning systems, Costs, Heuristic algorithms, Design methodology, Dynamics, Graph neural networks, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342326&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342326&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Symbolic State Space Optimization for Long Horizon Mobile Manipulation Planning</td>
                <td>X. Zhang et al.</td>
                <td>2023</td>
                <td>In existing task and motion planning (TAMP) research, it is a common assumption that experts manually specify the state space for task-level planning. A well-developed state space enables the desirable distribution of limited computational resources between task planning and motion planning. However, developing such task-level state spaces can be non-trivial in practice. In this paper, we consider a long horizon mobile manipulation domain including repeated navigation and manipulation. We propose Symbolic State Space Optimization (S3O) for computing a set of abstracted locations and their 2D geometric groundings for generating task-motion plans in such domains. Our approach has been extensively evaluated in simulation and demonstrated on a real mobile manipulator working on clearing up dining tables. Results show the superiority of the proposed method over TAMP baselines in task completion rate and execution time.</td>
                <td>Navigation, Grounding, Probabilistic logic, Manipulators, Planning, Task analysis, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342224&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342224&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Fast and Map-Free Model for Trajectory Prediction in Traffics</td>
                <td>J. Xiang, J. Zhang and Z. Nan</td>
                <td>2023</td>
                <td>To handle the two shortcomings of existing methods, (i) nearly all models rely on high-definition (HD) maps, yet the map information is not always available in real traffic scenes and HD map-building is expensive and time-consuming and (ii) existing models usually focus on improving prediction accuracy at the expense of reducing computing efficiency, yet the efficiency is crucial for various real applications, this paper proposes an efficient trajectory prediction model that is not dependent on traffic maps. The core idea of our model is encoding single-agent's spatial-temporal information in the first stage and exploring multi-agents' spatial-temporal interactions in the second stage. By comprehensively utilizing attention mechanism, LSTM, graph convolution network and temporal transformer in the two stages, our model is able to learn rich dynamic and interaction information of all agents. Our model achieves the highest performance when comparing with existing map-free methods and also exceeds most map-based state-of-the-art methods on the Argoverse dataset. In addition, our model also exhibits a faster inference speed than the baseline methods.</td>
                <td>Convolution, Computational modeling, Predictive models, Transformer cores, Transformers, Encoding, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342199&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342199&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Local Non-Cooperative Games with Principled Player Selection for Scalable Motion Planning</td>
                <td>M. Chahine, R. Firoozi, W. Xiao, M. Schwager and D. Rus</td>
                <td>2023</td>
                <td>Game-theoretic motion planners are a powerful tool for the control of interactive multi-agent robot systems. Indeed, contrary to predict-then-plan paradigms, game-theoretic planners do not ignore the interactive nature of the problem, and simultaneously predict the behaviour of other agents while considering change in one's policy. This, however, comes at the expense of computational complexity, especially as the number of agents considered grows. In fact, planning with more than a handful of agents can quickly become intractable, disqualifying game-theoretic planners as possible candidates for large scale planning. In this paper, we propose a planning algorithm enabling the use of game-theoretic planners in robot systems with a large number of agents. Our planner is based on the reality of locality of information and thus deploys local games with a selected subset of agents in a receding horizon fashion to plan collision avoiding trajectories. We propose five different principled schemes for selecting game participants and compare their collision avoidance performance. We observe that the use of Control Barrier Functions for priority ranking is a potent solution to the player selection problem for motion planning.</td>
                <td>Jacobian matrices, Costs, Scalability, Games, Planning, Trajectory, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341677&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341677&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Target Attribute Perception Based UAV Real-Time Task Planning in Dynamic Environments</td>
                <td>J. He, Z. Sun, N. Cao, D. Ming and C. Cai</td>
                <td>2023</td>
                <td>In this paper, a comprehensive solution for enabling unmanned aerial vehicle (UAV) to autonomously fly through complex and dynamic environments is proposed. Moving objects all have unique property information, we propose a method that utilizes deep learning for 3D dynamic environment perception, while taking into account limitations in computing resources. For safer dynamic avoidance, we first model the dynamic target and integrate it into a static grid occupancy map, and then construct a gradient field based on its attribute information. To achieve autonomous UAV flight in dynamic environments, we design an adaptive planning method based on gradient optimization, which achieves significant computational savings by autonomously adjusting the planning frequency and using manually constructed gradients instead of maintaining a signed distance field (SDF). We have integrated the above approach into a customised quadrotor system and thoroughly tested it in real-world, verifying its flexibility to handle multiple objects with variable speed motion in complex enviroment.</td>
                <td>Three-dimensional displays, Heuristic algorithms, Dynamics, Autonomous aerial vehicles, Dynamic scheduling, Planning, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341486&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341486&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Visibility-Based Escort Problem</td>
                <td>Kane</td>
                <td>2023</td>
                <td>This paper introduces and solves a visibility-based escort planning problem. This novel problem, which is closely related to the well-researched family of visibility-based pursuit-evasion problems in robotics, entails an escort agent tasked with escorting a vulnerable agent, called the VIP, in a 2-dimensional environment. The escort protects the VIP from adversaries that pose line-of-sight threats. We describe a correct and complete planning algorithm whose inputs are a simply-connected polygonal map of the environment, starting locations for the escort and the VIP, along with a goal location to which the VIP agent should be safely moved. The algorithm computes trajectories for the escort and VIP which allow the VIP to reach its goal without coming into the line-of-sight of the adversary at any time. During the execution of these trajectories, the adversary is allowed to move along any continuous path that does not enter into the line-of-sight of the escort. The algorithm proceeds by dividing the environment into a collection of conservative regions and planning the escort's movements as a sequence of these regions via breadth-first search over an information graph. The trajectory of the VIP can then be constructed by tracing the ‘safe zones' swept out by the escort's trajectory. We describe an implementation of this algorithm and present computed examples of escort agent strategies in diverse environments.</td>
                <td>Green products, Line-of-sight propagation, Trajectory, Planning, Task analysis, Computational complexity, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341574&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341574&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhancing Value Estimation Policies by Post-Hoc Symmetry Exploitation in Motion Planning Tasks</td>
                <td>Suarez, E. C. Carter, A. Faust and L. Tapia</td>
                <td>2023</td>
                <td>Motion planning tasks are often innately invariant to certain geometric transformations, or in other words, symmetric. This property, however, is not always reflected in learned policies that are trained on these tasks. Although this asymmetry can be addressed through data augmentation or additional training samples, doing so comes at a cost of increased training time. Instead of trying to remedy this issue during the learning process, we leverage this disparity during execution. We propose the symmetry exploitation policy, an augmentation in the post-hoc execution stage of RL policies. During the planning stage, we present the learned policy with an invariant, geometrically transformed version of the observation as an alternate perspective of the state. This allows the policy to produce multiple possible actions for a single state, and choose the action with the highest estimated value. Unlike other symmetry exploitation methods for learning solutions in motion planning, this method completely bypasses the need for additional training. We show the effect of the symmetry exploitation policy on DQN, A2C, and PPO policies, in three motion problems with different dimensions, observation types, and symmetries. The results show that by exploiting the symmetry of the task, a trained model achieves improved performance and better generalization, and can achieve comparable results to retraining, augmentation, or extended training, without incurring any additional training time. The efficacy is most prominent in more complex tasks, as 89 of the 100 models involved in the case study improve when using the method.</td>
                <td>Training, Costs, Dynamics, Estimation, Data augmentation, Planning, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341746&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341746&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Risk-Aware Emergency Landing Planning for Gliding Aircraft Model in Urban Environments</td>
                <td>J. Sláma, J. Herynek and J. Faigl</td>
                <td>2023</td>
                <td>An in-flight loss of thrust poses a risk to the aircraft, its passengers, and people on the ground. When a loss of thrust happens, the (auto)pilot is forced to perform an emergency landing, possibly toward one of the reachable airports. If none of the airports is reachable, the aircraft is forced to land at another location, which can be risky in urban environments. In this work, we present a generalization of the previous work on planning safe emergency landing in the case of in-flight loss of thrust such that the risk induced by the loss of thrust can be assessed if none of the airports are reachable. The proposed method relies on planning space discretization and efficient risk propagation through the risk map. The approach can find the least risky landing site and corresponding forced landing trajectory for any configuration in the planning space. The method has been empirically evaluated in a realistic urban scenario. The results support its suitability for risk-aware planning of an emergency landing in the case of in-flight loss of thrust.</td>
                <td>Atmospheric modeling, Urban areas, Airports, Planning, Trajectory, Aircraft, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341622&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341622&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SCTOMP: Spatially Constrained Time-Optimal Motion Planning</td>
                <td>J. Arrizabalaga and M. Ryll</td>
                <td>2023</td>
                <td>This work focuses on spatial time-optimal motion planning, a generalization of the exact time-optimal path following problem that allows a system to plan within a predefined space. In contrast to state-of-the-art methods, we drop the assumption of a given collision-free geometric reference. Instead, we present a three-stage motion planning method that solely relies on start and goal locations and a geometric representation of the environment to compute a time-optimal trajectory that is compliant with system dynamics and constraints. The proposed scheme first finds collision-free navigation corridors, second computes an obstacle-free Pythagorean Hodograph parametric spline along each corridor, and third, solves a spatially reformulated minimum-time optimization problem at each of these corridors. The spline obtained in the second stage is not a geometric reference, but an extension of the free space associated with its corridor, and thus, time-optimality of the solution is guaranteed. The validity of the proposed approach is demonstrated by a well-established planar example and benchmarked in a spatial system against state-of-the-art methodologies across a wide range of scenarios in highly congested environments. Video: https://youtu.be/zGExvnUEfOY</td>
                <td>System dynamics, Navigation, Benchmark testing, Minimization, Planning, Trajectory, Splines (mathematics)</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341500&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341500&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Consecutive Inertia Drift of Autonomous RC Car via Primitive-Based Planning and Data-Driven Control</td>
                <td>Y. Lu, B. Yang, J. Li, Y. Zhou, H. Chen and Y. Mo</td>
                <td>2023</td>
                <td>Inertia drift is an aggressive transitional driving maneuver, which is challenging due to the high nonlinearity of the system and the stringent requirement on control and planning performance. This paper presents a solution for the consecutive inertia drift of an autonomous RC car based on primitive-based planning and data-driven control. The planner generates complex paths via the concatenation of path segments called primitives, and the controller eases the burden on feedback by interpolating between multiple real trajectories with different initial conditions into one near-feasible reference trajectory. The proposed strategy is capable of drifting through various paths containing consecutive turns, which is validated in both simulation and reality.</td>
                <td>Interpolation, Planning, Trajectory, Automobiles, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341461&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341461&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Hybrid-State Path Planner for ASV Formations with Full-Scale Experiments</td>
                <td>L. M. Ruud, M. S. Rundhovde, J. Sandrib and G. Bitar</td>
                <td>2023</td>
                <td>We present a hybrid-state path planner for autonomous surface vehicles (ASVs) constrained by a min-imum turning radius. The work is motivated by the future Norwegian naval mine countermeasures (NMCM) concept, which includes mine sweeping operations with ASVs that operate alone or in a formation of two, with and without mine sweeping equipment attached. Our path-planning approach is a variant of hybrid A* search, with the goal of finding the shortest path for transit in an environment consisting of an a priori map, operator-defined avoid areas and static obstacles detected by the vehicle. The planner is capable of finding traversable paths for ASV formations and single vehicles using a simple geometric model based on an estimated minimum turning radius for a given configuration. The method is modular as it is uncoupled from the path-following control method, making it applicable to many different systems, including our ASV configurations for mine sweeping operations. We present results from path planning in simulation for a single vehicle and two vehicles in formation. The method has been validated through a full-scale experiment with one of our ASVs in our testing area for autonomous platforms.</td>
                <td>Navigation, Geometric modeling, Kinematics, Turning, Path planning, Intelligent robots, Testing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341696&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341696&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CAT-RRT: Motion Planning that Admits Contact One Link at a Time</td>
                <td>N. Nechyporenko, C. Escobedo, S. Kadekodi and A. Roncone</td>
                <td>2023</td>
                <td>Current motion planning approaches rely on binary collision checking to evaluate the validity of a state and thereby dictate where the robot is allowed to move. This approach leaves little room for robots to engage in contact with an object, as is often necessary when operating in densely cluttered spaces. In this work, we propose an alternative method that considers contact states as high-cost states that the robot should avoid but can traverse if necessary to complete a task. More specifically, we introduce Contact Admissible Transition-based Rapidly exploring Random Trees (CAT-RRT)11Supplementary video and open source code [1]., a planner that uses a novel per-link cost heuristic to find a path by traversing high-cost obstacle regions. Through extensive testing, we find that state-of-the-art optimization planners tend to over-explore low-cost states, which leads to slow and inefficient convergence to contact regions. Conversely, CAT-RRT searches both low and high-cost regions simultaneously with an adaptive thresholding mechanism carried out at each robot link. This leads to paths with a balance between efficiency, path length, and contact cost.</td>
                <td>Costs, Source coding, Planning, Collision avoidance, Task analysis, Optimization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341668&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341668&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Novel Gripper with Rotatable Distal Joints for Home Robots: Picking and Placing Tableware</td>
                <td>W. Kim, C. g. Hwang, S. Yoo, Y. Ko and S. Kang</td>
                <td>2023</td>
                <td>A convenient situation can be realized if home robots replace housework. However, tasks in an actual home environment are challenging for robots. Particularly, cleaning the table after eating is challenging because of the cluttered environments and various tableware shapes. This study presents a new type of gripper appropriate for picking and placing various tableware in narrow and cluttered environments. The gripper comprises a 1-DOF gripper and 2-DOF distal joints. The rotatable distal joints enable reducing the effective workspace when reorientating tableware and accessing tableware in narrow spaces. In addition, the gripper can transform into three types of grasping modes by actively rotating the distal joints for handling three types of tableware, namely cutlery, cups, and dishes. The gripper is experimentally demonstrated to handle various tableware and the tables can be cleaned after eating in real-world environments.</td>
                <td>Shape, 2-DOF, Transforms, Grasping, Cleaning, Grippers, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342249&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342249&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Prioritized Planning for Target-Oriented Manipulation via Hierarchical Stacking Relationship Prediction</td>
                <td>Z. Wu, J. Tang, X. Chen, C. Ma, X. Lan and N. Zheng</td>
                <td>2023</td>
                <td>In scenarios involving grasping multiple targets, the learning of stacking relationships between objects is fundamental for robots to execute safely and efficiently. However, current methods lack subdivision for the hierarchy of stacking relationship types. In scenes where objects are mostly stacked in an orderly manner, they are incapable of performing human-like and high-efficient grasping decisions. This paper proposes a perception-planning method to distinguish different stacking forms between objects and generate prioritized manipulation sequences based on given target designations. We utilize a Hierarchical Stacking Relationship Network (HSRN) to discriminate the hierarchy of stacking and generate a refined Stacking Relationship Tree (SRT) for relationship description. Considering objects with high stacking stability can be processed together if necessary, we introduce an elaborate decision-making planner based on Partially Observable Markov Decision Process (POMDP), which leverages observations and generates the least grasp-consuming decision chain with robustness and is suitable for simultaneously specifying multiple targets. To verify our work, we set the scene to the dining table and augment REGRAD dataset for network training. Experiments show that our method effectively generates grasping decisions that conform to human requirements, and improves the implementation efficiency compared with existing methods on the basis of guaranteeing success rate.</td>
                <td>Training, Stacking, Decision making, Grasping, Markov processes, Prediction algorithms, Stability analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342196&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342196&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Task-Oriented Grasp Prediction with Visual-Language Inputs</td>
                <td>C. Tang, D. Huang, L. Meng, W. Liu and H. Zhang</td>
                <td>2023</td>
                <td>To perform household tasks, assistive robots receive commands in the form of user language instructions for tool manipulation. The initial stage involves selecting the intended tool (i.e., object grounding) and grasping it in a task-oriented manner (i.e., task grounding). Nevertheless, prior researches on visual-language grasping (VLG) focus on object grounding, while disregarding the fine-grained impact of tasks on object grasping. Task-incompatible grasping of a tool will inevitably limit the success of subsequent manipulation steps. Motivated by this problem, this paper proposes GraspCLIP, which addresses the challenge of task grounding in addition to object grounding to enable task-oriented grasp prediction with visual-language inputs. Evaluation on a custom dataset demonstrates that GraspCLIP achieves superior performance over established baselines with object grounding only. The effectiveness of the proposed method is further validated on an assistive robotic arm for grasping previously unseen kitchen tools given the task specification. Our presentation video is available at: https://www.youtube.com/watch?v=e1wfYQPeAXU.</td>
                <td>Grounding, Grasping, Manipulators, Assistive robots, 6-DOF, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342268&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342268&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning to Grasp Clothing Structural Regions for Garment Manipulation Tasks</td>
                <td>W. Chen, D. Lee, D. Chappell and N. Rojas</td>
                <td>2023</td>
                <td>When performing cloth-related tasks, such as garment hanging, it is often important to identify and grasp certain structural regions—a shirt's collar as opposed to its sleeve, for instance. However, due to cloth deformability, these manipulation activities, which are essential in domestic, health care, and industrial contexts, remain challenging for robots. In this paper, we focus on how to segment and grasp structural regions of clothes to enable manipulation tasks, using hanging tasks as case study. To this end, a neural network-based perception system is proposed to segment a shirt's collar from areas that represent the rest of the scene in a depth image. With a 10-minute video of a human manipulating shirts to train it, our perception system is capable of generalizing to other shirts regardless of texture as well as to other types of collared garments. A novel grasping strategy is then proposed based on the segmentation to determine grasping pose. Experiments demonstrate that our proposed grasping strategy achieves 92%, 80%, and 50% grasping success rates with one folded garment, one crumpled garment and three crumpled garments, respectively. Our grasping strategy performs considerably better than tested baselines that do not take into account the structural nature of the garments. With the proposed region segmentation and grasping strategy, challenging garment hanging tasks are successfully implemented using an open-loop control policy. Supplementary material is available at https://sites.google.com/view/garment-hanging</td>
                <td>Image segmentation, Service robots, Clothing, Grasping, Medical services, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342086&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342086&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>External Sensor-Less in-Hand Object Position Manipulation for an Under-Actuated Hand Using Data-Driven-Based Methods to Compensate for the Nonlinearity of Self-Locking Mechanism</td>
                <td>H. T. L. Doan, H. Arita and K. Tahara</td>
                <td>2023</td>
                <td>Dexterous manipulation using an under-actuated hand has been a challenging task due to its non-linear dynamical characteristics. For a linkage-based under-actuated hand designed to be used to grasp and manipulate large, heavy, and rigid objects stably, precision grasping is necessary, which makes the task even more difficult to deal with. While approaches based on external sensors have been introduced throughout the years, to create a robotic hand that can be used for various tasks in unstructured environments, this paper takes the standpoint that control techniques that do not fully depend on utilizing additional sensing elements need to be further developed. This paper applies the hybrid method using analytics models and data-driven-based approaches to analyze internal sensors' data during the operation of the robot and introduces novel data-driven-based techniques to compensate for the limitations of controlling a linkage-based under-actuated hand with a self-locking mechanism. Then, a within-hand object position manipulation framework with proposed methodologies is presented and experimented with to show its effectiveness.</td>
                <td>Force, Estimation, Grasping, Sensor phenomena and characterization, Robot sensing systems, Real-time systems, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341517&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341517&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Contact-Aware Shaping and Maintenance of Deformable Linear Objects With Fixtures</td>
                <td>K. Chen et al.</td>
                <td>2023</td>
                <td>Studying the manipulation of deformable linear objects has significant practical applications in industry, including car manufacturing, textile production, and electronics automation. However, deformable linear object manipulation poses a significant challenge in developing planning and control algorithms, due to the precise and continuous control required to effectively manipulate the deformable nature of these objects. In this paper, we propose a new framework to control and maintain the shape of deformable linear objects with two robot manipulators utilizing environmental contacts. The framework is composed of a shape planning algorithm which automatically generates appropriate positions to place fixtures, and an object-centered skill engine which includes task and motion planning to control the motion and force of both robots based on the object status. The status of the deformable linear object is estimated online utilizing visual as well as force information. The framework manages to handle a cable routing task in real-world experiments with two Panda robots and especially achieves contact-aware and flexible clip fixing with challenging fixtures.</td>
                <td>Visualization, Shape, Fixtures, Force, Routing, Robot sensing systems, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341726&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341726&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Evaluation of Action Segmentation Algorithms on Bimanual Manipulation Datasets</td>
                <td>A. Meixner, F. Krebs, N. Jaquier and T. Asfour</td>
                <td>2023</td>
                <td>Humans naturally execute many everyday manipulation actions with both arms simultaneously. Similarly, endowing robots with bimanual manipulation task models is key to efficiently perform complex manipulation tasks. To do so, a promising approach is to learn a library of task models from human demonstrations. However, this requires human motions to be meaningfully segmented. In this paper, we propose to segment the motion of each hand individually to account for different bimanual coordination patterns and provide a thorough evaluation of state-of-the-art segmentation algorithms on bimanual manipulation datasets. In particular, we compare segmentation algorithms at trajectory and semantic level with hierarchical algorithms. Moreover, our evaluation extensively studies the performances of various segmentation algorithms over a novel extension of the KIT Bimanual Manipulation Dataset featuring ~ 176 minutes of human motion recordings in household scenarios.</td>
                <td>Motion segmentation, Robot kinematics, Semantics, Libraries, Trajectory, Recording, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341956&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341956&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SoftGPT: Learn Goal-Oriented Soft Object Manipulation Skills by Generative Pre-Trained Heterogeneous Graph Transformer</td>
                <td>J. Liu, Z. Li, W. Lin, S. Calinon, K. C. Tan and F. Chen</td>
                <td>2023</td>
                <td>Soft object manipulation tasks in domestic scenes pose a significant challenge for existing robotic skill learning techniques due to their complex dynamics and variable shape characteristics. Since learning new manipulation skills from human demonstration is an effective way for robot applications, developing prior knowledge of the representation and dynamics of soft objects is necessary. In this regard, we propose a pretrained soft object manipulation skill learning model, namely SoftGPT, that is trained using large amounts of exploration data, consisting of a three-dimensional heterogeneous graph representation and a GPT-based dynamics model. For each downstream task, a goal-oriented policy agent is trained to predict the subsequent actions, and SoftGPT generates the consequences of these actions. Integrating these two approaches establishes a thinking process in the robot's mind that provides rollout for facilitating policy learning. Our results demonstrate that leveraging prior knowledge through this thinking process can efficiently learn various soft object manipulation skills, with the potential for direct learning from human demonstrations.</td>
                <td>Solid modeling, Shape, Transformers, Data models, Task analysis, Robots, Manipulator dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341846&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341846&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Geometric Fault-Tolerant Control of Quadrotors in Case of Rotor Failures: An Attitude Based Comparative Study</td>
                <td>J. Yeom, G. Li and G. Loianno</td>
                <td>2023</td>
                <td>The ability of aerial robots to operate in the presence of failures is crucial in various applications that demand continuous operations, such as surveillance, monitoring, and inspection. In this paper, we propose a fault-tolerant control strategy for quadrotors that can adapt to single and dual complete rotor failures. Our approach augments a classic geometric tracking controller on $S{O}(3)\times \mathbb{R}^{3}$ to accommodate the effects of rotor failures. We provide an in-depth analysis of several attitude error metrics to identify the most appropriate design choice for fault-tolerant control strategies. To assess the effectiveness of these metrics, we evaluate trajectory tracking accuracies. Simulation results demonstrate the performance of the proposed approach.</td>
                <td>Measurement, Fault tolerance, Attitude control, Trajectory tracking, Surveillance, Simulation, Fault tolerant systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341669&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341669&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>UAV-Based Trilateration System for Localization and Tracking of Radio-Tagged Flying Insects: Development and Field Evaluation</td>
                <td>J. Pak, B. Kim, C. Ju, S. H. You and H. I. Son</td>
                <td>2023</td>
                <td>As the interest in ecosystem protection increases, many researchers are focusing on tracking flying insects to preserve biodiversity. Invasive alien species (IAS) such as Vespa velutina nigrithorax require extensive consideration in this regard owing to size and weight limitations. In this paper, we propose and experimentally validate an unmanned aerial vehicle (UAV)-based trilateration system for localization and tracking of flying insects. The proposed tracking algorithm is based on the use of multiple omnidirectional antennas, and it improves the tracking accuracy and shortens the tracking time compared to those of the existing unidirectional antennas. In addition, we reduce the measurement error and noise by applying a finite impulse response (FIR) filter to the received signal strength (RSS) emitted from the radio-telemetry transmitter. A field experiment involving the proposed trilateration system was conducted in an apiary. This experiment was divided into three sub-experiments as follows: 1) behavior experiment, 2) ground truth experiment, and 3) localization and tracking experiment. The proposed system contributes to the research on UAV-based tracking systems that respond to the speed of spreading of invasive V. velutina.</td>
                <td>Location awareness, Measurement errors, Target tracking, Finite impulse response filters, Insects, Radio transmitters, Omnidirectional antennas</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341725&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341725&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Canfly: A Can-Sized Autonomous Mini Coaxial Helicopter</td>
                <td>N. Pan, R. Jin, C. Xu and F. Gao</td>
                <td>2023</td>
                <td>The development of autonomous rotary-wing UAVs has shown an evident tendency in miniaturization. However, the side effects brought by miniaturization, such as decreased load capability, shorter flight duration and reduced autonomous ability, seriously hinder its process. In this paper, we first investigate the configurations of different rotary-wing aircraft and optimize the configuration selection. Afterward, with several elaborate mechanisms contributing to the miniaturization, we present the hardware design and control strategy of a mini coaxial helicopter, which is 62% smaller than the state-of-the-art autonomous mini quadrotor so far in collision area [1]. Meanwhile, abundant experiments reveal that it achieves impressive traversability and is capable of conducting autonomous tasks in unknown dense scenarios, while maintaining satisfactory performance regarding loadability and flight duration.</td>
                <td>Propellers, Atmospheric modeling, Stacking, Aerodynamics, Controllability, Hardware, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341490&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341490&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Aerial Manipulator Systems Gain a New Skill: Achieve Contact-based Landing on a Mobile Platform</td>
                <td>X. Meng, Y. He, H. Xi, J. Han and A. Song</td>
                <td>2023</td>
                <td>This paper studies a novel application of an aerial manipulator (AM)-the contact-based landing on a mobile platform. An AM is inherently unstable, under-actuated, and usually loses some DOFs while contacting environments. Meanwhile, the AM's flight state is susceptible to uncertain movements of the mobile platform, such as acceleration, sudden stopping, and reversing. To accomplish the contact-based landing mission, a robust controller is first designed to maintain a steady contact-based flight. Then a hierarchical control framework is applied, integrating the controllers in free-flight and restricted-flight stages. An AM and a mobile platform are developed for contact-based flight experiments. The proposed scheme is reliable and has good repeatability in experiments. To the best of our knowledge, this is the first time an AM has been implemented to conduct a contact-based landing, which is also an innovative landing approach for rotorcraft UAVs.</td>
                <td>Trajectory planning, Laboratories, Force, Manipulators, Autonomous aerial vehicles, Reliability, State estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342395&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342395&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>AOSoar: Autonomous Orographic Soaring of a Micro Air Vehicle</td>
                <td>S. Hwang, B. D. W. Remes and G. C. H. E. De Croon</td>
                <td>2023</td>
                <td>Utilizing wind hovering techniques of soaring birds can save energy expenditure and improve the flight endurance of micro air vehicles (MAVs). Here, we present a novel method for fully autonomous orographic soaring without a priori knowledge of the wind field. Specifically, we devise an Incremental Nonlinear Dynamic Inversion (INDI) controller with control allocation, adapting it for autonomous soaring. This allows for both soaring and the use of the throttle if necessary, without changing any gain or parameter during the flight. Furthermore, we propose a simulated-annealing-based optimization method to search for soaring positions. This enables for the first time an MAV to autonomously find a feasible soaring position while minimizing throttle usage and other control efforts. Autonomous orographic soaring was performed in the wind tunnel. The wind speed and incline of a ramp were changed during the soaring flight. The MAV was able to perform autonomous orographic soaring for flight times of up to 30 minutes. The mean throttle usage was only 0.25% for the entire soaring flight, whereas normal powered flight requires 38%. Also, it was shown that the MAV can find a new soaring spot when the wind field changes during the flight.</td>
                <td>Trajectory planning, Wind tunnels, Wind speed, Search methods, Optimization methods, Manuals, Autonomous aerial vehicles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341699&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341699&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Error-State Kalman Filter Based External Wrench Estimation for MAVs Under a Cascaded Architecture</td>
                <td>Y. Yin, Q. Yang and H. Fang</td>
                <td>2023</td>
                <td>In many applications such as aerial transportation, delivery, and manipulation, it is essential to know the external wrench exerted on multirotor aerial vehicles precisely. This paper presents an algorithm to estimate external wrench using a rotor speed measurement unit, an inertial measurement unit and a motion capture system. Under a cascaded architecture containing two sub-systems, one error-state Kalman Filter is designed to estimate velocity and attitude and eliminate the bias of the measurement from the inertial measurement unit, the other error-state Kalman Filter is designed to estimate the external wrench. Observability of the two estimation subsystems is verified by the Lie derivative method. The proposed algorithm has been tested in simulations and real-world experiments, which demonstrates its superiority in providing real-time and accurate external wrench estimation.</td>
                <td>Measurement units, Estimation, Transportation, Rotors, Computer architecture, Inertial navigation, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342358&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342358&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Minimally Actuated Tiltrotor for Perching and Normal Force Exertion</td>
                <td>D. Lee, S. Hwang, C. Kim, S. J. Lee and H. J. Kim</td>
                <td>2023</td>
                <td>This study presents a new hardware design and control of a minimally actuated 5 control degrees of freedom (CDoF) quadrotor-based tiltrotor. The proposed tiltrotor possesses several characteristics distinct from those found in existing works, including: 1) minimal number of actuators for 5 CDoF, 2) large margin to generate interaction force during aerial physical interaction (APhI), and 3) no mechanical obstruction in thrust direction rotation. Thanks to these properties, the proposed tiltrotor is suitable for perching-enabled APhI since it can hover parallel to an arbitrarily oriented surface and can freely adjust its thrust direction. To fully control the 5-CDoF of the designed tiltrotor, we construct an asymptotically stabilizing controller with stability analysis. The proposed tiltrotor design and controller are validated in experiments where the first two experiments of x, y position tracking and pitch tracking show controllability of the added CDoF compared to a conventional quadrotor. Finally, the last experiment of perching and cart pushing demonstrates the proposed tiltrotor's applicability to perching-enabled APhI.</td>
                <td>Actuators, Force, Controllability, Stability analysis, Hardware, Intelligent robots, Quadrotors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341910&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341910&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Nonlinear Model Predictive Control for Cooperative Transportation and Manipulation of Cable Suspended Payloads with Multiple Quadrotors</td>
                <td>G. Li and G. Loianno</td>
                <td>2023</td>
                <td>Autonomous Micro Aerial Vehicles (MAVs) such as quadrotors equipped with manipulation mechanisms have the potential to assist humans in tasks such as construction and package delivery. Cables are a promising option for manipulation mechanisms due to their low weight, low cost, and simple design. However, designing control and planning strategies for cable mechanisms presents challenges due to indirect load actuation, nonlinear configuration space, and highly coupled system dynamics. In this paper, we propose a novel Nonlinear Model Predictive Control (NMPC) method that enables a team of quadrotors to manipulate a rigid-body payload in all 6 degrees of freedom via suspended cables. Our approach can concurrently exploit, as part of the receding horizon optimization, the available mechanical system redundancies to perform additional tasks such as inter-robot separation and obstacle avoidance while respecting payload dynamics and actuator constraints. To address real-time computational requirements and scalability, we employ a lightweight state vector parametrization that includes only payload states in all six degrees of freedom. This also enables the planning of trajectories on the SE (3) manifold load configuration space, thereby also reducing planning complexity. We validate the proposed approach through simulation and real-world experiments.</td>
                <td>Scalability, Termination of employment, Aerospace electronics, Planning, Task analysis, Vehicle dynamics, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341785&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341785&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Image-Based Visual Servo Control for Aerial Manipulation Using a Fully-Actuated UAV</td>
                <td>G. He, Y. Jangir, J. Geng, M. Mousaei, D. Bai and S. Scherer</td>
                <td>2023</td>
                <td>Using Unmanned Aerial Vehicles (UAVs) to per-form high-altitude manipulation tasks beyond just passive visual application can reduce the time, cost, and risk of human workers. Prior research on aerial manipulation has relied on either ground truth state estimate or GPS/total station with some Simultaneous Localization and Mapping (SLAM) algorithms, which may not be practical for many applications close to infrastructure with degraded GPS signal or featureless environments. Visual servo can avoid the need to estimate robot pose. Existing works on visual servo for aerial manipulation either address solely end-effector position control or rely on precise velocity measurement and pre-defined visual visual marker with known pattern. Furthermore, most of previous work used under-actuated UAVs, resulting in complicated mechanical and hence control design for the end-effector. This paper develops an image-based visual servo control strategy for bridge maintenance using a fully-actuated UAV. The main components are (1) a visual line detection and tracking system, (2) a hybrid impedance force and motion control system. Our approach does not rely on either robot pose/velocity estimation from an external localization system or pre-defined visual markers. The complexity of the mechanical system and controller architecture is also minimized due to the fully-actuated nature. Experiments show that the system can effectively execute motion tracking and force holding using only the visual guidance for the bridge painting. To the best of our knowledge, this is one of the first studies on aerial manipulation using visual servo that is capable of achieving both motion and force control without the need of external pose/velocity information or pre-defined visual guidance.</td>
                <td>Bridges, Visualization, Simultaneous localization and mapping, Tracking, Force, Autonomous aerial vehicles, End effectors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342145&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342145&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DroNeRF: Real-Time Multi-Agent Drone Pose Optimization for Computing Neural Radiance Fields</td>
                <td>D. Patel, P. Pham and A. Bera</td>
                <td>2023</td>
                <td>We present a novel optimization algorithm called DroNeRF for the autonomous positioning of monocular camera drones around an object for real-time 3D reconstruction using only a few images. Neural Radiance Fields, or NeRF, is a novel view synthesis technique used to generate new views of an object or scene from a set of input images. Using drones in conjunction with NeRF provides a unique and dynamic way to generate novel views of a scene, especially with limited scene capabilities of restricted movements. Our approach focuses on calculating optimized pose for individual drones while solely depending on the object geometry without using any external localization system. The unique camera positioning during the data capturing phase significantly impacts the quality of the 3D model. To evaluate the quality of our generated novel views, we compute different perceptual metrics like the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). Our work demonstrates the benefit of using an optimal placement of various drones with limited mobility to generate perceptually better results.</td>
                <td>Measurement, Location awareness, Solid modeling, Three-dimensional displays, PSNR, Cameras, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342420&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342420&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Generation of Time-Varying Impedance Attacks Against Haptic Shared Control Steering Systems</td>
                <td>A. Mohammadi and H. Malik</td>
                <td>2023</td>
                <td>The safety-critical nature of vehicle steering is one of the main motivations for exploring the space of possible cyber-physical attacks against the steering systems of modern vehicles. This paper investigates the adversarial capabilities for destabilizing the interaction dynamics between human drivers and vehicle haptic shared control (HSC) steering systems. In contrast to the conventional robotics literature, where the main objective is to render the human-automation interaction dynamics stable by ensuring passivity, this paper takes the exact opposite route. In particular, to investigate the damaging capabilities of a successful cyber-physical attack, this paper demonstrates that an attacker who targets the HSC steering system can destabilize the interaction dynamics between the human driver and the vehicle HSC steering system through synthesis of time-varying impedance profiles. Specifically, it is shown that the adversary can utilize a properly designed non-passive and time-varying adversarial impedance target dynamics, which are fed with a linear combination of the human driver and the steering column torques. Using these target dynamics, it is possible for the adversary to generate in realtime a reference angular command for the driver input device and the directional control steering assembly of the vehicle. Furthermore, it is shown that the adversary can make the steering wheel and the vehicle steering column angular positions to follow the reference command generated by the time-varying impedance target dynamics using proper adaptive control strategies. Numerical simulations demonstrate the effectiveness of such time-varying impedance attacks, which result in a non-passive and inherently unstable interaction between the driver and the HSC steering system.</td>
                <td>Space vehicles, Steering systems, Wheels, Control systems, Numerical simulation, Space exploration, Haptic interfaces</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342459&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342459&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Haptic Dataset Augmentation with Subjective QoE Labels using Conditional Generative Adversarial Network</td>
                <td>Z. Wang, X. Xu, D. Yang, Z. Wang, S. Shtaierman and E. Steinbach</td>
                <td>2023</td>
                <td>This paper proposes a novel Generative Adversarial Network (GAN)-based strategy to augment subjective haptic Quality of Experience (QoE) datasets for bilateral teleoperation with haptic feedback without conducting time-consuming subjective experiments. In our previous work, we proposed a multi-assessment fusion approach to predict subjective haptic quality using a collection of objective metrics. This method requires a sufficiently large haptic dataset with QoE labels. The proposed generative approach automatically expands the existing haptic quality dataset by combining a modified conditional GAN (CGAN) and Style GAN (StyleGAN) architecture. The most important feature of our method is that it learns from the labeled training data and focuses on synthesizing signals with artifacts according to new input labels containing the QoE score, time delay, control method, and data reduction information. Extensive experiments are conducted to validate the suitability of the expanded dataset. The results show that our approach is able to generate new data, which match the label and signal distribution of the original data with categorical rank and linear correlation of over 0.85.</td>
                <td>Training, Measurement, Correlation, Training data, Generative adversarial networks, Real-time systems, Haptic interfaces</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341967&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341967&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Physically Based Deformable Model with Haptic Feedback for Real-Time Robotic Surgery Simulation</td>
                <td>S. Heredia, H. Masuda, A. Miyamoto and Y. Kuroda</td>
                <td>2023</td>
                <td>Surgical simulators have been under development for years, formerly intended for surgical training and recently applied for training machine learning models. These systems often employ computationally efficient methods such as mass-spring models or position-based dynamics that prioritize real-time execution over physical accuracy, while the use of the finite element method (FEM) has been limited due its computational cost. In consequence, there has been little improvement in the accuracy of the deformable models and the haptics, relying on hand-tuned stiffness parameters, and empirical solutions to estimate the contact forces. To solve these limitations, we propose to develop a new surgical simulator for laparoscopic cholecystectomy employing the extended position-based dynamics method in conjunction with FEM to compute physically based deformation and obtain accurate contact forces during the collision response. While dense organs like the liver are modeled using tetrahedrons and addressed the element inversion problem in FEM, we propose to simulate the gallbladder as a closed elastic membrane using two-dimensional FEM elements with volume preserving constraints to model the inner incompressible fluid. Because continuous position and contact force measurement on real materials to verify the simulation is challenging, we employ a bilateral robotic system equipped with Fiber Bragg Grating-based force sensors.</td>
                <td>Deformable models, Training, Solid modeling, Force measurement, Computational modeling, Machine learning, Robot sensing systems, Simulation and Animation, Haptics and Haptic Interfaces, Surgical Robotics, Laparoscopy</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341694&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341694&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Contact-Based State Estimation for Assembly Tasks</td>
                <td>J. Pankert and M. Hutter</td>
                <td>2023</td>
                <td>Robotic object manipulation requires knowledge of the environment's state. In particular, the object poses of fixed elements in the environment relative to the robot and the in-hand poses of grasped objects are of interest. For insertion tasks with tight tolerances, the accuracy of vision systems to estimate the object and in-hand pose is not high enough. This work proposes a state estimation system that delivers precise estimates for both estimation problems. It uses contact detections and the precise forward kinematics that robot arms provide thanks to their high-resolution joint encoders. We propose a reinforcement-learning-based exploration strategy that decides how the robot should engage with the environment to reduce state uncertainty. The system is evaluated in several simulation and hardware experiments. We show that the learned policy can propose meaningful actions for object localization. In hardware experiments with precision-milled objects, sub-millimeter accuracy is achieved for the in-hand pose estimation task. With objects relevant to industrial tasks, i.e., a melting fuse and a fuse box, millimeter-level accuracy can be reached for both in-hand pose estimation and fixed object localization. In an integrated experiment, we show how a robot grasps a fuse, estimates the in-hand pose, and inserts it into a fuse box.</td>
                <td>Location awareness, Solid modeling, Uncertainty, Fuses, Service robots, Pose estimation, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342219&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342219&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Evaluation of a 7-DoFs Robotic Manipulator as Haptic Interface During Planar Reaching Tasks</td>
                <td>A. Noccaro, S. Buscaglione, M. Pinardi, G. Di Pino and D. Formica</td>
                <td>2023</td>
                <td>In this work, we evaluated the suitability of using a 7 degrees of freedom robotic manipulator as a planar haptic interface for studies in motor neuroscience. In particular, we assessed to what extent it can measure human movement and forces without applying undesired perturbations. To this aim, we evaluated the amount of perturbation exerted by the robot during planar reaching movements when controlled to be as transparent as possible in the 2D task space, through an impedance control. Two planar specular configurations of the robot were tested, namely G1 and G2, which differ in the position of the “elbow joint” in the workspace. For both configurations, we estimated the inertial ellipsoids and simulated the forces for human-like forward movements. Performance was then experimentally assessed on 8 healthy participants, in 15 different positions in the workspace. The average handpath perturbation decreased and settled to 6 mm after 2 minutes of interaction. Interaction forces resulted specular for G1 and G2, with mean values below 5 N. Overall, the robotic manipulator resulted suitable for studies on planar reaching movements in both configurations, with a preference for the G1 configuration due to its symmetrical distribution of trajectory deviations, which anyway remain well below 1 cm for movements of 15 cm.</td>
                <td>Neuroscience, Perturbation methods, Manipulators, Particle measurements, Haptic interfaces, Trajectory, Motion measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342470&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342470&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Soft, Modular, Shape-Changing Displays with Hyperelastic Bubble Arrays</td>
                <td>M. R. Devlin, T. Liu, M. Zhu, N. S. Usevitch, N. Colonnese and A. H. Memar</td>
                <td>2023</td>
                <td>Incorporating compliance into shape-changing displays can improve their wearability and actuation modalities. While recent advances in soft actuators highlight promising paths for soft shape-changing displays, these displays currently face some practical challenges of device failure and limited actuator displacement. A monolithic fabrication processes means the device is challenging to repair, for a single point of failure often renders the whole device ineffective. We have leveraged a modular hyperelastic bubble array layer to create a soft shape-changing skin. The modularity of this device allows for rapid repair of individual bubbles and fast prototyping, and the spherical, hyperelastic actuators enable an increase in degrees of freedom due to bubble-to-bubble interactions. Furthermore, we present a forward kinematic description of our device, incorporating these bubble-to-bubble interactions and the nonlinear instabilities unique to hyperelastic actuator inflation. We demonstrate the utility of this soft shape-changing skin as a haptic display that can be worn comfortably or applied to passive interactive objects such as a computer mouse.</td>
                <td>Fabrication, Actuators, Kinematics, Maintenance engineering, Soft robotics, Skin, Mice</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341591&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341591&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Soft Optical Sensor and Haptic Feedback System for Remote and Robot-Assisted Palpation</td>
                <td>A. Gerald, J. Ye, R. Batliwala, P. Hsu, J. Pang and S. Russo</td>
                <td>2023</td>
                <td>Robotic palpation shows significant potential to improve the accuracy and speed of tumor identification. How-ever, robotic palpation mechanisms often lack haptic feedback, making it difficult for the surgeon to identify variations in tissue stiffness. This paper presents a soft optical sensor integrated with a wearable haptic glove for tumor detection during robotic palpation. The sensor contains an array of optical waveguides that can detect the presence of tumors embedded within a tissue phantom. Detection of a tumor results in an optical loss from the waveguide signal, triggering proportional inflation of the soft microfluidic actuators in the glove. The glove consists of four modular actuators placed at the fingertips, each corresponding to a sensing location on the waveguide array. The inflation of each actuator is proportional to the incident loss on the palpation sensor array, which is dependent on tumor depth. Thus, the glove is capable of alerting the user to the location of tumors during remote palpation.</td>
                <td>Optical losses, Actuators, Optical feedback, Optical device fabrication, Robot sensing systems, Haptic interfaces, Optical sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341754&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341754&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TIMS: A Tactile Internet-Based Micromanipulation System with Haptic Guidance for Surgical Training</td>
                <td>J. Lin et al.</td>
                <td>2023</td>
                <td>Microsurgery involves the dexterous manipulation of delicate tissue or fragile structures, such as small blood vessels and nerves, under a microscope. To address the limitations of imprecise manipulation of human hands, robotic systems have been developed to assist surgeons in performing complex microsurgical tasks with greater precision and safety. However, the steep learning curve for robot-assisted microsurgery (RAMS) and the shortage of well-trained surgeons pose significant challenges to the widespread adoption of RAMS. Therefore, the development of a versatile training system for RAMS is necessary, which can bring tangible benefits to both surgeons and patients. In this paper, we present a Tactile Internet-Based Micromanipulation System (TIMS) based on a ROS-Django web-based architecture for microsurgical training. This system can provide tactile feedback to operators via a wearable tactile display (WTD), while real-time data is transmitted through the internet via a ROS-Django framework. In addition, TIMS integrates haptic guidance to ‘guide’ the trainees to follow a desired trajectory provided by expert surgeons. Learning from demonstration based on Gaussian Process Regression (GPR) was used to generate the desired trajectory. We conducted user studies to verify the effectiveness of our proposed TIMS, comparing users' performance with and without tactile feedback and/or haptic guidance. For more details of this project, please view our website: https://sites.google.com/view/viewtims/home.</td>
                <td>Training, Microscopy, Random access memory, Tactile sensors, Microsurgery, Real-time systems, Haptic interfaces</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341980&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341980&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Teleoperated MR-Safe Haptic System for Magnetic Resonance Imaging-Guided Prostate Needle Biopsies</td>
                <td>E. Mendoza and J. P. Whitney</td>
                <td>2023</td>
                <td>Real-time magnetic resonance imaging (MRI) in-terventions are significantly impacted by material compatibility problems and size constraints in the MRI bore. Limited physi-cian access to patients within the bore of the MRI necessitates iterative positioning and imaging, which prolongs the duration of the procedure and increases patient risk. We present a passive MR-safe haptic teleoperation device for prostate needle biopsy inside an MRI machine. The device uses a low-friction hydrostatic transmission based on paired rolling diaphragm actuators, linear and rotary. The device has two degrees of freedom, allowing needle insertion and rotation. The robot produces negligible MR imaging artifacts, has effective positioning tracking, and can reliably detect needle punctures in the clinically relevant range. We describe the design components, system transparency, and perform a needle insertion test.</td>
                <td>Performance evaluation, Magnetic resonance imaging, Biopsy, Magnetic resonance, Needles, Real-time systems, Haptic interfaces, Haptics and Haptic Interfaces, Medical Robots and Systems, Telerobotics and Teleoperation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342113&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342113&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Symmetry-Based Modeling and Hybrid Orientation-Force Control of Wearable Cutaneous Haptic Device</td>
                <td>S. Lee, H. Kim and D. Lee</td>
                <td>2023</td>
                <td>We propose novel symmetry-based modeling and hybrid orientation-force control frameworks for cutaneous haptic device (CHD) to generate precise three degree-of-freedom (DoF) contact force on the fingertip robustly against user variability. The CHD hardware is designed in a form of an underactuated cable-driven parallel mechanism, with springs placed along the tendon to stabilize the pose. We analyze the kinematics of the CHD and propose a pose estimator by exploiting the symmetrical nature of the mechanism. We then devise a hybrid orientation-force controller to track the direction and magnitude of the desired contact force simultaneously in a feedback manner for control accuracy and robustness. We also adopt a tension regulator to mitigate friction effect during the actuation. Experimental validation and demonstration show the efficacy of the CHD with our proposed estimation and control framework.</td>
                <td>Performance evaluation, Regulators, Force, Kinematics, Systems modeling, Hybrid power systems, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341859&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341859&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning-Augmented Model-Based Planning for Visual Exploration</td>
                <td>Y. Li, A. Debnath, G. J. Stein and J. Košecká</td>
                <td>2023</td>
                <td>We consider the problem of time-limited robotic exploration in previously unseen environments where exploration is limited by a predefined amount of time. We propose a novel exploration approach using learning-augmented model-based planning. We generate a set of sub goals associated with frontiers on the current map and derive a Bellman Equation for exploration with these subgoals. Visual sensing and advances in semantic mapping of indoor scenes are exploited for training a deep convolutional neural network to estimate properties associated with each frontier: the expected unobserved area beyond the frontier and the expected time steps (discretized actions) required to explore it. The proposed model-based planner is guaranteed to explore the whole scene if time permits. We thoroughly evaluate our approach on a large-scale pseudo-realistic indoor dataset (Matterport3D) with the Habitat simulator. We compare our approach with classical and more recent RL-based exploration methods. Our approach surpasses the greedy strategies by 2.1% and the RL-based exploration methods by 8.4% in terms of coverage.</td>
                <td>Training, Visualization, Three-dimensional displays, Semantics, Robot sensing systems, Mathematical models, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341773&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341773&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DMCL: Robot Autonomous Navigation via Depth Image Masked Contrastive Learning</td>
                <td>J. Jiang, P. Li, X. Lv and Y. Yang</td>
                <td>2023</td>
                <td>Achieving high performance in deep reinforcement learning relies heavily on the ability to obtain good state representations from pixel inputs. However, learning an observation-space-to-action-space mapping from high-dimensional inputs is challenging in reinforcement learning, particularly when dealing with consecutive depth images as input states. In addition, we observe that the consecutive inputs of depth images are highly correlated for the autonomous navigation of a mobile robot, which inspires us to capture temporal correlations between consecutive inputs and infer scene change relationships. To this end, we propose a novel end-to-end robot vision navigation method dubbed DMCL, which obtains good spatial-temporal state representation via Depth image Masked Contrastive Learning. It reconstructs the latent representation from consecutive depth images masked in both spatial and temporal dimensions, resulting in a complete environment state representation. To obtain the optimal navigation policy, we leverage the Soft Actor-Critic reinforcement learning in conjunction with the above representation learning. Extensive experiments demonstrate that the proposed DMCL outperforms representative state-of-the-art methods. The source code will be made publicly available.</td>
                <td>Representation learning, Deep learning, Navigation, Source coding, Robot vision systems, Reinforcement learning, Mobile robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341836&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341836&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Image-based Regularization for Action Smoothness in Autonomous Miniature Racing Car with Deep Reinforcement Learning</td>
                <td>G. Cao et al.</td>
                <td>2023</td>
                <td>Deep reinforcement learning has achieved signif-icant results in low-level controlling tasks. However, for some applications like autonomous driving and drone flying, it is difficult to control behavior stably since the agent may suddenly change its actions which often lowers the controlling sys-tem's efficiency, induces excessive mechanical wear, and causes uncontrollable, dangerous behavior to the vehicle. Recently, a method called conditioning for action policy smoothness (CAPS) was proposed to solve the problem of jerkiness in low-dimensional features for applications such as quadrotor drones. To cope with high-dimensional features, this paper proposes image-based regularization for action smoothness (1-RAS) for solving jerky control in autonomous miniature car racing. We also introduce a control based on impact ratio, an adaptive regularization weight to control the smoothness constraint, called IR control. In the experiment, an agent with 1- RAS and IR control significantly improves the success rate from 59% to 95%. In the real-world-track experiment, the agent also outperforms other methods, namely reducing the average finish lap time, while also improving the completion rate even without real world training. This is also justified by an agent based on I-RAS winning the 2022 AWS DeepRacer Final Championship Cup.</td>
                <td>Deep learning, Training, Reinforcement learning, Robustness, Trajectory, Behavioral sciences, Automobiles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342029&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342029&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>UnLoc: A Universal Localization Method for Autonomous Vehicles using LiDAR, Radar and/or Camera Input</td>
                <td>M. Ibrahim, N. Akhtar, S. Anwar and A. Mian</td>
                <td>2023</td>
                <td>Localization is a fundamental task in robotics for autonomous navigation. Existing localization methods rely on a single input data modality or train several computational models to process different modalities. This leads to stringent computational requirements and sub-optimal results that fail to capitalize on the complementary information in other data streams. This paper proposes UnLoc, a novel unified neural modeling approach for localization with multi-sensor input in all weather conditions. Our multi-stream network can handle LiDAR, Camera and RADAR inputs for localization on demand, i.e., it can work with one or more input sensors, making it robust to sensor failure. UnLoc uses 3D sparse convolutions and cylindrical partitioning of the space to process LiDAR frames and implements ResNet blocks with a slot attention-based feature filtering module for the Radar and image modalities. We introduce a unique learnable modality encoding scheme to distinguish between the input sensor data. Our method is extensively evaluated on Oxford Radar RobotCar, ApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our technique. The dataset, results, and codes are available at https://github.com/IbrahimUWA/UnLoc</td>
                <td>Location awareness, Laser radar, Three-dimensional displays, Filtering, Radar, Radar imaging, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342046&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342046&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Complementarity-Based Switch-Fuse System for Improved Visual Place Recognition</td>
                <td>Maier and S. Ehsan</td>
                <td>2023</td>
                <td>Recently several fusion and switching based approaches have been presented to solve the problem of Visual Place Recognition. In spite of these systems demonstrating significant boost in VPR performance they each have their own set of limitations. The multi-process fusion systems usually involve employing brute force and running all available VPR techniques simultaneously while the switching method attempts to negate this practise by only selecting the best suited VPR technique for given query image. But switching does fail at times when no available suitable technique can be identified. An innovative solution would be an amalgamation of the two otherwise discrete approaches to combine their competitive advantages while negating their shortcomings. The proposed, Switch-Fuse system, is an interesting way to combine both the robustness of switching VPR techniques based on complementarity and the force of fusing the carefully selected techniques to significantly improve performance. Our system holds a structure superior to the basic fusion methods as instead of simply fusing all or any random techniques, it is structured to first select the best possible VPR techniques for fusion, according to the query image. The system combines two significant processes, switching and fusing VPR techniques, which together as a hybrid model substantially improve performance on all major VPR data sets illustrated using PR curves.</td>
                <td>Visualization, Force, Switches, Robustness, Data models, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341876&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341876&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RREx-BoT: Remote Referring Expressions with a Bag of Tricks</td>
                <td>G. A. Sigurdsson, J. Thomason, G. S. Sukhatme and R. Piramuthu</td>
                <td>2023</td>
                <td>Household robots operate in the same space for years. Such robots incrementally build dynamic maps that can be used for tasks requiring remote object localization. However, benchmarks in robot learning often test generalization through inference on tasks in unobserved environments. In an observed environment, locating an object is reduced to choosing from among all object proposals in the environment, which may number in the 100,000s. Armed with this intuition, using only a generic vision-language scoring model with minor modifications for 3d encoding and operating in an embodied environment, we demonstrate an absolute performance gain of 9.84% on remote object grounding above state of the art models for REVERIE and of 5.04% on FAO. When allowed to pre-explore an environment, we also exceed the previous state of the art pre-exploration method on REVERIE. Additionally, we demonstrate our model on a real-world TurtleBot platform, highlighting the simplicity and usefulness of the approach. Our analysis outlines a “bag of tricks” essential for accomplishing this task, from utilizing 3d coordinates and context, to gener-alizing vision-language models to large 3d search spaces.</td>
                <td>Training, Location awareness, Solid modeling, Three-dimensional displays, Grounding, Robot kinematics, Performance gain</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342093&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342093&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PLPL-VIO: A Novel Probabilistic Line Measurement Model for Point-Line-Based Visual-Inertial Odometry</td>
                <td>Z. Xu et al.</td>
                <td>2023</td>
                <td>Point and line features are complementary in Visual-Inertial Odometry (VIO) or Visual-Inertial Simultaneous Localization And Mapping (VI-SLAM) systems. The advantage of combining these two types of features relies on their proper weighting in the cost function, usually set by their uncertainty. Compared with point features, setting line segment endpoints' uncertainty with isotropic distribution is unreasonable. But the uncertainty of line feature observation, especially for the endpoints' uncertainty along the line, is difficult to set due to occlusion and fragmentation problems. In this article, we use infinite lines as the line feature observations and prove that the uncertainty of these observations is only related to the vertical uncertainty of the endpoints, thus avoiding setting the parallel uncertainty of the endpoints. Besides, we introduce a novel consistent measurement model for line features. Furthermore, for long-time constraints, we add 3D line segments into the state vector and derive how to update them properly. Finally, we construct a point-line-based VIO system that takes into account the uncertainty of line feature observations and the consistency of line feature measurements. The proposed VIO system is validated on two public datasets. The results show that the proposed method obtains the best accuracy compared with the state-of-the-art point-based VIO systems (OpenVINS, VINS-Mono), a point-line-based VIO system (PL-VINS), and a structural line-based system (StructVIO).</td>
                <td>Weight measurement, Measurement errors, Uncertainty, Three-dimensional displays, Simultaneous localization and mapping, Measurement uncertainty, Probabilistic logic</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342387&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342387&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Goal Audio-Visual Navigation Using Sound Direction Map</td>
                <td>H. Kondoh and A. Kanezaki</td>
                <td>2023</td>
                <td>Over the past few years, there has been a great deal of research on navigation tasks in indoor environments using deep reinforcement learning agents. Most of these tasks use only visual information in the form of first-person images to navigate to a single goal. More recently, tasks that simultaneously use visual and auditory information to navigate to the sound source and even navigation tasks with multiple goals instead of one have been proposed. However, there has been no proposal for a generalized navigation task combining these two types of tasks and using both visual and auditory information in a situation where multiple sound sources are goals. In this paper, we propose a new framework for this generalized task: multi-goal audio-visual navigation. We first define the task in detail, and then we investigate the difficulty of the multi-goal audio-visual navigation task relative to the current navigation tasks by conducting experiments in various situations. The research shows that multi-goal audio-visual navigation has the difficulty of the implicit need to separate the sources of sound. Next, to mitigate the difficulties in this new task, we propose a method named sound direction map (SDM), which dynamically localizes multiple sound sources in a learning-based manner while making use of past memories. Experimental results show that the use of SDM significantly improves the performance of multiple baseline methods, regardless of the number of goals.</td>
                <td>Degradation, Deep learning, Visualization, Source separation, Navigation, Reinforcement learning, Indoor environment</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341819&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341819&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Directed Real-World Learned Exploration</td>
                <td>Buysse et al.</td>
                <td>2023</td>
                <td>Automated Guided Vehicles (AGV) are omnipresent, and are able to carry out various kind of preprogrammed tasks. Unfortunately, a lot of manual configuration is still required in order to make these systems operational, and configuration needs to be re-done when the environment or task is changed. As an alternative to current inflexible methods, we employ a learning based method in order to perform directed exploration of a previously unseen environment. Instead of relying on handcrafted heuristic representations, the agent learns its own environmental representation through its embodiment. Our method offers loose coupling between the Reinforcement Learning (RL) agent, which is trained in simulation, and a separate, on real-world images trained task module. The uncertainty of the task module is used to direct the exploration behavior. As an example, we use a warehouse inventory task, and we show how directed exploration can improve the task performance through active data collection. We also propose a novel environment representation to efficiently tackle the sim2real gap in both sensing and actuation. We empirically evaluate the approach both in simulated environments and a real-world warehouse.</td>
                <td>Training, Learning systems, Uncertainty, Remotely guided vehicles, Navigation, Robot vision systems, Reinforcement learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341504&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341504&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Whom to Trust in Navigation: Dynamically Switching Between Classical and Neural Planning</td>
                <td>S. Dey, A. Sadek, G. Monaci, B. Chidlovskii and C. Wolf</td>
                <td>2023</td>
                <td>Navigation of terrestrial robots is typically addressed either with localization and mapping (SLAM) followed by classical planning on the dynamically created maps, or by machine learning (ML), often through end-to-end training with reinforcement learning (RL) or imitation learning (IL). Recently, modular designs have achieved promising results, and hybrid algorithms that combine ML with classical planning have been proposed. Existing methods implement these combinations with handcrafted functions, which cannot fully exploit the complementary nature of the policies and the complex regularities between scene structure and planning performance. Our work builds on the hypothesis that the strengths and weaknesses of neural planners and classical planners follow some regularities, which can be learned from training data, in particular from interactions. This is grounded on the assumption that, both, trained planners and the mapping algorithms underlying classical planning are subject to failure cases depending on the semantics of the scene and that this dependence is learnable: for instance, certain areas, objects or scene structures can be reconstructed easier than others. We propose a hierarchical method composed of a high-level planner dynamically switching between a classical and a neural planner. We fully train all neural policies in simulation and evaluate the method in both simulation and real experiments with a LoCoBot robot, showing significant gains in performance, in particular in the real environment. We also qualitatively conjecture on the nature of data regularities exploited by the high-level planner.</td>
                <td>Training, Three-dimensional displays, Simultaneous localization and mapping, Navigation, Heuristic algorithms, Semantics, Training data</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342308&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342308&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Deep Sensorimotor Policies for Vision-Based Autonomous Drone Racing</td>
                <td>J. Fu, Y. Song, Y. Wu, F. Yu and D. Scaramuzza</td>
                <td>2023</td>
                <td>The development of effective vision-based algorithms has been a significant challenge in achieving autonomous drones, which promise to offer immense potential for many real-world applications. This paper investigates learning deep sensorimotor policies for vision-based drone racing, which is a particularly demanding setting for testing the limits of an algorithm. Our method combines feature representation learning to extract task-relevant feature representations from high-dimensional image inputs with a learning-by-cheating framework to train a deep sensorimotor policy for vision-based drone racing. This approach eliminates the need for globally-consistent state estimation, trajectory planning, and handcrafted control design, allowing the policy to directly infer control commands from raw images, similar to human pilots. We conduct experiments using a realistic simulator and show that our vision-based policy can achieve state-of-the-art racing performance while being robust against unseen visual disturbances. Our study suggests that consistent feature embeddings are essential for achieving robust control performance in the presence of visual disturbances. The key to acquiring consistent feature embeddings is utilizing contrastive learning along with data augmentation. Video: https://youtu.be/AX_fcnW9yqE</td>
                <td>Robust control, Visualization, Trajectory planning, Feature extraction, Robot sensing systems, Robustness, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341805&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341805&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Magnetic Navigation Using Attitude-Invariant Magnetic Field Information for Loop Closure Detection</td>
                <td>Guay and J. R. Forbes</td>
                <td>2023</td>
                <td>Indoor magnetic fields are a combination of Earth's magnetic field and disruptions induced by ferromag-netic objects, such as steel structural components in buildings. As a result of these disruptions, pervasive in indoor spaces, mag-netic field data is often omitted from navigation algorithms in indoor environments. This paper leverages the spatially-varying disruptions to Earth's magnetic field to extract positional information for use in indoor navigation algorithms. The algorithm uses a rate gyro and an array of four magnetometers to estimate the robot's pose. Additionally, the magnetometer array is used to compute attitude-invariant measurements associated with the magnetic field and its gradient. These measurements are used to detect loop closure points. Experimental results indicate that the proposed approach can estimate the pose of a ground robot in an indoor environment within meter accuracy.</td>
                <td>Earth, Meters, Magnetic field measurement, Magnetometers, Indoor navigation, Indoor environment, Magnetic fields</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342466&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342466&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to Improve Visual Localization</td>
                <td>S. Hausler, S. Garg, P. Chakravarty, S. Shrivastava, A. Vora and M. Milford</td>
                <td>2023</td>
                <td>Most 6-DoF localization and SLAM systems use static landmarks but ignore dynamic objects because they cannot be usefully incorporated into a typical pipeline. Where dynamic objects have been incorporated, typical approaches have attempted relatively sophisticated identification and localization of these objects, limiting their robustness or general utility. In this research, we propose a middle ground, demonstrated in the context of autonomous vehicles, using dynamic vehicles to provide limited pose constraint information in a 6-DoF frame-by-frame PnP-RANSAC localization pipeline. We refine initial pose estimates with a motion model and propose a method for calculating the predicted quality of future pose estimates, triggered by whether or not the autonomous vehicle's motion is constrained by the relative frame-to-frame location of dynamic vehicles in the environment. Our approach detects and identifies suitable dynamic vehicles to define these pose constraints to modify a pose filter, resulting in improved recall across a range of localization tolerances from 0.25m to 5m, compared to a state-of-the-art baseline single image PnP method and its vanilla pose filtering. Our constraint detection system is active for approximately 35% of the time on the Ford AV dataset and localization is particularly improved when the constraint detection is active.</td>
                <td>Location awareness, Visualization, Tracking, Dynamics, Pipelines, 6-DOF, Vehicle dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341635&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341635&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Uncertainty-Aware Gaussian Mixture Model for UWB Time Difference of Arrival Localization in Cluttered Environments</td>
                <td>W. Zhao, A. Goudar, M. Tang, X. Qiao and A. P. Schoellig</td>
                <td>2023</td>
                <td>Ultra-wideband (UWB) time difference of arrival (TDOA)-based localization has emerged as a low-cost and scalable indoor positioning solution. However, in cluttered environments, the performance of UWB TDOA-based localization deteriorates due to the biased and non-Gaussian noise distributions induced by obstacles. In this work, we present a bi-level optimization-based joint localization and noise model learning algorithm to address this problem. In particular, we use a Gaussian mixture model (GMM) to approximate the measurement noise distribution. We explicitly incorporate the estimated state's uncertainty into the GMM noise model learning, referred to as uncertainty-aware GMM, to improve both noise modeling and localization performance. We first evaluate the GMM noise model learning and localization performance in numerous simulation scenarios. We then demonstrate the effectiveness of our algorithm in extensive real-world experiments using two different cluttered environments. We show that our algorithm provides accurate position estimates with low-cost UWB sensors, no prior knowledge about the obstacles in the space, and a significant amount of UWB radios occluded.</td>
                <td>Location awareness, Uncertainty, Time difference of arrival, Atmospheric measurements, Particle measurements, Sensors, Noise measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342365&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342365&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CREPES: Cooperative RElative Pose Estimation System</td>
                <td>Z. Xun et al.</td>
                <td>2023</td>
                <td>Mutual localization plays a crucial role in multi-robot cooperation. CREPES, a novel system that focuses on six degrees of freedom (DOF) relative pose estimation for multi-robot systems, is proposed in this paper. CREPES has a compact hardware design using active infrared (IR) LEDs, an IR fish-eye camera, an ultra-wideband (UWB) module and an inertial measurement unit (IMU). By leveraging IR light communication, the system solves data association between visual detection and UWB ranging. Ranging measurements from the UWB and directional information from the camera offer relative 3-DOF position estimation. Combining the mutual relative position with neighbors and the gravity constraints provided by IMUs, we can estimate the 6-DOF relative pose from a single frame of sensor measurements. In addition, we design an estimator based on the error-state Kalman filter (ESKF) to enhance system accuracy and robustness. When multiple neighbors are available, a Pose Graph Optimization (PGO) algorithm is applied to further improve system accuracy. We conduct enormous experiments to demonstrate CREPES’ accuracy between robot pairs and a team of robots, as well as performance under challenging conditions.</td>
                <td>Location awareness, Visualization, Pose estimation, Position measurement, Robot sensing systems, Cameras, Hardware, Distance measurement, Multi-robot systems, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342523&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342523&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>navlie: A Python Package for State Estimation on Lie Groups</td>
                <td>C. C. Cossette, M. Cohen, V. Korotkine, A. Del Castillo Bernal, M. A. Shalaby and J. R. Forbes</td>
                <td>2023</td>
                <td>The ability to rapidly test a variety of algorithms for an arbitrary state estimation task is valuable in the prototyping phase of navigation systems. Lie group theory is now mainstream in the robotics community, and hence estimation prototyping tools should allow state definitions that belong to manifolds. A new package, called navlie, provides a framework that allows a user to model a large class of problems by implementing a set of classes complying with a generic interface. Once accomplished, navlie provides a variety of on-manifold estimation algorithms that can run directly on these classes. The package also provides a built-in library of common models, as well as many useful utilities. The open-source project can be found at https://github.com/decargroup/navlie</td>
                <td>Manifolds, Estimation error, Navigation, Prototypes, Libraries, State estimation, Task analysis, Localization, Sensor Fusion, Software Tools for Robot Programming</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342362&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342362&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Relative Infrastructure-less Localization Algorithm for Decentralized and Autonomous Swarm Formation</td>
                <td>D. Schindler, V. Niculescu, T. Polonelli, D. Palossi, L. Benini and M. Magno</td>
                <td>2023</td>
                <td>Decentralized and autonomous control of Unmanned Aerial Vehicle (UAV) swarms is a key enabler for cooperative systems and infrastructure-less formation flights. However, UAVs often lack reliable heading angle measurements, especially in indoor scenarios, space, and GNSS-denied environments, posing an additional observability challenge on range-based relative localization. We tackle this problem by proposing a novel solution enhancing the classical tag-and-anchor trilateration. The proposed solution relies on Ultra-wideband range measurements and addresses the relative pose estimation between pairs of UAVs under relative motion. Furthermore, it does not require any explicit motion pattern or initialization procedure and leverages an approximate maximum-likelihood algorithm to recursively solve the relative localization problem with constant computational complexity. The method has been implemented and demonstrated through field experiments, where a swarm of nano-UAVs positioned themselves with respect to a leader in a nearly-static formation with an average error of 38.5 cm and a convergence time of 25 s. The achieved formation accuracy is similar to the one achieved by the state-of-the-art EKF-based leader-follower methods.</td>
                <td>Location awareness, Pose estimation, Autonomous aerial vehicles, Robustness, Motion measurement, Noise measurement, Observability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342168&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342168&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RaSpectLoc: RAman SPECTroscopy-dependent robot LOCalisation</td>
                <td>C. Thirgood, O. Mendez, E. C. Ling, J. Storey and S. Hadfield</td>
                <td>2023</td>
                <td>This paper presents a new information source for supporting robot localisation: material composition. The proposed method complements the existing visual, structural, and semantic cues utilized in the literature. However, it has a distinct advantage in its ability to differentiate structurally [23], visually [25] or categorically [1] similar objects such as different doors, by using Raman spectrometers. Such devices can identify the material of objects it probes through the bonds between the material's molecules. Unlike similar sensors, such as mass spectroscopy, it does so without damaging the material or environment. In addition to introducing the first material-based localisation algorithm, this paper supports the future growth of the field by presenting a gazebo plugin for Raman spectrometers, material sensing demonstrations, as well as the first-ever localisation data-set with benchmarks for material-based localisation. This benchmarking shows that the proposed technique results in a significant improvement over current state-of-the-art localisation techniques, achieving 16 % more accurate localisation than the leading baseline. The code and dataset will be released at: https://github.com/ThirgoodC/RaSpectLoc</td>
                <td>Visualization, Simultaneous localization and mapping, Navigation, Semantics, Benchmark testing, Mass spectroscopy, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342198&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342198&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Need for Speed: Fast Correspondence-Free Lidar-Inertial Odometry Using Doppler Velocity</td>
                <td>D. J. Yoon et al.</td>
                <td>2023</td>
                <td>In this paper, we present a fast, lightweight odometry method that uses the Doppler velocity measurements from a Frequency-Modulated Continuous-Wave (FMCW) lidar without data association. FMCW lidar is a recently emerging technology that enables per-return relative radial velocity measurements via the Doppler effect. Since the Doppler measurement model is linear with respect to the 6-degrees-of-freedom (DOF) vehicle velocity, we can formulate a linear continuous-time estimation problem for the velocity and numerically integrate for the 6-DOF pose estimate afterward. The caveat is that angular velocity is not observable with a single FMCW lidar. We address this limitation by also incorporating the angular velocity measurements from a gyroscope. This results in an extremely efficient odometry method that processes lidar frames at an average wall-clock time of 5.64ms on a single thread, well below the 10Hz operating rate of the lidar we tested. We show experimental results on real-world driving sequences and compare against state-of-the-art Iterative Closest Point (ICP)-based odometry methods, presenting a compelling tradeoff between accuracy and computation. We also present an algebraic observability study, where we demonstrate in theory that the Doppler measurements from multiple FMCW lidars are capable of observing all 6 degrees of freedom (translational and angular velocity).</td>
                <td>Laser radar, Angular velocity, 6-DOF, Gyroscopes, Sensors, Velocity measurement, Odometry</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341596&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341596&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Uncertainty Analysis for Accurate Ground Truth Trajectories with Robotic Total Stations</td>
                <td>M. Vaidis, W. Dubois, E. Daum, D. LaRocque and F. Pomerleau</td>
                <td>2023</td>
                <td>In the context of robotics, accurate ground truth positioning is essential for the development of Simultaneous Localization and Mapping (SLAM) and control algorithms. Robotic Total Stations (RTSs) provide accurate and precise reference positions in different types of outdoor environments, especially when compared to the limited accuracy of Global Navigation Satellite System (GNSS) in cluttered areas. Three RTSs give the possibility to obtain the six-Degrees Of Freedom (DOF) reference pose of a robotic platform. However, the uncertainty of every pose is rarely computed for trajectory evaluation. As evaluation algorithms are getting increasingly precise, it becomes crucial to take into account this uncertainty. We propose a method to compute this six-DOF uncertainty from the fusion of three RTSs based on Monte Carlo (MC) methods. This solution relies on point-to-point minimization to propagate the noise of RTSs on the pose of the robotic platform. Five main noise sources are identified to model this uncertainty: noise inherent to the instrument, tilt noise, atmospheric factors, time synchronization noise, and extrinsic calibration noise. Based on extensive experimental work, we compare the impact of each noise source on the prism uncertainty and the final estimated pose. Tested on more than 50 km of trajectories, our comparison highlighted the importance of the calibration noise and the measurement distance, which should be ideally under 75 m. Moreover, it has been noted that the uncertainty on the pose of the robot is not prominently affected by one particular noise source, compared to the others.</td>
                <td>Global navigation satellite system, Uncertainty, Simultaneous localization and mapping, Atmospheric modeling, Instruments, Computational modeling, Measurement uncertainty</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341529&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341529&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Graph Matching Optimization Network for Point Cloud Registration</td>
                <td>Q. Wu et al.</td>
                <td>2023</td>
                <td>Point Cloud Registration is a fundamental and challenging problem in 3D computer vision. Recent works often utilize geometric structure features in downsampled points (patches) to seek correspondences, then propagate these sparse patch correspondences to the dense level in the corresponding patches' neighborhood. However, they neglect the explicit global scale rigid constraint at the dense level point matching. We claim that the explicit isometry-preserving constraint in the dense level on a global scale is also important for improving feature representation in the training stage. To this end, we propose a Graph Matching Optimization based Network (GMONet for short), which utilizes the graph-matching optimizer to explicitly exert the isometry preserving constraints in the point feature training to improve the point feature representation. Specifically, we exploit a partial graph-matching optimizer to enhance the super point (i.e., down-sampled key points) features and a full graph-matching optimizer to improve the dense level point features in the overlap region. Meanwhile, we leverage the inexact proximal point method and the mini-batch sampling technique to accelerate these two graph-matching optimizers. Given high discriminative point features in the evaluation stage, we utilize the RANSAC approach to estimate the transformation between the scanned pairs. The proposed method has been evaluated on the 3DMatch/3DLoMatch and the KITTI datasets. The experimental results show that our method performs competitively compared to state-of-the-art baselines.</td>
                <td>Point cloud compression, Training, Representation learning, Computer vision, Three-dimensional displays, Task analysis, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342346&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342346&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SEAL: Simultaneous Exploration and Localization for Multi-Robot Systems</td>
                <td>E. Latif and R. Parasuraman</td>
                <td>2023</td>
                <td>The availability of accurate localization is critical for multi-robot exploration strategies; noisy or inconsistent localization causes failure in meeting exploration objectives. We aim to achieve high localization accuracy with contemporary exploration map belief and vice versa without needing global localization information. This paper proposes a novel simultaneous exploration and localization (SEAL) approach, which uses Gaussian Processes (GP)-based information fusion for maximum exploration while performing communication graph optimization for relative localization. Both these cross-dependent objectives were integrated through the Rao-Blackwellization technique. Distributed linearized convex hull optimization is used to select the next-best unexplored region for distributed exploration. SEAL outperformed cutting-edge methods on exploration and localization performance in extensive ROS-Gazebo simulations, illustrating the practicality of the approach in real-world applications.</td>
                <td>Location awareness, Seals, Gaussian processes, Sensor phenomena and characterization, Robot sensing systems, Multi-robot systems, Noise measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342157&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342157&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Discrete-Time Adaptive Control Algorithm for Coordination of Multiagent Systems in the Presence of Coupled Dynamics</td>
                <td>I. A. Aly and K. M. Dogan</td>
                <td>2023</td>
                <td>Discrete-time architectures have an advantage over their continuous counterparts as they can be directly executed on embedded hardware without the need for dis-cretization, and discretization can result in a loss of stability margin. This paper presents a discrete-time adaptive control architecture for uncertain scalar multi agent systems with coupled dynamics. Our strategy includes observer dynamics to handle the unmeasurable coupled dynamics and a user-assigned Laplacian matrix for coordination of the multiagent system. An illustrative example is presented to show the efficacy of the proposed control architectures.</td>
                <td>Asymptotic stability, Laplace equations, Heuristic algorithms, Observers, Hardware, Adaptive control, Numerical stability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341404&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341404&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Risk-Tolerant Task Allocation and Scheduling in Heterogeneous Multi-Robot Teams</td>
                <td>J. Park, A. Messing, H. Ravichandar and S. Hutchinson</td>
                <td>2023</td>
                <td>Effective coordination of heterogeneous multi-robot teams requires optimizing allocations, schedules, and motion plans in order to satisfy complex multi-dimensional task requirements. This challenge is exacerbated by the fact that real-world applications inevitably introduce uncertainties into robot capabilities and task requirements. In this paper, we extend our previous work on trait-based time-extended task allocation to account for such uncertainties. Specifically, we leverage the Sequential Probability Ratio Test to develop an algorithm that can guarantee that the probability of failing to satisfy task requirements is below a user-specified threshold. We also improve upon our prior approach by accounting for temporal deadlines in addition to synchronization and precedence constraints in a Mixed-Integer Linear Programming model. We evaluate our approach by benchmarking it against three baselines in a simulated battle domain in a city environment and compare its performance against a state-of-the-art framework in a pandemic-inspired multi-robot service coordination problem. Results demonstrate the effectiveness and advantages of our approach, which leverages redundancies to manage risk while simultaneously minimizing makespan.</td>
                <td>Schedules, Uncertainty, Robot kinematics, Urban areas, Redundancy, Robot sensing systems, Mixed integer linear programming</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341837&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341837&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PuSHR: A Multirobot System for Nonprehensile Rearrangement</td>
                <td>S. Talia, A. Thareja, C. Mavrogiannis, M. Schmittle and S. S. Srinivasa</td>
                <td>2023</td>
                <td>We focus on the problem of rearranging a set of objects with a team of car-like robot pushers built using off-the-shelf components. Maintaining control of pushed objects while avoiding collisions in a tight space demands highly coordinated motion that is challenging to execute on constrained hardware. Centralized replanning approaches become intractable even for small-sized problems whereas decentralized approaches often get stuck in deadlocks. Our key insight is that by carefully assigning pushing tasks to robots, we could reduce the complexity of the rearrangement task, enabling robust performance via scalable decentralized control. Based on this insight, we built PuSHR, a system that optimally assigns pushing tasks and trajectories to robots offline, and performs trajectory tracking via decentralized control online. Through an ablation study in simulation, we demonstrate that PuSHR dominates baselines ranging from purely centralized to fully decentralized in terms of success rate and time efficiency across challenging tasks with up to 4 robots. Hardware experiments demonstrate the transfer of our system to the real world and highlight its robustness to model inaccuracies. Our code can be found at https://github.com/prl-mushr/pushr, and videos from our experiments at https://youtu.be/nyUn9mHoR8Y.</td>
                <td>Trajectory tracking, Robot kinematics, Decentralized control, System recovery, Hardware, Robustness, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341853&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341853&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Game-Theoretical Approach to Multi-Robot Task Allocation Using a Bio-Inspired Optimization Strategy</td>
                <td>S. Chen, T. X. Lin and F. Zhang</td>
                <td>2023</td>
                <td>This paper introduces a game-theoretical approach to the multi-robot task allocation problem, where each robot is considered as self-interested and cannot share its personal utility functions. We consider the case where each robot can execute multiple tasks and each task requires only one robot. For real-world applications with mobile robots, we design a utility function that includes both assignment conflict penalties and path-dependent execution cost. For a robot to maximize its own utility, it needs to select a subset of conflict-free tasks that minimizes its total travel distance. Our approaches utilize a consensus communication scheme to share robots' task selection and the Speeding-Up and Slowing-Down (SUSD) strategy to search in a combinatorial action (task selection) space for a subset of tasks that can achieve a higher utility at each iteration. The SUSD strategy can perform a gradient-like search without calculating the derivatives, which allows robots to improve upon their current task selections. Simulation results show that robots using the proposed algorithms can successfully find Nash equilibria for effective coordination.</td>
                <td>Robot kinematics, Simulation, Nash equilibrium, Space exploration, Resource management, Mobile robots, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341947&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341947&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Robot Planning on Dynamic Topological Graphs Using Mixed- Integer Programming</td>
                <td>C. A. Dimmig, K. C. Wolfe and J. Moore</td>
                <td>2023</td>
                <td>Planning for multi-robot teams in complex environments is a challenging problem, especially when these teams must coordinate to accomplish a common objective. In general, optimal solutions to these planning problems are computationally intractable, since the decision space grows exponentially with the number of robots. In this paper, we present a novel approach for multi-robot planning on topological graphs using mixed-integer programming. Central to our approach is the notion of a dynamic topological graph, where edge weights vary dynamically based on the locations of the robots in the graph. We construct this graph using the critical features of the planning problem and the relationships between robots; we then leverage mixed-integer programming to minimize a shared cost that depends on the paths of all robots through the graph. To improve computational tractability, we formulated our optimization problem with a fully convex relaxation and designed our decision space around eliminating the exponential dependence on the number of robots. We test our approach on a multi-robot reconnaissance scenario, where robots must coordinate to minimize detectability and maximize safety while gathering information. We demonstrate that our approach is able to scale to a series of representative scenarios and is capable of computing optimal coordinated strategic behaviors for autonomous multi-robot teams in seconds.</td>
                <td>Costs, Robot kinematics, Reconnaissance, Programming, Search problems, Planning, Dynamic programming</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341497&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341497&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Heterogeneous Coalition Formation and Scheduling with Multi-Skilled Robots</td>
                <td>A. Aswale and C. Pinciroli</td>
                <td>2023</td>
                <td>We present an approach to task scheduling in heterogeneous multi-robot systems. In our setting, the tasks to complete require diverse skills. We assume that each robot is multi-skilled, i.e., each robot offers a subset of the possible skills. This makes the formation of heterogeneous teams (coalitions) a requirement for task completion. We present two centralized algorithms to schedule robots across tasks and to form suitable coalitions, assuming stochastic travel times across tasks. The coalitions are dynamic, in that the robots form and disband coalitions as the schedule is executed. The first algorithm we propose guarantees optimality, but its runtime is acceptable only for small problem instances. The second algorithm we propose can tackle large problems with short runtimes, and is based on a heuristic approach that typically reaches 1x-2x of the optimal solution cost.</td>
                <td>Schedules, Runtime, Costs, Heuristic algorithms, Parallel processing, Multi-robot systems, Resource management</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342489&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342489&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Measuring Human-Robot Team Benefits Under Time Pressure in a Virtual Reality Testbed</td>
                <td>K. Popović, M. Schlafly, A. Prabhakar, C. Kim and T. D. Murphey</td>
                <td>2023</td>
                <td>During a natural disaster such as hurricane, earthquake, or fire, robots have the potential to explore vast areas and provide valuable aid in search & rescue efforts. These scenarios are often high-pressure and time-critical with dynamically-changing task goals. One limitation to these large scale deployments is effective human-robot interaction. Prior work shows that collaboration between one human and one robot benefits from shared control. Here we evaluate the efficacy of shared control for human-swarm teaming in an immersive virtual reality environment. Although there are many human-swarm interaction paradigms, few are evaluated in high-pressure settings representative of their intended end use. We have developed an open-source virtual reality testbed for realistic evaluation of human-swarm teaming performance under pressure. We conduct a user study ($\mathrm{n}=16$) comparing four human-swarm paradigms to a baseline condition with no robotic assistance. Shared control significantly reduces the number of instructions needed to operate the robots. While shared control leads to marginally improved team performance in experienced participants, novices perform best when the robots are fully autonomous. Our experimental results suggest that in immersive, high-pressure settings, the benefits of robotic assistance may depend on how the human and robots interact, and the human operator's expertise.</td>
                <td>Human-robot interaction, Virtual reality, Particle measurements, Time measurement, Hurricanes, Pressure measurement, Time factors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341794&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341794&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Electric Vehicle Balancing of Autonomous Mobility-on-Demand System: A Multi-Agent Reinforcement Learning Approach</td>
                <td>S. He, S. Han and F. Miao</td>
                <td>2023</td>
                <td>Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust EAVs balancing policy to balance both the supply-demand ratio and charging utilization rate across the whole city. Experiments show that our proposed robust method performs better compared with a non-robust MARL method that does not consider state uncertainties; it improves the reward, charging utilization fairness, and supply-demand fairness by 19.28%, 28.18%, and 3.97%, respectively. Compared with a robust optimization-based method, the proposed MARL algorithm can improve the reward, charging utilization fairness, and supply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively.</td>
                <td>Training, Time-frequency analysis, Uncertainty, Supply and demand, Urban areas, Reinforcement learning, Prediction algorithms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342263&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342263&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments</td>
                <td>C. Tanama, K. Peng, Z. Marinov, R. Stiefelhagen and A. Roitberg</td>
                <td>2023</td>
                <td>Deep learning-based models are at the top of most driver observation benchmarks due to their remarkable accuracies but come with a high computational cost, while the resources are often limited in real-world driving scenarios. This paper presents a lightweight framework for resource- efficient driver activity recognition. We enhance 3D MobileNet, a speed-optimized neural architecture for video classification, with two paradigms for improving the trade-off between model accuracy and computational efficiency: knowledge distillation and model quantization. Knowledge distillation prevents large drops in accuracy when reducing the model size by harvesting knowledge from a large teacher model (I3D) via soft labels instead of using the original ground truth. Quantization further drastically reduces the memory and computation requirements by representing the model weights and activations using lower precision integers. Extensive experiments on a public dataset for in-vehicle monitoring during autonomous driving show that our proposed framework leads to an 3- fold reduction in model size and 1.4-fold improvement in inference time compared to an already speed-optimized architecture. Our code is available at https://github.com/calvintanama/qd-driver-activity-reco.</td>
                <td>Solid modeling, Quantization (signal), Three-dimensional displays, Computational modeling, Memory management, Activity recognition, Computational efficiency</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342203&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342203&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Joint Out-of-Distribution Detection and Uncertainty Estimation for Trajectory Prediction</td>
                <td>J. Wiederer, J. Schmidt, U. Kressel, K. Dietmayer and V. Belagiannis</td>
                <td>2023</td>
                <td>Despite the significant research efforts on trajectory prediction for automated driving, limited work exists on assessing the prediction reliability. To address this limitation we propose an approach that covers two sources of error, namely novel situations with out-of-distribution (OOD) detection and the complexity in in-distribution (ID) situations with uncertainty estimation. We introduce two modules next to an encoder-decoder network for trajectory prediction. Firstly, a Gaussian mixture model learns the probability density function of the ID encoder features during training, and then it is used to detect the OOD samples in regions of the feature space with low likelihood. Secondly, an error regression network is applied to the encoder, which learns to estimate the trajectory prediction error in supervised training. During inference, the estimated prediction error is used as the uncertainty. In our experiments, the combination of both modules outperforms the prior work in OOD detection and uncertainty estimation, on the Shifts robust trajectory prediction dataset by 2.8 % and 10.1%, respectively. The code is publicly available44project page: https://github.com/againerju/joodu.</td>
                <td>Training, Uncertainty, Computational modeling, Estimation, Predictive models, Probability density function, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341616&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341616&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Deep Reinforcement Learning-Based Intelligent Traffic Signal Controls with Optimized CO2 Emissions</td>
                <td>P. Agand, A. Iskrov and M. Chen</td>
                <td>2023</td>
                <td>Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, waiting time, and stopped time. Our evaluation considers multiple scenarios that encompass a range of road users (trucks, buses, cars) with varying pollution levels.</td>
                <td>Measurement, Q-learning, Sensitivity analysis, Roads, Urban areas, Transportation, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341972&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341972&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hierarchical Attention Network for Planning-Informed Multi-Agent Trajectory Prediction</td>
                <td>W. Xiong, J. Chen, X. Zhang, Q. Wang and Z. Qi</td>
                <td>2023</td>
                <td>The accurate prediction of the neighboring vehicles' trajectories affects the security of autonomous driving vehicles. However, it is challenging for existing methods to anticipating the trajectories of vehicles in the vicinity due to the uncertainty of driving behaviors and the complex interaction patterns of traffic flows. In this study, incorporating the planning information of the ego vehicle, we propose a novel trajectory prediction approach based on the hierarchical attention mechanism. Firstly, a spatio-temporary attention module is presented to extract the social interaction of surrounding vehicles and capture the temporal dependence of continuous frame historical information and planning information. Then, a hard-soft attention module is designed to perform two tasks: weighing the importance of both historical and future information, and learning different location information about the target vehicles. Our method is evaluated on two national highway datasets. The experimental results show that our algorithm achieves the state-of-the-art performance.</td>
                <td>Road transportation, TV, Uncertainty, Prediction algorithms, Trajectory, Planning, Data mining</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341557&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341557&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Two-Stage Based Social Preference Recognition in Multi-Agent Autonomous Driving System</td>
                <td>J. Xue, D. Zhang, R. Xiong, Y. Wang and E. Liu</td>
                <td>2023</td>
                <td>Multi-Agent Reinforcement Learning (MARL) has become a promising solution for constructing a multi-agent autonomous driving system (MADS) in complex and dense scenarios. But most methods consider agents acting selfishly, which leads to conflict behaviors. Some existing works incorporate the concept of social value orientation (SVO) to promote coordination, but they lack the knowledge of other agents' SVOs, resulting in conservative maneuvers. In this paper, we aim to tackle the mentioned problem by enabling the agents to understand other agents' SVOs. To accomplish this, we propose a two-stage system framework. Firstly, we train a policy by allowing the agents to share their ground truth SVOs to establish a coordinated traffic flow. Secondly, we develop a recognition network that estimates agents' SVOs and integrates it with the policy trained in the first stage. Experiments demonstrate that our developed method significantly improves the performance of the driving policy in MADS compared to two state-of-the-art MARL algorithms.</td>
                <td>Reinforcement learning, Behavioral sciences, Autonomous vehicles, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341803&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341803&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RLPG: Reinforcement Learning Approach for Dynamic Intra-Platoon Gap Adaptation for Highway On-Ramp Merging</td>
                <td>S. R. Yadavalli, L. C. Das and M. Won</td>
                <td>2023</td>
                <td>A platoon refers to a group of vehicles traveling together in very close proximity using automated driving technology. Owing to its immense capacity to improve fuel efficiency, driving safety, and driver comfort, platooning technology has garnered substantial attention from the autonomous vehicle research community. Although highly advantageous, recent research has uncovered that an excessively small intra-platoon gap can impede traffic flow during highway on-ramp merging. While existing control-based methods allow for adaptation of the intra-platoon gap to improve traffic flow, making an optimal control decision under the complex dynamics of traffic conditions remains a challenge due to the massive computational complexity. In this paper, we present the design, implementation, and evaluation of a novel reinforcement learning framework that adaptively adjusts the intra-platoon gap of an individual platoon member to maximize traffic flow in response to dynamically changing, complex traffic conditions for highway on-ramp merging. The framework's state space has been meticulously designed in consultation with the transportation literature to take into account critical traffic parameters that bear direct relevance to merging efficiency. An intra-platoon gap decision making method based on the deep deterministic policy gradient algorithm is created to incorporate the continuous action space to ensure precise and continuous adaptation of the intra-platoon gap. An extensive simulation study demonstrates the effectiveness of the reinforcement learning-based approach for significantly improving traffic flow in various highway on-ramp merging scenarios.</td>
                <td>Adaptation models, Roads, Merging, Optimal control, Reinforcement learning, Safety, Vehicle dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341918&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341918&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>P4P: Conflict-Aware Motion Prediction for Planning in Autonomous Driving</td>
                <td>Q. Sun, X. Huang, B. C. Williams and H. Zhao</td>
                <td>2023</td>
                <td>Motion prediction is crucial in enabling safe motion planning for autonomous vehicles in interactive scenarios. It allows the planner to identify potential conflicts with other traffic agents and generate safe plans. Existing motion predictors often focus on reducing prediction errors, yet it remains an open question on how well they help identify conflicts for the planner, which are critical to the safety of autonomous vehicles. In this paper, we evaluate state-of-the-art predictors through novel conflict-related metrics, such as the success rate of identifying conflicts. Surprisingly, the predictors suffer from a low success rate and thus lead to a large percentage of collisions when we test the prediction-planning system in an interactive simulator. To fill the gap, we propose a simple but effective alternative that combines a physics-based trajectory generator and a learning-based relation predictor to identify conflicts and infer conflict relations. We demonstrate that our predictor, P4P, achieves superior performance over existing learning-based predictors in realistic interactive driving scenarios from Waymo Open Motion Dataset.</td>
                <td>Measurement, Generators, Trajectory, Safety, Planning, Autonomous vehicles, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342247&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342247&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Thousand Worlds: Scenery Specification and Generation for Simulation-Based Testing of Mobile Robot Navigation Stacks</td>
                <td>S. Parra, A. Ortega, S. Schneider and N. Hochgeschwender</td>
                <td>2023</td>
                <td>Is mobile robot navigation a solved problem? We asked this question to 14 professional robot software engineers who work with navigation stacks of mobile, wheeled robots on a daily basis. They unanimously report that it remains challenging to ensure the performance of their mobile robots. We find that the method of choice to verify a robot's performance is to expose it to different environments under varying conditions. Unfortunately, these field tests are costly to set up and often too risky to execute. Therefore, robot software engineers want to replicate real-world environments in simulated sceneries (i) to test their navigation stack or parts of it prior to deployment; and (ii) to reproduce erroneous behaviour observed in the real world. Motivated by these insights, we have developed a domain-specific language and associated tooling which enables engineers (i) to specify sceneries of indoor environments; and (ii) to automatically generate variants thereof that resemble changes in the real world. We demonstrate how our approach enables the simulation-based replication of real-world environments and allows us to recreate erroneous robot behaviour as reported in the interviews. While performing simulation-based tests, we discovered an 8-year dormant bug in the ROS navigation stack.</td>
                <td>Navigation, Computer bugs, Software, Indoor environment, Mobile robots, Interviews, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342315&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342315&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RAIST: Learning Risk Aware Traffic Interactions via Spatio-Temporal Graph Convolutional Networks</td>
                <td>V. Suman, P. Pham and A. Bera</td>
                <td>2023</td>
                <td>A key aspect of driving a road vehicle is to interact with other road users, assess their intentions and make riskaware tactical decisions. An intuitive approach to enabling an intelligent automated driving system would be incorporating some aspects of human driving behavior. To this end, we propose a novel driving framework for egocentric views based on spatio-temporal traffic graphs. The traffic graphs model not only the spatial interactions amongst the road users but also their individual intentions through temporally associated message passing. We leverage a spatio-temporal graph convolutional network (ST-GCN) to train the graph edges. These edges are formulated using parameterized functions of 3D positions and scene-aware appearance features of road agents. Along with tactical behavior prediction, it is crucial to evaluate the risk-assessing ability of the proposed framework. We claim that our framework learns risk-aware representations by improving on the task of risk object identification, especially in identifying objects with vulnerable interactions like pedestrians and cyclists.</td>
                <td>Three-dimensional displays, Pedestrians, Roads, Message passing, Road vehicles, Behavioral sciences, Object recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341578&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341578&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Look Before You Drive: Boosting Trajectory Forecasting via Imagining Future</td>
                <td>Y. Fan, X. Liu, Y. Li and S. Wang</td>
                <td>2023</td>
                <td>Predicting the future trajectories of other agents in the scene fast and effectively is crucial for autonomous driving systems. We note that high-quality predictions require us to take into account the subjective initiative of the target agents, which is reflected by the fact that they themselves make decisions based on their own predictions about the future, just like our ego vehicle's prediction-planning system. However, this characteristic has been neglected in previous studies. We introduce Look Before You Drive (LBYD), a two-stage approach that explicitly incorporates both past observations and future estimates to make predictions. To get a preliminary estimate of the future, we propose a neat and effective baseline capable of making predictions for multiple agents simultaneously. We use only the most basic structures, mainly Transformer, to ensure sufficient inference speed and room for expansion. On this basis, we cooperatively train two networks to enable the coarse estimates to boost final forecasting. Our experiments demonstrate that LBYD can significantly surpass the baseline performance. Moreover, while state-of-the-art methods rely on considering heterogeneity and artificially designed inductive biases for attention modeling, LBYD performs on par with SOTA without them on both the Argoverse 1 and the large scale Argoverse 2 datasets, and can run at 67 FPS on an RTX 3090 GPU.</td>
                <td>Training, Scalability, Predictive models, Transformers, Trajectory, Proposals, History</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341509&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341509&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Leveraging Cloud Computing to Make Autonomous Vehicles Safer</td>
                <td>P. Schafhalter, S. Kalra, L. Xu, J. E. Gonzalez and I. Stoica</td>
                <td>2023</td>
                <td>The safety of autonomous vehicles (AVs) depends on their ability to perform complex computations on high-volume sensor data in a timely manner. Their ability to run these computations with state-of-the-art models is limited by the processing power and slow update cycles of their onboard hardware. In contrast, cloud computing offers the ability to burst computation to vast amounts of the latest generation of hardware. However, accessing these cloud resources requires traversing wireless networks that are often considered to be too unreliable for real-time AV driving applications. Our work seeks to harness this unreliable cloud to enhance the accuracy of an AV's decisions, while ensuring that it can always fall back to its on-board computational capabilities. We identify three mechanisms that can be used by AVs to safely leverage the cloud for accuracy enhancements, and elaborate why current execution systems fail to enable these mechanisms. To address these limitations, we provide a system design based on the speculative execution of an AV's pipeline in the cloud, and show the efficacy of this approach in simulations of complex real-world scenarios that apply these mechanisms.</td>
                <td>Cloud computing, Computational modeling, Wireless networks, Pipelines, Robot sensing systems, Hardware, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341821&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341821&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Effective Traffic Signal Control with Offline-to-Online Reinforcement Learning</td>
                <td>J. Ma and F. Wu</td>
                <td>2023</td>
                <td>Reinforcement learning (RL) has emerged as a promising approach for optimizing traffic signal control (TSC) to ensure the efficient operation of transportation networks. However, the traditional trial-and-error technique in RL is usually impractical in real-world applications. Offline RL, which trains models using pre-collected datasets, is a more practical approach. However, this presents challenges such as suboptimal datasets and limited generalization of pre-trained models. To address this, we propose an offline-to-online RL framework for TSC that pre-trains a generalized model and quickly adapts to new traffic scenarios through online refinement. In the offline stage, we augment the pre-collected datasets to cover a diverse set of possible scenarios and use an offline RL method to pretrain a control model. To ensure generalization, we use FRAP-like network as our base model, which is designed to learn the basic logic for signal control. In the online stage, we introduce a discrepancy measure to tackle inconsistencies between offline pre-trained models and online scenarios and prioritize samples based on it. In the experiments, the proposed approach achieves competitive performance and reduces the training time needed for learning in new scenarios, compared to several baselines.</td>
                <td>Training, Adaptation models, Transportation, Reinforcement learning, Computational efficiency, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341718&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341718&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning from Symmetry: Meta-Reinforcement Learning with Symmetrical Behaviors and Language Instructions</td>
                <td>X. Yao et al.</td>
                <td>2023</td>
                <td>Meta-reinforcement learning (meta-RL) is a promising approach that enables the agent to learn new tasks quickly. However, most meta-RL algorithms show poor generalization in multi-task scenarios due to the insufficient task information provided only by rewards. Language-conditioned meta-RL improves the generalization capability by matching language instructions with the agent's behaviors. While both behaviors and language instructions have symmetry, which can speed up human learning of new knowledge. Thus, combining symmetry and language instructions into meta-RL can help improve the algorithm's generalization and learning efficiency. We propose a dual-MDP meta-reinforcement learning method that enables learning new tasks efficiently with symmetrical behav-iors and language instructions. We evaluate our method in mul-tiple challenging manipulation tasks, and experimental results show that our method can greatly improve the generalization and learning efficiency of meta-reinforcement learning. Videos are available at https://tumi6robot.wixsite.com/symmetry/.</td>
                <td>Learning systems, Multitasking, Behavioral sciences, Task analysis, Intelligent robots, Videos</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341769&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341769&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Multiplicative Value Function for Safe and Efficient Reinforcement Learning</td>
                <td>N. Bührer, Z. Zhang, A. Liniger, F. Yu and L. Van Gool</td>
                <td>2023</td>
                <td>An emerging field of sequential decision problems is safe Reinforcement Learning (RL), where the objective is to maximize the reward while obeying safety constraints. Being able to handle constraints is essential for deploying RL agents in real-world environments, where constraint violations can harm the agent and the environment. To this end, we propose a safe model-free RL algorithm with a novel multiplicative value function consisting of a safety critic and a reward critic. The safety critic predicts the probability of constraint violation and discounts the reward critic that only estimates constraint-free returns. By splitting responsibilities, we facilitate the learning task leading to increased sample efficiency. We integrate our approach into two popular RL algorithms, Proximal Policy Optimization and Soft Actor-Critic, and evaluate our method in four safety-focused environments, including classical RL benchmarks augmented with safety constraints and robot navigation tasks with images and raw Lidar scans as observations. Finally, we make the zero-shot sim-to-real transfer where a differential drive robot has to navigate through a cluttered room. Our code can be found at https://github.com/nikeke19/Safe-Mult-RL.</td>
                <td>Laser radar, Codes, Navigation, Reinforcement learning, Benchmark testing, Prediction algorithms, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342288&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342288&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Energy Constrained Multi-Agent Reinforcement Learning for Coverage Path Planning</td>
                <td>U. Yoon, X. Li, H. Li and Z. Zhang</td>
                <td>2023</td>
                <td>For multi-agent area coverage path planning problem, existing researches regard it as a combination of Traveling Salesman Problem (TSP) and Coverage Path Planning (CPP). However, these approaches have disadvantages of poor observation ability in online phase and high computational cost in offline phase, making it difficult to be applied to energy-constrained Unmanned Aerial Vehicles (UAVs) and adjust strategy dynamically. In this paper, we decompose the task into two sub-problems: multi-agent path planning and sub-region CPP. We model the multi-agent path planning problem as a Collective Markov Decision Process (C-MDP), and design an Energy Constrained Multi-Agent Reinforcement Learning (ECMARL) algorithm based on the centralized training and distributed execution concept. Taking into account energy constraint of UAVs, the UAV propulsion power model is established to measure the energy consumption of UAVs, and load balancing strategy is applied to dynamically allocate target areas for each UAV. If the UAV is under energy-depleted situation, ECMARL can adjust the mission strategy in real time according to environmental information and energy storage conditions of other UAVs. When UAVs reach each sub-region of interest, Back-an-Forth Paths (BFPs) are adopted to solve CPP problem, which can ensure full coverage, optimality and complexity of the sub-problem. Comprehensive theoretical analysis and experiments demonstrate that ECMARL is superior to the traditional offline TSP-CPP strategy in terms of solution quality and computational time, and can effectively deal with the energy-constrained UAVs.</td>
                <td>Training, Power measurement, Reinforcement learning, Traveling salesman problems, Propulsion, Markov processes, Autonomous aerial vehicles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341412&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341412&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Air-M: A Visual Reality Many-Agent Reinforcement Learning Platform for Large-Scale Aerial Unmanned System</td>
                <td>J. Lou, W. Wu, S. Liao and R. Shi</td>
                <td>2023</td>
                <td>Reinforcement learning for swarms of flying robots is a challenging task that requires a large number of data samples. Moreover, the problem of sim-to-real transfer has long been a challenge in robotics algorithm deployment. To address these issues, we propose Air-M, a platform that facilitates large-scale drone swarm learning in a distributed docker container environment and deployment in a virtual reality setting. Air-M trains the policy network using physics engines and creates replicas of agents in docker containers, which helps amortize the computational cost. In addition, Air-M establishes an intermediate link between the simulation and the real world, allowing real drones to interact with virtual objects via virtual sensors. This enables the policy network to be trained using virtual agents and seamlessly transferred to real drones. Air-Mis highly scalable, accommodating hundreds of agents with dynamic models and virtual sensors. We evaluate the effectiveness of our approach by conducting experiments in three representative virtual scenarios with an increasing number of agents. Our results demonstrate that our method outperforms the state-of- the-art in terms of training efficiency and transferability, making it a promising platform for swarm robotics applications.</td>
                <td>Training, Soft sensors, Reinforcement learning, Containers, Autonomous aerial vehicles, Libraries, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341405&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341405&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration</td>
                <td>J. Del Aguila Ferrandis, J. Moura and S. Vijayakumar</td>
                <td>2023</td>
                <td>Developing robot controllers capable of achieving dexterous nonprehensile manipulation, such as pushing an object on a table, is challenging. The underactuated and hybrid-dynamics nature of the problem, further complicated by the uncertainty resulting from the frictional interactions, requires sophisticated control behaviors. Reinforcement Learning (RL) is a powerful framework for developing such robot controllers. However, previous RL literature addressing the nonprehensile pushing task achieves low accuracy, non-smooth trajectories, and only simple motions, i.e. without rotation of the manipulated object. We conjecture that previously used unimodal exploration strategies fail to capture the inherent hybrid-dynamics of the task, arising from the different possible contact interaction modes between the robot and the object, such as sticking, sliding, and separation. In this work, we propose a multimodal exploration approach through categorical distributions, which enables us to train planar pushing RL policies for arbitrary starting and target object poses, i.e. positions and orientations, and with improved accuracy. We show that the learned policies are robust to external disturbances and observation noise, and scale to tasks with multiple pushers. Furthermore, we validate the transferability of the learned policies, trained entirely in simulation, to a physical robot hardware using the KUKA iiwa robot arm. See our supplemental video: https://youtu.be/vTdvalmgrk4.</td>
                <td>Uncertainty, Reinforcement learning, Manipulators, Hardware, Trajectory, Behavioral sciences, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341629&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341629&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Domain Coverage for Vehicles with Second-Order Dynamics via Multi-Agent Reinforcement Learning</td>
                <td>X. Zhao, R. C. Fetecau and M. Chen</td>
                <td>2023</td>
                <td>Collaborative autonomous multi-agent systems covering a specified area have many potential applications. Traditional approaches for such problems involve designing model-based control policies; however, state-of-the-art classical control policy still exhibits a large degree of sub-optimality. We present a combined reinforcement learning (RL) and control approach for the multi-agent coverage problem involving agents with second-order dynamics, with the RL component being based on the Multi-Agent Proximal Policy Optimization Algorithm (MAPPO). Our proposed network architecture includes the incorporation of LSTM and self-attention, which allows the trained policy to adapt to a variable number of agents. Our trained policy significantly outperforms the state-of-the-art classical control policy. We demonstrate our proposed method in a variety of simulated experiments.</td>
                <td>Heuristic algorithms, Collaboration, Reinforcement learning, Network architecture, Vehicle dynamics, Task analysis, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341748&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341748&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Domains as Objectives: Multi-Domain Reinforcement Learning with Convex-Coverage Set Learning for Domain Uncertainty Awareness</td>
                <td>W. E. L. Ilboudo, T. Kobayashi and T. Matsubara</td>
                <td>2023</td>
                <td>Domain randomization (DR) is a powerful framework that has allowed the transfer of policies from randomized domain (a.k.a. simulation) to real robots with little to no retraining requirement. However, because the policy has to perform well for many different domain conditions, DR tends to produce sub-optimal policies that can be too conservative on the target real system. This problem is further exacerbated the larger the randomized domain is. To tackle this issue, recent works have proposed to learn universal policies (UP) with domain knowledge such that they can adapt their behavior to each domain when paired with an online system identifier (OSI). However, in most applications, perfect identifications of the target domain can be impossible. In this paper, by drawing similarities between DR as a multi-domain reinforcement learning and multi-objective reinforcement learning (MORL), we propose to learn a UP over the convex coverage set borrowed from the MORL theory. Thanks to this, our method learns a UP that effectively captures different sub-domains of the uncertainty set and can therefore adapt its behavior based on an OSI uncertainty, unlocking the power of stochastic system identification with no retraining requirement. This pseudo-MORL framework also contains previous works in DR and robust reinforcement learning. We conduct simulations on Mujoco tasks and experiments on a real D'Claw robot, revealing the effectiveness of our domain-uncertainty-aware UP for sim-to-real transfer.</td>
                <td>Uncertainty, Stochastic systems, Neural networks, Reinforcement learning, Open systems, Behavioral sciences, System identification</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342236&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342236&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PIMbot: Policy and Incentive Manipulation for Multi-Robot Reinforcement Learning in Social Dilemmas</td>
                <td>S. Nikkhoo, Z. Li, A. Samanta, Y. Li and C. Liu</td>
                <td>2023</td>
                <td>Recent research has demonstrated the potential of reinforcement learning (RL) in enabling effective multi-robot collaboration, particularly in social dilemmas where robots face a trade-off between self-interests and collective benefits. However, environmental factors such as miscommunication and adversarial robots can impact cooperation, making it crucial to explore how multi-robot communication can be manipulated to achieve different outcomes. This paper presents a novel approach, namely PIMbot, to manipulating the reward function in multi-robot collaboration through two distinct forms of manipulation: policy and incentive manipulation. Our work introduces a new angle for manipulation in recent multi-agent RL social dilemmas that utilize a unique reward function for incentivization. By utilizing our proposed PIMbot mechanisms, a robot is able to manipulate the social dilemma environment effectively. PIMbot has the potential for both positive and negative impacts on the task outcome, where positive impacts lead to faster convergence to the global optimum and maximized rewards for any chosen robot. Conversely, negative impacts can have a detrimental effect on the overall task performance. We present comprehensive experimental results that demonstrate the effectiveness of our proposed methods in the Gazebo-simulated multi-robot environment. Our work provides insights into how inter-robot communication can be manipulated and has implications for various robotic applications.</td>
                <td>Collaboration, Reinforcement learning, Environmental factors, Behavioral sciences, Multi-robot systems, Task analysis, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341884&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341884&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Robust and Constrained Multi-Agent Reinforcement Learning Electric Vehicle Rebalancing Method in AMoD Systems</td>
                <td>S. He, Y. Wang, S. Han, S. Zou and F. Miao</td>
                <td>2023</td>
                <td>Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand (AMoD) systems, but their unique charging patterns increase the model uncertainties in AMoD systems (e.g. state transition probability). Since there usually exists a mismatch between the training and test/true environments, incorporating model uncertainty into system design is of critical importance in real-world applications. However, model uncertainties have not been considered explicitly in EV AMoD system rebalancing by existing literature yet, and the coexistence of model uncertainties and constraints that the decision should satisfy makes the problem even more challenging. In this work, we design a robust and constrained multi-agent reinforcement learning (MARL) framework with state transition kernel uncertainty for EV AMoD systems. We then propose a robust and constrained MARL algorithm (ROCOMA) with robust natural policy gradients (RNPG) that trains a robust EV rebalancing policy to balance the supply-demand ratio and the charging utilization rate across the city under model uncertainty. Experiments show that the ROCOMA can learn an effective and robust rebalancing policy. It outperforms non-robust MARL methods in the presence of model uncertainties. It increases the system fairness by 19.6% and decreases the rebalancing costs by 75.8%.</td>
                <td>Training, Uncertainty, Heuristic algorithms, Urban areas, Reinforcement learning, Electric vehicles, Vehicle dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342342&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342342&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Model-Free Deep Reinforcement Learning for Force Control of a Series Elastic Actuator</td>
                <td>R. Sambhus, A. Gokce, S. Welch, C. W. Herron and A. Leonessa</td>
                <td>2023</td>
                <td>Many state-of-the-art robotic applications utilize series elastic actuators (SEAs) with closed-loop force control to achieve complex tasks such as walking, lifting, and manipulation. Model-free PID control methods are more prone to instability due to nonlinearities in the SEA where cascaded model-based robust controllers can remove these effects to achieve stable force control. However, these model-based methods require detailed investigations to characterize the system accurately. Deep reinforcement learning (DRL) has proved to be an effective model-free method for continuous control tasks, where few works deal with hardware learning. This paper describes the training process of a DRL policy on the hardware of an SEA pendulum system for tracking force control trajectories from 0.05 - 0.35 Hz at 50 N amplitude using the Proximal Policy Optimization (PPO) algorithm. Safety mechanisms are developed and utilized for training the policy for over 21 hours (including overnight) without an operator present. The tracking performance is evaluated showing improvements of 25 N in mean absolute error when comparing the first 18 minutes of training to the full 21 hours for a 50 N amplitude, 0.1 Hz sinusoid desired force trajectory. Finally, the DRL policy exhibits better tracking and stability margins when compared to a model-free PID controller for a 50 N chirp force trajectory.</td>
                <td>Training, Legged locomotion, Force, Reinforcement learning, Hardware, Stability analysis, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341751&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341751&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Approach to Design a Biomechanically-Inspired Reward Function to Solve a Patience Cube Under Reinforcement Learning Framework</td>
                <td>H. Yoo, S. W. Kim and H. U. Yoon</td>
                <td>2023</td>
                <td>This paper presents an approach to design a reward function by adopting both control theoretic and biomechanical perspectives. In reinforcement learning (RL), a reward function plays a crucial role for an RL agent training; especially, a task learning time and a task performance. Accordingly, designing a reward function becomes a key issue to train an RL agent generating human-like policy/strategy to perform dexterous manipulation. Since human beings are good at producing heuristic approaches to complete a given task, determining a set of basis functions as well as corresponding weights used not to be so straightforward. In this study, we consider solving a patience cube as an example of a dexterous manipulation task. In our approach, we first employed a quadratic regulator form as a backbone of a desired reward function. Next, the kinematic data of a controlled object and the sEMG data of a human expert were measured while performing a demonstration to solve a patience cube. Then, from the measured data, the weights of the basis functions were determined by utilizing muscle synergy extraction and inverse optimal control as two key tools. Finally, an RL agent was trained by the designed reward function and comparative analysis versus the other RL agents trained by prototypical weight settings was followed. The result showed that the RL agent trained by our approach yielded human-like learning curve as well as policy successfully and outperformed the others in terms of a task success rate and a task completion time. These findings substantiated the feasibility of extending our approach to an assistive robotic manipulator or prosthesis design to perform the activities of daily living.</td>
                <td>Biomechanics, Weight measurement, Training, Optimal control, Reinforcement learning, Kinematics, Muscles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341831&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341831&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MaskBEV: Joint Object Detection and Footprint Completion for Bird's-Eye View 3D Point Clouds</td>
                <td>M. Fortin, F. Pomerleau and P. Giguère</td>
                <td>2023</td>
                <td>Recent works in object detection in LiDAR point clouds mostly focus on predicting bounding boxes around objects. This prediction is commonly achieved using anchor-based or anchor-free detectors that predict bounding boxes, requiring significant explicit prior knowledge about the objects to work properly. To remedy these limitations, we propose MaskBEV, a bird's-eye view (BEV) mask-based object detector neural architecture. MaskBEV predicts a set of BEV instance masks that represent the footprints of detected objects. Moreover, our approach allows object detection and footprint completion in a single pass. MaskBEV also reformulates the detection problem purely in terms of classification, doing away with regression usually done to predict bounding boxes. We evaluate the performance of MaskBEV on both SemanticKITTI and KITTI datasets while analyzing the architecture advantages and limitations.</td>
                <td>Point cloud compression, Training, Three-dimensional displays, Laser radar, Shape, Training data, Object detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342294&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342294&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Disentangled Discriminator for Unsupervised Domain Adaptation on Object Detection</td>
                <td>Y. Zhu, P. Guo, H. Wei, X. Zhao and X. Wu</td>
                <td>2023</td>
                <td>Object detection plays an important role in computer vision tasks such as autonomous driving, robotics, etc. Typically, a detection model is firstly trained on collected data and then deployed in real world. However, the discrepancy exists between training (source) and testing (target) data, which degrades the detection model's performance in the real world. To mitigate the negative effects, Unsupervised Domain Adaptation (UDA) methods learn the features of a shared domain via a discriminator. However, existing discriminators consider only the in-distribution adversarial learning, which ignore the out-of-distribution data of individual domains. In this paper, we propose a disentangled discriminator to consider the in-distribution and outliers separately. It aligns the source and target data with split branches under a gated strategy. We combine the disentangled discriminator with a Teacher-Student (T-S) framework that trains the student using labeled source data and unlabeled target data under a self-training mechanism. Specifically, the teacher network, that is updated with the parameters of student network via the exponential moving average, predicts pseudo labels for unlabeled data. The quality of pseudo labels can be improved after alleviating the domain discrepancy thanks to the disentangled discriminator. Extensive experiments on benchmarks demonstrate the superiority of the proposed method. Specifically, we achieve 53.9% mAP on Foggy Cityscapes, which is 7.2% higher than the Oracle.</td>
                <td>Training, Adaptation models, Computational modeling, Object detection, Logic gates, Data models, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341878&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341878&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Open-Vocabulary Affordance Detection in 3D Point Clouds</td>
                <td>T. Nguyen et al.</td>
                <td>2023</td>
                <td>Affordance detection is a challenging problem with a wide variety of robotic applications. Traditional affordance detection methods are limited to a predefined set of affordance labels, hence potentially restricting the adaptability of intelligent robots in complex and dynamic environments. In this paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method, which is capable of detecting an unbounded number of affordances in 3D point clouds. By simultaneously learning the affordance text and the point feature, OpenAD successfully exploits the semantic relationships between affordances. Therefore, our proposed method enables zero-shot detection and can be able to detect previously unseen affordances without a single annotation example. Intensive experimental results show that OpenAD works effectively on a wide range of affordance detection setups and outperforms other baselines by a large margin. Additionally, we demonstrate the practicality of the proposed OpenAD in real-world robotic applications with a fast inference speed. Our project is available at https://openad2023.github.io.</td>
                <td>Point cloud compression, Three-dimensional displays, Annotations, Affordances, Semantics, Usability, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341553&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341553&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>EvCenterNet: Uncertainty Estimation for Object Detection Using Evidential Learning</td>
                <td>H. Cheng and A. Valada</td>
                <td>2023</td>
                <td>Uncertainty estimation is crucial in safety-critical settings such as automated driving as it provides valuable information for several downstream tasks including high-level decision making and path planning. In this work, we propose EvCenterNet, a novel uncertainty-aware 2D object detection framework using evidential learning to directly estimate both classification and regression uncertainties. To employ evidential learning for object detection, we devise a combination of evidential and focal loss functions for the sparse heatmap inputs. We introduce class-balanced weighting for regression and heatmap prediction to tackle the class imbalance encountered by evidential learning. Moreover, we propose a learning scheme to actively utilize the predicted heatmap uncertainties to improve the detection performance by focusing on the most uncertain points. We train our model on the KITTI dataset and evaluate it on challenging out-of-distribution datasets including BDD100K and nuImages. Our experiments demonstrate that our approach improves the precision and minimizes the execution time loss in relation to the base model.</td>
                <td>Heating systems, Uncertainty, Three-dimensional displays, Decision making, Estimation, Focusing, Object detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341826&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341826&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SemanticBEVFusion: Rethinking LiDAR-Camera Fusion in Unified Bird's-Eye View Representation for 3D Object Detection</td>
                <td>Q. Jiang and H. Sun</td>
                <td>2023</td>
                <td>LiDAR and cameras are two essential sensors for 3D object detection in autonomous driving. LiDAR provides accurate and reliable 3D geometry information while the camera provides rich texture with color. Despite the increasing popularity of fusing these two complementary sensors, the challenge remains in how to effectively fuse 3D LiDAR point cloud with 2D camera images. Recent methods focus on point-level fusion which paints the LiDAR point cloud with camera features in the perspective view or bird's-eye view (BEV)-level fusion which unifies multi-modality features in the BEV representation. In this paper, we rethink these previous fusion strategies and analyze their information loss and influences on geometric and semantic features. We present SemanticBEVFusion to deeply fuse camera features with LiDAR features in a unified BEV representation while maintaining per-modality strengths for 3D object detection. Our method achieves state-of-the-art performance on the large-scale nuScenes dataset, especially for challenging distant objects. The code will be made publicly available.</td>
                <td>Point cloud compression, Three-dimensional displays, Laser radar, Fuses, Semantics, Object detection, Sensor fusion</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342368&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342368&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RFDNet: Real-Time 3D Object Detection Via Range Feature Decoration</td>
                <td>H. Chang, L. Wang and J. Cheng</td>
                <td>2023</td>
                <td>High-performance real-time 3D object detection is crucial in autonomous driving perception systems. Voxel-or point-based 3D object detectors are highly accurate but inefficient and difficult to deploy, while other methods use 2D projection views to improve efficiency, but information loss usually degrades performance. To balance effectiveness and efficiency, we propose a scheme called RFDNet that uses range features to decorate points. Specifically, RFDNet adaptively aggregates point features projected to independent grids and nearby regions via Dilated Grid Feature Encoding (DGFE) to generate a range view, which can handle occlusion and multi-frame inputs while the established geometric correlation between grid with surrounding space weakens the effects of scale distortion. We also propose a Soft Box Regression (SBR) strategy that supervises 3D box regression on a more extensive range than conventional methods to enhance model robustness. In addition, RFDNet benefits from our designed Semantic-assisted Ground-truth Sample (SA-GTS) data augmentation, which additionally considers collisions and spatial distributions of objects. Experiments on the nuScenes benchmark show that RFDNet outperforms all LiDAR-only non-ensemble 3D object detectors and runs at high speed of 20 FPS, achieving a better effectiveness-efficiency trade-off. Code is available at https://github.com/wy17646051/RFDNet.</td>
                <td>Solid modeling, Three-dimensional displays, Graphical models, Detectors, Object detection, Feature extraction, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341482&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341482&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Object-Level Unknown Obstacle Detection</td>
                <td>W. Chen</td>
                <td>2023</td>
                <td>This paper presents a novel method for object-level unknown obstacle detection in driving scenes that reduces false positives. The proposed method combines existing anomaly detectors, depth estimation, and object detection techniques to achieve object-level predictions. Our method can predict anomalies as bound-box instance detections. These bounding boxes can then be used to refine anomaly detection by suppressing false positives outside of the bounding boxes. The proposed method has several advantages, including object-level detections that are more practical than pixel-level detections, and the ability to find and refine region proposals for obstacle detection. The paper provides a detailed explanation of all components of the system and includes an ablation study on the usage of depth estimation, as well as execution time averages on different hardware. The proposed method is evaluated using different metrics and benchmarks, demonstrating the effectiveness and relevance of the existing proposed methods. Overall, our proposed method has the potential to significantly improve object-level anomaly detection making it suitable for real-world applications.</td>
                <td>Measurement, Location awareness, Roads, Neural networks, Estimation, Object detection, Proposals</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342306&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342306&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>BSH-Det3D: Improving 3D Object Detection with BEV Shape Heatmap</td>
                <td>Y. Shen et al.</td>
                <td>2023</td>
                <td>The progress of LiDAR-based 3D object detection has significantly enhanced developments in autonomous driving and robotics. However, due to the limitations of LiDAR sensors, object shapes suffer from deterioration in occluded and distant areas, which creates a fundamental challenge to 3D perception. Existing methods estimate specific 3D shapes and achieve remarkable performance. However, these methods rely on extensive computation and memory, causing imbalances between accuracy and real-time performance. To tackle this challenge, we propose a novel LiDAR-based 3D object detection model named BSH-Det3D, which applies an effective way to enhance spatial features by estimating complete shapes from a bird's eye view (BEV). Specifically, we design the Pillar-based Shape Completion (PSC) module to predict the probability of occupancy whether a pillar contains object shapes. The PSC module generates a BEV shape heatmap for each scene. After integrating with heatmaps, BSH-Det3D can provide additional information in shape deterioration areas and generate high-quality 3D proposals. We also design an attention-based densification fusion module (ADF) to adaptively associate the sparse features with heatmaps and raw points. The ADF module integrates the advantages of points and shapes knowledge with negligible overheads. Extensive experiments on the KITTI benchmark achieve state-of-the-art (SOTA) performance in terms of accuracy and speed, demonstrating the efficiency and flexibility of BSH-Det3D. The source code is available on https://github.com/mystorm16/BSH-Det3D.</td>
                <td>Heating systems, Solid modeling, Three-dimensional displays, Shape, Source coding, Object detection, Benchmark testing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341930&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341930&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>I3DOD: Towards Incremental 3D Object Detection via Prompting</td>
                <td>W. Liang, G. Sun, C. Liu, J. Dong and K. Wang</td>
                <td>2023</td>
                <td>3D object detection have achieved significant performance in many fields, e.g., robotics system, autonomous driving, and augmented reality. However, most existing methods could cause catastrophic forgetting of old classes when performing on the class-incremental scenarios. Meanwhile, the current class-incremental 3D object detection methods neglect the relationships between the object localization information and category semantic information, and assume all the knowledge of old model is reliable. To address the above challenge, we present a novel Incremental 3D Object Detection framework with the guidance of prompting, i.e., I3DOD. Specifically, we propose a task-shared prompts mechanism to learn the matching relationships between the object localization information and category semantic information. After training on the current task, these prompts will be stored in our prompt pool, and perform the relationship of old classes in the next task. Moreover, we design a reliable distillation strategy to transfer knowledge from two aspects: a reliable dynamic distillation is developed to filter out the negative knowledge and transfer the reliable 3D knowledge to new detection model; the relation feature is proposed to capture the responses relation in feature space and protect plasticity of the model when learning novel 3D classes. To the end, we conduct comprehensive experiments on two benchmark datasets and our method outperforms the state-of-the-art object detection methods by 0.6% ∼ 2.7% in terms of mAP@0.25.</td>
                <td>Location awareness, Training, Solid modeling, Three-dimensional displays, Semantics, Object detection, Reliability engineering</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341834&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341834&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SpinDOE: A Ball Spin Estimation Method for Table Tennis Robot</td>
                <td>T. Gossard, J. Tebbe, A. Ziegler and A. Zell</td>
                <td>2023</td>
                <td>Spin plays a considerable role in table tennis, making a shot's trajectory harder to read and predict. However, the spin is challenging to measure because of the ball's high velocity and the magnitude of the spin values. Existing methods either require extremely high framerate cameras or are unreliable because they use the ball's logo, which may not always be visible. Because of this, many table tennis-playing robots ignore the spin, which severely limits their capabilities. This paper proposes an easily implementable and reliable spin estimation method. We developed a dotted-ball orientation estimation (DOE) method, that can then be used to estimate the spin. The dots are first localized on the image using a CNN and then identified using geometric hashing. The spin is finally regressed from the estimated orientations. Using our algorithm, the ball's orientation can be estimated with a mean error of 2.4° and the spin estimation has an relative error lower than 1%. Spins up to 175 rps are measurable with a camera of 350 fps in real time. Using our method, we generated a dataset of table tennis ball trajectories with position and spin, available on our project page. Project page: https://cogsys-tuebingen.github.io/spindoe/.</td>
                <td>Torque, Sports equipment, Robot vision systems, Estimation, Cameras, Time measurement, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342178&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342178&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhancing Fine-Grained 3D Object Recognition Using Hybrid Multi-Modal Vision Transformer-CNN Models</td>
                <td>S. Xiong, G. Tziafas and H. Kasaei</td>
                <td>2023</td>
                <td>Robots operating in human-centered environments, such as retail stores, restaurants, and households, are often required to distinguish between similar objects in different contexts with a high degree of accuracy. However, fine-grained object recognition remains a challenge in robotics due to the high intra-category and low inter-category dissimilarities. In addition, the limited number of fine-grained 3D datasets poses a significant problem in addressing this issue effectively. In this paper, we propose a hybrid multi-modal Vision Transformer (ViT) and Convolutional Neural Networks (CNN) approach to improve the performance of fine-grained visual classification (FGVC). To address the shortage of FGVC 3D datasets, we generated two synthetic datasets. The first dataset consists of 20 categories related to restaurants with a total of 100 instances, while the second dataset contains 120 shoe instances. Our approach was evaluated on both datasets, and the results indicate that our hybrid multi-modal model outperforms both CNN-only and ViT-only baselines, achieving a recognition accuracy of 94.50% and 93.51% on the restaurant and shoe datasets, respectively. Additionally, we have made our FGVC RGB-D datasets available to the research community to enable further experimentation and advancement. Furthermore, we integrated our proposed method with a robot framework and demonstrated its potential as a fine-grained perception tool in both simulated and real-world robotic scenarios.</td>
                <td>Visualization, Solid modeling, Three-dimensional displays, Footwear, Transformers, Object recognition, Convolutional neural networks</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342235&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342235&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ScAR: Scaling Adversarial Robustness for LiDAR Object Detection</td>
                <td>X. Lu and H. Radha</td>
                <td>2023</td>
                <td>The adversarial robustness of a model is its ability to resist adversarial attacks in the form of small perturbations to input data. Universal adversarial attack methods such as Fast Sign Gradient Method (FSGM) [1] and Projected Gradient Descend (PGD) [2] are popular for LiDAR object detection, but they are often deficient compared to task-specific adversarial attacks. Additionally, these universal methods typically require unrestricted access to the model's information, which is difficult to obtain in real-world applications. To address these limitations, we present a black-box Scaling Adversarial Robustness (ScAR) method for LiDAR object detection. By analyzing the statistical characteristics of 3D object detection datasets such as KITTI, Waymo, and nuScenes, we have found that the model's prediction is sensitive to scaling of 3D instances. We propose three black-box scaling adversarial attack methods based on the available information: model-aware attack, distribution-aware attack, and blind attack. We also introduce a strategy for generating scaling adversarial examples to improve the model's robustness against these three scaling adversarial attacks. Comparison with other methods on public datasets under different 3D object detection architectures demonstrates the effectiveness of our proposed method.</td>
                <td>Solid modeling, Three-dimensional displays, Laser radar, Sensitivity, Closed box, Object detection, Resists</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341583&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341583&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Formal Composition of Robotic Systems as Contract Programs</td>
                <td>M. Nakamura, J. Svegliato, S. B. Nashed, S. Zilberstein and S. Russell</td>
                <td>2023</td>
                <td>Robotic systems are often composed of modular algorithms that each perform a specific function within a larger architecture, ranging from state estimation and task planning to trajectory optimization and object recognition. Existing work for specifying these systems as a formal composition of contract algorithms has limited expressiveness compared to the variety of sophisticated architectures that are commonly used in practice. Therefore, in this paper, we (1) propose a novel metareasoning framework for formally composing robotic systems as a contract program with programming constructs for functional, conditional, and looping semantics and (2) introduce a recursive hill climbing algorithm that finds a locally optimal time allocation of a contract program. In our experiments, we demonstrate that our approach outperforms baseline techniques in a simulated pick-and-place robot domain.</td>
                <td>Semantics, Programming, Planning, Resource management, Object recognition, Task analysis, State estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342341&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342341&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Competitive Ant Coverage: The Value of Pursuit</td>
                <td>A. Shats, M. Amir and N. Agmon</td>
                <td>2023</td>
                <td>This paper studies the problem of Competitive Ant Coverage, in which two ant-like robots with very limited capabilities in terms of sensing range, computational power, and knowledge of the world compete in an area coverage task. We examine two variants of the problem that differ in the robot's objective: either being the First to Cover a Cell (FCC), or being the Last to Cover a Cell (LCC). Each robot's goal is to acquire (by visiting first or last, respectively) more cells than the opposing robot, and by that win the game. We examine the problem both theoretically and empirically, and show that the main strategy for dominance revolves around the ability to pursue: in LCC, we wish to pursue the opposing robot, whereas in FCC, we wish to create a scenario wherein the opposing robot pursues us. We find that this ability relies more heavily on knowledge of the opponent's strategy than on the robot's sensing capabilities. Moreover, given the robot's limited capabilities, we find that this knowledge-gap cannot be easily mitigated by learning.</td>
                <td>Heuristic algorithms, FCC, Games, Robot sensing systems, Sensors, Task analysis, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342063&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342063&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Human-Aware Navigation in Crowded Environments Using Adaptive Proxemic Area and Group Detection</td>
                <td>Sánchez, S. Janzon, M. Zella, J. Capitán and P. J. Marrón</td>
                <td>2023</td>
                <td>Navigation is an essential task for social robots. However, certain rules must be followed to allow them to move without causing distraction or discomfort to people. Considering that the context surrounding robots and persons affects the expected behavior, this work defines a social area around a person that adapts to the real situation. In addition, the social context of a person is extended to identify groups of people, which the robot should take into account while navigating. With this understanding of the surrounding of the robot together with the ability to predict the trajectory of individuals as well as groups, the proposed solution not only effectively addresses collision avoidance while promoting socially acceptable behavior but also outperforms the majority of recent works in terms of accuracy. Furthermore, a dedicated policy is introduced to react to social navigation conflicts. The evaluation performed in a simulated environment shows that the computation of our proposed solution is at least 8 times faster than the best state-of-the-art approach while preserving comparable social conduct. Also, the results of realistic experiments performed using Gazebo and a real robot are reported.</td>
                <td>Costs, Navigation, Social robots, Trajectory, Behavioral sciences, Resource management, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342385&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342385&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Heuristics for Multi-Robot Path Planning in Crowded Environments</td>
                <td>T. Guo and J. Yu</td>
                <td>2023</td>
                <td>Optimal Multi-Robot Path Planning (MRPP) has garnered significant attention due to its many applications in domains including warehouse automation, transportation, and swarm robotics. Current MRPP solvers can be divided into reduction-based, search-based, and rule-based categories, each with their strengths and limitations. Regardless of the methodology, however, the issue of handling dense MRPP instances remains a significant challenge, where existing approaches generally demonstrate a dichotomy regarding solution optimality and efficiency. This study seeks to bridge the gap in optimal MRPP resolution for dense, highly-entangled scenarios, with potential applications to high-density storage systems and traffic congestion control. Toward that goal, we analyze the behaviors of SOTA MRPP algorithms in dense settings and develop two hybrid algorithms leveraging the strengths of existing SOTA algorithms: DCBS (database-accelerated enhanced conflict-based search) and SCBS (sparsified enhanced conflict-based search). Experimental validations demonstrate that DCBS and SCBS deliver a significant reduction in computational time compared to existing bounded-suboptimal methods and improve solution quality compared to existing rule-based methods, achieving a desirable balance between computational efficiency and solution optimality. As a result, DCBS and SCBS are particularly suitable for quickly computing good-quality solutions for multi-robot routing in dense settings. Simulation video https://youtu.be/dZxMPUr7Bqg Upon the publication of the manuscript source code and data will be released at https://github.com/arc-l/dcbs</td>
                <td>Automation, Source coding, Transportation, Swarm robotics, Routing, Path planning, Computational efficiency</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341800&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341800&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Depth Vision-Based Personalized Robot Navigation From Dynamic Demonstrations in Virtual Reality</td>
                <td>J. de Heuvel, N. Corral, B. Kreis, J. Conradi, A. Driemel and M. Bennewitz</td>
                <td>2023</td>
                <td>For the best human-robot interaction experience, the robot's navigation policy should take into account personal preferences of the user. In this paper, we present a learning framework complemented by a perception pipeline to train a depth vision-based, personalized navigation controller from user demonstrations. Our virtual reality interface enables the demonstration of robot navigation trajectories under motion of the user for dynamic interaction scenarios. The novel perception pipeline enrolls a variational autoencoder in combination with a motion predictor. It compresses the perceived depth images to a latent state representation to enable efficient reasoning of the learning agent about the robot's dynamic environment. In a detailed analysis and ablation study, we evaluate different configurations of the perception pipeline. To further quantify the navigation controller's quality of personalization, we develop and apply a novel metric to measure preference reflection based on the Frechet Distance. We discuss the robot's navigation performance in various virtual scenes and demonstrate the first personalized robot navigation controller that solely relies on depth images. A supplemental video highlighting our approach is available online11Full video: hrl.uni-bonn.de/publications/deheuve123iros_learning.mp4.</td>
                <td>Measurement, Navigation, Pipelines, Dynamics, Virtual reality, Vision sensors, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341370&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341370&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>From Crowd Motion Prediction to Robot Navigation in Crowds</td>
                <td>S. Poddar, C. Mavrogiannis and S. S. Srinivasa</td>
                <td>2023</td>
                <td>We focus on robot navigation in crowded environments. To navigate safely and efficiently within crowds, robots need models for crowd motion prediction. Building such models is hard due to the high dimensionality of multiagent domains and the challenge of collecting or simulating interaction-rich crowd-robot demonstrations. While there has been important progress on models for offline pedestrian motion forecasting, transferring their performance on real robots is nontrivial due to close interaction settings and novelty effects on users. In this paper, we investigate the utility of a recent state-of-the-art motion prediction model (S-GAN) for crowd navigation tasks. We incorporate this model into a model predictive controller (MPC) and deploy it on a self-balancing robot which we subject to a diverse range of crowd behaviors in the lab. We demonstrate that while S-GAN motion prediction accuracy transfers to the real world, its value is not reflected on navigation performance, measured with respect to safety and efficiency; in fact, the MPC performs indistinguishably even when using a simple constant-velocity prediction model, suggesting that substantial model improvements might be needed to yield significant gains for crowd navigation tasks. Footage from our experiments can be found at https://youtu.be/mzFiXgSKsZ0.</td>
                <td>Pedestrians, Navigation, Buildings, Predictive models, Gain measurement, Safety, Motion measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341464&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341464&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Game-Theoretic Framework for Joint Forecasting and Planning</td>
                <td>K. Kedia, P. Dan and S. Choudhury</td>
                <td>2023</td>
                <td>Planning safe robot motions in the presence of humans requires reliable forecasts of future human motion. However, simply predicting the most likely motion from prior interactions does not guarantee safety. Such forecasts fail to model the long tail of possible events, which are rarely observed in limited datasets. On the other hand, planning for worst-case motions leads to overtly conservative behavior and a “frozen robot”. Instead, we aim to learn forecasts that predict counterfactuals that humans guard against. We propose a novel game-theoretic framework for joint planning and forecasting with the payoff being the performance of the planner against the demonstrator, and present practical algorithms to train models in an end-to-end fashion. We demonstrate that our proposed algorithm results in safer plans in a crowd navigation simulator and real-world datasets of pedestrian motion. We release our code at https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.</td>
                <td>Robot motion, Pedestrians, Navigation, Tail, Predictive models, Prediction algorithms, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341265&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341265&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Coordination of Bounded Rational Drones Through Informed Prior Policy</td>
                <td>D. Pushp, J. Xu and L. Liu</td>
                <td>2023</td>
                <td>Biological agents, such as humans and animals, are capable of making decisions out of a very large number of choices in a limited time. They can do so because they use their prior knowledge to find a solution that is not necessarily optimal but good enough for the given task. In this work, we study the motion coordination of multiple drones under the above-mentioned paradigm, Bounded Rationality (BR), to achieve cooperative motion planning tasks. Specifically, we design a prior policy that provides useful goal-directed navigation heuristics in familiar environments and is adaptive in unfamiliar ones via Reinforcement Learning augmented with an environment-dependent exploration noise. Integrating this prior policy in the game-theoretic bounded rationality framework allows agents to quickly make decisions in a group considering other agents' computational constraints. Our investigation assures that agents with a well-informed prior policy increase the efficiency of the collective decision-making capability of the group. We have conducted rigorous experiments in simulation and in the real world to demonstrate that the ability of informed agents to navigate to the goal safely can guide the group to coordinate efficiently under the BR framework.</td>
                <td>Uncertainty, Navigation, Decision making, Reinforcement learning, Robustness, Planning, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342006&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342006&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Mapless Urban Robot Navigation by Following Pedestrians</td>
                <td>Medrano, A. Cosgun, E. Croft and W. P. Chan</td>
                <td>2023</td>
                <td>Navigating effectively and safely in unknown urban environments is a crucial ability for service robot applications such as last-mile package delivery. To reach the entrance of its target destination, the robot must make informed local and global path planning decisions. We present a mapless global planning strategy based on pedestrian following. Our method allows the robot to exploit natural routes taken by surrounding pedestrians to make informed and efficient path planning decisions for reaching its goal. The algorithm also includes a recovery system to assist the robot when insufficient progress is made (i.e. robot stuck in dead end). Once the robot is within the vicinity of the target building, a wall following behaviour is used to reach the entrance of the target building. Simulated experiments and a proof-of-concept demonstration on a real robot were shown to validate the approach.</td>
                <td>Pedestrians, Navigation, Service robots, Buildings, Urban areas, Path planning, Product delivery</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341843&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341843&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Control of Cart-Like Nonholonomic Systems Using a Mobile Manipulator</td>
                <td>S. Aguilera and S. Hutchinson</td>
                <td>2023</td>
                <td>This work focuses on the capability of Mobile Manipulators to effectively control and maneuver cart-like non-holonomic systems. These cart-like systems are passive-wheeled objects with nonholonomic constraints with varying inertial parameters. We derive the dynamic equations of the cart-like system using a constrained Euler-Lagrange formulation and propose a Linear Quadratic Regulator controller to move the cart along a desired trajectory using external forces (applied by the MM) at a given contact point. For the MM, we present a control architecture to i) control the mobile base to keep the cart inside the workspace of the manipulator and ii) a control Lyapunov function formulation to control the manipulator in torque control, while decoupling the motion of the base from the arm and applying the required wrench onto the object. We validate our approach experimentally, using a MM to push a shopping cart and track desired trajectories. These experiments show the accuracy of the control architecture to track the desired trajectories for carts with different inertial parameters and improve the controllability of the system by changing the contact point on the cart.</td>
                <td>Torque, Regulators, Tracking, Dynamics, Torque control, Computer architecture, Mathematical models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342088&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342088&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FRoGGeR: Fast Robust Grasp Generation via the Min-Weight Metric</td>
                <td>A. H. Li, P. Culbertson, J. W. Burdick and A. D. Ames</td>
                <td>2023</td>
                <td>Many approaches to grasp synthesis optimize analytic quality metrics that measure grasp robustness based on finger placements and local surface geometry. However, generating feasible dexterous grasps by optimizing these metrics is slow, often taking minutes. To address this issue, this paper presents FRoGGeR: a method that quickly generates robust precision grasps using the min-weight metric, a novel, almost-everywhere differentiable approximation of the classical $\epsilon$ grasp metric. The min-weight metric is simple and interpretable, provides a reasonable measure of grasp robustness, and admits numerically efficient gradients for smooth optimization. We leverage these properties to rapidly synthesize collision-free robust grasps-typically in less than a second. FRoGGeR can refine the candidate grasps generated by other methods (heuristic, data-driven, etc.) and is compatible with many object representations (SDFs, meshes, etc.), We study FRoGGeR's performance on over 40 objects drawn from the YCB dataset, outperforming a competitive baseline in computation time, feasibility rate of grasp synthesis, and picking success in simulation. We conclude that FRoGGeR is fast: it has a median synthesis time of 0.834s over hundreds of experiments.</td>
                <td>Measurement, Geometry, Fingers, Robustness, Optimization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341806&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341806&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Minimal Collision Strategy of Synergy Between Pushing and Grasping for Large Clusters of Objects</td>
                <td>C. Chen, S. Yan, M. Yuan, C. Tay, D. Choi and Q. Dan Le</td>
                <td>2023</td>
                <td>Grasping and moving objects in a large cluster is a common real scenario. In such scenarios, tens of objects are adjacent to each other, even stacked layer by layer, so that simple grasp would not work due to obstruction. In this paper, we propose a well-designed strategy to use synergy of pushing and grasping to automatically push and grasp objects in a large tightly packed cluster of objects. Our strategy is to detect and grasp isolated graspable objects first before other actions. We then use a smart strategy that pushes objects at the narrowest edge of the clusters. For push action, the robot pushes the edge at the perpendicular direction relative to the cluster, thus improving the performance of isolation and minimizing collisions. We have conducted experiments in both simulation and real-world environments with more than 20 cluttered objects and demonstrated that our solution outperforms existing deep learning based methods, especially in challenging cases, and achieves significantly higher completion rate, grasp success rate, picked rate and efficiency.</td>
                <td>Deep learning, Image edge detection, Grasping, Collision avoidance, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341452&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341452&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Flexible Biolaboratory Automation: Container Taxonomy-Based, 3D-Printed Gripper Fingers*</td>
                <td>H. Zwirnmann, D. Knobbe, U. Culha and S. Haddadin</td>
                <td>2023</td>
                <td>Automation in the life science research laboratory is a paradigm that has gained increasing relevance in recent years. Current robotic solutions often have a limited scope, which reduces their acceptance and prevents the realization of complex workflows. The transport and manipulation of laboratory supplies with a robot is a particular case where this limitation manifests. In this paper, we deduce a taxonomy of biolaboratory liquid containers that clarifies the need for a flexible grasping solution. Using the taxonomy as a guideline, we design fingers for a parallel robotic gripper which are developed with a monolithic dual-extrusion 3D print that integrates rigid and soft materials to optimize gripping properties. We design fine-tuned fingertips that provide stable grasps of the containers in question. A simple actuation system and a low weight are maintained by adopting a passive compliant mechanism. The ability to resist chemicals and high temperatures and the integration with a tool exchange system render the fingers usable for daily laboratory use and complex workflows. We present the task suitability of the fingers in experiments that show the wide range of vessels that can be handled as well as their tolerance against displacements and their grasp stability.</td>
                <td>Three-dimensional displays, Automation, Temperature, Taxonomy, Grasping, Containers, Grippers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342218&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342218&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Pretouch Perception Algorithm for Object Material and Structure Mapping to Assist Grasp and Manipulation Using a DMDSM Sensor</td>
                <td>F. Guo, S. Xie, D. Wang, C. Fang, J. Zou and D. Song</td>
                <td>2023</td>
                <td>We report a new material and structure mapping (MSM) algorithm to assist robotic grasping and manipulation. Building on our new sensor development, the algorithm has four main components: 1) detection of time-of-flight (ToF) durations for the dual modalities of optoacoustic (OA) and pulse-echo ultrasound (US), 2) contour reconstruction by fusing OA and US signals, 3) local noise filtering by checking local consistency of material and structure label (MSL), and 4) medium boundary searching that identifies class boundaries through two-staged clustering and boundary establishment using support vector machine (SVM) hyperplanes. We have implemented our algorithm and tested it with multiple common household items. The experimental results have successfully validated our algorithm design which shows that the average error of contour reconstruction is 0.05 mm and the true positive rate of MSL is over 98%.</td>
                <td>Support vector machines, Ultrasonic imaging, Filtering, Buildings, Clustering algorithms, Grasping, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341560&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341560&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PoseFusion: Robust Object-in-Hand Pose Estimation with SelectLSTM</td>
                <td>Y. Tu, J. Jiang, S. Li, N. Hendrich, M. Li and J. Zhang</td>
                <td>2023</td>
                <td>Accurate estimation of the relative pose between an object and a robot hand is critical for many manipulation tasks. However, most of the existing object-in-hand pose datasets use two-finger grippers and also assume that the object remains fixed in the hand without any relative movements, which is not representative of real-world scenarios. To address this issue, a 6D object-in-hand pose dataset is proposed using a teleoperation method with an anthropomorphic Shadow Dexterous hand. Our dataset comprises RGB-D images, proprioception and tactile data, covering diverse grasping poses, finger contact states, and object occlusions. To overcome the significant hand occlusion and limited tactile sensor contact in real-world scenarios, we propose PoseFusion, a hybrid multi-modal fusion approach that integrates the information from visual and tactile perception channels. PoseFusion generates three candidate object poses from three estimators (tactile only, visual only, and visuo-tactile fusion), which are then filtered by a SelectLSTM network to select the optimal pose, avoiding inferior fusion poses resulting from modality collapse. Extensive experiments demonstrate the robustness and advantages of our framework. All data and codes are available on the project website: https://elevenjiang1.github.io/ObjectlnHand-Dataset/.</td>
                <td>Training, Visualization, Pose estimation, Tactile sensors, Grasping, Information filters, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341688&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341688&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation</td>
                <td>T. Yang et al.</td>
                <td>2023</td>
                <td>In this paper, we present a novel method for mobile manipulators to perform multiple contact-rich manipulation tasks. While learning-based methods have the potential to generate actions in an end-to-end manner, they often suffer from insufficient action accuracy and robustness against noise. On the other hand, classical control-based methods can enhance system robustness, but at the cost of extensive parameter tuning. To address these challenges, we present MOMA-Force, a visual-force imitation method that seamlessly combines representation learning for perception, imitation learning for complex motion generation, and admittance whole-body control for system robustness and controllability. MOMA-Force enables a mobile manipulator to learn multiple complex contact-rich tasks with high success rates and small contact forces. In a real household setting, our method outperforms baseline methods in terms of task success rates. Moreover, our method achieves smaller contact forces and smaller force variances compared to baseline methods without force imitation. Overall, we offer a promising approach for efficient and robust mobile manipulation in the real world. Videos and more details can be found on https://visual-force-imitation.github.io.</td>
                <td>Learning systems, Visualization, Force, Manipulators, Robustness, Trajectory, Admittance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342371&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342371&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Task-Oriented Grasping with Point Cloud Representation of Objects</td>
                <td>A. Patankar, K. Phi, D. Mahalingam, N. Chakraborty and I. Ramakrishnan</td>
                <td>2023</td>
                <td>In this paper, we study the problem of task-oriented grasp synthesis from partial point cloud data using an eye-in-hand camera configuration. In task-oriented grasp synthesis, a grasp has to be selected so that the object is not lost during manipulation, and it is also ensured that adequate force/moment can be applied to perform the task. We formalize the notion of a gross manipulation task as a constant screw motion (or a sequence of constant screw motions) to be applied to the object after grasping. Using this notion of task, and a corresponding grasp quality metric developed in our prior work, we use a neural network to approximate a function for predicting the grasp quality metric on a cuboid shape. We show that by using a bounding box obtained from the partial point cloud of an object, and the grasp quality metric mentioned above, we can generate a good grasping region on the bounding box that can be used to compute an antipodal grasp on the actual object. Our algorithm does not use any manually labeled data or grasping simulator, thus making it very efficient to implement and integrate with screw linear interpolation-based motion planners. We present simulation as well as experimental results that show the effectiveness of our approach. Website: https://irsl-sbu.github.io/Task-Oriented-Grasping-from-Point-Cloud-Representation/.</td>
                <td>Point cloud compression, Measurement, Shape, Neural networks, Grasping, Fasteners, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342318&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342318&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Refining 6-DoF Grasps with Context-Specific Classifiers</td>
                <td>T. Taunyazov, H. Zhang, J. P. Eala, N. Zhao and H. Soh</td>
                <td>2023</td>
                <td>In this work, we present GraspFlow, a refinement approach for generating context-specific grasps. We formulate the problem of grasp synthesis as a sampling problem: we seek to sample from a context-conditioned probability distribution of successful grasps. However, this target distribution is unknown. As a solution, we devise a discriminator gradient-flow method to evolve grasps obtained from a simpler distribution in a manner that mimics sampling from the desired target distribution. Unlike existing approaches, GraspFlow is modular, allowing grasps that satisfy multiple criteria to be obtained simply by incorporating the relevant discriminators. It is also simple to implement, requiring minimal code given existing auto-differentiation libraries and suitable discriminators. Experiments show that GraspFlow generates stable and executable grasps on a real-world Panda robot for a diverse range of objects. In particular, in 60 trials on 20 different household objects, the first attempted grasp was successful 94% of the time, and 100% grasp success was achieved by the second grasp. Moreover, incorporating a functional discriminator for robot-human handover improved the functional aspect of the grasp by up to 33%.</td>
                <td>Codes, Refining, MIMICs, Handover, Probability distribution, Libraries, 6-DOF</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341671&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341671&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sequential Manipulation Planning for Over-Actuated Unmanned Aerial Manipulators</td>
                <td>Y. Su et al.</td>
                <td>2023</td>
                <td>We investigate the sequential manipulation planning problem for unmanned aerial manipulators (UAMs). Unlike prior work that primarily focuses on one-step manipulation tasks, sequential manipulations require coordinated motions of a UAM's floating base, the manipulator, and the object being manipulated, entailing a unified kinematics and dynamics model for motion planning under designated constraints. By leveraging a virtual kinematic chain (VKC)-based motion planning framework that consolidates components' kinematics into one chain, the sequential manipulation task of a UAM can be planned as a whole, yielding more coordinated motions. Integrating the kinematics and dynamics models with a hierarchical control framework, we demonstrate, for the first time, an over-actuated UAM achieves a series of new sequential manipulation capabilities in both simulation and experiment.</td>
                <td>Wireless communication, Tracking, Toy manufacturing industry, Dynamics, Kinematics, Planning, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341441&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341441&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Force-Based Pose Regulation of a Cable-Suspended Load Using UAVs with Force Bias</td>
                <td>C. Gabellieri, M. Tognon, D. Sanalitro and A. Franchi</td>
                <td>2023</td>
                <td>This work studies how force measurement/estimation biases affect the force-based cooperative manipulation of a beam-like load suspended with cables by two aerial robots. Indeed, force biases are especially relevant in a force-based manipulation scenario in which direct communication is not relied upon. First, we compute the equilibrium configurations of the system. Then, we show that inducing an internal force in the load augments the robustness of the load attitude error and its sensitivity to force-bias variations. Eventually, we propose a method for zeroing the load position error. The results are validated through numerical simulations and experiments.</td>
                <td>Sensitivity, Force measurement, Force, Robot sensing systems, Numerical simulation, Autonomous aerial vehicles, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342240&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342240&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Roller-Quadrotor: A Novel Hybrid Terrestrial/Aerial Quadrotor with Unicycle-Driven and Rotor-Assisted Turning</td>
                <td>Z. Zheng et al.</td>
                <td>2023</td>
                <td>The Roller-Quadrotor is a novel quadrotor that combines the maneuverability of aerial drones with the endurance of ground vehicles. This work focuses on the design, modeling, and experimental validation of the Roller-Quadrotor. Flight capabilities are achieved through a quadrotor config-uration, with four thrust-providing actuators. Additionally, rolling motion is facilitated by a unicycle-driven and rotor-assisted turning structure. By utilizing terrestrial locomotion, the vehicle can overcome rolling and turning resistance, thereby conserving energy compared to its flight mode. This innovative approach not only tackles the inherent challenges of traditional rotorcraft but also enables the vehicle to roll through narrow gaps and overcome obstacles by taking advantage of its aerial mobility. We develop comprehensive models and controllers for the Roller-Quadrotor and validate their performance through experiments. The results demonstrate its seamless transition between aerial and terrestrial locomotion, as well as its ability to safely roll through gaps half the size of its diameter. Moreover, the terrestrial range of the vehicle is approximately 2.8 times greater, while the operating time is about 41.2 times longer compared to its aerial capabilities. These findings underscore the feasibility and effectiveness of the proposed structure and control mechanisms for efficient rolling through challenging terrains while conserving energy.</td>
                <td>Resistance, Actuators, Energy conservation, Rotors, Turning, Land vehicles, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341703&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341703&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>HALO: A Safe, Coaxial, and Dual-Ducted UAV Without Servo</td>
                <td>H. Li et al.</td>
                <td>2023</td>
                <td>This paper presents a novel uncrewed aerial vehicle (UAV) design named HALO, which stands for “harmless aerial limber robot”. HALO uses a swashplateless mechanism to generate a moment for pitch and roll control without requiring additional actuators such as servo, reducing the number of components needed for control and enhancing reliability. Its reduced weight and number of actuators improve payload capacity and maneuverability. Meanwhile, HALO's coaxial duct design improves safety and aerodynamic efficiency. Experimental tests, including figure-of-eight trajectory tracking, wind gust and stick poking disturbances, hover efficiency comparison, and actual flight with collision is conducted to confirm HALO's robustness and exceptional safety characteristics, suggesting it as a promising design for various applications.</td>
                <td>Actuators, Three-dimensional displays, Trajectory tracking, Ducts, Autonomous aerial vehicles, Robustness, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341923&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341923&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning to Open Doors with an Aerial Manipulator</td>
                <td>E. Cuniato, I. Geles, W. Zhang, O. Andersson, M. Tognon and R. Siegwart</td>
                <td>2023</td>
                <td>The field of aerial manipulation has seen rapid advances, transitioning from push-and-slide tasks to interaction with articulated objects. The motion trajectory of these complex actions is usually hand-crafted or a result of online optimization methods like Model Predictive Control (MPC) or Model Predictive Path Integral (MPPI) control. However, these methods rely on heuristics or model simplifications to efficiently run on onboard hardware, limiting their robustness, and making them sensitive to disturbances and differences between the real environment and its model. In this work, we propose a Reinforcement Learning (RL) approach to learn reactive motion behaviors for a manipulation task while producing policies that are robust to disturbances and modeling errors. Specifically, we train a policy to perform a door-opening task with an Omnidirectional Micro Aerial Vehicle (OMAV). The policy is trained in a physics simulator and shown in the real world, where it is able to generalize also to door closing tasks never seen in training. We also compare our method against a state-of-the-art MPPI solution in simulation, showing a considerable increase in robustness and speed.</td>
                <td>Training, Optimization methods, Reinforcement learning, Predictive models, Robustness, Trajectory, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342289&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342289&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Planning and Control for a Dynamic Morphing-Wing UAV Using a Vortex Particle Model</td>
                <td>G. Perrotta et al.</td>
                <td>2023</td>
                <td>Achieving precise, highly-dynamic maneuvers with Unmanned Aerial Vehicles (UAVs) is a major challenge due to the complexity of the associated aerodynamics. In particular, unsteady effectsas might be experienced in post-stall regimes or during sudden vehicle morphing-can have an adverse impact on the performance of modern flight control systems. In this paper, we present a vortex particle model and associated model-based controller capable of reasoning about the unsteady aerodynamics during aggressive maneuvers. We evaluate our approach in hardware on a morphing-wing UAV executing post-stall perching maneuvers. Our results show that the use of the unsteady aerodynamics model improves performance during both fixed-wing and dynamic-wing perching, while the use of wing-morphing planned with quasi-steady aerodynamics results in reduced performance. While the focus of this paper is a pre-computed control policy, we believe that, with sufficient computational resources, our approach could enable online planning in the future.</td>
                <td>Computational modeling, Aerodynamics, Autonomous aerial vehicles, Robot sensing systems, Real-time systems, Planning, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342191&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342191&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Physical Contact with Wall using a Multirotor UAV Equipped with Add-On Thruster for Inspection Work</td>
                <td>T. Kominami, Z. Liang, R. R. Martinez, H. Paul and K. Shimonomura</td>
                <td>2023</td>
                <td>Inspection and maintenance work at heights car-ries significant risks and is time consuming for human workers. Therefore, aerial manipulators are expected to replace these tasks. This paper presents a multirotor UAV equipped with a single horizontal thruster. This minimal configuration en-ables physical contact while keeping the airframe's attitude horizontal for non-destructive inspection work on vertical wall surfaces. The thrust required to move forward and backward during a physical contact task is independent of the thrust required for hovering, simplifying control of the UAV. Utilizing onboard sensors, the UAV automatically maintains a forward-facing posture against the wall, initiates and sustains contact, and disengages when necessary. Additionally, the UAV in this study incorporates an ultrasonic thickness measurement device, allowing for the verification of automated measurements while in flight.</td>
                <td>Attitude control, Ultrasonic variables measurement, Inspection, Position measurement, Maintenance engineering, Autonomous aerial vehicles, Acoustics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341576&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341576&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DoubleBee: A Hybrid Aerial-Ground Robot with Two Active Wheels</td>
                <td>M. Cao, X. Xu, S. Yuan, K. Cao, K. Liu and L. Xie</td>
                <td>2023</td>
                <td>In this paper, we present the dynamic model and control of DoubleBee, a novel hybrid aerial-ground vehicle consisting of two propellers mounted on tilting servo motors and two motor-driven wheels. DoubleBee exploits the high energy efficiency of a bicopter configuration in aerial mode, and enjoys the low power consumption of a two-wheel self-balancing robot on the ground. Furthermore, the propeller thrusts act as additional control inputs on the ground, enabling a novel decoupled control scheme where the attitude of the robot is controlled using thrusts and the translational motion is realized using wheels. A prototype of DoubleBee is constructed using commercially available components. The power efficiency and the control performance of the robot are verified through comprehensive experiments. Challenging tasks in indoor and outdoor environments demonstrate the capability of DoubleBee to traverse unstructured environments, fly over and move under barriers, and climb steep and rough terrains.</td>
                <td>Propellers, Attitude control, Wheels, Energy efficiency, Stability analysis, Mobile robots, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341984&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341984&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Swashplateless-Elevon Actuation for a Dual-Rotor Tail-Sitter VTOL UAV</td>
                <td>N. Chen et al.</td>
                <td>2023</td>
                <td>In this paper, we propose a novel swashplateless-elevon actuation (SEA) for dual-rotor tail-sitter vertical takeoff and landing (VTOL) unmanned aerial vehicles (UAVs). In contrast to the conventional elevon actuation (CEA) which controls both pitch and yaw using elevons, the SEA adopts swash-plateless mechanisms to generate an extra moment through motor speed modulation to control pitch and uses elevons solely for controlling yaw, without requiring additional actuators. This decoupled control strategy mitigates the saturation of elevons' deflection needed for large pitch and yaw control actions, thus improving the UAV's control performance on trajectory tracking and disturbance rejection performance in the presence of large external disturbances. Furthermore, the SEA overcomes the actuation degradation issues experienced by the CEA when the UAV is in close proximity to the ground, leading to a smoother and more stable take-off process. We validate and compare the performances of the SEA and the CEA in various real-world flight conditions, including take-off, trajectory tracking, and hover flight and position steps under external disturbance. Experimental results demonstrate that the SEA has better performances than the CEA. Moreover, we verify the SEA's feasibility in the attitude transition process and fixed-wing-mode flight of the VTOL UAV. The results indicate that the SEA can accurately control pitch in the presence of high-speed incoming airflow and maintain a stable attitude during fixed-wing mode flight. Video of all experiments can be found in youtube.com/watch?v=Sx9Rk4Zf7sQ</td>
                <td>Degradation, Actuators, Fans, Three-dimensional displays, Trajectory tracking, Attitude control, Modulation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341861&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341861&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SBlimp: Design, Model, and Translational Motion Control for a Swing-Blimp</td>
                <td>Antonio, D. J. Ammirato and D. Saldaña</td>
                <td>2023</td>
                <td>We present an aerial vehicle composed of a custom quadrotor with tilted rotors and a helium balloon, called SBlimp. We propose a novel control strategy that takes advantage of the natural stable attitude of the blimp to control translational motion. Different from cascade controllers in the literature that controls attitude to achieve desired translational motion, our approach directly controls the linear velocity regardless of the heading orientation of the vehicle. As a result, the vehicle swings during the translational motion. We provide a planar analysis of the dynamic model, demonstrating stability for our controller. Our design is evaluated in numerical simulations with different physical factors and validated with experiments using a real-world prototype, showing that the SBlimp is able to achieve stable translation regardless of its orientation.</td>
                <td>Attitude control, Rotors, Prototypes, Numerical simulation, Stability analysis, Numerical models, Vehicle dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341796&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341796&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>On Semi-Autonomous Robotic Telemanipulation Employing Electromyography Based Motion Decoding and Potential Fields</td>
                <td>B. Guan, R. V. Godoy, F. Sanches, A. Dwivedi and M. Liarokapis</td>
                <td>2023</td>
                <td>Telemanipulation is widely used in robotics applications, ranging from maintenance of various industrial systems to search and rescue response in remote and/or hazardous environments. Human operators are often responsible for the control of such robotic systems. However, these remote interactions require highly trained and experienced operators owing to their complex nature. Semi-autonomous systems are presented as an alternative to complex and counter-intuitive manual systems, combining decoded user intentions with autonomous control modules. This paper proposes a semi-autonomous framework for robotic telemanipulation that employs Electromyography (EMG) based motion decoding and potential fields to execute complex object stacking tasks with a dexterous robot arm-hand system. Even though simple EMG-based teleoperation is promising, the signals are often noisy leading to induced randomness and control errors. To assist the user during task executions, potential fields are utilized to avoid obstacles and guide the robot end-effector toward the objects of interest, thus reducing the cognitive load on the user and the need for accurate predictions. The user's motion is decoded from the myoelectric activations of the human upper arm and upper torso using a Random Forest-based regression methodology. The objects are detected in the environment with an external camera that provides their goal poses to the potential fields scheme. EMG control and potential fields work in a synergistic manner simplifying the system's operation. The framework performance is experimentally validated in real-time experiments involving complex cube and cylinder stacking tasks.</td>
                <td>Electric potential, Visualization, Service robots, Stacking, Robot vision systems, Cameras, Electromyography</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342155&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342155&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Affordances and Electromyography Based Telemanipulation Framework for Control of Robotic Arm-Hand Systems</td>
                <td>R. V. Godoy, B. Guan, A. Dwivedi and M. Liarokapis</td>
                <td>2023</td>
                <td>Over the last decades, significant research effort has been put into creating Electromyography (EMG) based controllers for intuitive, hands-free control of robotic arms and hands. To achieve this, machine learning models have been employed to decode human motion and intention using EMG signals as input and to deliver several applications, such as prosthesis control using gesture classification. Despite the advances introduced by new deep learning techniques, real-time control of robot arms and hands using EMG signals as input still lacks accuracy, especially when a plethora of gestures are included as labels in the case of classification. This has been observed to be due to the noise and non-stationarity of the EMG signals and the increased dimensionality of the problem. In this paper, we propose an intuitive, affordances-oriented EMG-based telemanipulation framework for a robot arm-hand system that allows for dexterous control of the device. An external camera is utilized to perform scene understanding and object detection and recognition, providing grasping and manipulation assistance to the user and simplifying control. Object-specific Transformer-based classifiers are employed based on the affordances of the object of interest, reducing the number of possible gesture outputs, dividing and conquering the problem, and resulting in a more robust and accurate gesture decoding system when compared to a single generic classification model. The performance of the proposed system is experimentally validated in a remote telemanipulation setting, where the user successfully performs a set of dexterous manipulation tasks.</td>
                <td>Affordances, Robot vision systems, Transformers, Manipulators, Cameras, Electromyography, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341955&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341955&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Orbital Head-Mounted Display: A Novel Interface for Viewpoint Control during Robot Teleoperation in Cluttered Environments</td>
                <td>S. Kuitert, J. Hofland, C. J. M. Heemskerk, D. A. Abbink and L. Peternel</td>
                <td>2023</td>
                <td>Robotic teleoperation is used in various applications, including the nuclear industry, where the experience and intelligence of a human operator are necessary for making complex decisions that are beyond the autonomy of robots. Human-robot interfaces that help strengthen an operators situational awareness without inducing excessive cognitive load are crucial to the success of teleoperation. This paper presents a novel visual interface that allows operators to simultaneously control a 6-DoF camera platform and a robotic manipulator whilst experiencing the remote environment through a virtual reality head-mounted display (HMD). The proposed system, Orbital Head-Mounted Display (OHMD), utilizes head rotation tracking to command camera movement in azimuth and elevation directions around a fixation point located at a robot's end-effector. A human factor study was conducted to compare the interface acceptance, perceived workload, and task performance of OHMD with a conventional interface utilizing multiple fixed cameras (Array) and a standard head-mounted display implementation (HMD). Results show that both the OHMD and HMD interfaces significantly improve task performance, reduce perceived workload and increase interface acceptance compared to the Array interface. Participants reported they preferred OHMD due to the increased assistance and freedom in viewpoint selection. Whilst OHMD excelled in usefulness, the standard HMD interface allowed operators to perform robotic welding tasks significantly faster.</td>
                <td>Visualization, Head-mounted displays, Welding, Robot vision systems, Resists, Cameras, Orbits</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341733&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341733&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Exploiting Task Tolerances in Mimicry-Based Telemanipulation</td>
                <td>Y. Wang, C. Sifferman and M. Gleicher</td>
                <td>2023</td>
                <td>We explore task tolerances, i.e., allowable position or rotation inaccuracy, as an important resource to facilitate smooth and effective telemanipulation. Task tolerances provide a robot flexibility to generate smooth and feasible motions; however, in teleoperation, this flexibility may make the user's control less direct. In this work, we implemented a telema-nipulation system that allows a robot to autonomously adjust its configuration within task tolerances. We conducted a user study comparing a telemanipulation paradigm that exploits task tolerances (functional mimicry) to a paradigm that requires the robot to exactly mimic its human operator (exact mimicry), and assess how the choice in paradigm shapes user experience and task performance. Our results show that autonomous adjustments within task tolerances can lead to performance improvements without sacrificing perceived control of the robot. Additionally, we find that users perceive the robot to be more under control, predictable, fluent, and trustworthy in functional mimicry than in exact mimicry.</td>
                <td>Shape, User experience, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342536&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342536&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Light-Field Visual System for the Remote Robot Operation Interface</td>
                <td>T. Morimoto et al.</td>
                <td>2023</td>
                <td>Robotic automation is expected to be applicable in various fields. The utilization of robots requires human-robot interaction (HRI) for prolonged direct manipulation or learning. Recently, numerous studies on HRI have been conducted in virtual space using virtual, augmented, and mixed reality (VAM-HRI). In the future, VAM-HRI applications are expected to involve users wearing head-mounted displays (HMDs). However, HMDs present various problems, such as vergence and accommodation conflict (VAC) caused by eye strain, and thus cannot be used for long periods. In this study, a remote robot operation interface is developed to solve this problem. Experiments are conducted from the viewpoint of ophthalmology to determine whether “removal of VAC” and “improvement of depth perception” by Light Field HMDs (LFHMD), and “reduction of eye strain” by TransRay are feasible. The results reveal that the LFHMD is superior to the general HMDs for all items.</td>
                <td>Visualization, Human-robot interaction, Mixed reality, Imaging, Visual systems, Fatigue, Light fields</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342529&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342529&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Performance Comparison of Teleoperation Interfaces for Ultra-Lightweight Anthropomorphic Arms</td>
                <td>F. Zorić, A. Suarez, G. Vasiljević, M. Orsag, Z. Kovačić and A. Ollero</td>
                <td>2023</td>
                <td>This paper presents a comparative performance evaluation of three different teleoperation interfaces for very low weight (<3 kg) anthropomorphic dual arms intended to conduct complex manipulation tasks involving a certain level of dexterity, accuracy and agility, either in ground service or in aerial manipulation applications. A visual human pose estimation system is developed to obtain the Cartesian and joint values of the user, which are mapped to the corresponding pose of the dual arm manipulator exploiting the equivalent human-robot kinematics. A leader-follower scheme is also presented, using a reduced scale dual arm that can replicate directly the joint positions of the leader arms to the follower arms. A 6-DOF (degrees of freedom) joystick is proposed to generate linear motions more accurately. A total of 60 ground tests were conducted involving 10 participants to determine the accuracy and time performance in two benchmarks (box edges and S contour tracking). Finally, the visual and leader-follower interfaces were evaluated with the dual arm aerial manipulator on flight tests, reporting several findings derived from the system evaluation.</td>
                <td>Performance evaluation, Visualization, Tracking, Pose estimation, Robot vision systems, Benchmark testing, Position measurement, teleoperation interfaces, human pose estimation, anthropomorphic robotic arms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342484&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342484&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Human Preferred Augmented Reality Visual Cues for Remote Robot Manipulation Assistance: from Direct to Supervisory Control</td>
                <td>C. Lin and Z. Li</td>
                <td>2023</td>
                <td>When humans control or supervise remote robot manipulation, augmented reality (AR) visual cues overlaid on the remote camera video stream can effectively enhance human's remote perception of task and robot states, and comprehension of the robot autonomy's capability and intent. In this work, we conducted a user study (N=18) to investigate: (RQ1) what AR cues humans prefer when controlling the robot with various levels of autonomy, and (RQ2) whether this preference can be influenced by the way humans learn to use the interface. We provided AR visual cues of various types (e.g., motion guidance, obstacle indicator, target hint, autonomy activation and intent) to assist humans to pick and place an object around an obstacle on a counter workspace. We found that: 1) Participants prefer different types of AR cues based on the level of robot autonomy; 2) The AR cues the participants prefer to use after hands-on robot operation converged to the recommendation of experienced users, and may largely differ from their initial selection based on video instruction.</td>
                <td>Visualization, Robot control, Stacking, Robot vision systems, Supervisory control, Streaming media, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341969&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341969&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Shared Autonomous Nursing Robot Assistant with Dynamic Workspace for Versatile Mobile Manipulation</td>
                <td>N. Boguslavskii, Z. Zhong, L. M. Genua and Z. Li</td>
                <td>2023</td>
                <td>This paper presents a novel integration of a shared autonomous mobile humanoid robot for remote nursing assistance. The proposed nursing robot has a motorized versatile supporting structure to allow flexible integration of the system components, autonomously adjust its mobile manipulation workspace and improve its reachability and manipulability to operate in a cluttered environment. The robot also provides a novel integration of robot autonomy to reduce the human effort to coordinate the motorized chest and arm motion, control the precise manipulation of objects and camera viewpoint, and handle complex collision avoidance in human-guided gross manipulation. Moreover, we developed an open-source virtual testbed that integrates ROS- and Unity-based robot simulation and benchmark mobile manipulation nursing tasks and scenarios in a realistic simulation of a hospital environment. The virtual testbed supports various contemporary gaming and AR/VR interfaces to control the virtual human and robots, and provides autonomy for navigation, manipulation, and remote active perception assistance. We conducted a user study (N=9) to validate that the versatile supporting structure and shared autonomy of the physical testbed can effectively reduce the human effort to control unstructured manipulation, and improve the robot's reachability and manipulability. In addition, we conducted a pilot study (N=8) to test the usability of the virtual testbed and collect feedback from representative users.</td>
                <td>Navigation, Robot kinematics, Robot vision systems, Humanoid robots, Collision avoidance, Usability, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342401&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342401&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Adaptive PD Control Using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays</td>
                <td>L. McCutcheon and S. Fallah</td>
                <td>2023</td>
                <td>Local-remote systems allow robots to execute complex tasks in hazardous environments such as space and nuclear power stations. However, establishing accurate positional mapping between local and remote devices can be difficult due to time delays that can compromise system performance and stability. Enhancing the synchronicity and stability of localremote systems is vital for enabling robots to interact with environments at greater distances and under highly challenging network conditions, including time delays. We introduce an adaptive control method employing reinforcement learning to tackle the time-delayed control problem. By adjusting controller parameters in real-time, this adaptive controller compensates for stochastic delays and improves synchronicity between local and remote robotic manipulators. To improve the adaptive PD controller's performance, we devise a model-based reinforcement learning approach that effectively incorporates multi-step delays into the learning framework. Utilizing this proposed technique, the local-remote system's performance is stabilized for stochastic communication time-delays of up to 290ms. Our results demonstrate that the suggested model-based reinforcement learning method surpasses the Soft-Actor Critic and augmented state Soft-Actor Critic techniques. Access the code at: https://github.com/CAV-Research-Lab/Predictive-Model-Delay-Correction</td>
                <td>Adaptation models, Delay effects, System performance, Computational modeling, Stochastic processes, Reinforcement learning, Stability analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341953&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341953&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Team Northeastern's Approach to ANA XPRIZE Avatar Final Testing: A Holistic Approach to Telepresence and Lessons Learned</td>
                <td>R. Luo et al.</td>
                <td>2023</td>
                <td>This paper reports on Team Northeastern's Avatar system for telepresence, and our holistic approach to meet the ANA Avatar XPRIZE Final testing task requirements. The system features a dual-arm configuration with hydraulically actuated glove-gripper pair for haptic force feedback. Our proposed Avatar system was evaluated in the ANA Avatar XPRIZE Finals and completed all 10 tasks, scored 14.5 points out of 15.0, and received the 3rd Place Award. We provide the details of improvements over our first generation Avatar, covering manipulation, perception, locomotion, power, network, and controller design. We also extensively discuss the major lessons learned during our participation in the competition.</td>
                <td>Telepresence, Avatars, Force feedback, System improvement, Task analysis, Intelligent robots, Testing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341475&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341475&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Avatar Robot Overlaid with the 3D Human Model of a Remote Operator</td>
                <td>R. Tejwani, C. Ma, P. Bonato and H. H. Asada</td>
                <td>2023</td>
                <td>Although telepresence assistive robots have made significant progress, they still lack the sense of realism and physical presence of the remote operator. This results in a lack of trust and adoption of such robots. In this paper, we introduce an Avatar Robot System which is a mixed real/virtual robotic system that physically interacts with a person in proximity of the robot. The robot structure is overlaid with the 3D model of the remote caregiver and visualized through Augmented Reality (AR). In this way, the person receives haptic feedback as the robot touches him/her. We further present an Optimal Non-Iterative Alignment solver that solves for the optimally aligned pose of 3D Human model to the robot (shoulder to the wrist non-iteratively). The proposed alignment solver is stateless, achieves optimal alignment and faster than the baseline solvers (demonstrated in our evaluations). We also propose an evaluation framework that quantifies the alignment quality of the solvers through multifaceted metrics. We show that our solver can consistently produce poses with similar or superior alignments as IK-based baselines without their potential drawbacks.</td>
                <td>Wrist, Solid modeling, Visualization, Three-dimensional displays, Telepresence, Sensitivity, Avatars</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341890&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341890&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Adaptive Robust Model Predictive Control for Bilateral Teleoperation</td>
                <td>F. A. Almasalmah, H. Omran, C. Liu and B. Bayle</td>
                <td>2023</td>
                <td>In this work, we use recent developments in the field of adaptive robust Model Predictive Control (MPC) to build a controller for bilateral teleoperation systems. To guarantee robust constraint satisfaction, we incorporate polytopic tube controllers in the MPC design. In addition, we use online learning methods to learn the environment model. Namely, we use set membership learning to learn the parametric uncertainty bounds and reduce the conservatism of the robust controller, and we combine it with least mean square method to learn a point estimate of the model parameters, which enhances the controller performance. Our simulation demonstrates the effectiveness of the proposed approach in maintaining robust constraint satisfaction and enhancing performance by learning during teleoperation tasks.</td>
                <td>Learning systems, Adaptation models, Adaptive systems, Uncertainty, Electron tubes, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341847&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341847&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Augmented Avatar Toward Both Remote Communication and Manipulation Tasks</td>
                <td>M. Haruna, M. Ogino, S. Tagashira and S. Morita</td>
                <td>2023</td>
                <td>A human being can communicate while working at the same time. However, teleoperated humanoid robots that can work and communicate simultaneously are currently too complex and expensive. We propose an “Augmented Avatar” that can perform both work and communication simultaneously at a low cost. The authors have developed two types of Avatars: a Manipulation Avatar with minimal functions tailored for work, and a Communication Avatar that mimics the human body structure to facilitate communication. The aim of this project is to develop an AVATAR system that enables operators to seamlessly operate the two avatars without using a wearable control interface. In this paper, we report on the construction and operation test results of this prototype. The prototype system has been well received in an international AVATAR competition in which 820 teams participated, placing 12th. This paper also discusses operability based on the experience gained at this competition.</td>
                <td>Avatars, Sociology, MIMICs, Prototypes, User interfaces, Manipulators, Remote working</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342284&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342284&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation</td>
                <td>C. Wu, J. Sun, Z. Shen and L. Zhang</td>
                <td>2023</td>
                <td>Simulating camera sensors is a crucial task in autonomous driving. Although neural radiance fields are exceptional at synthesizing photorealistic views in driving simulations, they still fail to generate extrapolated views. This paper proposes to incorporate map priors into neural radiance fields to synthesize out-of-trajectory driving views with semantic road consistency. The key insight is that map information can be utilized as a prior to guiding the training of the radiance fields with uncertainty. Specifically, we utilize the coarse ground surface as uncertain information to supervise the density field and warp depth with uncertainty from unknown camera poses to ensure multi-view consistency. Experimental results demonstrate that our approach can produce semantic consistency in deviated views for vehicle camera simulation. The supplementary video can be viewed at https://youtu.be/jEQWr-Rfh3A.</td>
                <td>Training, Uncertainty, Roads, Semantics, Robot vision systems, Cameras, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341681&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341681&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Object Goal Navigation with Recursive Implicit Maps</td>
                <td>S. Chen, T. Chabal, I. Laptev and C. Schmid</td>
                <td>2023</td>
                <td>Object goal navigation aims to navigate an agent to locations of a given object category in unseen environments. Classical methods explicitly build maps of environments and require extensive engineering while lacking semantic information for object-oriented exploration. On the other hand, end-to-end learning methods alleviate manual map design and predict actions using implicit representations. Such methods, however, lack an explicit notion of geometry and may have limited ability to encode navigation history. In this work, we propose an implicit spatial map for object goal navigation. Our implicit map is recursively updated with new observations at each step using a transformer. To encourage spatial reasoning, we introduce auxiliary tasks and train our model to reconstruct explicit maps as well as to predict visual features, semantic labels and actions. Our method significantly outperforms the state of the art on the challenging MP3D dataset and generalizes well to the HM3D dataset. We successfully deploy our model on a real robot and achieve encouraging object goal navigation results in real scenes using only a few real-world demonstrations. Code, trained models and videos are available at https://www.di.ens.fr/willow/research/onav_rim/.</td>
                <td>Learning systems, Visualization, Navigation, Object oriented modeling, Semantics, Manuals, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341827&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341827&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Vision-Based Autonomous Navigation for Unmanned Surface Vessel in Extreme Marine Conditions</td>
                <td>M. Ahmed et al.</td>
                <td>2023</td>
                <td>Visual perception is an important component for autonomous navigation of unmanned surface vessels (USV), particularly for the tasks related to autonomous inspection and tracking. These tasks involve vision-based navigation techniques to identify the target for navigation. Reduced visibility under extreme weather conditions in marine environments makes it difficult for vision-based approaches to work properly. To overcome these issues, this paper presents an autonomous vision-based navigation framework for tracking target objects in extreme marine conditions. The proposed framework consists of an integrated perception pipeline that uses a generative adversarial network (GAN) to remove noise and highlight the object features before passing them to the object detector (i.e., YOLOv5). The detected visual features are then used by the USV to track the target. The proposed framework has been thoroughly tested in simulation under extremely reduced visibility due to sandstorms and fog. The results are compared with state-of-the-art de-hazing methods across the benchmarked MBZIRC simulation dataset, on which the proposed scheme has outperformed the existing methods across various metrics.</td>
                <td>YOLO, Visualization, Target tracking, Navigation, Pipelines, Generative adversarial networks, Feature extraction, Navigation, Marine Robotics, Visual Control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341867&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341867&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Framework for Few-Shot Policy Transfer Through Observation Mapping and Behavior Cloning</td>
                <td>Y. Shukla, B. Kesari, S. Goel, R. Wright and J. Sinapov</td>
                <td>2023</td>
                <td>Despite recent progress in Reinforcement Learning for robotics applications, many tasks remain prohibitively difficult to solve because of the expensive interaction cost. Transfer learning helps reduce the training time in the target domain by transferring knowledge learned in a source domain. Sim2Real transfer helps transfer knowledge from a simulated robotic domain to a physical target domain. Knowledge transfer reduces the time required to train a task in the physical world, where the cost of interactions is high. However, most existing approaches assume exact correspondence in the task structure and the physical properties of the two domains. This work proposes a framework for Few-Shot Policy Transfer between two domains through Observation Mapping and Behavior Cloning. We use Generative Adversarial Networks (GANs) along with a cycle-consistency loss to map the observations between the source and target domains and later use this learned mapping to clone the successful source task behavior policy to the target domain. We observe successful behavior policy transfer with limited target task interactions and in cases where the source and target task are semantically dissimilar.</td>
                <td>Training, Costs, Transfer learning, Cloning, Reinforcement learning, Generative adversarial networks, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342477&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342477&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>UVIO: An UWB-Aided Visual-Inertial Odometry Framework with Bias-Compensated Anchors Initialization</td>
                <td>G. Delama, F. Shamsfakhr, S. Weiss, D. Fontanelli and A. Fomasier</td>
                <td>2023</td>
                <td>This paper introduces UVIO, a multi-sensor framework that leverages Ultra Wide Band (UWB) technology and Visual-Inertial Odometry (VIO) to provide robust and low-drift localization. In order to include range measurements in state estimation, the position of the UWB anchors must be known. This study proposes a multi-step initialization procedure to map multiple unknown anchors by an Unmanned Aerial Vehicle (UAV), in a fully autonomous fashion. To address the limitations of initializing UWB anchors via a random trajectory, this paper uses the Geometric Dilution of Precision (GDOP) as a measure of optimality in anchor position estimation, to compute a set of optimal waypoints and synthesize a trajectory that minimizes the mapping uncertainty. After the initialization is complete, the range measurements from multiple anchors, including measurement biases, are tightly integrated into the VIO system. While in range of the initialized anchors, the VIO drift in position and heading is eliminated. The effectiveness of UVIO and our initialization procedure has been validated through a series of simulations and real-world experiments.</td>
                <td>Location awareness, Uncertainty, Autonomous aerial vehicles, Real-time systems, Trajectory, Odometry, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342012&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342012&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Self-Supervised Object Goal Navigation with In-Situ Finetuning</td>
                <td>S. Y. Min et al.</td>
                <td>2023</td>
                <td>A household robot should be able to navigate to target objects without requiring users to first annotate everything in their home. Most current approaches to object navigation do not test on real robots and rely solely on reconstructed scans of houses and their expensively labeled semantic 3D meshes. In this work, our goal is to build an agent that builds self-supervised models of the world via exploration, the same as a child might - thus we (1) eschew the expense of labeled 3D mesh and (2) enable self-supervised in-situ finetuning in the real world. We identify a strong source of self-supervision (Location Consistency - LocCon) that can train all components of an ObjectNav agent, using unannotated simulated houses. Our key insight is that embodied agents can leverage location consistency as a self-supervision signal - collecting images from different views/angles and applying contrastive learning. We show that our agent can perform competitively in the real world and simulation. Our results also indicate that supervised training with 3D mesh annotations causes models to learn simulation artifacts, which are not transferrable to the real world. In contrast, our LocCon shows the most robust transfer in the real world among the set of models we compare to, and that the real-world performance of all models can be further improved with self-supervised LocCon in-situ training.</td>
                <td>Training, Solid modeling, Three-dimensional displays, Navigation, Annotations, Semantics, Object recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341959&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341959&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Tightly-Coupled Visual- DVL- Inertial Odometry for Robot-Based Ice-Water Boundary Exploration</td>
                <td>L. Zhao, M. Zhou and B. Loose</td>
                <td>2023</td>
                <td>Underwater robots, like Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs), are promising tools for the exploration and study of the under-ice environment and the ecosystems that thrive there. However, state estimation is a well-known problem for robotic systems, especially, for the ones that travel underwater. In this paper, $w$e present a tightly-coupled multi-sensors fusion framework to increase localization accuracy that is robust to sensor failure. Visual images, Doppler Velocity Log (DVL), Inertial Measurement Unit (IMU) and Pressure sensor are integrated using a Multi-State Constraint Kalman Filter (MSCKF) for state estimation. Besides, a modified keyframe-based clone marginalization and a new DVL-aided feature enhancement method are presented to further improve the localization performance. The proposed method is validated in the under-ice environment on Lake Michigan, USA, and the results are cross-compared with 10 other different sensor fusion setups. Overall, the integration of keyframe enabled and DVL-aided feature enhancement yielded the best performance with a Root-mean-square error of less than 2 m compared to the ground truth path over a total traveling distance of about 200 m.</td>
                <td>Location awareness, Pressure sensors, Autonomous underwater vehicles, Visualization, Remotely guided vehicles, Measurement units, Sensor fusion</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342024&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342024&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ProxMaP: Proximal Occupancy Map Prediction for Efficient Indoor Robot Navigation</td>
                <td>V. D. Sharma, J. Chen and P. Tokekar</td>
                <td>2023</td>
                <td>Planning a path for a mobile robot typically requires building a map (e.g., an occupancy grid) of the environment as the robot moves around. While navigating in an unknown environment, the map built by the robot online may have many as-yet-unknown regions. A conservative planner may avoid such regions taking a longer time to navigate to the goal. Instead, if a robot is able to correctly predict the occupancy in the occluded regions, the robot may navigate efficiently. We present a self-supervised occupancy prediction technique, ProxMaP, to predict the occupancy within the proximity of the robot to enable faster navigation. We show that ProxMaP generalizes well across realistic and real domains, and improves the robot navigation efficiency in simulation by 12.40% against a traditional navigation method. We share our findings and code at https://raaslab.org/projects/ProxMaP.</td>
                <td>Codes, Navigation, Semantics, Robot vision systems, Buildings, Cameras, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341435&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341435&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>USA-Net: Unified Semantic and Affordance Representations for Robot Memory</td>
                <td>B. Bolte, A. Wang, J. Yang, M. Mukadam, M. Kalakrishnan and C. Paxton</td>
                <td>2023</td>
                <td>In order for robots to follow open-ended instructions like “go open the brown cabinet over the sink,” they require an understanding of both the scene geometry and the semantics of their environment. Robotic systems often handle these through separate pipelines, sometimes using very different representation spaces, which can be suboptimal when the two objectives conflict. In this work, we present USA-Net, a simple method for constructing a world representation that encodes both the semantics and spatial affordances of a scene in a differentiable map. This allows us to build a gradient-based planner which can navigate to locations in the scene specified using open-ended vocabulary. We use this planner to consistently generate trajectories which are both shorter 5-10% shorter and 10-30% closer to our goal query in CLIP embedding space than paths from comparable grid-based planners which don't leverage gradient information. To our knowledge, this is the first end-to-end differentiable planner optimizes for both semantics and affordance in a single implicit map. Code and visuals are available at our website: usa.bolte.cc</td>
                <td>Geometry, Vocabulary, Visualization, Codes, Navigation, Affordances, Semantics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341737&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341737&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Pred-NBV: Prediction-Guided Next-Best-View Planning for 3D Object Reconstruction</td>
                <td>H. Dhami, V. D. Sharma and P. Tokekar</td>
                <td>2023</td>
                <td>Prediction-based active perception has shown the potential to improve the navigation efficiency and safety of the robot by anticipating the uncertainty in the unknown environment. The existing works for 3D shape prediction make an implicit assumption about the partial observations and therefore cannot be used for real-world planning and do not consider the control effort for next-best-view planning. We present Pred-NBV, a realistic object shape reconstruction method consisting of PoinTr-C, an enhanced 3D prediction model trained on the ShapeNet dataset, and an information and control effort-based next-best-view method to address these issues. Pred-NBV shows an improvement of 25.46% in object coverage over the traditional methods in the AirSim simulator, and performs better shape completion than PoinTr, the state-of-the-art shape completion model, even on real data obtained from a Velodyne 3D LiDAR mounted on DJI M600 Pro.</td>
                <td>Point cloud compression, Solid modeling, Three-dimensional displays, Uncertainty, Shape, Measurement uncertainty, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341650&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341650&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Active Visual SLAM Based on Hierarchical Reinforcement Learning</td>
                <td>W. Chen, W. Li, A. Yang and Y. Hu</td>
                <td>2023</td>
                <td>We present AVS-HRL, a modular Active visual SLAM system based on hierarchical reinforcement learning. The reward function explicitly considers the efficiency of exploration and the accuracy of mapping by utilizing the internal variables of SLAM, such as feature points distribution and loop-closure signal. Compared to end-to-end active SLAM methods, we designed a map reconstruction module that can correct the cumulative error in the incremental mapping process. Furthermore, the inputs of all neural network modules use more abstract and general information, such as grid maps, rather than raw sensor observations. We conducted experiments in two different simulators and real-world environments. In the noisy setting of Habitat environments, our method improves the accuracy of the mapped areas by 68.48% as an average of Gibson and MP3D datasets. Moreover, our method's generalization performance was demonstrated through direct transfer across different simulators and real-world environments.</td>
                <td>Visualization, Simultaneous localization and mapping, Neural networks, Habitats, Reinforcement learning, Noise measurement, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341780&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341780&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Point Cloud Registration with Geometry-based Transformation Invariant Descriptor</td>
                <td>J. Lin, M. Rickert, L. Wen, Y. Hu and A. Knoll</td>
                <td>2023</td>
                <td>This work presents a novel method for point registration in 3D space. The proposed algorithm utilizes transformation-invariant geometry information to estimate the pose of objects based on correspondences between points in two sets. Conventional methods use geometry descriptors to find these correspondences, which can result in a large number of outliers. Most existing algorithms are error-prone when outliers are present. Instead of formulating point registration as a non-convex optimization problem, we propose an intuitive method that filters out spurious correspondences. This is achieved by evaluating three different geometry-based transformation-invariant descriptors for outlier removal. We construct fully connected graphs with the proposed descriptors on correspondences, and convert the outlier removal problem into a subgraph isomorphism problem that is solved using a binary clustering approach. The resulting inlier clustering is used to estimate the transformation between the two point sets. The effectiveness of the proposed approach is evaluated on standard 3D data and the 3DMatch scan matching dataset, and compared against existing state-of-the-art methods. Results show that our method effectively reduces outliers and performs similarly to these methods.</td>
                <td>Point cloud compression, Geometry, Three-dimensional displays, Buildings, Filtering algorithms, Complexity theory, Standards</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342244&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342244&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Online Monocular Lane Mapping Using Catmull-Rom Spline</td>
                <td>Z. Qiao, Z. Yu, H. Yin and S. Shen</td>
                <td>2023</td>
                <td>In this study, we introduce an online monocular lane mapping approach that solely relies on a single camera and odometry for generating spline-based maps. Our proposed technique models the lane association process as an assignment issue utilizing a bipartite graph, and assigns weights to the edges by incorporating Chamfer distance, pose uncertainty, and lateral sequence consistency. Furthermore, we meticulously design control point initialization, spline parameterization, and optimization to progressively create, expand, and refine splines. In contrast to prior research that assessed performance using self-constructed datasets, our experiments are conducted on the openly accessible OpenLane dataset. The experimental outcomes reveal that our suggested approach enhances lane association and odometry precision, as well as overall lane map quality. We have open-sourced out code11https://github.com/HKUST-Aerial-Robotics/MonoLaneMapping for this project.</td>
                <td>Uncertainty, Cameras, Bipartite graph, Odometry, Splines (mathematics), Optimization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341707&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341707&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>VDBblox: Accurate and Efficient Distance Fields for Path Planning and Mesh Reconstruction</td>
                <td>Y. Bai, Z. Miao, X. Wang, Y. Liu, H. Wang and Y. Wang</td>
                <td>2023</td>
                <td>Highly accurate and efficient map in unknown and complex environments is essential for robotics navigation. Traditionally, mobile robot platforms are often computationally constrained when using multiple sensors to process large amounts of input data. In previous works, some of them have been deployed to embedded platforms in real-time. However, how to balance accuracy and efficiency while reducing the computational resources and the memory footprint is still the bottleneck. Motivated by these challenges, we proposed a mapping framework called VDBblox to incrementally build Euclidean Signed Distance Fields (ESDFs) map from Truncated Signed Distance Fields (TSDFs) mapping. We use a novel weight function to update the non-projective TSDFs, thus improving the quality of the mesh reconstruction with higher accuracy than up-to-date methods. Meanwhile, the generated ESDFs map is maintained by the least recently used (LRU) cache to dynamically handle the obstacle changes with less runtime than state-of-the-art. We show VDBblox performance in terms of accuracy and efficiency by benchmark comparison on RGB-D and LiDAR public datasets. Moreover, we demonstrate that VDBblox can be integrated into a completed quadrotor system as a sub-module. Then we validate it through online obstacle avoidance and high-quality mesh reconstruction in real-world experiments. Finally, we release our method as open-source code to the community11Code - https://github.com/yinloonga/vdbblox.</td>
                <td>Runtime, Laser radar, Navigation, Memory management, Robot sensing systems, Real-time systems, Path planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342123&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342123&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Rollvox: Real-Time and High-Quality LiDAR Colorization with Rolling Shutter Camera</td>
                <td>S. Hong, C. Zheng, H. Yin and S. Shen</td>
                <td>2023</td>
                <td>In this study, we propose a novel system for real-time coloring LiDAR point clouds with a low-cost RS camera. The main challenges are dealing with the motion distortion of the RS camera and the multi-sensor time synchronization. To tackle these challenges, we carefully design a hardware synchronizer to ensure the strict alignment of the LiDAR, inertial measurement unit, and RS camera. With accurate timestamps, we first use LiDAR-inertial odometry (LIO) for pose estimation, and the poses of image line exposure are calculated by forward propagation based on a constant velocity motion model. Then, we propose our method based on the RS constraint for colorizing the LiDAR point cloud. For comparison, we colorize the LiDAR point cloud with conventional rolling shutter image undistortion. In the real-world tests, The results show that our proposed method produces more accurate and efficient colorization of point clouds. Besides, considering the situation of readout time not being provided, we propose a method to calibrate the readout time by minimizing the reprojection error of LIO's inter-frame pose and image optical flows. We release our code and self-collected datasets on Github33https://github.com/sheng00125/Rollvox to benefit the community.</td>
                <td>Point cloud compression, Laser radar, Robot vision systems, Pose estimation, Cameras, Real-time systems, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342172&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342172&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>360FusionNeRF: Panoramic Neural Radiance Fields with Joint Guidance</td>
                <td>S. Kulkarni, P. Yin and S. Scherer</td>
                <td>2023</td>
                <td>Based on the neural radiance fields (NeRF), we present a pipeline for generating novel views from a single 360° panoramic image. Prior research relied on the neighborhood interpolation capability of multi-layer perceptions to complete missing regions caused by occlusion. This resulted in artifacts in their predictions. We propose 360FusionNeRF, a semi-supervised learning framework that employs geometric supervision and semantic consistency to guide the progressive training process. Firstly, the input image is reprojected to 360° images, and depth maps are extracted at different camera positions. In addition to the NeRF color guidance, the depth supervision enhances the geometry of the synthesized views. Furthermore, we include a semantic consistency loss that encourages realistic renderings of novel views. We extract these semantic features using a pre-trained visual encoder CLIP, a Vision Transformer (ViT) trained on hundreds of millions of diverse 2D photographs mined from the web with natural language supervision. Experiments indicate that our proposed method is capable of producing realistic completions of unobserved regions while preserving the features of the scene. 360FusionNeRF consistently delivers state-of-the-art performance when transferring to synthetic Structured3D dataset (PSNR ~ 5%, SSIM ~3% LPIPS ~13%), real-world Matterport3D dataset (PSNR ~3%, SSIM ~3% LPIPS ~9%) and Replica360 dataset (PSNR ~8%, SSIM ~2% LPIPS ~18%). We provide the source code at https://github.com/MetaSLAM/360FusionNeRF.</td>
                <td>Training, Visualization, Image color analysis, Source coding, Semantics, Semisupervised learning, Feature extraction, Scene representation, View synthesis, Neural Radiance Field, 360° image, 3D deep learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341346&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341346&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Session, Localization-Oriented and Lightweight LiDAR Mapping Using Semantic Lines and Planes</td>
                <td>Z. Yu, Z. Qiao, L. Qiu, H. Yin and S. Shen</td>
                <td>2023</td>
                <td>In this paper, we present a centralized framework for multi-session LiDAR mapping in urban environments, by utilizing lightweight line and plane map representations instead of widely used point clouds. The proposed framework achieves consistent mapping in a coarse-to-fine manner. Global place recognition is achieved by associating lines and planes on the Grassmannian manifold, followed by an outlier rejection-aided pose graph optimization for map merging. Then a novel bundle adjustment is also designed to improve the local consistency of lines and planes. In the experimental section, both public and self-collected datasets are used to demonstrate efficiency and effectiveness. Extensive results validate that our LiDAR mapping framework could merge multi-session maps globally, optimize maps incrementally, and is applicable for lightweight robot localization.</td>
                <td>Point cloud compression, Manifolds, Laser radar, Urban areas, Semantics, Merging, Pipelines</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341462&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341462&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Elevation Mapping with Bayesian Ground Filling and Traversability Analysis for UGV Navigation</td>
                <td>H. Xie, X. Zhong, B. Chen, P. Peng, H. Hu and Q. Liu</td>
                <td>2023</td>
                <td>Unmanned ground vehicles (UGVs) require effective perception and analysis of their surrounding terrain for safe operation. This paper presents a novel approach to their local elevation mapping and traversability analysis using sparse data from a single LiDAR sensor, which can generate a dense local traversability map in real-time. By preserving ground height information, our method can differentiate between vertical obstacles, suspended objects and other terrains in the elevation map. The modified Bayesian generalized kernel elevation inference is utilized to predict and fill in sparse elevation maps to achieve local dense terrain traversability mapping. The system uses GPU parallel processing to accelerate calculations, ensuring real-time perception and dynamic processing. The proposed system was tested in both structured and unstructured environments, and achieved better performances in map filling and handling of suspended and vertical objects compared to other existing approaches.</td>
                <td>Point cloud compression, Visualization, Navigation, Parallel processing, Robot sensing systems, Real-time systems, Filling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341662&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341662&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Geometrically Consistent Monocular Metric-Semantic 3D Mapping for Indoor Environments with Transparent and Reflecting Objects</td>
                <td>M. Mohrat, A. Berkaev, A. Burkov and S. Kolyubin</td>
                <td>2023</td>
                <td>3D mapping is crucial for many applications in robotics and related industries. To build dense high-quality point clouds accurate depth estimation or completion is needed. This paper presents the development of a metric-semantic mapping pipeline based on Deep Neural Networks (DNN) which assures geometrical consistency with enhancements for chal-lenging environments with transparent and reflecting objects like glass walls, doors, and mirrors. The suggested approach uses camera ego-motion alongside its sparse visual features to avoid the scale ambiguity issue caused by monocular depth affine-invariant estimations and to able to restore metric consistent depth information. Visual-inertial odometry data is used for camera pose graph optimization with no need to use RGB-D cameras. The proposed pipeline incorporates semantic segmentation and robust filtering to refine point clouds by removing outliers associated with mirrors and glass surfaces. Latency-aware performance and quality evaluation of 3D scene reconstruction were carried out on both a specially prepared dataset that reflects office-like scenes with multiple transparent objects and a public ScanNet dataset. The quantitative and qualitative results show that the proposed solution outperforms other state-of-art DNN-based models and algorithms as well as RGB-D cameras in terms of metric depth geometric consistency, 3D reconstruction accuracy, and the ability to preserve mesh quality in challenging scenarios with transparent and reflective surfaces.</td>
                <td>Measurement, Point cloud compression, Surface reconstruction, Three-dimensional displays, Pipelines, Estimation, Glass</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342329&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342329&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>PanopticNDT: Efficient and Robust Panoptic Mapping</td>
                <td>M. Gross</td>
                <td>2023</td>
                <td>As the application scenarios of mobile robots are getting more complex and challenging, scene understanding becomes increasingly crucial. A mobile robot that is supposed to operate autonomously in indoor environments must have precise knowledge about what objects are present, where they are, what their spatial extent is, and how they can be reached; i.e., information about free space is also crucial. Panoptic mapping is a powerful instrument providing such information. However, building 3D panoptic maps with high spatial resolution is challenging on mobile robots, given their limited computing capabilities. In this paper, we propose PanopticNDT – an efficient and robust panoptic mapping approach based on occupancy normal distribution transform (NDT) mapping. We evaluate our approach on the publicly available datasets Hypersim and ScanNetV2. The results reveal that our approach can represent panoptic information at a higher level of detail than other state-of-the-art approaches while enabling real-time panoptic mapping on mobile robots. Finally, we prove the real-world applicability of PanopticNDT with qualitative results in a domestic application.</td>
                <td>Three-dimensional displays, Instruments, Buildings, Transforms, Gaussian distribution, Real-time systems, Indoor environment</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342137&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342137&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Range-based GP Maps: Local Surface Mapping for Mobile Robots using Gaussian Process Regression in Range Space</td>
                <td>M. Hansen and D. Wettergreen</td>
                <td>2023</td>
                <td>This work introduces range-based GP maps, which directly represent terrain by modeling the range from a LiDAR sensor as a Gaussian process (GP) in spherical space. Such a model aligns the predicted uncertainty from the GP regression with the uncertainty in the underlying sensor observations. Experimental evaluation on simulated natural terrain indicates that local range-based GP maps perform comparably to elevation-based methods when predicting terrain height, with the former producing more stable parameters and providing a better uncertainty representation. An aggregation method is proposed using the pose as an additional input to the GP. Unlike their elevation-based counterparts, range-based GP maps are capable of modeling overhangs and vertical obstacles with ease, demonstrated with examples of maps built on real-world data from a fully 3D subterranean environment.</td>
                <td>Location awareness, Solid modeling, Uncertainty, Laser radar, Three-dimensional displays, Runtime, Navigation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341949&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341949&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Pseudo Inputs Optimisation for Efficient Gaussian Process Distance Fields</td>
                <td>Calleja</td>
                <td>2023</td>
                <td>Robots reason about the environment through dedicated representations. Despite the fact that Gaussian Process (GP)-based representations are appealing due to their probabilistic and continuous nature, the cubic computational complexity is a concern. In this paper, we present a novel efficient GP-based representation that has the ability to produce accurate distance fields and is parameterised by the optimal locations of pseudo inputs. When applying the proposed method together with a kernel approximation approach, we show it outperforms well-established sparse GP frameworks in efficiency and accuracy. Moreover, we extend the proposed method to work in a dynamic setting, where a map is built iteratively and the scene dynamics are accounted for by adding or removing objects from the environment representation. In a nutshell, our method provides the ability to infer dynamic distance fields and achieve state-of-the-art reconstruction efficiently.</td>
                <td>Uncertainty, Optimization methods, Gaussian processes, Probabilistic logic, Computational efficiency, Kernel, Computational complexity, Gaussian Process, Euclidean Distance Fields, Mapping</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342483&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342483&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Computing Motion Plans for Assembling Particles with Global Control</td>
                <td>P. Blumenberg, A. Schmidt and A. T. Becker</td>
                <td>2023</td>
                <td>We investigate motion planning algorithms for the assembly of shapes in the tilt model in which unit-square tiles move in a grid world under the influence of uniform external forces and self-assemble according to certain rules. We provide several heuristics and experimental evaluation of their success rate, solution length, and runtime.</td>
                <td>Runtime, Shape, Computational modeling, Planning, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341556&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341556&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Constrained Multi-Agent Trajectory Optimization Using Dynamic Potential Games</td>
                <td>M. Bhatt, Y. Jia and N. Mehr</td>
                <td>2023</td>
                <td>Although dynamic games provide a rich paradigm for modeling agents' interactions, solving these games for real-world applications is often challenging. Many real-world interactive settings involve general nonlinear state and input constraints that couple agents' decisions with one another. In this work, we develop an efficient and fast planner for interactive trajectory optimization in constrained setups using a constrained game-theoretical framework. Our key insight is to leverage the special structure of agents' objective and constraint functions that are common in multi-agent interactions for fast and reliable planning. More precisely, we identify the structure of agents' cost and constraint functions under which the resulting dynamic game is an instance of a constrained dynamic potential game. Constrained dynamic potential games are a class of games for which instead of solving a set of coupled constrained optimal control problems, a constrained Nash equilibrium, i.e. a Generalized Nash equilibrium, can be found by solving a single constrained optimal control problem. This simplifies constrained interactive trajectory optimization significantly. We compare the performance of our method in a navigation setup involving four planar agents and show that our method is on average 20 times faster than the state-of-the-art. We further provide experimental validation of our proposed method in a navigation setup involving two quadrotors carrying a rigid object while avoiding collisions with two humans.</td>
                <td>Costs, Navigation, Optimal control, Games, Nash equilibrium, Planning, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342328&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342328&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bidirectional Search Strategy for Incremental Search-based Path Planning</td>
                <td>H. Meng</td>
                <td>2023</td>
                <td>Planning a collision-free path efficiently among obstacles is crucial in robotics. Conventional one-shot unidirectional path planning algorithms work well in the static environment, but cannot respond to the environment changes timely in the dynamic environment. To tackle this issue and improve the search efficiency, we propose a bidirectional incremental search method, Bidirectional Lifelong Planning A* (BLPA*), which searches in the forward and backward directions and performs incremental search bidirectionally when the environment changes. Furthermore, inspired by the robot perception range limitation and BLPA*, we propose the fractional bidirectional D* Lite (fBD* Lite(dp)), which constraints the forward search to the robot perception range and uses the backward search to expand the rest area. Our simulation results demonstrate BLPA* and mD* Lite(dp) can achieve superior performance in the dynamic environment. It reveals that the bidirectional incremental search strategy can be a general and efficient technique for graph-search-based robot path planning methods.</td>
                <td>Heuristic algorithms, Simulation, Search problems, Path planning, Planning, Collision avoidance, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342039&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342039&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>HELSA: Hierarchical Reinforcement Learning with Spatiotemporal Abstraction for Large-Scale Multi-Agent Path Finding</td>
                <td>Z. Song, R. Zhang and X. Cheng</td>
                <td>2023</td>
                <td>The Multi-Agent Path Finding (MAPF) problem is a critical challenge in dynamic multi-robot systems. Recent studies have revealed that multi-agent reinforcement learning (MARL) is a promising approach to solving MAPF problems in a fully decentralized manner. However, as the size of the multi-robot system increases, sample inefficiency becomes a major impediment to learning-based methods. This paper presents a hierarchical reinforcement learning (HRL) framework for large-scale multi-agent path finding, featuring applying spatial and temporal abstraction to capture intermediate reward and thus encourage efficient exploration. Specifically, we introduce a meta controller that partitions the map into interconnected regions and optimizes agents' region-wise paths towards globally better solutions. Additionally, we design a lower-level controller that efficiently solves each sub-problem by incorporating heuristic guidance and an inter-agent communication mechanism with RL-based policies. Our empirical results on test instances of various scales demonstrate that our method outperforms existing approaches in terms of both success rate and makespan.</td>
                <td>Learning systems, Reinforcement learning, Routing, Spatiotemporal phenomena, Multi-robot systems, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342261&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342261&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning to Schedule in Multi-Agent Pathfinding</td>
                <td>K. Ahn, H. Park and J. Park</td>
                <td>2023</td>
                <td>In this work, we consider the problem of allocating tasks and planning paths for multiple robots to operate cooper-atively. We formulate the problem as a bi-level optimization that involves optimizing the scheduling of robots and the collision-free path for each robot. To address the complexity of the environment with obstacles, we introduce a congestion-aware state representation technique with the aid of graph neural networks. We also derive a joint scheduling policy using multi-agent reinforcement learning, and we propose an additional auxiliary loss that promotes coordination among the robots. Through empirical evaluation, we demonstrate the effectiveness of our approach in solving the combined task allocation and path planning problem in a cooperative multi-robot system.</td>
                <td>Schedules, Robot kinematics, Reinforcement learning, Path planning, Resource management, Multi-robot systems, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342073&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342073&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation</td>
                <td>J. Blumenkamp, Q. Li, B. Wang, Z. Liu and A. Prorok</td>
                <td>2023</td>
                <td>We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person- view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate to the target along the shortest path. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to 2.0 × improvement in SPL (Success weighted by Path Length) when compared to a communication-free baseline. This is done without requiring a global map, positioning data, nor pre-calibration of the sensor network. Second, we perform a zero-shot transfer of our model from simulation to the real world. Laboratory experiments demonstrate the feasibility of our approach in various cluttered environments. Finally, we showcase examples of successful navigation to the target while both the sensor network layout as well as obstacles are dynamically reconfigured as the robot navigates. We provide a video demo11https://www.youtube.com/watch?v=kcrnr6RUgucw, the dataset, trained models, and source code22https://github.com/proroklab/sensor-guided-visual-nav.</td>
                <td>Visualization, Navigation, Layout, Robot sensing systems, Graph neural networks, Sensor systems, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342535&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342535&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Counterexample Guided Abstraction Refinement with Non-Refined Abstractions for Multi-Goal Multi-Robot Path Planning</td>
                <td>P. Surynek</td>
                <td>2023</td>
                <td>We address the problem of multi-goal multi robot path planning (MG-MRPP) via counterexample guided abstraction refinement (CEGAR) framework. MG-MRPP generalizes the standard discrete multi-robot path planning (MRPP) problem. While the task in MRPP is to navigate robots in an undirected graph from their starting vertices to one individual goal vertex per robot, MG-MRPP assigns each robot multiple goal vertices and the task is to visit each of them at least once. Solving MG-MRPP not only requires finding collision free paths for individual robots but also determining the order of visiting robot's goal vertices so that common objectives like the sum-of-costs are optimized. We use the Boolean satisfiability (SAT) techniques as the underlying paradigm. A specifically novel in this work is the use of non-refined abstractions when formulating the MG-MRPP problem as SAT. While the standard CEGAR approach for MG-MRPP does not encode collision elimination constraints in the initial abstraction and leave them to subsequent refinements. The novel CEGAR approach leaves some abstractions deliberately non-refined. This adds the necessity to post-process the answers obtained from the underlying SAT solver as these answers slightly differ from the correct MG-MRPP solutions. Non-refining however yields order-of-magnitude smaller SAT encodings than those of the previous CEGAR approach and speeds up the overall solving process.</td>
                <td>Navigation, Path planning, Encoding, Collision avoidance, Task analysis, Robots, Standards</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341952&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341952&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach</td>
                <td>Dehkordi, M. Chen and Y. Zhang</td>
                <td>2023</td>
                <td>Cooperative multi-agent problems often require coordination between agents, which can be achieved through a centralized policy that considers the global state. Multi-agent policy gradient (MAPG) methods are commonly used to learn such policies, but they are often limited to problems with low-level action spaces. In complex problems with large state and action spaces, it is advantageous to extend MAPG methods to use higher-level actions, also known as options, to improve the policy search efficiency. However, multi-robot option executions are often asynchronous, that is, agents may select and complete their options at different time steps. This makes it difficult for MAPG methods to derive a centralized policy and evaluate its gradient, as centralized policy always select new options at the same time. In this work, we propose a novel, conditional reasoning approach to address this problem and demonstrate its effectiveness on representative option-based multi-agent cooperative tasks through empirical validation. Find code and videos at: https://sites.google.com/view/mahrlsupp/</td>
                <td>Codes, Search problems, Cognition, Multi-robot systems, Task analysis, Intelligent robots, Videos</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342281&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342281&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Decentralised Multi-Robot Exploration Using Monte Carlo Tree Search</td>
                <td>Maushart and M. Chli</td>
                <td>2023</td>
                <td>Autonomous robotic systems are useful in automating tasks such as inspection and surveying of unknown areas, where speed is often an important factor. In order to effectively reduce the time required to complete missions, an efficient exploration and coordination strategy is needed. In this spirit, this work proposes an approach based on the Monte Carlo Tree Search (MCTS) algorithm to guide robots during exploration missions. Our method first expands a search tree of possible actions from the robot's position towards unknown regions, and then selects the sequence of movements that best drive the exploration process forward with respect to a given reward function. The proposed approach, which is able to balance short- and long-term decision-making, is then extended to accommodate the presence of multiple robots, in a bid to push the efficiency of exploration further. Our method allows for the coordination of the robots' movements in a decentralized manner, relying on point-to-point communication. This results in an efficient strategy, which we refer to as Decentralized Monte Carlo Exploration (DMCE). The experimental results demonstrate that our pipeline outperforms a greedy exploration approach, as well as state-of-the-art planners, with up to 30% reduction in exploration times in a series of real-world maps.</td>
                <td>Monte Carlo methods, Robot kinematics, Pipelines, Decision making, Inspection, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341485&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341485&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Task Assignment, Scheduling, and Motion Planning for Automated Warehouses for Million Product Workloads</td>
                <td>C. Leet, C. Oh, M. Lora, S. Koenig and P. Nuzzo</td>
                <td>2023</td>
                <td>We address the Warehouse Servicing Problem (WSP) in automated warehouses, which use teams of mobile robots to move products from shelves to packaging stations. Given a list of products, the WSP amounts to finding a motion plan which brings every product on the list from a shelf to a packaging station within a given time limit. The WSP consists of four subproblems, namely, deciding where to source and deposit a product (task formulation), who should transport each product (task assignment) and when (scheduling) and how (motion planning). These problems are NP-Hard individually and made more challenging by their interdependence. The difficulty of the WSP is compounded by the scale of automated warehouses, which use teams of hundreds of agents to transport thousands of products. In this paper, we present Contract-based Cyclic Motion Planning (CCMP), a novel contract-based methodology for solving the WSP at scale. CCMP decomposes a warehouse into a set of traffic system components. By assigning each component a contract which describes the traffic flows it can support, CCMP can generate a traffic flow which satisfies a given WSP instance. CCMP then uses a novel motion planner to transform this traffic flow into a motion plan for a team of robots. Evaluation shows that CCMP can solve WSP instances taken from real industrial scenarios with up to 1 million products while outperforming other methodologies for solving the WSP by up to 2.9×.</td>
                <td>Job shop scheduling, Service robots, Transforms, Packaging, Planning, Mobile robots, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341755&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341755&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LiDAR Missing Measurement Detection for Autonomous Driving in Rain</td>
                <td>C. Zhang, Z. Huang, M. H. Ang and D. Rus</td>
                <td>2023</td>
                <td>Autonomous driving in rain remains challenging. Rain causes sensor performance degradation that can affect sensor measurement quality. During the rain, lasers may suffer from energy loss due to raindrop absorption. As a result, some laser measurements reflected from obstacles may not be recognized by the LiDAR sensor, thus raising potential risks for autonomous vehicles. This work investigates a novel task that aims to detect those missing measurements. Our solution uses a two-stage learning method to generate an anomaly score for each missing measurement, representing the likelihood of being caused by rain. We evaluate our method with real-world data and demonstrate its effectiveness in identifying anomalous missing measurements through qualitative and quantitative experiments.</td>
                <td>Degradation, Rain, Laser radar, Snow, Measurement by laser beam, Robot sensing systems, Loss measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341932&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341932&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SMART-Degradation: A Dataset for LiDAR Degradation Evaluation in Rain</td>
                <td>C. Zhang, Z. Huang, B. X. L. Tung, M. H. Ang and D. Rus</td>
                <td>2023</td>
                <td>Sensor degradation is one of the major challenges for autonomous driving. During the rain, the interference from raindrops can negatively influence LiDAR measurements. For example, valid measurements could be reduced during the rain, and some measurements may become noisy. Unreliable measurements can lead to potential safety issues if autonomous driving systems are unaware of these changes. In this work, we will release a naturalistic driving dataset to advance the research in studying LiDAR degradation. Our dataset consists of 3D LiDAR scans collected by a data collection vehicle under various rainy conditions. Besides these raw scans, we also release LiDAR scan pairs (each pair consists of one scan from rainy weather and one scan from clear weather at the same location). These LiDAR pairs are developed to help researchers identify LiDAR degradation. Finally, we will release a toolbox integrated with mapping, localization, and scan synthesis functions used to create this dataset. This toolbox can facilitate dataset creation for studying degradation in other harsh weather conditions. More information can be found at https://smart-rain-dataset.github.io/.</td>
                <td>Degradation, Location awareness, Laser radar, Rain, Three-dimensional displays, Safety, Noise measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342323&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342323&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Streaming Motion Forecasting for Autonomous Driving</td>
                <td>X. Wang</td>
                <td>2023</td>
                <td>Trajectory forecasting is a widely-studied problem for autonomous navigation. However, existing benchmarks evaluate forecasting based on independent snapshots of trajectories, which are not representative of real-world applications that operate on a continuous stream of data. To bridge this gap, we introduce a benchmark that continuously queries future trajectories on streaming data and we refer to it as “streaming forecasting.” Our benchmark inherently captures the disappearance and re-appearance of agents, presenting the emergent challenge of forecasting for occluded agents, which is a safetycritical problem yet overlooked by snapshot-based benchmarks. Moreover, forecasting in the context of continuous timestamps naturally asks for temporal coherence between predictions from adjacent timestamps. Based on this benchmark, we further provide solutions and analysis for streaming forecasting. We propose a plug-and-play meta-algorithm called “Predictive Streamer” that can adapt any snapshot-based forecaster into a streaming forecaster. Our algorithm estimates the states of occluded agents by propagating their positions with multi-modal trajectories, and leverages differentiable filters to ensure temporal consistency. Both occlusion reasoning and temporal coherence strategies significantly improve forecasting quality, resulting in 25% smaller endpoint errors for occluded agents and 10-20% smaller fluctuations of trajectories. Our work is intended to generate interest within the community by highlighting the importance of addressing motion forecasting in its intrinsic streaming setting. Code is available at https://github.com/ziqipang/StreamingForecasting.</td>
                <td>Fluctuations, Codes, Coherence, Benchmark testing, Filtering algorithms, Prediction algorithms, Cognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341894&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341894&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Leveraging Multimodal Sensing and Topometric Mapping for Human-Like Autonomous Navigation in Complex Environments</td>
                <td>K. Tsiakas, D. Alexiou, D. Giakoumis, A. Gasteratos and D. Tzovaras</td>
                <td>2023</td>
                <td>Autonomous vehicle navigation in complex and unpredictable outdoor environments requires extensive and detailed understanding of the surrounding area and compliance with the traffic rules. In this paper, we attempt to imitate human driver behavior towards autonomous navigation that is suitable for diverse, challenging environments, whether urban, semi-structured or rural-like. Our approach starts with a novel method that we propose for extracting free space area using RGB and LiDAR data, in combination with a rough topometric map for route planning. Local goals are extracted in the final drivable region, and the vehicle draws a local path in the free space that is approximately in line with the overall path via a lattice planner. Our method is evaluated both in the publicly available KITTI urban dataset and a custom-made dataset of a semi-structured environment. In both cases, the results highlight the potential of our approach for further advancements in autonomous navigation and the development of safer and more human-like behaviors in driverless vehicles compared to the existing trajectory prediction state-of-the-art methods that make use of a topometric map.</td>
                <td>Space vehicles, Visualization, Navigation, Multimodal sensors, MIMICs, Behavioral sciences, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341358&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341358&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Autonomous Robot Navigation in Human Populated Environments Using an Universal SFM and Parametrized MPC</td>
                <td>E. Fiasché, P. Martinet and E. Malis</td>
                <td>2023</td>
                <td>Autonomous mobile robot navigation in a human populated and encumbered environment is recognized as a hard problem to be solved in real-time. Most of the time, robots face the so-called ‘Freezing Robot Problem’, that occurs when the robot stops because no feasible and safe motion can be found. In order to provide to the robot the capability of proactive navigation, in this work we generalize the classical Social Force Model into a Universal Social Force Model (USFM) that attributes to any object surrounding the robot (humans, robots, obstacles) a social behavior. Nonlinear Model Predictive Control (MPC) can be used to solve the autonomous navigation problem since it can take into account all the possible constraints coming from the interaction model between the robot and the different surrounding objects. However, to be effective, MPC requires a sufficiently large prediction horizon, which generally implies a high computational cost. In order to considerably reduce the computational cost, we propose a new control parametrisation based on Thin Plate Spline Radial Basis Functions that allow us to have a large prediction horizon with fewer parameters. The global control framework is validated in simulation with virtual pedestrians, and in real world environments.</td>
                <td>Pedestrians, Navigation, Force, Real-time systems, Computational efficiency, Behavioral sciences, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341814&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341814&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Pseudo-Stereo++: Cycled Generative Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving</td>
                <td>A. Elhagry, H. Dai and A. El Saddik</td>
                <td>2023</td>
                <td>Recently, the feature-level generation has demonstrated the effectiveness of pseudo-stereo synthesis in Monocular 3D Detection (M3D). In this paper, we aim to further bridge the gap between the stereo and the monocular 3D object detectors in autonomous driving through direct image-level pseudo-stereo generation. We propose a novel Cycled Generative Pseudo-Stereo (CGPS) architecture to generate the right-view image from the left-view for constructing a pseudo-stereo pair to stereo 3D object detectors while maintaining the natural of M3D with the left-view image as the only input. Moreover, we use a triplet consistency loss to focus on the detected objects in the pseudo-stereo generation. Besides, we demonstrate that the proposed CGPS is an ad-hoc module to adapt top stereo 3D object detectors into monocular 3D object detectors. The proposed framework with CGPS achieves 74.80%, 55.28%, and 46.70% 3DAP for easy, moderate, and hard difficulty levels in monocular 3D detection on the KITTI benchmark with comparable performance to the stereo 3D object detectors but using a monocular image as the only input. Till the submission, the proposed M3D framework ranks 1st with dramatic improvements against the existing monocular 3D detectors on the KITTI benchmark.</td>
                <td>Three-dimensional displays, Detectors, Object detection, Benchmark testing, Feature extraction, Autonomous vehicles, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341610&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341610&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Aggressive Trajectory Generation for a Swarm of Autonomous Racing Drones</td>
                <td>Y. Shen et al.</td>
                <td>2023</td>
                <td>Autonomous drone racing is becoming an excellent platform to challenge quadrotors' autonomy techniques including planning, navigation and control technologies. However, most research on this topic mainly focuses on single drone scenarios. In this paper, we describe a novel time-optimal trajectory generation method for generating time-optimal trajectories for a swarm of quadrotors to fly through pre-defined waypoints with their maximum maneuverability without collision. We verify the method in the Gazebo simulations where a swarm of 5 quadrotors can fly through a complex 6-waypoint racing track in a $35m\times 35m$ space with a top speed of 14m/s. Flight tests are performed on two quadrotors passing through 3 waypoints in a $4m\times 2m$ flight arena to demonstrate the feasibility of the proposed method in the real world. Both simulations and real-world flight tests show that the proposed method can generate the optimal aggressive trajectories for a swarm of autonomous racing drones. The method can also be easily transferred to other types of robot swarms.</td>
                <td>Trajectory tracking, Navigation, Robot sensing systems, Trajectory, Sensors, Planning, Security</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341844&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341844&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Toward Human-Like Social Robot Navigation: A Large-Scale, Multi-Modal, Social Human Navigation Dataset</td>
                <td>D. M. Nguyen, M. Nazeri, A. Payandeh, A. Datar and X. Xiao</td>
                <td>2023</td>
                <td>Humans are well-adept at navigating public spaces shared with others, where current autonomous mobile robots still struggle: while safely and efficiently reaching their goals, humans communicate their intentions and conform to unwritten social norms on a daily basis; conversely, robots become clumsy in those daily social scenarios, getting stuck in dense crowds, surprising nearby pedestrians, or even causing collisions. While recent research on robot learning has shown promises in data-driven social robot navigation, good-quality training data is still difficult to acquire through either trial and error or expert demonstrations. In this work, we propose to utilize the body of rich, widely available, social human navigation data in many natural human-inhabited public spaces for robots to learn similar, human-like, socially compliant navigation behaviors. To be specific, we design an open-source egocentric data collection sensor suite wearable by walking humans to provide multimodal robot perception data; we collect a large-scale (~100 km, 20 hours, 300 trials, 13 humans) dataset in a variety of public spaces which contain numerous natural social navigation interactions; we analyze our dataset, demonstrate its usability, and point out future research directions and use cases.11Website: https://cs.gmu.edu/-xiao/Research/MuSoHu/</td>
                <td>Legged locomotion, Pedestrians, Navigation, Social robots, Training data, Robot sensing systems, Robot learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342447&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342447&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hilbert Space Embedding-Based Trajectory Optimization for Multi-Modal Uncertain Obstacle Trajectory Prediction</td>
                <td>B. Sharma, A. Sharma, K. M. Krishna and A. K. Singh</td>
                <td>2023</td>
                <td>Safe autonomous driving critically depends on how well the ego-vehicle can predict the trajectories of neighboring vehicles. To this end, several trajectory prediction algorithms have been presented in the existing literature. Many of these approaches output a multimodal distribution of obstacle trajectories instead of a single deterministic prediction to account for the underlying uncertainty. However, existing planners cannot handle the multimodality based on just sample-level information of the predictions. With this motivation, this paper proposes a trajectory optimizer that can leverage the distributional aspects of the prediction in a computationally tractable and sample-efficient manner. Our optimizer can work with arbitrarily complex distributions and thus can be used with output distribution represented as a deep neural network. The core of our approach is built on embedding distribution in Reproducing Kernel Hilbert Space (RKHS), which we leverage in two ways. First, we propose an RKHS embedding approach to select probable samples from the obstacle trajectory distribution. Second, we rephrase chance-constrained optimization as distribution matching in RKHS and propose a novel sampling-based optimizer for its solution. We validate our approach with handcrafted and neural network-based predictors trained on real-world datasets and show improvement over the existing stochastic optimization approaches in safety metrics.</td>
                <td>Measurement, Uncertainty, Prediction algorithms, Hilbert space, Trajectory, Safety, Kernel</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342490&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342490&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Risk-Sensitive Mobile Robot Navigation in Crowded Environment via Offline Reinforcement Learning</td>
                <td>J. Wu, Y. Wang, H. Asama, Q. An and A. Yamashita</td>
                <td>2023</td>
                <td>Mobile robot navigation in a human-populated environment has been of great interest to the research community in recent years, referred to as crowd navigation. Currently, offline reinforcement learning (RL)-based method has been introduced to this domain, for its ability to alleviate the sim2real gap brought by online RL which relies on simulators to execute training, and its scalability to use the same dataset to train for differently customized rewards. However, the performance of the navigation policy suffered from the distributional shift between the training data and the input during deployment, since when it gets an input out of the training data distribution, the learned policy has the risk of choosing an erroneous action that leads to catastrophic failure such as colliding with a human. To realize risk sensitivity and improve the safety of the offline RL agent during deployment, this work proposes a multipolicy control framework that combines offline RL navigation policy with a risk detector and a force-based risk-avoiding policy. In particular, a Lyapunov density model is learned using the latent feature of the offline RL policy and works as a risk detector to switch the control to the risk-avoiding policy when the robot has a tendency to go out of the area supported by the training data. Experimental results showed that the proposed method was able to learn navigation in a crowded scene from the offline trajectory dataset and the risk detector substantially reduces the collision rate of the vanilla offline RL agent while maintaining the navigation efficiency outperforming the state-of-the-art methods.</td>
                <td>Training, Navigation, Training data, Reinforcement learning, Detectors, Data models, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341948&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341948&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>GP-Guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments</td>
                <td>I. S. Mohamed, M. Ali and L. Liu</td>
                <td>2023</td>
                <td>Robotic navigation in unknown, cluttered environ-ments with limited sensing capabilities poses significant chal-lenges in robotics. Local trajectory optimization methods, such as Model Predictive Path Intergal (MPPI), are a promising solution to this challenge. However, global guidance is required to ensure effective navigation, especially when encountering challenging environmental conditions or navigating beyond the planning horizon. This study presents the GP-MPPI, an online learning-based control strategy that integrates MPPI with a local perception model based on Sparse Gaussian Process (SGP). The key idea is to leverage the learning capability of SGP to construct a variance (uncertainty) surface, which enables the robot to learn about the navigable space surrounding it, identify a set of suggested subgoals, and ultimately recommend the optimal subgoal that minimizes a predefined cost function to the local MPPI planner. Afterward, MPPI computes the optimal control sequence that satisfies the robot and collision avoidance constraints. Such an approach eliminates the necessity of a global map of the environment or an offline training process. We validate the efficiency and robustness of our proposed control strategy through both simulated and real-world experiments of 2D autonomous navigation tasks in complex unknown en-vironments, demonstrating its superiority in guiding the robot safely towards its desired goal while avoiding obstacles and escaping entrapment in local minima. The GPU implementation of GP-MPPI, including the supplementary video, is available at https://github.com/IhabMohamed/GP-MPPI.</td>
                <td>Training, Uncertainty, Navigation, Optimal control, Aerospace electronics, Robot sensing systems, Cost function</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341382&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341382&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>V2X-Lead: LiDAR-Based End-to-End Autonomous Driving with Vehicle-to-Everything Communication Integration</td>
                <td>Z. Deng, Y. Shi and W. Shen</td>
                <td>2023</td>
                <td>This paper presents a LiDAR-based end-to-end autonomous driving method with Vehicle-to-Everything (V2X) communication integration, termed V2X-Lead, to address the challenges of navigating unregulated urban scenarios under mixed-autonomy traffic conditions. The proposed method aims to handle imperfect partial observations by fusing the onboard LiDAR sensor and V2X communication data. A model-free and off-policy deep reinforcement learning (DRL) algorithm is employed to train the driving agent, which incorporates a carefully designed reward function and multi-task learning technique to enhance generalization across diverse driving tasks and scenarios. Experimental results demonstrate the effectiveness of the proposed approach in improving safety and efficiency in the task of traversing unsignalized intersections in mixed-autonomy traffic, and its generalizability to previously unseen scenarios, such as roundabouts. The integration of V2X communication offers a significant data source for autonomous vehicles (AVs) to perceive their surroundings beyond onboard sensors, resulting in a more accurate and comprehensive perception of the driving environment and more safe and robust driving behavior.</td>
                <td>Laser radar, Protocols, Soft sensors, Reinforcement learning, Multitasking, Robot sensing systems, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342375&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342375&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>C2: Co-design of Robots via Concurrent-Network Coupling Online and Offline Reinforcement Learning</td>
                <td>C. Chen, P. Xiang, H. Lu, Y. Wang and R. Xiong</td>
                <td>2023</td>
                <td>With the increasing computing power, using data-driven approaches to co-design a robot's morphology and controller has become a promising way. However, most existing data-driven methods require training the controller for each morphology to calculate fitness, which is time-consuming. In contrast, the dual-network framework utilizes data collected by individual networks under a specific morphology to train a population network that provides a surrogate function for morphology optimization. This approach replaces the traditional evaluation of a diverse set of candidates, thereby speeding up the training. Despite considerable results, the online training of both networks impedes their performance. To address this issue, we propose a concurrent network framework that combines online and offline reinforcement learning (RL) methods. By leveraging the behavior cloning term in a flexible manner, we achieve an effective combination of both networks. We conducted multiple sets of comparative experiments in the simulator and found that the proposed method effectively addresses issues present in the dual-network framework, leading to overall algorithmic performance improvement. Furthermore, we validated the algorithm on a real robot, demonstrating its feasibility in a practical application.</td>
                <td>Training, Sociology, Morphology, Reinforcement learning, Robot sensing systems, Control systems, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341983&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341983&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Autonomous Exploration and Mapping for Mobile Robots via Cumulative Curriculum Reinforcement Learning</td>
                <td>Z. Li, J. Xin and N. Li</td>
                <td>2023</td>
                <td>Deep reinforcement learning (DRL) has been widely applied in autonomous exploration and mapping tasks, but often struggles with the challenges of sampling efficiency, poor adaptability to unknown map sizes, and slow simulation speed. To speed up convergence, we combine curriculum learning (CL) with DRL, and first propose a Cumulative Curriculum Reinforcement Learning (CCRL) training framework to alleviate the issue of catastrophic forgetting faced by general CL. Besides, we present a novel state representation, which considers a local egocentric map and a global exploration map resized to the fixed dimension, so as to flexibly adapt to environments with various sizes and shapes. Additionally, for facilitating the fast training of DRL models, we develop a lightweight grid-based simulator, which can substantially accelerate simulation compared to popular robot simulation platforms such as Gazebo. Based on the customized simulator, comprehensive experiments have been conducted, and the results show that the CCRL framework not only mitigates the catastrophic forgetting problem, but also improves the sample efficiency and generalization of DRL models, compared to general CL as well as without a curriculum. Our code is available at https://github.com/BeamanLi/CCRL_Exploration.</td>
                <td>Training, Deep learning, Adaptation models, Codes, Shape, Reinforcement learning, Mobile robots, Task analysis, Intelligent robots, Convergence</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342066&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342066&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning to Solve Tasks with Exploring Prior Behaviours</td>
                <td>R. Zhu, S. Li, T. Dai, C. Zhang and O. Celiktutan</td>
                <td>2023</td>
                <td>Demonstrations are widely used in Deep Reinforcement Learning (DRL) for facilitating solving tasks with sparse rewards. However, the tasks in real-world scenarios can often have varied initial conditions from the demonstration, which would require additional prior behaviours. For example, consider we are given the demonstration for the task of picking up an object from an open drawer, but the drawer is closed in the training. Without acquiring the prior behaviours of opening the drawer, the robot is unlikely to solve the task. To address this, in this paper we propose an Intrinsic Reward Driven Example-based Control (IRDEC). Our method can endow agents with the ability to explore and acquire the required prior behaviours and then connect to the task-specific behaviours in the demonstration to solve sparse-reward tasks without requiring additional demonstration of the prior behaviours. The performance of our method outperforms other baselines on three navigation tasks and one robotic manipulation task with sparse rewards. Codes are available at https://github.com/Ricky-Zhu/IRDEC.</td>
                <td>Training, Deep learning, Codes, Navigation, Reinforcement learning, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342272&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342272&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Comparing Quadrotor Control Policies for Zero-Shot Reinforcement Learning under Uncertainty and Partial Observability</td>
                <td>S. Gronauer, D. Stümke and K. Diepold</td>
                <td>2023</td>
                <td>To alleviate the sample complexity of reinforcement learning algorithms, simulations are a common approach to train control policies before deploying the policy on a real-world robot. However, a gap between simulation and reality generally persists, which endorses the aim to train robust policies already in simulation such that those can be transferred to a real robot at a high success rate. In this paper, we investigate history-dependent policies for drone control in the context of zero-shot transfer learning, where the training is conducted exclusively in simulation. We compare policies represented by feed-forward neural networks with recurrent neural networks and assess both performance and robustness on a real-world quadrotor. Furthermore, we study if an end-to-end learned representation can control a quadrotor based on raw onboard-sensor information only, rendering accurate state estimation from a Kalman filter obsolete. Our results show that recurrent control policies achieve similar performance and robustness as feed-forward policies when acting on state estimates. With raw sensory data, however, recurrent networks offer higher success rates for sim-to-real transfer than feed-forward networks. We also find that recurrent architectures are advantageous when system parameters such as latency are uncertain.</td>
                <td>Training, Uncertainty, Transfer learning, Reinforcement learning, Robot sensing systems, Rendering (computer graphics), Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341941&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341941&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Reducing Safety Interventions in Provably Safe Reinforcement Learning</td>
                <td>J. Thumm, G. Pelat and M. Althoff</td>
                <td>2023</td>
                <td>Deep Reinforcement Learning (RL) has shown promise in addressing complex robotic challenges. In real-world applications, RL is often accompanied by failsafe controllers as a last resort to avoid catastrophic events. While necessary for safety, these interventions can result in undesirable behaviors, such as abrupt braking or aggressive steering. This paper proposes two safety intervention reduction methods: proactive replacement and proactive projection, which change the action of the agent if it leads to a potential failsafe intervention. These approaches are compared to state-of-the-art constrained RL on the OpenAI safety gym benchmark and a human-robot collab-oration task. Our study demonstrates that the combination of our method with provably safe RL leads to high-performing policies with zero safety violations and a low number of failsafe interventions. Our versatile method can be applied to a wide range of real-world robotic tasks, while effectively improving safety without sacrificing task performance.</td>
                <td>Deep learning, Reinforcement learning, Manipulators, Robustness, Production facilities, Safety, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342464&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342464&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning</td>
                <td>J. Gao, S. Reddy, G. Berseth, A. D. Dragan and S. Levine</td>
                <td>2023</td>
                <td>Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated navigation task by using their eye gaze to modulate a 128-dimensional command signal from their webcam. The results show that our method enables successful goal navigation more often than a baseline directional interface, by learning to denoise user commands signals and provide shared autonomy assistance. We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated user commands, and find that our method improves over baseline interfaces in these domains as well. Extensive ablation experiments with simulated user commands empirically motivate each component of our method.</td>
                <td>Training, Space vehicles, Machine learning algorithms, Navigation, Webcams, Moon, Reinforcement learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341779&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341779&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>End-to-End Reinforcement Learning for Torque Based Variable Height Hopping</td>
                <td>R. Soni, D. Harnack, H. Isermann, S. Fushimi, S. Kumar and F. Kirchner</td>
                <td>2023</td>
                <td>Legged locomotion is arguably the most suited and versatile mode to deal with natural or unstructured terrains. Intensive research into dynamic walking and running controllers has recently yielded great advances, both in the optimal control and reinforcement learning (RL) literature. Hopping is a challenging dynamic task involving a flight phase and has the potential to increase the traversability of legged robots. Model based control for hopping typically relies on accurate detection of different jump phases, such as lift-off or touch down, and using different controllers for each phase. In this paper, we present a end-to-end RL based torque controller that learns to implicitly detect the relevant jump phases, removing the need to provide manual heuristics for state detection. We also extend a method for simulation to reality transfer of the learned controller to contact rich dynamic tasks, resulting in successful deployment on the robot after training without parameter tuning.</td>
                <td>Training, Legged locomotion, Torque, Torque control, Neural networks, Propioception, Optimal control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342187&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342187&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Offline Reinforcement Learning for Quadrotor Control: Overcoming the Ground Effect</td>
                <td>L. Sacchetto, M. Korte, S. Gronauer, M. Kissel and K. Diepold</td>
                <td>2023</td>
                <td>Applying Reinforcement Learning to solve real-world optimization problems presents significant challenges because of the large amount of data normally required. A popular solution is to train the algorithms in a simulation and transfer the weights to the real system. However, sim-to-real approaches are prone to fail when the Reality Gap is too big, e.g. in robotic systems with complex and non-linear dynamics. In this work, we propose the use of Offline Reinforcement Learning as a viable alternative to sim-to-real policy transfer to address such instances. On the example of a small quadrotor, we show that the ground effect causes problems in an otherwise functioning zero-shot sim-to-real framework. Our sim-to-real experiments show that, even with the explicit modelling of the ground effect and the employing of popular transfer techniques, the trained policies fail to capture the physical nuances necessary to perform a real-world take-off maneuver. Contrariwise, we show that state-of-the-art Offline Reinforcement Learning algorithms represent a feasible, reliable and sample efficient alternative in this use case.</td>
                <td>Training, Heuristic algorithms, Reinforcement learning, Reliability, Optimization, Intelligent robots, Quadrotors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341599&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341599&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dynamic Decision Frequency with Continuous Options</td>
                <td>A. Karimi, J. Jin, J. Luo, A. R. Mahmood, M. Jagersand and S. Tosatto</td>
                <td>2023</td>
                <td>In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the problem's difficulty by requiring the agent to make numerous decisions to achieve its goal while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods on simulated continuous control tasks with various action-cycle times. We show that our algorithm's performance is not affected by the choice of environment interaction frequency. Furthermore, we demonstrate the efficacy of CTCO in facilitating exploration in a real-world visual reaching task for a 7 DOF robotic arm with sparse rewards.</td>
                <td>Visualization, Time-frequency analysis, Heuristic algorithms, Reinforcement learning, Manipulators, Control systems, High frequency</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342408&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342408&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios</td>
                <td>Y. Lu et al.</td>
                <td>2023</td>
                <td>Imitation learning (IL) is a simple and powerful way to use high-quality human driving data, which can be collected at scale, to produce human-like behavior. However, policies based on imitation learning alone often fail to sufficiently account for safety and reliability concerns. In this paper, we show how imitation learning combined with reinforcement learning using simple rewards can substan-tially improve the safety and reliability of driving policies over those learned from imitation alone. In particular, we train a policy on over lOOk miles of urban driving data, and measure its effectiveness in test scenarios grouped by different levels of collision likelihood. Our analysis shows that while imitation can perform well in low-difficulty scenarios that are well-covered by the demonstration data, our proposed approach significantly improves robustness on the most challenging scenarios (over 38 % reduction in failures). To our knowledge, this is the first application of a combined imitation and reinforcement learning approach in autonomous driving that utilizes large amounts of real- world human driving data.</td>
                <td>Training, Atmospheric measurements, Reinforcement learning, Particle measurements, Robustness, Safety, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342038&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342038&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores</td>
                <td>S. Liu, C. Wu, Y. Li and L. Zhang</td>
                <td>2023</td>
                <td>Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of a large amount of interactive feedback. This paper presents a new method that uses scores provided by humans instead of pairwise preferences to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by humans negatively impacting the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method for robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores while requiring less feedback compared to pairwise preference learning methods. The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL.</td>
                <td>Training, Learning systems, Adaptive learning, Source coding, Reinforcement learning, Boosting, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341990&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341990&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dual Variable Actor-Critic for Adaptive Safe Reinforcement Learning</td>
                <td>J. Lee, J. Heo, D. Kim, G. Lee and S. Oh</td>
                <td>2023</td>
                <td>Satisfying safety constraints in reinforcement learning (RL) is an important issue, especially in real-world applications. Many studies have approached safe RL with the Lagrangian method, which introduces dual variables. However, applying a trained policy with the optimal dual variable to a new environment can be hazardous since the optimal value of the dual variable, which represents a level of safety, depends on the environmental setting. To this end, we propose a new framework, dual variable actor-critic (DVAC), that solves the safe RL problem by simultaneously training a single policy over different safety levels. We introduce a universal policy and universal Q-function, which have a dual variable as an argument. Then, we extend the soft actor-critic so that the universal policy is guaranteed to converge to the Pareto optimal policy sets. We evaluate the proposed method in simulation and real-world environments. The universal policy learned with the proposed method ranges from extremely safe to high performance according to the dual variables, and is nearly Pareto optimal compared to policies learned with the baseline methods. In addition, the agent is able to adapt to environments with unseen state distributions without additional training by identifying a suitable dual variable using the proposed method.</td>
                <td>Training, Reinforcement learning, Pareto optimization, Safety, Trajectory, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341973&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341973&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multimodal Diffusion Segmentation Model for Object Segmentation from Manipulation Instructions</td>
                <td>Y. Iioka, Y. Yoshida, Y. Wada, S. Hatanaka and K. Sugiura</td>
                <td>2023</td>
                <td>In this study, we aim to develop a model that comprehends a natural language instruction (e.g., “Go to the living room and get the nearest pillow to the radio art on the wall”) and generates a segmentation mask for the target everyday object. The task is challenging because it requires (1) the understanding of the referring expressions for multiple objects in the instruction, (2) the prediction of the target phrase of the sentence among the multiple phrases, and (3) the generation of pixel-wise segmentation masks rather than bounding boxes. Studies have been conducted on language-based segmentation methods; however, they sometimes mask irrelevant regions for complex sentences. In this paper, we propose the Multimodal Diffusion Segmentation Model (MDSM), which generates a mask in the first stage and refines it in the second stage. We introduce a crossmodal parallel feature extraction mechanism and extend diffusion probabilistic models to handle crossmodal features. To validate our model, we built a new dataset based on the well-known Matterport3D and REVERIE datasets. This dataset consists of instructions with complex referring expressions accompanied by real indoor environmental images that feature various target objects, in addition to pixel-wise segmentation masks. The performance of MDSM surpassed that of the baseline method by a large margin of +10.13 mean IoU.</td>
                <td>Image segmentation, Art, Natural languages, Object segmentation, Feature extraction, Transformers, Probabilistic logic</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341402&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341402&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>InsMOS: Instance-Aware Moving Object Segmentation in LiDAR Data</td>
                <td>N. Wang, C. Shi, R. Guo, H. Lu, Z. Zheng and X. Chen</td>
                <td>2023</td>
                <td>Identifying moving objects is a crucial capability for autonomous navigation, consistent map generation, and future trajectory prediction of objects. In this paper, we propose a novel network that addresses the challenge of segmenting moving objects in 3D LiDAR scans. Our approach not only predicts point-wise moving labels but also detects instance information of main traffic participants. Such a design helps determine which instances are actually moving and which ones are temporarily static in the current scene. Our method exploits a sequence of point clouds as input and quantifies them into 4D voxels. We use 4D sparse convolutions to extract motion features from the 4D voxels and inject them into the current scan. Then, we extract spatio-temporal features from the current scan for instance detection and feature fusion. Finally, we design an upsample fusion module to output point-wise labels by fusing the spatio-temporal features and predicted instance information. We evaluated our approach on the LiDAR-MOS benchmark based on SemanticKITTI and achieved better moving object segmentation performance compared to state-of-the-art methods, demonstrating the effectiveness of our approach in integrating instance information for moving object segmentation. Furthermore, our method shows superior performance on the Apollo dataset with a pre-trained model on SemanticKITTI, indicating that our method generalizes well in different scenes. The code and pre-trained models of our method will be released at https://github.com/nubot-nudt/InsMOS.</td>
                <td>Point cloud compression, Laser radar, Three-dimensional displays, Motion segmentation, Object segmentation, Feature extraction, Prediction algorithms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342277&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342277&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ElC-OIS: Ellipsoidal Clustering for Open-World Instance Segmentation on LiDAR Data</td>
                <td>W. Deng, K. Huang, Q. Yu, H. Lu, Z. Zheng and X. Chen</td>
                <td>2023</td>
                <td>Open-world Instance Segmentation (OIS) is a challenging task that aims to accurately segment every object instance appearing in the current observation, regardless of whether these instances have been labeled in the training set. This is important for safety-critical applications such as robust autonomous navigation. In this paper, we present a flexible and effective OIS framework for LiDAR point cloud that can accurately segment both known and unknown instances (i.e., seen and unseen instance categories during training). It first identifies points belonging to known classes and removes the back-ground by leveraging close-set panoptic segmentation networks. Then, we propose a novel ellipsoidal clustering method that is more adapted to the characteristic of LiDAR scans and allows precise segmentation of unknown instances. Furthermore, a diffuse searching method is proposed to handle the common over-segmentation problem presented in the known instances. With the combination of these techniques, we are able to achieve accurate segmentation for both known and unknown instances. We evaluated our method on the SemanticKITTI open-world LiDAR instance segmentation dataset. The experimental results suggest that it outperforms current state-of-the-art methods, especially with a 10.0% improvement in association quality. The source code of our method will be publicly available at https://github.com/nubot-nudt/ElC-OIS.</td>
                <td>Instance segmentation, Training, Point cloud compression, Laser radar, Clustering methods, Source coding, Benchmark testing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342356&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342356&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Spatio-Temporal Attention Network for Persistent Monitoring of Multiple Mobile Targets</td>
                <td>Y. Wang, Y. Wang, Y. Cao and G. Sartoretti</td>
                <td>2023</td>
                <td>This work focuses on the persistent monitoring problem, where a set of targets moving based on an unknown model must be monitored by an autonomous mobile robot with a limited sensing range. To keep each target's position estimate as accurate as possible, the robot needs to adaptively plan its path to (re-)visit all the targets and update its belief from measurements collected along the way. In doing so, the main challenge is to strike a balance between exploitation, i.e., re-visiting previously-located targets, and exploration, i.e., finding new targets or re-acquiring lost ones. Encouraged by recent advances in deep reinforcement learning, we introduce an attention-based neural solution to the persistent monitoring problem, where the agent can learn the inter-dependencies between targets, i.e., their spatial and temporal correlations, conditioned on past measurements. This endows the agent with the ability to determine which target, time, and location to attend to across multiple scales, which we show also helps relax the usual limitations of a finite target set with prior positional information. We experimentally demonstrate that our method outperforms other baselines in terms of number of targets visits and average estimation error in complex environments. Finally, we implement and validate our model in a drone-based simulation experiment to monitor mobile ground targets in a high-fidelity simulator.</td>
                <td>Deep learning, Estimation error, Reinforcement learning, Position measurement, Robot sensing systems, Particle measurements, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341674&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341674&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Subtask Aware End-to-End Learning for Visual Room Rearrangement</td>
                <td>H. Kim</td>
                <td>2023</td>
                <td>The goal of intelligent embodied agents is to learn how to explore within the environment, interact with objects, and understand the environment in order to achieve task objectives. There are two main approaches to training such agents: one is to train an action policy that performs the task goal through end-to-end learning, and the other is to construct a policy by implementing the necessary abilities according to the task goal in a modular manner. For complex and long-horizon tasks, such as visual room rearrangement, a modular approach that infers task sequence by identifying the causality of actions through prior knowledge shows higher performance. Based on this insight, we propose an Online Subtask Prediction Network (OSPNet) that determines the subtask to be performed at each moment based on the environment information and past subtask inference history to train an embodied agent for long-horizon tasks through an end-to-end manner, and also propose a Subtask Aware Policy Network (SAPNet) as the action policy that decides actions based on the reasoning of the OSPNet. We implement an embodied agent that performs visual room rearrangement using the proposed SAPNet and train it through imitation learning, demonstrating similar or better performance with much fewer training steps than previous works.</td>
                <td>Training, Visualization, Cognition, History, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342320&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342320&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Less Than Human: How Different Users of Telepresence Robots Expect Different Social Norms</td>
                <td>C. Lin, J. Rhim and A. J. Moon</td>
                <td>2023</td>
                <td>Does the norm of first-come-first-serve (FCFS) equally apply to those piloting a Mobile Remote Presence (MRP) system as to those who are physically present with it? While telepresence robots could make social interactions more accessible and enjoyable for geographically-constrained individuals, such an outcome requires both pilots and local users of MRPs to share the same social norm expectations that govern their use. To address this question, we conducted an online study $(N=903)$ involving simulated human-MRP interaction scenarios. Our results suggest that those remotely piloting the MRP-rather than local users-assign the robot to a lower social priority; they find it more appropriate when local users ignore queue order than when pilots ignore queue order. Furthermore, we provide significant empirical evidence that local users expect different social norms to be upheld depending on how they perceive the robot. Those who perceive MRPs simply as robots-rather than an extension of a person-do not expect the FCFS norm to be respected for MRPs.</td>
                <td>Telepresence, Human-robot interaction, Elevators, Robots, Intelligent robots, Materials requirements planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341962&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341962&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Do Hierarchies in a Robot Team Impact the Service Evaluation by Users?</td>
                <td>S. Shin and S. S. Kwak</td>
                <td>2023</td>
                <td>Customer satisfaction is not only determined by how workers treat customers but also by how they treat their coworkers. In this sense, we examined whether the dynamics of robot workers influence user satisfaction. To optimize the robot team's atmosphere for Korean culture, we adopted the Korean honorific language system to express hierarchy. We set four types of relationships between two robots: Case 1) both using non honorific language, indicating intimacy and a flat hierarchy; Case 2) both using honorific language, indicating non intimacy and a flat hierarchy; Case 3) delivery robot using non honorific language and monitoring robot using honorific language, indicating non intimacy and a tall hierarchy; Case 4) vice versa. People preferred a robot team with a flat hierarchy and both robots using honorific language, common in the service industry. Interviews revealed discomfort with tall hierarchies. Case 4 was least satisfactory in service, while Case 3 was least socially intelligent. People perceive a robot team mirroring human society as socially intelligent, but negative representations like Case 4 led to less satisfaction.</td>
                <td>Industries, Service robots, Hospitals, Employment, Organizations, Robot sensing systems, Interviews</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341364&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341364&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robot-Induced Group Conversation Dynamics: A Model to Balance Participation and Unify Communities</td>
                <td>L. Grassi, C. T. Recchiuto and A. Sgorbissa</td>
                <td>2023</td>
                <td>The purpose of this research is to study the impact of robot participation in group conversations and assess the effectiveness of different addressing policies. The study involved a total of 300 participants, who were divided into groups of four and engaged in a dialogue with a humanoid robot. The robot acted as a moderator, using information obtained during the conversation to determine which speaker to address. The study found that the policy used by the robot significantly impacted the conversation dynamics. Specifically, the robot provided more balanced attention to each participant and reduced the number of subgroups.</td>
                <td>Visualization, Sociology, Social robots, Humanoid robots, Oral communication, Medical services, User experience</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342510&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342510&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts</td>
                <td>P. Tuttosi, E. Hughson, A. Matsufuji, C. Zhang and A. Lim</td>
                <td>2023</td>
                <td>How should a robot speak in a formal, quiet and dark, or a bright, lively and noisy environment? By designing robots to speak in a more social and ambient-appropriate manner we can improve perceived awareness and intelligence for these agents. We describe a process and results toward selecting robot voice styles for perceived social appropriateness and ambiance awareness. Understanding how humans adapt their voices in different acoustic settings can be challenging due to difficulties in voice capture in the wild. Our approach includes 3 steps: (a) Collecting and validating voice data interactions in virtual Zoom ambiances, (b) Exploration and clustering human vocal utterances to identify primary voice styles, and (c) Testing robot voice styles in recreated ambiances using projections, lighting and sound. We focus on food service scenarios as a proof-of-concept setting. We provide results using the Pepper robot's voice with different styles, towards robots that speak in a contextually appropriate and adaptive manner. Our results with N=120 participants provide evidence that the choice of voice style in different ambiances impacted a robot's perceived intelligence in several factors including: social appropriateness, comfort, awareness, human-likeness and competency.</td>
                <td>Lighting, Acoustics, Noise measurement, Robots, Intelligent robots, Testing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341925&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341925&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Persuasive Polite Robots in Free-Standing Conversational Groups</td>
                <td>S. Zojaji, A. B. Latupeirissa, I. Leite, R. Bresin and C. Peters</td>
                <td>2023</td>
                <td>Politeness is at the core of the common set of behavioral norms that regulate human communication and is therefore of significant interest in the design of Human-Robot Interactions. In this paper, we investigate how the politeness behaviors of a humanoid robot impact human decisions about where to join a group of two robots. We also evaluate the resulting impact on the perception of the robot's politeness. In a study involving 59 participants, the main (Pepper) robot in the group invited participants to join using six politeness behaviors derived from Brown and Levinson's politeness theory. It requests participants to join the group at the furthest side of the group which involves more effort to reach than a closer side that is also available to the participant but would ignore the request of the robot. We evaluated the robot's effectiveness in terms of persuasiveness, politeness, and clarity. We found that more direct and explicit politeness strategies derived from the theory have a higher level of success in persuading participants to join at the furthest side of the group. We also evaluated participants' adherence to social norms i.e. not walking through the center, or o-space, of the group when joining it. Our results showed that participants tended to adhere to social norms when joining at the furthest side by not walking through the center of the group of robots, even though they were informed that the robots were fully automated.</td>
                <td>Legged locomotion, Humanoid robots, Human-robot interaction, Behavioral sciences, Robots, Intelligent robots, Social robotics, Politeness, Persuasiveness, Social norms, Human-Robot interaction, free-standing conversational groups</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341830&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341830&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Social Triangles and Aggressive Lines: Multi-Robot Formations Impact Navigation and Approach</td>
                <td>A. Bacula, E. Villalovoz, D. Flynn, A. Mehta and H. Knight</td>
                <td>2023</td>
                <td>Spatial formations can give many social cues, such as illustrating a group of people are having a conversation (social affiliation), or that they are trying to move swiftly through a space (functional goal). This work explored how people perceive varied robots formations while navigating through a space and approaching people. Evaluation occurred across four different geometric formations: wedge, v-shape, vertical line, and horizontal line (Fig 3). Two studies were conducted: the first being an exploratory study of three robots navigating through a public space, and the second being a controlled user study of the same robots approaching humans in different formations. Results showed that triangle shapes were generally received more positively than lines, with wedge being the viewed as harmless, polite, welcoming, and encouraging the human to join the robot group, whereas horizontal line was seen as threatening and unwelcoming. From a path planning perspective, v-shape and wedge were also more robust to controller variance. Results from this work show that formation impacts how people perceive robots, and as a result may impact task success. Future researchers can use these results to inform their behavior design for multi-robot groups to increase task success and desired communication effects.</td>
                <td>Navigation, Shape, Aerospace electronics, Robot sensing systems, Path planning, Behavioral sciences, Security</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342372&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342372&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>How to Make a Robot Grumpy Teaching Social Robots to Stay in Character with Mood Steering</td>
                <td>E. Nichols, D. Szapiro, Y. Vasylkiv and R. Gomez</td>
                <td>2023</td>
                <td>Conveying a robot's target mood is crucial to successful social interactions. The robot's expressive performance must be appropriate, persuasive, and consistent. However, this is challenging when interactions contain a mixture of scripted and improvised content, such as those generated by language models. In this paper, we take on the task of teaching robots to stay in character, that is to say, exhibit consistency in mood during interactions. We start by defining a communication strategy module that allows for the top-down specification of a target robot mood for a given task, goal, or context. We then propose a mood steering framework for enforcing robot mood consistency throughout an interaction that supports several target moods. Our framework consists of two components: 1. expressivity steering specifies the speech and behavior to be used by the robot to convey a target mood, and 2. language model steering ensures that improvised language is consistent with the robot's target mood. As a first step toward identifying effective communication strategies, we implement grumpy and cheerful strategies for a collaborative storytelling game and compare them to a neutral baseline. Evaluation in a collaborative storytelling game shows that our approach generates robot behavior that successfully conveys the robot's target mood throughout gameplay and language model steering generates story contributions that capture the target mood without quality degradation and raises important issues for communication strategy design.</td>
                <td>Degradation, Mood, Education, Social robots, Collaboration, Games, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341906&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341906&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhancing Teleoperated Robot Customer Service through Speech Monitoring and Filtering</td>
                <td>K. Yamada, J. Even and T. Kanda</td>
                <td>2023</td>
                <td>In this paper, we propose a system that supports operators who provide services to customers using teleoperated robots. We observed that unprofessional or lazy operators of teleoperated robots are a risk for businesses as they are likely to speak in ways that are inappropriate for customer services. The proposed system lets competent operators talk freely to customers and thus provide high quality service. For subpar operators, the proposed system filters inappropriate utterances to improve the service they provide. We conducted a user study with 21 participants to compare the proposed support system to a baseline system where operators talk freely to customers. For subpar operators, the quality of the service is significantly higher in terms of perceived politeness and reported customer satisfaction when using the proposed support system compared to when using the baseline system. For competent operators, we found no significant differences in the quality of the service between the two systems.</td>
                <td>Customer services, Customer satisfaction, Speech enhancement, Behavioral sciences, Research and development, Monitoring, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342098&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342098&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>T2FPV: Dataset and Method for Correcting First-Person View Errors in Pedestrian Trajectory Prediction</td>
                <td>B. Stoler, M. Jana, S. Hwang and J. Oh</td>
                <td>2023</td>
                <td>Predicting pedestrian motion is essential for developing socially-aware robots that interact in a crowded environment. While the natural visual perspective for a social interaction setting is an egocentric view, the majority of existing work in trajectory prediction therein has been investigated purely in the top-down trajectory space. To support first-person view trajectory prediction research, we present T2FPV, a method for constructing high-fidelity first-person view (FPV) datasets given a real-world, top-down trajectory dataset; we showcase our approach on the ETH/UCY pedestrian dataset to generate the egocentric visual data of all interacting pedestrians, creating the T2FPV-ETH dataset. In this setting, FPV-specific errors arise due to imperfect detection and tracking, occlusions, and field-of-view (FOV) limitations of the camera. To address these errors, we propose CoFE, a module that further refines the imputation of missing data in an end-to-end manner with trajectory forecasting algorithms. Our method reduces the impact of such FPV errors on downstream prediction performance, decreasing displacement error by more than 10% on average. To facilitate research engagement, we release our T2FPV-ETH dataset and software tools§§https://github.com/cmubig/T2FPV.</td>
                <td>Training, Visualization, Pedestrians, Navigation, Tracking, Software algorithms, Prediction algorithms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341874&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341874&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Interactive Task Learning for Social Robots: A Pilot Study</td>
                <td>Y. G. Louie</td>
                <td>2023</td>
                <td>For socially assistive robots to achieve widespread adoption, the ability to learn new tasks in the wild is critical. Learning from Demonstration (LfD) approaches are a popular method for learning in the wild, but current methods require significant amounts of data and can be difficult to interpret. Interactive Task Learning (ITL) is an emerging learning paradigm that aims to teach tasks in a structured manner, minimizing the need for data and increasing transparency. However, to date ITL has only been explored for physical robotics applications. Additionally, minimal research has explored how usable existing ITL systems are for non-expert users. In this work, we propose a novel approach to learn social tasks via ITL. This system utilizes recent advances in Natural Language Understanding (NLU) to learn from natural dialogue. We conducted a pilot study to compare the ITL system against an LfD approach to investigate differences in teaching performance as well as teachers' perceptions of trust and workload towards these systems. Additionally, we analyzed the teaching behavior of participants to identify successful and unsuccessful teaching strategies. Our findings suggest ITL could provide more transparency to users and improve performance by correcting speech recognition errors. However, participants generally preferred LfD and found it an easier teaching method. From the observed teaching behavior, we identify existing challenges in ITL for non-experts to teach social tasks. Using this, we propose areas of improvement toward future ITL learning paradigms that are intuitive, transparent, and performant.</td>
                <td>Education, Social robots, Speech recognition, Assistive robots, Natural language processing, Behavioral sciences, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341713&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341713&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Proactive Opinion-Driven Robot Navigation Around Human Movers</td>
                <td>C. Cathcart, M. Santos, S. Park and N. E. Leonard</td>
                <td>2023</td>
                <td>We propose, analyze, and experimentally verify a new proactive approach for robot social navigation driven by the robot's “opinion” for which way and by how much to pass human movers crossing its path. The robot forms an opinion over time according to nonlinear dynamics that depend on the robot's observations of human movers and its level of attention to these social cues. For these dynamics, it is guaranteed that when the robot's attention is greater than a critical value, deadlock in decision making is broken, and the robot rapidly forms a strong opinion, passing each human mover even if the robot has no bias nor evidence for which way to pass. We enable proactive rapid and reliable social navigation by having the robot grow its attention across the critical value when a human mover approaches. With human-robot experiments we demonstrate the flexibility of our approach and validate our analytical results on deadlock-breaking. We also show that a single design parameter can tune the trade-off between efficiency and reliability in human-robot passing. The new approach has the additional advantage that it does not rely on a predictive model of human behavior.</td>
                <td>Navigation, Decision making, System recovery, Predictive models, Reliability engineering, Nonlinear dynamical systems, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341745&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341745&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Emotionally Specific Backchanneling in Social Human-Robot Interaction and Human-Human Interaction</td>
                <td>Y. G. Louie</td>
                <td>2023</td>
                <td>Backchanneling models, designed to enhance the interactive capabilities of robots, have primarily been trained on human-human interaction data. However, applying such data directly to social robots raises concerns due to dissimilarities in the way humans and robots exhibit verbal and nonverbal behaviors, particularly in the domain of emotional backchannels. This research aims to address this gap by conducting an exploratory study on the differences in human backchanneling behaviors during interactions with humans and social robots in various emotional contexts (e.g., happy and sad). Our findings reveal significant variations in emotionally specific backchannels between human-human and human-robot interactions under different emotional contexts. These results highlight the importance of designing backchanneling models that are tailored for human-robot interactions.</td>
                <td>Social robots, Data models, Behavioral sciences, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341823&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341823&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Impact of Imperfect Exoskeleton Algorithms on Step Characteristics, Task Performance, and Perception of Exoskeleton Performance</td>
                <td>M. I. Wu and L. Stirling</td>
                <td>2023</td>
                <td>Lower-limb exoskeletons may experience errors in operational settings, where an expected assistive torque is missing. These errors may affect user's gait strategies and perception of the exoskeleton's performance, leading to impacted human-exoskeleton fluency and user trust in the system. In this study, we introduced five different exoskeleton control algorithms with fixed error rates up to 10% error (90% accuracy). Two groups of participants (N=22, 11 per group) walked with a bilateral ankle exoskeleton while completing a targeted stepping task and experienced each controller twice, but in different orders. The impact of exoskeleton error rates was assessed on step characteristics (step length and width), task performance (absolute task error), and perception of exoskeleton performance (survey responses). Step character-istics were not impacted by exoskeleton errors, but multiple participants were not able to achieve acceptable task accuracy and increased task error over time across all error rates. Increasing error rates negatively impacted users' perception of algorithm predictability, exoskeleton supportiveness, and probability of future usage. Perceived predictability and future usage probability transitioned from positive to negative between 2% and 5% error. Understanding the effect of increasing exoskeleton error rates informs minimum algorithm accuracy to support human-exoskeleton fluency and performance for gait-assist exoskeletons.</td>
                <td>Surveys, Torque, Error analysis, System performance, Exoskeletons, Prediction algorithms, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341368&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341368&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Can Quadruped Guide Robots be Used as Guide Dogs?</td>
                <td>L. Wang et al.</td>
                <td>2023</td>
                <td>Quadruped robots have the potential to guide blind and low vision (BLV) people due to their highly flexible locomotion and emotional value provided by their bionic forms. However, the development of quadruped guide robots rarely involves BLV users' participatory designs and evaluations. In this paper, we conducted two empirical experiments both in indoor controlled and outdoor field scenarios, exploring the benefits and drawbacks of quadruped guide robots. The results show that the nowadays commercial quadruped robots exposed significant disadvantages in usability and trust compared with wheeled robots. It is concluded that the moving gait and walking noise of quadruped robots would limit the guiding effectiveness to a certain extent, and the empathetic effect of its bionic form for BLV users could not be fully reflected. Based on the findings of wheeled robots and quadruped robots' advantages, we discuss the design implications for the future guide robot design for BLV users. This paper reports the first empirical experiment about quadruped guide robots with BLV users and preliminary explores their potential improvement space in substituting guide dogs, which can inspire the further specialized design of quadruped guide robots.</td>
                <td>Legged locomotion, Navigation, Dogs, Market research, Biology, Quadrupedal robots, Usability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341792&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341792&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Investigating the Usability of Collaborative Robot Control Through Hands-Free Operation Using Eye Gaze and Augmented Reality</td>
                <td>J. Lee, T. Lim and W. Kim</td>
                <td>2023</td>
                <td>This paper proposes a novel operation for controlling a mobile robot using a head-mounted device. Conventionally, robots are operated using computers or a joystick, which creates limitations in usability and flexibility because control equipment has to be carried by hand. This lack of flexibility may prevent workers from multitasking or carrying objects while operating the robot. To address this limitation, we propose a hands-free method to operate the mobile robot with a human gaze in an Augmented Reality (AR) environment. The proposed work is demonstrated using the HoloLens 2 to control the mobile robot, Robotnik Summit-XL, through the eye-gaze in AR. Stable speed control and navigation of the mobile robot were achieved through admittance control which was calculated using the gaze position. The experiment was conducted to compare the usability between the joystick and the proposed operation, and the results were validated through surveys (i.e., SUS, SEQ). The survey results from the participants after the experiments showed that the wearer of the HoloLens accurately operated the mobile robot in a collaborative manner. The results for both the joystick and the HoloLens were marked as easy to use with above-average usability. This suggests that the HoloLens can be used as a replacement for the joystick to allow hands-free robot operation and has the potential to increase the efficiency of human-robot collaboration in situations when hands-free controls are needed.</td>
                <td>Surveys, Velocity control, Robot control, Collaboration, Trajectory, Mobile robots, Synchronization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342045&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342045&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Development and Evaluation of Exploratory Experiences to Facilitate Reasoning About Robotic Systems</td>
                <td>S. Balali et al.</td>
                <td>2023</td>
                <td>This paper introduces a novel interactive approach —Exploratory Experiences— that aims to improve the ability of people to reason about the capabilities and limitations of robotic technology. We focus on two areas: robot navigation and object detection. We evaluate the Exploratory Experiences with a novel approach that measures the participant's ability to predict when the robot will fail, following up with asking the reason and a possible fix. We show that our approach is effective at improving participants' understanding of potential robot navigation failures and that they already have the skills to detect potential object detection failures when presented with the correct stimuli.</td>
                <td>Navigation, Atmospheric measurements, Object detection, Particle measurements, Cognition, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342409&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342409&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Decoding sEMG Under Non-Ideal Conditions Toward Robust Muscle-Machine Interface Control</td>
                <td>Filho and R. M. Andrade</td>
                <td>2023</td>
                <td>The evaluation of systems under non-ideal conditions is a research problem, particularly in robotic applications for the rehabilitation of people with disabilities. Accordingly, the evaluation of algorithmic strategies for robustness validation under different non-ideal conditions is a current challenge for the scientific community. Therefore, in this study, a computational methodology based on Extreme Learning Machine (ELM) was evaluated for the recognition of seven hand gestures using sEMG under five non-ideal conditions. The shift of eight sEMG electrodes, three upper-limb postures, increased muscle fatigue, and inter-subject and inter-day variabilities were evaluated. The results indicate that the proposed methodology performs well under specific conditions in comparison with previous strategies reported in the literature using Machine Learning classifiers. Therefore, the findings of this study are potentially important for the field of robotics; however, more efforts are still needed to develop more robust computational methods to obtain higher accuracy under non-ideal conditions, with the aim of implementing more controllable, usable, and reliable systems.</td>
                <td>Machine learning algorithms, Extreme learning machines, Scholarships, Finance, Machine learning, Muscles, Fatigue</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341503&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341503&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>From “Thumbs Up” to “10 out of 10”: Reconsidering Scalar Feedback in Interactive Reinforcement Learning</td>
                <td>H. Yu, R. M. Aronson, K. H. Allen and E. S. Short</td>
                <td>2023</td>
                <td>Learning from human feedback is an effective way to improve robotic learning in exploration-heavy tasks. Compared to the wide application of binary human feedback, scalar human feedback has been used less because it is believed to be noisy and unstable. In this paper, we compare scalar and binary feedback, and demonstrate that scalar feedback benefits learning when properly handled. We collected binary or scalar feedback respectively from two groups of crowdworkers on a robot task. We found that when considering how consistently a participant labeled the same data, scalar feedback led to less consistency than binary feedback; however, the difference vanishes if small mismatches are allowed. Additionally, scalar and binary feedback show no significant differences in their correlations with key Reinforcement Learning targets. We then introduce Stabilizing TEacher Assessment DYnamics (STEADY) to improve learning from scalar feedback. Based on the idea that scalar feedback is muti-distributional, STEADY reconstructs underlying positive and negative feedback distributions and re-scales scalar feedback based on feedback statistics. We show that models trained with scalar feedback + STEADY outperform baselines, including binary feedback and raw scalar feedback, in a robot reaching task with non-expert human feedback. Our results show that both binary feedback and scalar feedback are dynamic, and scalar feedback is a promising signal for use in interactive Reinforcement Learning.</td>
                <td>Negative feedback, Heuristic algorithms, Education, Thumb, Human-robot interaction, Reinforcement learning, Noise measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342458&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342458&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Continuous Identification of Passive Human Joint Impedance Using Physical Human-Robot Interaction System</td>
                <td>B. Tout, J. Chevrie, A. Dequidt and L. Vermeiren</td>
                <td>2023</td>
                <td>The identification of human joint impedance is necessary for various applications, such as improving rehabilitation efficiency or monitoring the human operator's state (fatigue, stress). To this end, in this paper we combine robot's payload identification methods with sliding window recursive least squares algorithm allowing a continuous identification of the varying human joint model without the need for external sensors. We also propose a threshold for detecting fake changes in the identified model parameters due to numerical issues. The presented approach is validated by simulations and experiments using elastic rubber bands representing a simplified passive human joint model attached to a one degree of freedom robotic system. Comparison with simple recursive least squares shows that the proposed method is promising, as it converges to the new parameter in a single window length, whereas the other method takes much longer. In addition, it distinguishes real from fake changes depending on the validity of the used model.</td>
                <td>Robot sensing systems, Numerical models, Sensors, Impedance, Rubber, Object recognition, Stress</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341372&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341372&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>The MyoPassivity Puzzle: How Does Muscle Fatigue Affect Energetic Behavior of the Human Upper-Limb During Physical Interaction with Robots?</td>
                <td>S. Oliver, P. Paik, X. Zhou and S. F. Atashzar</td>
                <td>2023</td>
                <td>The human limb possesses a remarkable capacity to absorb energy during physical human-robot interaction (pHRI), which can be quantified as the biomechanical “Excess of Passivity” (EoP) using non-linear control theory. This biome-chanical passivity index can be used to reduce conservatism and increase the transparency of pHRI stabilizers. Previous work on EoP has used system identification techniques to compute EoP offline. However, for use in real-time controllers, an instantaneous method for EoP estimation would be desired. This paper hypothesizes that muscle fatigue can potentially be a complicating factor which can cumulatively affect the ability of human biomechanics to absorb mechanical energy over time during physical interaction with robots. In this work, we focused on the energetic behavior of the human wrist during pHRI, and, for the first time, we investigated the effect of fatigue on EoP. The EoP for five participants was computed throughout one hundred-second trials of high-frequency wrist perturbations in four directions. Subjects maintained a stiff and consistent grip throughout each trial, causing an accumulation of fatigue in the forearm muscles. Muscle activity was recorded using an array of sixteen sEMG sensors. It was found that the EoP degraded (in a statistically significant manner) with increased muscle fatigue in all directions, even when the level of muscle co-contraction was controlled consistently through a visual myofeedback mechanism. 100% of the subjects exhibited this decline in energy absorption capacity in all directions studied. The median drop in EoP after one-hundred seconds of perturbation was 11% for trials in the abduction and adduction directions and 22% in the pronation and supination directions. These results indicate a need for more robust estimation methods or new modalities to account for muscle fatigue in the control architectures of physical human-robot interaction.</td>
                <td>Wrist, Absorption, Perturbation methods, Estimation, Human-robot interaction, Muscles, Fatigue</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341902&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341902&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Harnessing the Power of Human Biomechanics in Force-Position Domain: A 3D Passivity Index Map for Upper Limb Physical Human-(Tele) Robot Interaction</td>
                <td>X. Zhou, P. Paik and S. F. Atashzar</td>
                <td>2023</td>
                <td>In the context of physical human-(tele)robot interaction, passivity-based stabilizers have been used to guarantee the physical or (tele) physical stability. In most of these examples, human biomechanics is considered an inherently passive system that dissipates energy. This assumption may not hold true when the interaction is implemented in the force-position domain, even though such a setting would be needed to boost positional accuracy and avoid the common kinematic drifts in the force-velocity domains. The aforementioned topic is examined in this paper using the concept of shortage versus excess of passivity index for human biomechanics in the force-position domain. We also investigate the compounding effect of the frequency of interaction. The outcomes of this paper will be imperative for the design of force-position domain pURI stabilizers when the classical assumption of passivity of human biomechanics can lead to serious safety issues. In this work, for the first time, we quantitatively present the passivity margin and, thus, the energetic behavior of the human arm's biomechanics under various interaction scenarios in the Force-Position domain. The outcome of this work includes a three-dimensional passivity index map (3DPiM) that is validated on five healthy participants. The goal is to illustrate the passivity margin of the human upper limb biomechanics for two distinct levels of muscle co-contractions, as indicated by the Electromyography (EMG) signal, across four interaction frequencies and eight geometric directions. This outcome enables the future development of biomechanics-aware stabilizers in the force-position domain, quantifying the passivity margin in real-time and thus significantly reducing the stabilizer's conservatism while ensuring the safety of human-robot interactions.</td>
                <td>Biomechanics, Three-dimensional displays, Frequency-domain analysis, Stability criteria, Muscles, Rendering (computer graphics), Electromyography</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341393&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341393&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>No Contact Needed: Humans Adapt Their Gait to Suit Legged Robot Companions</td>
                <td>P. M. Riek and A. R. Wu</td>
                <td>2023</td>
                <td>Legged robots companions may one day assist humans with everyday tasks, but their possible impact on human gait is unknown. While previous studies have shown that humans adjust their gait when walking with other humans, it is uncertain whether walking with legged robots would yield similar results. In this study, we measured the gait of healthy participants (N = 14) while they walked alone and with a small quadruped robot. Spatiotemporal and stability gait parameters were calculated to determine whether the presence of the robot affected participants' gait. We found that walking with robots primarily affected measures in the walking direction. Participants walked slower and with altered anterior-posterior stability with the robot, but we did not find significant differences in the mediolateral direction in terms of step width or stability. However, we also observed that variability in the mediolateral distance between the robot and our participants also influenced participant gait behaviour. Our results demonstrate that robots do influence human gait even without physical contact and through seemingly innocuous actions such as walking near them.</td>
                <td>Legged locomotion, Atmospheric measurements, Particle measurements, Spatiotemporal phenomena, Quadrupedal robots, Task analysis, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341255&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341255&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Implications of Personality on Cognitive Workload, Affect, and Task Performance in Remote Robot Control</td>
                <td>C. Min</td>
                <td>2023</td>
                <td>This paper explores how the personality traits of robot operators can influence their task performance during remote control of robots. It is essential to explore the impact of personal dispositions on information processing, both directly and indirectly, when working with robots on specific tasks. To investigate this relationship, we utilize the open-access multi-modal dataset MOCAS to examine the robot operator's personality traits, affect, cognitive load, and task performance. Our objective is to confirm if personality traits have a total effect, including both direct and indirect effects, that could significantly impact the performance levels of operators. Specifically, we examine the relationship between personality traits such as extroversion, conscientiousness, and agreeableness, and task performance. We conduct a correlation analysis between cognitive load, self-ratings of workload and affect, and quantified individual personality traits along with their experimental scores. The findings show that personality traits do not have a total effect on task performance. A supplementary video can be accessed at: https://youtu.be/h3XUtVn7nzg.</td>
                <td>Correlation, Robot control, Information processing, Cognitive load, Task analysis, Remote control, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341633&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341633&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Optimizing Algorithms from Pairwise User Preferences</td>
                <td>L. Keselman, K. Shih, M. Hebert and A. Steinfeld</td>
                <td>2023</td>
                <td>Typical black-box optimization approaches in robotics focus on learning from metric scores. However, that is not always possible, as not all developers have ground truth available. Learning appropriate robot behavior in human-centric contexts often requires querying users, who typically cannot provide precise metric scores. Existing approaches leverage human feedback in an attempt to model an implicit reward function; however, this reward may be difficult or impossible to effectively capture. In this work, we introduce SortCMA to optimize algorithm parameter configurations in high dimensions based on pairwise user preferences. SortCMA efficiently and robustly leverages user input to find parameter sets without directly modeling a reward. We apply this method to tuning a commercial depth sensor without ground truth, and to robot social navigation, which involves highly complex preferences over robot behavior. We show that our method succeeds in optimizing for the user's goals and perform a user study to evaluate social navigation results.</td>
                <td>Measurement, Navigation, Closed box, Robot sensing systems, Robustness, Behavioral sciences, Tuning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342081&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342081&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Sensitivity Analysis of an Economic Value Metric for Quantifying the Success of Lower-Limb Exoskeletons and Their Assistance</td>
                <td>N. D. Rawal, R. L. Medrano, G. C. Thomas and E. J. Rouse</td>
                <td>2023</td>
                <td>Modern exoskeletons are typically developed to optimize for a single, physiological objective, the “gold standard” of which is a reduction of the wearer's metabolic rate. However, recent research suggests that these changes in metabolic rate are not yet perceivable on average. To address this gap, this study explores a novel economic value metric to quantify the value of exoskeleton assistance. The overarching goal of this work is the development of a perceptible metric that leverages the user experience to quantify exoskeleton success. We use the Vickrey second-price auction to obtain the monetary compensation needed for participants to continue walking for consecutive two-minute bouts. Comparing the participant's bidding trends when wearing and not wearing an exoskeleton captures the economic value of the experience, termed Marginal Value (MV). To reduce the logistical burden of the auction, we simulated human participants (robo-bidders) to compete alongside real participants. This work presents a sensitivity analysis to understand how the number and bidding behavior of the robo-bidders affects our economic value metric, MV. We found that MV was not significantly affected by the number of robo-bidders or their bidding behavior (i.e. effort rate). The bidding behavior of the human participants was affected by the robo-bidder effort rate, indicating that there is interplay in the bidding dynamics among the auction participants, but these changes do not significantly affect the marginal value. This study tentatively validates the current approach in generating our proposed metric for exoskeleton success, paving the way for economic value to be further explored as a holistic, personalized metric for the development of lower-limb exoskeletons.</td>
                <td>Measurement, Economics, Legged locomotion, Sensitivity analysis, Exoskeletons, Market research, User experience</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342452&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342452&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Arm Robot Task Planning for Fruit Harvesting Using Multi-Agent Reinforcement Learning</td>
                <td>T. Li, F. Xie, Q. Qiu and Q. Feng</td>
                <td>2023</td>
                <td>The emergence of harvesting robotics offers a promising solution to the issue of limited agricultural labor resources and the increasing demand for fruits. Despite notable advancements in the field of harvesting robotics, the utilization of such technology in orchards is still limited. The key challenge for harvesting robots is to improve the operational efficiency. Taking into account inner-arm conflicts, couplings of DoFs, and the dynamic tasks, we propose a task planning strategy for a harvesting robot with four arms in this paper. The proposed method employs a Markov game framework to formulate the four-arm robotic harvesting task, which avoids the computational complexity of solving an NP-hard scheduling problem. Furthermore, a multi-agent reinforcement learning (MARL) structure with a fully centralized collaboration protocol is used to train a MARL-based task planning network. Several simulations and orchard experiments are conducted to validate the effectiveness of the proposed method for a multi-arm harvesting robot in comparison with the existing method.</td>
                <td>Protocols, Processor scheduling, Reinforcement learning, Markov processes, Manipulators, Planning, Labor resources</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341822&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341822&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design, Modeling, and Control of a Low-Cost and Rapid Response Soft-Growing Manipulator for Orchard Operations</td>
                <td>R. Dorosh et al.</td>
                <td>2023</td>
                <td>Tree fruit growers around the world are facing labor shortages for critical operations, including harvest and pruning. There is a great interest in developing robotic solutions for these labor-intensive tasks, but current efforts have been prohibitively costly, slow, or require a reconfiguration of the orchard in order to function. In this paper, we introduce an alternative approach to robotics using a novel and low-cost soft-growing robotic platform. Our platform features the ability to extend up to 1.2 m linearly at a maximum speed of 0.27 m/s. The soft-growing robotic arm can operate with a terminal payload of up to 1.4 kg (4.4 N), more than sufficient for carrying an apple. This platform decouples linear and steering motions to simplify path planning and the controller design for targeting. We anticipate our platform being relatively simple to maintain compared to rigid robotic arms. Herein we also describe and experimentally verify the platform's kinematic model, including the prediction of the relationship between the steering angle and the angular positions of the three steering motors. Information from the model enables the position controller to guide the end effector to the targeted positions faster and with higher stability than without this information. Overall, our research show promise for using soft-growing robotic platforms in orchard operations.</td>
                <td>Vibrations, Robot kinematics, Kinematics, Bending, End effectors, Steady-state, Feedforward systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341507&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341507&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lightweight Real-Time Detection Model for Multi-Sheep Abnormal Behaviour Based on Yolov7-Tiny</td>
                <td>H. Zhang, Y. Ma, X. Wang, R. Mao and M. Wang</td>
                <td>2023</td>
                <td>Animal behaviour can reflect the health and physiological stage of the animal. Animal behaviour recognition is a vital part of automated farming systems. Although image-based deep learning algorithms can accurately identify animal behaviour, the lack of data on animal abnormal behaviour makes the practical deployment of models of limited significance. At the same time, the ageing of farm monitoring equipment is also a key factor hindering automated farming. This paper constructs a sheep abnormal behaviour dataset ABSB to address these issues and proposes a lightweight real-time multi-sheep abnormal behaviour detection model YOLOv7-Lrab based on the YOLOv7-tiny network. The abnormal behaviour dataset includes four normal behaviours: standing, lying, eating and drinking, and three abnormal behaviours: lameness, attack and death. In the proposed YOLOv7-Lrab model, the small target detection layer, Coordinate attention module, SPD-Conv and Mobileone module are added compared to YOLOv7-tiny. The experimental results show that with a 7:3 ratio of training data to test data, 96.5% recognition accuracy and 95.5% recall can be achieved, and the model size is only 4.5MB with fps of 156. The model is compressed to a minimum without loss of accuracy, providing a new idea for deploying deep learning model in practical application scenarios.</td>
                <td>Deep learning, Animals, Target recognition, Training data, Object detection, Real-time systems, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342186&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342186&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>NBV-SC: Next Best View Planning Based on Shape Completion for Fruit Mapping and Reconstruction</td>
                <td>R. Menon, T. Zaenker, N. Dengler and M. Bennewitz</td>
                <td>2023</td>
                <td>Active perception for fruit mapping and harvesting is a difficult task since occlusions occur frequently and the location as well as size of fruits change over time. State-of-the-art viewpoint planning approaches utilize computationally expensive ray casting operations to find good viewpoints aiming at maximizing information gain and covering the fruits in the scene. In this paper, we present a novel viewpoint planning approach that explicitly uses information about the predicted fruit shapes to compute targeted viewpoints that observe as yet unobserved parts of the fruits. Furthermore, we formulate the concept of viewpoint dissimilarity to reduce the sampling space for more efficient selection of useful, dissimilar viewpoints. Our simulation experiments with a UR5e arm equipped with an RGB-D sensor provide a quantitative demonstration of the efficacy of our iterative next best view planning method based on shape completion. In comparative experiments with a state-of-the-art viewpoint planner, we demonstrate improvement not only in the estimation of the fruit sizes, but also in their reconstruction, while significantly reducing the planning time. Finally, we show the viability of our approach for mapping sweet pepper plants with a real robotic system in a commercial glasshouse.</td>
                <td>Casting, Shape, Estimation, Active perception, Robot sensing systems, Planning, Iterative methods</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341855&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341855&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Knowledge Distillation for Efficient Panoptic Semantic Segmentation: Applied to Agriculture</td>
                <td>M. Li, M. Hasltead and C. McCool</td>
                <td>2023</td>
                <td>Panoptic segmentation provides both holistic and detailed image parsing information at both the pixel and the instance level. However, the computational burdens restrict its applications in real-time scenarios. A potential approach to learn more efficient models is to employ knowledge distillation. However, previous knowledge distillation schemes have focused mainly on classification with limited attention given to rearession-related tasks which is key for panoptic segmentation. In this paper, we establish a logits-based, a hints-based, and a combination-based scheme for panoptic knowledge distillation by using logits from the final layers and features in the middle layers. Then we explore different combinations of balancing weights for optimal solutions according to different network structures and datasets. To validate our proposed approach, various experiments on different datasets have been conducted and efficient networks with higher performance have been obtained. We show that knowledge distillation can be applied to develop accurate ResNet-34 networks improving their panoptic quality on things by an absolute amount of 4.1 points for sweet pepper (glasshouse environment) and 2.2 points for sugar beet (arable farming environment). These student ResNet-34 networks are able to run inference at faster than a framerate of 53Hz on computing infrastructure similar to PATHoBot (a glasshouse robot). To the best of our knowledge, this is the first work to propose knowledge distillation schemes for panoptic semantic segmentation.</td>
                <td>Knowledge engineering, Computer vision, Semantic segmentation, Computational modeling, Agriculture, Sugar industry, Real-time systems, Computer Vision for Agriculture Automation, Knowledge Distillation, Efficient Panoptic Segmentation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342527&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342527&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Estimating 4D Data Associations Towards Spatial-Temporal Mapping of Growing Plants for Agricultural Robots</td>
                <td>L. Lobefaro, M. V. R. Malladi, O. Vysotska, T. Guadagnino and C. Stachniss</td>
                <td>2023</td>
                <td>Our world is non-static, and robots should be able to track its changing geometry. For tracking changes, data asso-ciations between 3D points over time are key. In this paper, we investigate the problem of associating 3D points on plant organs from different mapping runs over time while the plants grow. We achieve a high spatial-temporal matching performance by combining 3D RGB-D SLAM, visual place recognition, and 2D/3D matching exploiting background knowledge. We showcase our approach in a real agricultural glasshouse used to grow sweet peppers, using RGB-D observations from a mobile robot traversing the environment. Our experiments suggest that with our approach, we can robustly make data associations in highly repetitive scenes and under changing geometries caused by plant growth. We see our approach as an important step towards spatial-temporal data association for robotic agriculture.</td>
                <td>Geometry, Point cloud compression, Visualization, Three-dimensional displays, Simultaneous localization and mapping, Shape, Plants (biology)</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342449&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342449&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Graph-Based View Motion Planning for Fruit Detection</td>
                <td>T. Zaenker, J. Rückin, R. Menon, M. Popović and M. Bennewitz</td>
                <td>2023</td>
                <td>Crop monitoring is crucial for maximizing agricultural productivity and efficiency. However, monitoring large and complex structures such as sweet pepper plants presents significant challenges, especially due to frequent occlusions of the fruits. Traditional next-best view planning can lead to unstructured and inefficient coverage of the crops. To address this, we propose a novel view motion planner that builds a graph network of viable view poses and trajectories between nearby poses, thereby considering robot motion constraints. The planner searches the graphs for view sequences with the highest accumulated information gain, allowing for efficient pepper plant monitoring while minimizing occlusions. The generated view poses aim at both sufficiently covering already detected and discovering new fruits. The graph and the corresponding best view pose sequence are computed with a limited horizon and are adaptively updated in fixed time intervals as the system gathers new information. We demonstrate the effectiveness of our approach through simulated and real-world experiments using a robotic arm equipped with an RGB-D camera and mounted on a trolley. As the experimental results show, our planner produces view pose sequences to systematically cover the crops and leads to increased fruit coverage when given a limited time in comparison to a state-of-the-art single next-best view planner.</td>
                <td>Robot motion, Productivity, Motion segmentation, Robot vision systems, Crops, Manipulators, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342532&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342532&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Panoptic Mapping with Fruit Completion and Pose Estimation for Horticultural Robots</td>
                <td>Y. Pan et al.</td>
                <td>2023</td>
                <td>Monitoring plants and fruits at high resolution play a key role in the future of agriculture. Accurate 3D information can pave the way to a diverse number of robotic applications in agriculture ranging from autonomous harvesting to precise yield estimation. Obtaining such 3D information is non-trivial as agricultural environments are often repetitive and cluttered, and one has to account for the partial observability of fruit and plants. In this paper, we address the problem of jointly estimating complete 3D shapes of fruit and their pose in a 3D multi-resolution map built by a mobile robot. To this end, we propose an online multi-resolution panoptic mapping system where regions of interest are represented with a higher resolution. We exploit data to learn a general fruit shape representation that we use at inference time together with an occlusion-aware differentiable rendering pipeline to complete partial fruit observations and estimate the 7 DoF pose of each fruit in the map. The experiments presented in this paper, evaluated both in the controlled environment and in a commercial greenhouse, show that our novel algorithm yields higher completion and pose estimation accuracy than existing methods, with an improvement of 41 % in completion accuracy and 52 % in pose estimation accuracy while keeping a low inference time of 0.6 s in average.</td>
                <td>Three-dimensional displays, Shape, Plants (biology), Pose estimation, Pipelines, Green products, Rendering (computer graphics)</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342067&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342067&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Vision-Based Vineyard Navigation Solution with Automatic Annotation</td>
                <td>Davidson, D. Combs and Y. Jiang</td>
                <td>2023</td>
                <td>Autonomous navigation is crucial for achieving the full automation of agricultural research and production management using agricultural robots. In this paper, we present a vision-based autonomous navigation approach for agriculture robots in trellised cropping systems, which stands out for its remarkable performance achieved entirely without human annotation. We propose a novel learning-based method that directly estimates the path traversibility heatmap from an RGB-D image and subsequently converts it into a preferred traversal path. One key advantage of our approach lies in its capability to predict the robot's preferred path directly, allowing us to obtain training labels without manual annotation. Specifically, we propose an automatic annotation pipeline that leverages the robot's path recorded during data collection. Furthermore, we develop a full navigation framework by integrating our path detection model with row switching modules, enabling the robot to smoothly transition between crop rows within the vineyard. We conduct extensive field trials in three different vineyards to validate the performance of our autonomous navigation framework. The results demonstrate that our approach provides a cost-effective, accurate, and robust solution for vineyard navigation.</td>
                <td>Heating systems, Training, Production management, Annotations, Navigation, Pipelines, Switches</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341261&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341261&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Detecting Olives with Synthetic or Real Data? Olive the Above</td>
                <td>Y. Karabatis, X. Lin, N. J. Sanket, M. G. Lagoudakis and Y. Aloimonos</td>
                <td>2023</td>
                <td>Modern robotics has enabled the advancement in yield estimation for precision agriculture. However, when applied to the olive industry, the high variation of olive colors and their similarity to the background leaf canopy presents a challenge. Labeling several thousands of very dense olive grove images for segmentation is a labor-intensive task. This paper presents a novel approach to detecting olives without the need to manually label data. In this work, we present the world's first olive detection dataset comprised of synthetic and real olive tree images. This is accomplished by generating an auto-labeled photorealistic 3D model of an olive tree. Its geometry is then simplified for lightweight rendering purposes. In addition, experiments are conducted with a mix of synthetically generated and real images, yielding an improvement of up to 66% compared to when only using a small sample of real data. When access to real, human-labeled data is limited, a combination of mostly synthetic data and a small amount of real data can enhance olive detection.</td>
                <td>Industries, Solid modeling, Three-dimensional displays, Service robots, Rendering (computer graphics), Yield estimation, Labeling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341765&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341765&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Grasp State Classification in Agricultural Manipulation</td>
                <td>B. Walt and G. Krishnan</td>
                <td>2023</td>
                <td>The agricultural setting poses additional challenges for robotic manipulation, as fruit is firmly attached to plants and the environment is cluttered and occluded. Therefore, accurate feedback about the grasp state is essential for effective harvesting. This study examines the different states involved in fruit picking by a robot, such as successful grasp, slip, and failed grasp, and develops a learning-based classifier using low-cost, computationally light sensors (IMU and IR reflectance). The Random Forest multi-class classifier accurately determines the current state and along with the sensors can operate in the occluded environment of a plant. The classifier was successfully trained and tested in the lab and showed 100% success at identifying slip and grasp failure and 80% success identifying successful picks on a real cherry tomato plant. By using this classifier, corrective actions can be planned based on the current state, thus leading to more efficient fruit harvesting.</td>
                <td>Reflectivity, Plants (biology), Robot sensing systems, Agriculture, Sensors, Research initiatives, Artificial intelligence</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341881&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341881&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Skirting Line Estimation Using Sparse to Dense Deformation</td>
                <td>D. P. Banuelos, R. Falque, T. Patten and A. Alempijevic</td>
                <td>2023</td>
                <td>Automating the process of fleece contaminant removal has the potential to drastically improve the quality of wool leaving the farm gate. Towards this goal, we present a method to automatically extract skirting lines, i.e., the separations between clean and contaminated wool of a fleece using RGB images. We propose a learning-based sparse-to-dense approach for estimating the non-rigid deformation of fleeces in order to estimate the skirting lines. Our method is bootstrapped from a set of sparse inlier feature correspon-dences, which are heavily filtered through a set of strict criteria. The inlier correspondences are then greedily expanded by adding correspondences from a denser set through a filtering process. This process is based on a learning approach that takes as inputs the pixel similarity and the consistency with their inlier neighbours. Each greedy iteration is initialised with a non-rigid deformation using as-rigid-as-possible as a prior to the filtering process. The proposed method outperforms both a rigid deformation baseline and optic flow deep learning approach, as evidenced by the quantitative evaluation of pixel location error in controlled experiments. To further prove its practicality, we demonstrate qualitative results comparing the predicted skirting line from various methods on images of skirted fleeces collected from several wool sheds.</td>
                <td>Optical filters, Deep learning, Deformation, Filtering, Estimation, Logic gates, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342451&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342451&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bird-View 3D Reconstruction for Crops with Repeated Textures</td>
                <td>G. Lu</td>
                <td>2023</td>
                <td>Large-scale in-situ 3D reconstruction of crop fields presents a challenging task, as the 3D crop structures play a crucial role in plant phenotyping and significantly influence crop growth and yield. While existing efforts focus on close-range plants, only a limited number of deep learning-based methods have been developed explicitly for large-scale 3D crop reconstruction, mainly due to the scarcity of large-scale crop sensing data. In this paper, we leverage unmanned aerial vehicles (UAVs) in agriculture and utilize a recently captured multi-view real-world snap beans crop dataset to develop an unsupervised structure-from-motion (SfM) framework. Our framework is designed specifically for reconstructing large-scale 3D crop structures. It addresses the challenge of inaccurate depth inference caused by excessively repeated patterns in the crop dataset, resulting in highly accurate 3D crop reconstruction for large-scale scenarios. Through experiments conducted on the crop dataset, we demonstrate the accuracy and robustness of our 3D crop reconstruction algorithm. The application of our proposed framework has the potential to advance research in agriculture, enabling better plant phenotyping and understanding of crop growth and yield.</td>
                <td>Three-dimensional displays, Neural networks, Crops, Robot sensing systems, Agriculture, Robustness, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341478&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341478&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Generalized Robot Dynamics Learning and Gen2Real Transfer</td>
                <td>D. Xing, Y. Yang, Z. Wang, J. Li and B. Xu</td>
                <td>2023</td>
                <td>Acquiring dynamics is critical for robot learning and is fundamental to planning and control. This paper concerns two fundamental questions: How can we learn a model that covers massive, diverse robot dynamics? Can we construct a model that lifts the data-collection pain and domain expertise required for building specific robot models? We learn the dynamics involved in a dataset containing a large number of serial articulated robots and propose a new concept, “Gen2Real”, to transfer simulated, generalized models to physical, specific robots. We generate a large-scale dataset by randomizing dynamics parameters, topology configurations, and model dimensions, which, in sequence, correspond to different properties, connections, and numbers of robot links. A structure modified from the generative pre-trained transformer is applied to approximate the dynamics of massive heterogeneous robots. In Gen2Real, we transfer the pre-trained model to a target robot using distillation, for the sake of real-time computation. The results demonstrate the superiority of the proposed method in terms of its accuracy in learning a tremendous amount of robot dynamics and its generality to transfer to different robots.</td>
                <td>Parallel robots, Parameter estimation, Pain, Computational modeling, Transformers, Robot learning, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342406&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342406&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dynamic Modeling and Analysis of Impact-Resilient MAVs Undergoing High-Speed and Large-Angle Collisions with the Environment</td>
                <td>Z. Liu and K. Karydis</td>
                <td>2023</td>
                <td>Micro Aerial Vehicles (MAVs) often face a high risk of collision during autonomous flight, particularly in cluttered and unstructured environments. To mitigate the collision impact on sensitive onboard devices, resilient MAVs with mechanical protective cages and reinforced frames are commonly used. However, compliant and impact-resilient MAVs offer a promising alternative by reducing the potential damage caused by impacts. In this study, we present novel findings on the impact-resilient capabilities of MAVs equipped with passive springs in their compliant arms. We analyze the effect of compliance through dynamic modeling and demonstrate that the inclusion of passive springs enhances impact resilience. The impact resilience is extensively tested to stabilize the MAV following wall collisions under high-speed and large-angle conditions. Additionally, we provide comprehensive comparisons with rigid MAVs to better determine the tradeoffs in flight by embedding compliance onto the robot's frame.</td>
                <td>Analytical models, Tracking, Motion capture, Collision avoidance, Vehicle dynamics, Springs, State estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341848&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341848&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Legged Locomotion Control of an Under-Actuated Eccentric Paddle Mechanism with Torso Stabilization</td>
                <td>Y. Zheng, L. Li and S. Ma</td>
                <td>2023</td>
                <td>Rescue robots require versatility and the capability to operate in various environments to carry out a diverse set of tasks effectively. The eccentric paddle (ePaddle) mechanism stands out for its high efficiency and adaptability. Generally, it is designed as a quadruped robot with a combined structure for fully-actuated control, this approach is often both inefficient and inflexible due to the requirement for repeated front-to-back paths. Unlike the fully-actuated controller that assume torso is fixed, this study proposes an under-actuated controller, consisting of a single ePaddle mechanism and a free torso for more efficient and flexible movement. Inspired by human gait, precision walking, and non-precision walking are introduced to discuss the stability of zero dynamics. Additionally, the stability condition is presented and demonstrated by numerical simulation. Since this control is based on robot dynamics, it has a high fault-tolerance and benefited from its dynamics attractor. The concept of under-actuated controller we proposed in this study is not only applicable to the ePaddle mechanism, but also to other under-actuated legged locomotion models.</td>
                <td>Legged locomotion, Torso, Numerical simulation, Stability analysis, Numerical models, Quadrupedal robots, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342488&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342488&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Improving Deep Dynamics Models for Autonomous Vehicles with Multimodal Latent Mapping of Surfaces</td>
                <td>J. Vertens, N. Dorka, T. Welschehold, M. Thompson and W. Burgard</td>
                <td>2023</td>
                <td>The safe deployment of autonomous vehicles relies on their ability to effectively react to environmental changes. This can require maneuvering on varying surfaces which is still a difficult problem, especially for slippery terrains. To address this issue we propose a new approach that learns a surface-aware dynamics model by conditioning it on a latent variable vector storing surface information about the current location. A latent mapper is trained to update these latent variables during inference from multiple modalities on every traversal of the corresponding locations and stores them in a map. By training everything end-to-end with the loss of the dynamics model, we enforce the latent mapper to learn an update rule for the latent map that is useful for the subsequent dynamics model. We implement and evaluate our approach on a real miniature electric car. The results show that the latent map is updated to allow more accurate predictions of the dynamics model compared to a model without this information. We further show that by using this model, the driving performance can be improved on varying and challenging surfaces.</td>
                <td>Training, Temperature sensors, Temperature, Roads, Vehicle safety, Tire pressure, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341771&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341771&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Data-Based MHE for Agile Quadrotor Flight</td>
                <td>W. Choo and E. Kayacan</td>
                <td>2023</td>
                <td>This paper develops a data-based moving horizon estimation (MHE) method for agile quadrotors. Accurate state estimation of the system is paramount for precise trajectory control for agile quadrotors; however, the high level of aerodynamic forces experienced by the quadrotors during high-speed flights make this task extremely challenging. These complex turbulent effects are difficult to model and the unmodelled dynamics introduce inaccuracies in the state estimation. In this work, we propose a method to model these aerodynamic effects using Gaussian Processes which we integrate into the MHE to achieve efficient and accurate state estimation with minimal computational burden. Through extensive simulation and experimental studies, this method has demonstrated significant improvement in state estimation performance displaying superior robustness to poor state measurements.</td>
                <td>Accelerometers, Training, Computational modeling, Current measurement, Predictive models, Aerodynamics, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342084&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342084&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Novel Approximation for the Spring Loaded Inverted Pendulum Model of Locomotion</td>
                <td>A. U. Kilic and D. J. Braun</td>
                <td>2023</td>
                <td>The Spring-Loaded Inverted Pendulum (SLIP) is one of the simplest models of robot locomotion. SLIP is commonly used to predict the center of mass motion and derive simple control laws for stable locomotion. However, the SLIP model is not integrable, which means that no closed-form relation can be derived to understand how the design and control parameters of the SLIP model affect stable locomotion. There exist a number of different analytical approximations to the SLIP model when considering small step lengths and symmetric steps. In this paper, we present a novel approximation to the SLIP model without relying on the small step length and the symmetric step assumption. The model was found to accurately predict the stability of the SLIP model for large and asymmetric steps and was used to design a controller to stabilize the SLIP model in a couple of steps.</td>
                <td>Analytical models, Predictive models, Stability analysis, Springs, Intelligent robots, Load modeling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341418&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341418&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Fast Steerable Soft Robot for Navigating a Pipe Network</td>
                <td>I. Stewart and P. Tallapragada</td>
                <td>2023</td>
                <td>Soft vibrational bristlebots are robots with deformable bristles on their outside that propel the robot through the stick slip motion of the bristle tips interacting with the ground when a vibration is induced in the robot. Experimental results and theoretical analysis of the dynamics for this style of robot have been investigated on flat surfaces. However, for the soft vibrational bristlebots traveling through pipes, controlled steering and the ability for the robot to navigate intersections has not been achieved or understood. This paper presents a two unit vibrational bristlebot with connecting helical coils. Through models and experiments it is demonstrated that a full range of controlled motion (traveling forwards, turning left and right and going up or down a vertical pipe) within a pipe network is possible for such soft vibrational bristlebots. Such fast steerable motion is all achieved merely by changing the polarity and spin speed of the vibrational motors.</td>
                <td>Vibrations, Navigation, Friction, Soft robotics, Propulsion, Turning, Nonlinear dynamical systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341708&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341708&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Orthogonal Collocation Method for Static and Dynamic Cosserat Rods</td>
                <td>F. Villard and E. Kerrien</td>
                <td>2023</td>
                <td>We propose an orthogonal collocation method (CM) for solving Cosserat rod Dirichlet-Neumann boundary value problems in static and dynamic modes. We interpolate the internal loading and collocate the strong form of the differential equations. The method uses Chebyshev polynomials in order to minimize Runge's phenomenon. The time derivatives are implicitly discretized using the backward differentiation formula BDF- $\alpha$ We compare our method with the shooting method (SM), multiple shooting method (MSM) and two isogeometric CM against three static and one dynamic applications. The results show that our CM is more stable than SM and faster than MSM.</td>
                <td>Friction, Loading, Force, Dynamics, Chebyshev approximation, Bending, Stability analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341631&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341631&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design, Characterization and Control of a Whole-body Grasping and Perching (WHOPPEr) Drone</td>
                <td>W. Tao, K. Patnaik, F. Chen, Y. Kumar and W. Zhang</td>
                <td>2023</td>
                <td>Flying robots can exploit perching abilities to position themselves on strategically-chosen locations and monitor the areas of interest from a critical vantage point. Moreover, they can significantly extend their battery life by turning off the propulsion systems when carrying out a surveillance mission. However, unknown disturbances arise from the physical interactions between the robot and the object, making it challenging to stabilize the robot during perching. In this paper, we present a Whole-body Grasping and Perching (WHOPPEr) Drone, which is capable of fast and robust perching by utilizing its entire body as the grasper in lieu of an add-on grasper. We first present the design concept, parameter selection and characterization of the novel whole-body grasping drone. Next, we analyze the grasping ability of the morphing chassis and present an aerodynamic analysis for the effect of motor thrust on the compliant arm. We finally demonstrate, via real-time experiments, the performance of WHOPPEr in autonomous perching and payload delivery tasks.</td>
                <td>Computational modeling, Surveillance, Grasping, Propulsion, Turning, Robustness, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341722&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341722&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Rotor Flywheel Robot: Land-air Amphibious Design and Control</td>
                <td>C. Wang, Y. Zhang, C. Li, W. Wang and Y. Li</td>
                <td>2023</td>
                <td>Most land-air amphibious UAVs feature a four-wheel design that limits their adaptability in narrow and uneven spaces. This study proposes the rotor flywheel as a new land-air design that integrates a one-wheel structure and eight-rotor wings for more flexible motion. The dynamics model is then conducted with the Kane method, finding two power-saving self-balance state while rolling. Its controller design highlights the multi-input decoupling approach utilizing feedback, along with the dynamic model-based component to enable efficient control of its intricate operations. Results of simulations and experimental tests have validated the stability and adaptability of the mode-switching and rolling of the robot in ground motion.</td>
                <td>Analytical models, Adaptation models, Dynamics, Rotors, Aerospace electronics, Stability analysis, Flywheels</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341371&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341371&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>System Identification and Control of Front-Steered Ackermann Vehicles Through Differentiable Physics</td>
                <td>B. M. Gonultas, P. Mukherjee, O. G. Poyrazoglu and V. Isler</td>
                <td>2023</td>
                <td>In this paper, we address the problem of system identification and control of a front-steered vehicle which abides by the Ackermann geometry constraints. This problem arises naturally for on-road and off-road vehicles that require reliable system identification and basic feedback controllers for various applications such as lane keeping and way-point navigation. Traditional system identification requires expensive equipment and is time consuming. In this work we explore the use of differentiable physics for system identification and controller design and make the following contributions: i) We develop a differentiable physics simulator (DPS) to provide a method for the system identification of front-steered class of vehicles whose system parameters are learned using a gradient-based method; ii) We provide results for our gradient-based method that exhibit better sample efficiency in comparison to other gradient-free methods; iii) We validate the learned system parameters by implementing a feedback controller to demonstrate stable lane keeping performance on a real front-steered vehicle, the F1TENTH; iv) Further, we provide results exhibiting comparable lane keeping behavior for system parameters learned using our gradient-based method with lane keeping behavior of the actual system parameters of the F1TENTH.</td>
                <td>Geometry, Navigation, Control systems, System identification, Behavioral sciences, Reliability, Adaptive control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342391&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342391&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hardware-in-the-Loop Simulation of Vehicle-Manipulator Systems for Physical Interaction Tasks</td>
                <td>H. Das, B. K. Sæbø, K. Y. Pettersen and C. Ott</td>
                <td>2023</td>
                <td>Hardware-in-the-loop simulation (HILS) allows a more realistic evaluation of control approaches than what is possible with pure software simulations, but without the actual complexity of the complete system. This is important for some complex systems such as orbital robots, where testing of the system is typically not possible after its launch, and an on-ground replica is used to validate the performance of such a system. In this article, an impedance-matching approach is presented to match the end-effector dynamics of a fixed-base robot manipulator with that of a target vehicle-manipulator system (VMS), while taking into account the redundant nullspace dynamics in a connected real-time simulation framework. This approach ensures that the forces and torques exerted by the system on the environment matches with that of the simulated system. The contact wrenches used in our approach are not obtained from numerical simulations, but rather from real physical interaction, which is one of the main advantages of our approach. The effectiveness of our method is validated by demonstrating various physical interaction tasks with the environment, using a suspended aerial manipulator as the target system.</td>
                <td>Hardware-in-the-loop simulation, Dynamics, Numerical simulation, Software, Real-time systems, Orbits, Vehicle dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342250&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342250&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Modeling and Workspace Characterization of Continuously Compliant Robotic Legs</td>
                <td>R. Bendfeld and C. D. Remy</td>
                <td>2023</td>
                <td>This work introduces a new design paradigm for robotic legs. Our concept extends upon classical series elastic actuation and directly integrates the series compliance into the structure of the leg. This integration reduces the mechanical design complexity and can potentially reduce the overall weight of the leg. In this paper, we introduce a prototype leg with a continuously compliant shank and derive and analyze a non-linear beam model that is used to predict its contact forces. The model is validated in two static experiments: one on the isolated shank and one on the full leg. It shows good agreement with measurements. In our validation, we also studied the influence of the model discretization, showing that about 10 nodes are sufficent. Furthermore, we introduce the concept of a force workspace: the range of forces that can be created by controlling the joint angles of the leg. Due to the coupling of nonlinear deformations and nonlinearities in the kinematics, this workspace is non-trivial. In particular, we demonstrate that it is bounded by a force-singularity in which the force/joint-angle relationship cannot be inverted. The results presented in this work can be applied in the development of state-estimators, set-point filters and controllers, and they can inform the future design of suitable geometries of compliant elements.</td>
                <td>Legged locomotion, Geometry, Couplings, Deformation, Force, Prototypes, Kinematics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342327&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342327&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficiency Estimation and Optimization of Multistage Compound Planetary Gearboxes and Application to the Design of the Active Skin Propulsion of EELS</td>
                <td>N. Georgiev</td>
                <td>2023</td>
                <td>This paper outlines a novel versatile geometric method for the estimation of the efficiency of multistage compound planetary gearboxes. The approach is based on a virtual pitch point modeling that allows for accurate representation of the gear interaction with forces applied at the pitch point. When this modeling is applied to all gears in a multistage compound planetary gearbox, the gearbox efficiency may be estimated directly from the free body diagram of the compound planets in a simple and easy to implement fashion. The gear design parameters that determine the gearing efficiency are identified and a consistent strategy for maximizing the efficiency of such gearboxes is outlined. Finally, the introduced methods are applied to the design of a three-stage compound planetary gearbox used in a novel snake-like robot.</td>
                <td>Gears, Planets, Estimation, Propulsion, Fasteners, Skin, Compounds</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341515&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341515&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Impact-Friendly Object Catching at Non-Zero Velocity Based on Combined Optimization and Learning</td>
                <td>J. Zhao, G. J. G. Lahr, F. Tassi, A. Santopaolo, E. De Momi and A. Ajoudani</td>
                <td>2023</td>
                <td>This paper proposes a combined optimization and learning method for impact-friendly, non-prehensile catching of objects at non-zero velocity. Through a constrained Quadratic Programming problem, the method generates optimal trajectories up to the contact point between the robot and the object to minimize their relative velocity and reduce the impact forces. Next, the generated trajectories are updated by Kernelized Movement Primitives, which are based on human catching demonstrations to ensure a smooth transition around the catching point. In addition, the learned human variable stiffness (HVS) is sent to the robot's Cartesian impedance controller to absorb the post-impact forces and stabilize the catching position. Three experiments are conducted to compare our method with and without HVS against a fixed-position impedance controller (FP-IC). The results showed that the proposed methods outperform the FP-IC while adding HVS yields better results for absorbing the post-impact forces.</td>
                <td>Measurement, Learning systems, Torque, Redundancy, Trajectory, Impedance, Quadratic programming</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341600&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341600&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Online Estimation of 2D Human Arm Stiffness for Peg-in-Hole Tasks with Variable Impedance Control</td>
                <td>H. Wu, H. Yang and Y. Li</td>
                <td>2023</td>
                <td>This paper proposes an online estimation model for 2D arm stiffness in humans. The proposed model is based on recent physiological findings which suggest that: (1) joint stiffness is linearly related to the magnitude of joint torque and increases to compensate for environmental disturbances; and (2) the endpoint stiffness of the arm is proportional to grasp force. To validate the proposed model, perturbation experiments were conducted under different grasp forces. The model parameters were identified and the accuracy of the model was assessed. The results showed that the proposed model has advantages over previous models for estimating human arm endpoint stiffness, in the sense of simplicity and robustness. The proposed model was also used to design a variable stiffness controller for peg-in-hole tasks, demonstrating the potential of the model for human-robot collaboration.</td>
                <td>Solid modeling, Torque, Three-dimensional displays, Human-robot interaction, Estimation, Robot sensing systems, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341960&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341960&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Workspace Force/Acceleration Disturbance Observer for Precise and Safe Motion Control</td>
                <td>W. Han, W. Yun and S. Oh</td>
                <td>2023</td>
                <td>The use of impedance control has become widespread in applications requiring simultaneous position tracking and compliance in contact. However, disturbances such as friction and model uncertainties can adversely affect the performance of impedance-based motion control. The disturbance observer (DOB) has been proposed to address this issue, which is a widely-utilized robust controller that eliminates observed disturbances with the nominal model. However, current DOB applications fail to consider the aspect of interactive force control properly. This study proposes a novel Workspace Force/Acceleration Disturbance Observer (WFADOB) controller, which utilizes both interaction force and acceleration to design a disturbance observer loop, enabling precise motion tracking even with low-impedance gain settings. Additionally, the proposed controller offers fine impedance rendering performance, offering safe contact while maintaining low impedance. This paper discusses the problem of motion tracking performance due to friction and the interaction force that arises during contact. The proposed controller is theoretically analyzed and experimentally verified, demonstrating its performance compared to conventional methods.</td>
                <td>Uncertainty, Tracking, Friction, Force, Stability criteria, Disturbance observers, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341933&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341933&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Orientation Control with Variable Stiffness Dynamical Systems</td>
                <td>Dakka and D. Lee</td>
                <td>2023</td>
                <td>Recently, several approaches have attempted to combine motion generation and control in one loop to equip robots with reactive behaviors, that cannot be achieved with traditional time-indexed tracking controllers. These approaches however mainly focused on positions, neglecting the orientation part which can be crucial to many tasks e.g. screwing. In this work, we propose a control algorithm that adapts the robot's rotational motion and impedance in a closed-loop manner. Given a first-order Dynamical System representing an orientation motion plan and a desired rotational stiffness profile, our approach enables the robot to follow the reference motion with an interactive behavior specified by the desired stiffness, while always being aware of the current orientation, represented as a Unit Quaternion (UQ). We rely on the Lie algebra to formulate our algorithm, since unlike positions, UQ feature constraints that should be respected in the devised controller. We validate our proposed approach in multiple robot experiments, showcasing the ability of our controller to follow complex orientation profiles, react safely to perturbations, and fulfill physical interaction tasks.</td>
                <td>Tracking loops, Tracking, Heuristic algorithms, Perturbation methods, Aerospace electronics, Behavioral sciences, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342531&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342531&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>UMIRobot: An Open-{Software, Hardware} Low-Cost Robotic Manipulator for Education</td>
                <td>C. Lin and J. Zhao</td>
                <td>2023</td>
                <td>Robot teleoperation has been studied for the past 70 years and is relevant in many contexts, such as in the handling of hazardous materials and telesurgery. The COVID19 pandemic has rekindled interest in this topic, but the existing robotic education kits fall short of being suitable for teleoperated robotic manipulator learning. In addition, the global restrictions of motion motivated large investments in online/hybrid education. In this work, a newly developed robotics education kit and its ecosystem are presented which is used as the backbone of an online/hybrid course in teleoperated robots. The students are divided into teams. Each team designs, fabricates (3D printing and assembling), and implements a control strategy for a master device and gripper. Coupling those with the UMIRobot, provided as a kit, the students compete in a teleoperation challenge. The kit is low cost (<100USD), which allows higher-learning institutions to provide one kit per student and they can learn in a risk-free environment. As of now, 73 such kits have been assembled and sent to course participants in eight countries. As major success stories, we show an example of gripper and master designed for the proposed course. In addition, we show a teleoperated task between Japan and Bangladesh executed by course participants. Design files, videos, source code, and more information are available at https://mmmarinho.github.io/UMIRobot/</td>
                <td>Temperature sensors, Three-dimensional displays, Education, Ecosystems, Manipulators, Three-dimensional printing, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341347&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341347&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>I2mpedance - A Passivity Based Integrative Impedance Controller for Precise and Compliant Manipulation and Interaction</td>
                <td>F. Voigt, A. Naceri and S. Haddadin</td>
                <td>2023</td>
                <td>Sophisticated manipulation requires both compliance and accuracy. While tactile robots excel at being compliant, their accuracy is often inadequate for complex manipulation. Contact-rich assembly tasks, such as the insertion and manipulation of objects with small tolerances pose an enormous challenge. Complex, highly integrated assemblies, especially in high-tech areas such as robotics, sensors, or machines, still require human personnel, as they cannot be automated in a satisfactory way. To automate such tasks, especially in the context of labor shortage and Industry 4.0, these limitations must be overcome. Robots need to guarantee force limits for active environments in order to avoid harm or damage. Therefore, in this work, we adapt standard Cartesian impedance control by introducing an integration term for position accuracy and wrench limits for safe compliant interaction with unknown and active environments. We combine this with a virtual energy tank to guarantee the general passivity of the controller. Our controller is benchmarked against standard impedance control for absolute positioning accuracy across the robot workspace. Furthermore, we show its applicability to an industrial insertion task. We demonstrate absolute positioning accuracy (residual error| Ax| < 4e – 4) comparable to rigid robots while preserving compliant behavior.</td>
                <td>Service robots, Force, Robot sensing systems, Sensors, Fourth Industrial Revolution, Impedance, Personnel</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342370&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342370&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real is Better than Perfect: Sim-to-Real Robotic System in Secondary School Education</td>
                <td>J. Gao, H. Guo, Z. Cao, P. Huang and G. Zhou</td>
                <td>2023</td>
                <td>Simulation systems of robots can facilitate the prediction, development, and debugging of robotic systems. However, they seldom applied in robotics education for primary and secondary school students. In this paper, we present a sim-to-real robotic system that enables students to optimize their algorithms in a simulated environment and validate them in a remote physical laboratory with data logs and remote cameras. Moreover, the system employs an automated submit-test-reset subsystem that minimizes the need for human intervention and provides 24/7 testing support. Experimental data from a trial with 28 students in remote areas show that the sim-to-real robotic experimental environment has comparable learning outcomes to a pure real robot environment and is significantly better than a pure simulation environment. Given the results, we validate that our system can substantially reduce the costs of teaching equipment and space while maintaining high-quality robotics education.</td>
                <td>Learning systems, Codes, Education, Robot vision systems, Laboratories, Cameras, Prediction algorithms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341903&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341903&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Soft, Multi-Layer, Kirigami Inspired Robotic Gripper with a Compact, Compression-Based Actuation System</td>
                <td>J. Buzzatto et al.</td>
                <td>2023</td>
                <td>Over the last decade, a plethora of soft robotic devices have been proposed for the execution of complex grasping and dexterous manipulation tasks. Tasks requiring such increased dexterity are typically executed using fully-actuated, rigid end-effectors equipped with sophisticated sensing and controlled with complex control laws. The new class of soft robotic devices offers an alternative to the traditional end-effectors and facilitates the development of robotic grasping and manipulation solutions that are lightweight, safe to interact with, affordable, and easy to use and control. Within the class of soft robotic grippers and hands, promising recent developments were made in ultra-affordable, even disposable mechanisms based on origami and kirigami structures. This paper proposes a new kirigami-inspired robotic gripper geometry employing compression-based actuation. The compression actuation fundamentally differentiates this new design class from previous kirigami grippers, resulting in more compact robotic grippers with superior grasping capabilities. In particular, we investigate how the shapes of the internal cuts of the kirigami geometries can affect the gripper performance in terms of force exertion and grasping capabilities. A series of experiments are conducted to understand better the working principles behind this new type of kirigami grippers and experimentally validate their efficacy in the execution of complex, everyday life tasks. Further demonstrations of the gripper's capabilities include the pick-and-placing of human hair, egg yolk, and even liquids.</td>
                <td>Hair, Geometry, Liquids, Shape, Force, Grasping, Soft robotics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341893&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341893&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Visuotactile Sensor Enabled Pneumatic Device Towards Compliant Oropharyngeal Swab Sampling</td>
                <td>S. Li et al.</td>
                <td>2023</td>
                <td>Manual oropharyngeal (OP) swab sampling is an intensive and risky task. In this article, a novel OP swab sampling device of low cost and high compliance is designed by combining the visuotactile sensor and the pneumatic actuator-based gripper. Here, a concave visuotactile sensor called CoTac is first proposed to address the problems of high cost and poor reliability of traditional multi-axis force sensors. Besides, by imitating the doctor's fingers, a soft pneumatic actuator with a rigid skeleton structure is designed, which is demonstrated to be reliable and safe via finite element modeling and experiments. Furthermore, we propose a sampling method that adopts a compliant control algorithm based on the adaptive virtual force to enhance the safety and compliance of the swab sampling process. The effectiveness of the device has been verified through sampling experiments as well as in vivo tests, indicating great application potential. The cost of the device is around 30 US dollars and the total weight of the functional part is less than 0.1 kg, allowing the device to be rapidly deployed on various robotic arms.</td>
                <td>In vivo, Costs, Force, Tactile sensors, Sampling methods, Skeleton, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342266&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342266&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Mathematical Modelling and Experimental Validation of an Articulated Vacuum Gripper</td>
                <td>M. Maggi, G. Reina and G. Mantriota</td>
                <td>2023</td>
                <td>This paper presents the underactuated vacuum gripper named Polypus. What is unique in Polypus is that it combines under-actuation and vacuum grasping to apply both power and unilateral grasp to objects of various shape and geometry. In addition, the gripper features modularity, i.e., single phalanges can be added or removed based on the application. The high flexibility also comes with a cost-effective (less than 100€ and simple design that can be manufactured with a consumer-grade 3D printer using FDM technology. While the analytical model has been introduced by the authors in previous research, here its experimental validation is described using a physical prototype that shows that the theoretical assumptions are reasonable. Experimental results also suggest a small variation in the original theoretical model.</td>
                <td>Analytical models, Three-dimensional displays, Shape, Force, Prototypes, Programmable logic arrays, Predictive models, Underactuated grippers, vacuum grasping, robotic gripper, prototyping, experimental validation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342100&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342100&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lip-Inspired Passive Jamming Gripper with Teeth Structure</td>
                <td>J. Hong, K. Shin, D. C. Mathur, S. Yamsani, J. Yim and J. Kim</td>
                <td>2023</td>
                <td>In this paper, we propose a lip-inspired passive jamming gripper by mimicking teeth structures from a dog's oral structure. Animal lips are hydrostatic structures. To mimic the features, which are usually soft but rigid when contacted, we used the passive particle jamming effect. To increase the adaptability of our previous gripper to grasp various shaped objects, we focused on the dogs' oral structure and holding behaviors. Dogs use spaces inside their mouths to hold sticks firmly when moving. The grasping force of the upgraded gripper was improved by generating teeth structures with the mimicked lip structures. Experiments were conducted to demonstrate the grasping ability of the gripper with the teeth structures by comparing it to other types of grippers with cylindrical and cuboid objects of various dimensions. We also showed that the proposed gripper could hold daily kitchen objects better than the previous version gripper by using closing lip-pouches.</td>
                <td>Shape, Atmospheric measurements, Lips, Force, Grasping, Teeth, Dogs</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342175&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342175&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Two-Fingered Hand with Gear-Type Synchronization Mechanism with Magnet for Improved Small and Offset Objects Grasping: F2 Hand</td>
                <td>I. Maeda</td>
                <td>2023</td>
                <td>A problem that plagues robotic grasping is the misalignment of the object and gripper due to difficulties in precise localization, actuation, etc. Under-actuated robotic hands with compliant mechanisms are used to adapt and compensate for these inaccuracies. However, these mechanisms come at the cost of controllability and coordination. For instance, adaptive functions that let the fingers of a two-fingered gripper adapt independently may affect the coordination necessary for grasping small objects. In this work, we develop a two-fingered robotic hand capable of grasping objects that are offset from the gripper's center, while still having the requisite coordination for grasping small objects via a novel gear-type synchronization mechanism with a magnet. This gear synchronization mechanism allows the adaptive finger's tips to be aligned enabling it to grasp objects as small as toothpicks and washers. The magnetic component allows this coordination to automatically turn off when needed, allowing for the grasping of objects that are offset/misaligned from the gripper. This equips the hand with the capability of grasping light, fragile objects (strawberries, creampuffs, etc.) to heavy frying pan lids, all while maintaining their position and posture which is vital in numerous applications that require precise positioning or careful manipulation.</td>
                <td>Location awareness, Manufacturing processes, Costs, Gears, Robot kinematics, Grasping, Synchronization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342060&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342060&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Inflatable Eversible Finger Pad for Variable-Stiffness Grasping with Parallel-Jaw Grippers</td>
                <td>R. Deimel and A. Kugi</td>
                <td>2023</td>
                <td>We present an inflatable finger pad that allows regular parallel-jaw grippers to vary their grasp stiffness while maintaining a contact force and contact to non-planar surfaces. An eversible radial bellows structure made of silicone rubber allows the pad to extend to four times its original height and to retract into a rigid pod when not needed. The bellows act as passive universal joints when everted, enabling aerial contact with surfaces inclined by up to 45 degree. The bellow geometry is intentionally nonlinear but avoids bistable configurations to facilitate control. We find that the nonlinear stiffness behavior allows a pair of opposing pads to increase the compliance of a grasp twenty fold, resulting in a total of two orders of magnitude difference between the most stiff and most compliant configuration. Crucially, high compliance can be achieved while exerting a contact force between 0.7 N and 5 N, allowing for compliant but firm grasps. Pads are manufactured using printed molds and sacrificial-mold casting.</td>
                <td>Geometry, Surface impedance, Bellows, Fingers, Force, Grasping, Rubber</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341676&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341676&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>D-PALI: A Low-Cost Open Source Robotic Gripper Platform for Planar In-Hand-Manipulation</td>
                <td>A. Patra and A. J. Spiers</td>
                <td>2023</td>
                <td>Robot grippers are widely used in industrial automation for pick-and-place tasks on a variety of objects. Whilst the majority of commercial grippers are capable of establishing stable grasps, few can perform in-hand-manipulation (IHM). IHM is has the potential to increase robotic motion efficiency, yet most IHM-capable manipulation platforms are anthropomorphic in nature and cost over $10,000, posing a barrier to entry for many. In this work we propose a IHM capable gripper platform that is open-source and may be assembled for £150 ($162) and access to a 3D printer. The gripper consists of two fully actuated 2DOF fingers, each of which is based on a five-bar linkage mechanism with one link extended. The fingers are modular, allowing the gripper to be easily expanded into 3+ finger configurations via simple modification of the central mount. We define the inverse kinematics and effective workspace of the gripper (via the use of Freudenstein equations), providing guidance for translation and rotation of gripped objects. We demonstrate the gripper's ability to manipulate a 1-inch cube's pose within a ±5% error margin and rotate various other YCB objects via open-loop position control.</td>
                <td>Robot motion, Three-dimensional displays, Service robots, Fingers, Position control, Kinematics, Printers, Grippers and Other End-Effectors, In-Hand Manipulation, Dexterous Manipulation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341860&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341860&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Adaptive and Fail-Safe Magnetic Gripper with Charging Function for Drones on Power Lines</td>
                <td>V. D. Hoang, A. Kramberger and E. Ebeid</td>
                <td>2023</td>
                <td>Drone grasping on power lines for recharging is challenging since it requires the gripper to be lightweight, carried by a drone, and efficient for a firm grasp. A deep understanding of the power line nature and its magnetic characteristic helps ease such challenges and bring new knowledge to gripper design. In this work, a novel adaptive, lightweight, and fail-safe magnetic gripper with a recharging feature is presented. The gripper exploits the radiated magnetic field of the lines for charging and holding the drone and can easily detach from the line. The gripper design has been validated in the lab and on a quadcopter with a real power line.</td>
                <td>Power cables, Grasping, Switches, Inspection, Magnetic fields, Grippers, Magnetic switching</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341434&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341434&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>InstaGrasp: An Entirely 3D Printed Adaptive Gripper with TPU Soft Elements and Minimal Assembly Time</td>
                <td>X. Zhou and A. J. Spiers</td>
                <td>2023</td>
                <td>Fabricating existing and popular open-source adaptive robotic grippers commonly involves using multiple professional machines, purchasing a wide range of parts, and tedious, time-consuming assembly processes. This poses a significant barrier to entry for some robotics researchers and drives others to opt for expensive commercial alternatives. To provide both parties with an easier and cheaper (under £100) solution, we propose a novel adaptive gripper design where every component (with the exception of actuators and the screws that come packaged with them) can be fabricated on a hobby-grade 3D printer, via a combination of inexpensive and readily available PLA and TPU filaments. This approach means that the gripper's tendons, flexure joints and finger pads are now printed, as a replacement for traditional string-tendons and molded urethane flexures / pads. A push-fit systems results in an assembly time of under 10 minutes. The gripper design is also highly modular and requires only a few minutes to replace any part, leading to extremely user-friendly maintenance and part modifications. An extensive stress test has shown a level of durability more than suitable for research, whilst grasping experiments (with perturbations) using items from the YCB object set has also proven its mechanical adaptability to be highly satisfactory.</td>
                <td>Three-dimensional displays, Pulleys, Perturbation methods, Grasping, Programmable logic arrays, Printers, Grippers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341385&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341385&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Bio-Inspired Robotic Finger: Mechanics and Control</td>
                <td>W. Chungsangsatiporn and R. Chancharoen</td>
                <td>2023</td>
                <td>A robotic finger is successfully designed, fabricated, analyzed, and examined. The finger consists of bones, joint ligaments, and an extensor hood. Driven by two tendons, it is two degrees on the freedom finger. Although the behavior of this design is not uniform, it provides useful dexterity, sensitivity, and versatility. The artificial bone is lightweight and compact. The actuation is backdrivable in good visibility. Tendon muscles effectively actuate the finger in a grasp direction. This exceptional behavior has the potential to be suitable for a flexible task and may lead to a next-generation gripper and prosthetic hand.</td>
                <td>Fabrication, Sensitivity, Muscles, Bones, Robot sensing systems, Behavioral sciences, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342443&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342443&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Autonomous Robotic Drilling System for Mice Cranial Window Creation: An Evaluation with an Egg Model</td>
                <td>E. Zhao, M. M. Marinho and K. Harada</td>
                <td>2023</td>
                <td>Robotic assistance for experimental manipulation in the life sciences is expected to enable precise manipulation of valuable samples, regardless of the skill of the scientist. Experimental specimens in the life sciences are subject to individual variability and deformation, and therefore require autonomous robotic control. As an example, we are studying the installation of a cranial window in a mouse. This operation requires the removal of the skull, which is approximately 300 um thick, to cut it into a circular shape 8 mm in diameter, but the shape of the mouse skull varies depending on the strain of mouse, sex and week of age. The thickness of the skull is not uniform, with some areas being thin and others thicker. It is also difficult to ensure that the skulls of the mice are kept in the same position for each operation. It is not realistically possible to measure all these features and pre-program a robotic trajectory for individual mice. The paper therefore proposes an autonomous robotic drilling method. The proposed method consists of drilling trajectory planning and image-based task completion level recognition. The trajectory planning adjusts the z-position of the drill according to the task completion level at each discrete point, and forms the 3D drilling path via constrained cubic spline interpolation while avoiding overshoot. The task completion level recognition uses a DSSD-inspired deep learning model to estimate the task completion level of each discrete point. Since an egg has similar characteristics to a mouse skull in terms of shape, thickness and mechanical properties, removing the egg shell without damaging the membrane underneath was chosen as the simulation task. The proposed method was evaluated using a 6-DOF robotic arm holding a drill and achieved a success rate of 80% out of 20 trials.</td>
                <td>Drilling, Trajectory planning, Shape, Skull, Mice, Real-time systems, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341693&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341693&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards an Accurate Augmented-Reality-Assisted Orthopedic Surgical Robotic System Using Bidirectional Generalized Point Set Registration</td>
                <td>H. Meng</td>
                <td>2023</td>
                <td>This paper presents a novel augmented reality (AR)-assisted orthopedic surgical robotic system based on Head-Mounted Display (HMD) devices. The proposed system can overlay the preoperative plans over the patient's anatomy and provide useful guidance for surgeons during interventions, with integrated calibration and registration components. A novel bi-directional generalised point set registration algorithm that utilises robust features is developed to accurately align the pre-operative CT and intra-operative patient spaces, which has been demonstrated to outperform existing registration methods. The efficacy of the system is both qualitatively and quantitatively assessed with an in vitro study simulating a total knee arthroplasty (TKA) procedure. The experimental results showed that 1) the system can successfully align the preoperative and intraoperative spaces, with the mean target registration error (TRE) being 2.7771 mm; 2) the models can be properly overlaid to the physical scenarios with the mean AR visualization accuracy being 6.9726 mm.</td>
                <td>Visualization, Medical robotics, Simultaneous localization and mapping, Robot kinematics, Robot vision systems, Surgery, Resists</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341401&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341401&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Generalizing Surgical Instruments Segmentation to Unseen Domains with One-to-Many Synthesis</td>
                <td>A. Wang, M. Islam, M. Xu and H. Ren</td>
                <td>2023</td>
                <td>Despite their impressive performance in various surgical scene understanding tasks, deep learning-based methods are frequently hindered from deploying to real-world surgical applications for various causes. Particularly, data collection, annotation, and domain shift in-between sites and patients are the most common obstacles. In this work, we mitigate data-related issues by efficiently leveraging minimal source images to generate synthetic surgical instrument segmentation datasets and achieve outstanding generalization performance on unseen real domains. Specifically, in our framework, only one background tissue image and at most three images of each foreground instrument are taken as the seed images. These source images are extensively transformed and employed to build up the foreground and background image pools, from which randomly sampled tissue and instrument images are composed with multiple blending techniques to generate new surgical scene images. Besides, we introduce hybrid training-time augmentations to diversify the training data further. Extensive evaluation on three real-world datasets, i.e., Endo2017, Endo2018, and RoboTool, demonstrates that our one-to-many synthetic surgical instruments datasets generation and segmentation framework can achieve encouraging performance compared with training with real data. Notably, on the RoboTool dataset, where a more significant domain gap exists, our framework shows its superiority of generalization by a considerable margin. We expect that our inspiring results will attract research attention to improving model generalization with data synthesizing.</td>
                <td>Training, Learning systems, Image segmentation, Annotations, Instruments, Training data, Data collection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341609&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341609&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Reducing Workload During Brain Surgery with Robot-Assisted Autonomous Exoscope</td>
                <td>E. Iovene et al.</td>
                <td>2023</td>
                <td>In this paper, a position-based visual-servoing control approach is introduced for a robotic camera holder to improve ergonomics and reduce mental stress during brain surgery. The visual tracking system controls and moves the robotic camera holder by following a selected surgical instrument without the need for artificial markers. The system was validated using a 7 Degree-of-Freedoms (DoFs) redundant robotic manipulator with an eye-in-hand stereo camera configuration and compared with conventional control methods using NASA TLX survey and four objective metrics, including execution time, time out of field of view (FoV), target score, and path length. Experimental results demonstrate that the proposed system can reduce the surgeon's workload during brain surgery-related task execution, improve ergonomics and achieve higher performance than traditional control methods.</td>
                <td>Surveys, Visualization, Tracking, Instruments, Robot vision systems, Cameras, Control systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341799&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341799&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Augmented Reality Navigation in Robot-Assisted Surgery with a Teleoperated Robotic Endoscope</td>
                <td>V. Penza et al.</td>
                <td>2023</td>
                <td>Augmented reality (AR) is considered one of the most promising solutions for safer procedures in several surgical specialities. Fusing patient-specific pre-operative information, typically 3D models extracted from CT scans or MRI, with real-time surgical images allows the surgeon to have detailed information on the anatomical structure of the surgical target intra-operatively. The coupling of AR and Robotics represents the next step towards introducing awareness into the surgical room, thus enhancing the surgeon's perceptual, cognitive and manipulative capabilities. This paper presents a novel integrated system for real-time AR navigation in robotic minimally invasive surgery (RMIS), composed of a robotic endoscopic camera, a robotic teleoperation implementing a software-based Remote Center of Motion (RCM), and an AR navigation software based on an initial manual registration of virtual 3D models with the real anatomy. The integrated system, as well as the individual modules, were evaluated in simulated surgical-like setups for accuracy and repeatability. The proposed system can perform high-precision tasks (position accuracy around $1 mm$ and AR error lower than 7%), showing potential for application in different surgical procedures and setting the basis for autonomous robotic surgery operations.</td>
                <td>Solid modeling, Three-dimensional displays, Minimally invasive surgery, Navigation, Robot vision systems, Real-time systems, Software</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342282&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342282&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>See What a Strabismus Patient Sees Using Eye Robots</td>
                <td>Y. Huang, Q. Wei, J. L. Demer and N. Yao</td>
                <td>2023</td>
                <td>Ocular mobility disorders such as strabismus af-fect millions of people. Patients' descriptions of their symptoms, such as what they see and how their vision has changed, are important for ophthalmologists to diagnose, monitor pro-gression, and evaluate treatment effectiveness. However, such verbal depiction may be vague and Subjective. A data-driven simulator that visualizes abnormal vision experienced by a strabismic patient can be helpful to objectively illustrate each individual's vision condition and thus to better understand and manage strabismus. To fulfill this technical void, this paper presents the first vision visualization robot that uses human eye movement data to simulate strabismic vision. We developed a robotic binocular eye platform, which is capable of displaying simulated visual scenes using its onboard cameras. Based on the hypothesis that a human's binocular vision fusion process can be mimicked as a homography transformation from one view to another view, we developed a pipeline to estimate the time-varying homography matrix, and generate the fused view of a human's binocular vision. The effectiveness of the proposed method is demonstrated through experiments with eye movement data from both healthy individuals and strabismic patients.</td>
                <td>Visualization, Robot vision systems, Pipelines, Data visualization, Cameras, Monitoring, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342099&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342099&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Visual-Kinematics Graph Learning for Procedure-Agnostic Instrument Tip Segmentation in Robotic Surgeries</td>
                <td>J. Liu, Y. Long, K. Chen, C. H. Leung, Z. Wang and Q. Dou</td>
                <td>2023</td>
                <td>Accurate segmentation of surgical instrument tip is an important task for enabling downstream applications in robotic surgery, such as surgical skill assessment, tool-tissue interaction and deformation modeling, as well as surgical autonomy. However, this task is very challenging due to the small sizes of surgical instrument tips, and significant variance of surgical scenes across different procedures. Although much effort has been made on visual-based methods, existing segmentation models still suffer from low robustness thus not usable in practice. Fortunately, kinematics data from the robotic system can provide reliable prior for instrument location, which is consistent regardless of different surgery types. To make use of such multi-modal information, we propose a novel visual-kinematics graph learning framework to accurately segment the instrument tip given various surgical procedures. Specifically, a graph learning framework is proposed to encode relational features of instrument parts from both image and kinematics. Next, a cross-modal contrastive loss is designed to incorporate robust geometric prior from kinematics to image for tip segmentation. We have conducted experiments on a private paired visual-kinematics dataset including multiple procedures, i.e., prostatectomy, total mesorectal excision, fundoplication and distal gastrectomy on cadaver, and distal gastrectomy on porcine. The leave-one-procedure-out cross validation demon-strated that our proposed multi-modal segmentation method significantly outperformed current image-based state-of-the-art approaches, exceeding averagely 11.2% on Dice.</td>
                <td>Image segmentation, Visualization, Medical robotics, Deformation, Instruments, Surgery, Kinematics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342120&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342120&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dynamic Heart Simulator for Ultrasound-Guided Pericardiocentesis</td>
                <td>K. Yan, W. Yan and S. S. Cheng</td>
                <td>2023</td>
                <td>Pericardiocentesis is an important surgical intervention to treat a medical condition called pericardial effusion, during which excessive fluid accumulates around the heart, potentially leading to life-threatening situation. It involves the insertion of a needle and catheter towards the heart into the pericardial space to drain the excessive fluid under ultrasound (US) guidance. The risky procedure requires surgeons to acquire sufficient training to ensure safe execution of the procedure. However, existing heart simulators lack dynamic features, do not offer realistic images under US imaging, and are not reusable. This work presents a dynamic heart simulator (DHS) with pericardial effusion to mimic the beating motion of the human heart and the realistic US imaging results. The beating heart motion is realized using a hydraulic actuation system connected to a double-layer balloon set. The clear and realistic US imaging results are obtained through a unique formula proposed for the chest tissue and the cardiac muscle. A characterization method was also developed to allow customization of important anatomical parameters in the DHS. The experimental results show that the DHS allowed highly realistic simulation of the beating heart, cardiac muscle, and pericardium under US imaging and has been demonstrated to enable successful US-guided pericardiocentesis.</td>
                <td>Heart, Training, Ultrasonic imaging, Fluids, Heart beat, Dynamics, Surgery</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342122&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342122&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Method for Robotic Motion Compensation During PET Imaging of Mobile Subjects</td>
                <td>J. Wang, I. Iordachita and P. Kazanzides</td>
                <td>2023</td>
                <td>Studies of the human brain during natural activities, such as locomotion, would benefit from the ability to image deep brain structures during these activities. While Positron Emission Tomography (PET) can image these structures, the bulk and weight of current scanners are not compatible with the desire for a wearable device. This has motivated the design of a robotic system to support a PET imaging system around the subject's head and to move the system to accommodate natural motion. We report here the design and experimental evaluation of a prototype robotic system that senses motion of a subject's head, using parallel string encoders connected between the robot-supported imaging ring and a helmet worn by the subject. This measurement is used to robotically move the imaging ring (coarse motion correction) and to compensate for residual motion during image reconstruction (fine motion correction). Minimization of latency and measurement error are the key design goals, respectively, for coarse and fine motion correction. The system is evaluated using recorded human head motions during locomotion, with a mock imaging system consisting of lasers and cameras, and is shown to provide an overall system latency of about 80 ms, which is sufficient for coarse motion correction and collision avoidance, as well as a measurement accuracy of about 0.5 mm for fine motion correction.</td>
                <td>Weight measurement, Head, Imaging, Prototypes, Robot sensing systems, Magnetic heads, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341444&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341444&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>On the Potentials of Surface Tactile Imaging and Dilated Residual Networks for Early Detection of Colorectal Cancer Polyps</td>
                <td>N. Venkatayogi, Q. Hu, O. C. Kara, T. G. Mohanraj, S. F. Atashzar and F. Alambeigi</td>
                <td>2023</td>
                <td>This study proposes a novel diagnosis framework to decrease the early detection miss rate of colorectal cancer (CRC) polyps by using a hypersensitive vision-based tactile sensor (HySenSe) and a deep residual neural network. The HySenSe generates high-resolution 3D textural images of 160 realistic polyp phantoms for accurate classification via the proposed deep learning (DL) architecture. The DL module explores lightweight dilated convolutions, residual neural network architecture, and transfer learning to overcome the challenge of a small dataset of 229 images. Results show that the proposed architecture outperforms state-of-the-art DL models (i.e., EfficientNet and DenseNet) with a 94% accuracy, offering a promising solution for improving early detection of CRC polyps. The proposed framework can be used as a diagnostic module within tele-assessment medical robots, highlighting the potential of advanced technology and deep learning to revolutionize the early detection and treatment of CRC.</td>
                <td>Deep learning, Three-dimensional displays, Transfer learning, Tactile sensors, Phantoms, Imaging phantoms, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342161&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342161&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Smart Handheld Edge Device for on-Site Diagnosis and Classification of Texture and Stiffness of Excised Colorectal Cancer Polyps</td>
                <td>O. C. Kara et al.</td>
                <td>2023</td>
                <td>This paper proposes a smart handheld textural sensing medical device with complementary Machine Learning (ML) algorithms to enable on-site Colorectal Cancer (CRC) polyp diagnosis and pathology of excised tumors. The proposed unique handheld edge device benefits from a unique tactile sensing module and a dual-stage machine learning algorithms (composed of a dilated residual network and a t-SNE engine) for polyp type and stiffness characterization. Solely utilizing the occlusion-free, illumination-resilient textural images captured by the proposed tactile sensor, the framework is able to sensitively and reliably identify the type and stage of CRC polyps by classifying their texture and stiffness, respectively. Moreover, the proposed handheld medical edge device benefits from internet connectivity for enabling remote digital pathology (boosting the diagnosis in operating rooms and promoting accessibility and equity in medical diagnosis).</td>
                <td>Pathology, Machine learning algorithms, Image edge detection, Tactile sensors, Sensors, Reliability, Medical diagnostic imaging</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341678&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341678&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Underwater and Surface Aquatic Locomotion of Soft Biomimetic Robot Based on Bending Rolled Dielectric Elastomer Actuators</td>
                <td>C. Zhang, C. Zhang, J. Qu and X. Qian</td>
                <td>2023</td>
                <td>All-around, real-time navigation and sensing across the water environments by miniature soft robotics are promising, for their merits of small size, high agility and good compliance to the unstructured surroundings. In this paper, we propose and demonstrate a mantas-like soft aquatic robot which propels itself by flapping-fins using rolled dielectric elastomer actuators (DEAs) with bending motions. This robot exhibits fast-moving capabilities of swimming at 57mm/s or 1.25 body length per second (BL/s), skating on water surface at 64 mm/s (1.36 BL/s) and vertical ascending at 38mm/s (0.82 BL/s) at 1300 V, 17 Hz of the power supply. These results show the feasibility of adopting rolled DEAs for mesoscale aquatic robots with high motion performance in various water-related scenarios.</td>
                <td>Aquatic robots, Power supplies, Bending, Soft robotics, Propulsion, Robot sensing systems, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342144&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342144&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Swarm of One: Bottom-Up Emergence of Stable Robot Bodies from Identical Cells</td>
                <td>T. Smith, R. M. Butts, N. Adkins and Y. Gu</td>
                <td>2023</td>
                <td>Unlike most human-engineered systems, biological systems are emergent from low-level interactions, allowing much broader diversity and superior adaptation to complex environments. Inspired by the process of morphogenesis in nature, a bottom-up design approach for robot morphology is proposed to treat a robot's body as an emergent response to underlying processes rather than a predefined shape. This paper presents Loopy, a “Swarm-of-One” polymorphic robot testbed that can be viewed simultaneously as a robotic swarm and a single robot. Loopy's shape is determined jointly by self-organization and morphological computing using physically linked homogeneous cells. Experimental results show that Loopy can form symmetric shapes consisting of lobes. Using the same set of parameters, even small amounts of initial noise can change the number of lobes formed. However, once in a stable configuration, Loopy has an “inertia” to transfiguring in response to dynamic parameters. By making the connections among self-organization, morphological computing, and robot design, this work lays the foundation for more adaptable robot designs in the future.</td>
                <td>Shape, Morphology, Biological systems, Self-organizing networks, Modeling, Robots, Intelligent robots, Robotic Swarm, Robot Design, Morphogenesis, Morphological Computing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342118&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342118&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Pose Estimation of Rats Based on Stereo Vision Embedded in a Robotic Rat</td>
                <td>Khulaqui, Z. Chen, T. Fukuda and Q. Shi</td>
                <td>2023</td>
                <td>In this paper, we propose a system for real-time rat pose estimation based on stereo vision. The system is dedicated to robot-rat interaction research. First, we design a lightweight, high-resolution network (RRKDNet) for keypoint detection of the rat. The network is trained on a dataset of rat images, which are captured by the robotic rat in first-person view. Second, based on the keypoint detection results, the pose of the rat is obtained by stereo vision model calculation and robot coordinate transformation. At last, we complete a real-time simulation experiment to reproduce the pose of the rat and the robotic rat. The system has been subjected to a series of experiments and the results demonstrate that our network performs better in speed and performance than similar networks. Compared to similar networks, our network has about one-third the number of parameters, while the detection rate increases by 45.25% (the detection rate is 71.57%). The inference speed (34.42 FPS with dual model simultaneous inference) is also faster. The validation error is only 13.85 pixels on the homemade dataset, which is lower than all backbones in Deeplabcut (a toolbox more frequently used for rat keypoint detection). Thus, this work is a significant step in the autonomous intelligent interaction between robots and rats.</td>
                <td>Visualization, Three-dimensional displays, Robot kinematics, Pose estimation, Rats, Real-time systems, Stereo vision</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342475&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342475&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Development of a Rapidly Deployable Low-Cost Tensegrity In-Pipe Robot</td>
                <td>Y. Liu et al.</td>
                <td>2023</td>
                <td>Existing in-pipe robots have insufficient adaptability when dealing with accidents in unfamiliar pipe environments. Developing a pipe robot that can be designed and manufactured quickly is one solution. The tensegrity structure is a self-stressing spatial structure formed by the interaction of rigid members and flexible cables, which has the advantages of simple structure, good flexibility, deformability, and impact resistance. Inspired by this structure, we design a novel worm-like tensegrity robot for different pipe environments, which can be manufactured rapidly at low cost. Firstly, a robotic module based on the tensegrity structure is designed inspired by the motion patterns of worm-like organisms. Then, the design process of the module is presented based on the mathematical analysis of the deformation. Finally, a prototype of the tensegrity robot is developed using simple and low-cost parts in less than an hour. To test the motion performance, load performance, and inspection capability of the tensegrity robot, we designed a series of experiments on horizontal pipes, vertical pipes, elbows, and steel pipes. Experimental results show that the worm-like tensegrity robot is simple in structure, easy to manufacture, low in cost, and good in performance.</td>
                <td>Vibrations, Costs, Deformation, Prototypes, Inspection, Steel, Elbow</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341489&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341489&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Reinforcement Learning Based Multi-Layer Bayesian Control for Snake Robots in Cluttered Scenes</td>
                <td>J. Z. Qu, W. Z. Qu, L. Li and Y. Jia</td>
                <td>2023</td>
                <td>The majority of current research on reinforcement learning (RL) for snake robot control do not sufficiently account for the spatial and temporal dependencies within the robot or its interaction with its environment during movement. To address this issue, we propose an RL based multi-layer Bayesian method for autonomous snake robot control, which handles challenging scenarios and improves navigation efficiency. There are three major contributions: 1) An innovative hierarchical Bayesian framework unifies gait control, locomotion control, and stimulus reaction; 2) The dynamics of environment is modeled by density propagation and exploited by an LSTM-based agent to improve the learning process; 3) A stimulus reaction model is derived by combining spatial correlation among robot modules and temporal dependency along time sequence. Comparison experiments with a simulated snake robot show that performance of the proposed approach with challenging obstacles is superior to state-of-the-art baseline.</td>
                <td>Legged locomotion, Uncertainty, Navigation, Snake robots, Stochastic processes, Process control, Reinforcement learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342171&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342171&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Roblets: Robotic Tablets That Self-Assemble and Self-Fold into a Robot</td>
                <td>J. Han, D. Rus and S. Miyashita</td>
                <td>2023</td>
                <td>Inspired by human proteins that are synthesized from only 20 types of amino acids, the development of self-assembly methods that allow robots to be built simply by randomly stirring the parts has been explored for many years. The key challenges include how to synthesize parts in pieces into a three-dimensional functional structure in a practical time, and subsequently, achieve a controlled robotic motion, all with minimal human intervention. This study proposes a method of self-assembling a 3D robot by first self-assembling random parts into a 2D structure and then self-folding it into a 3D shape. Once self-folded, the robot, whose compositional parts contain magnets, becomes capable of performing basic tasks such as block-pushing upon an application of an external magnetic field. Self-assembly from parts into a two-dimensional structure was performed by repeatedly colliding the parts with each other, and combining them with complementary-shaped parts, like matching jigsaw puzzle pieces. Self-folding was performed by shrinking a heat-responsive film attached across the hinge of each assembly part in hot water, causing the entire 2D structure to self-fold. The experiment demonstrated a series of 13 parts self-assembling into the shape of a 3D beetle, then walking and pushing an object in 13 minutes. The self-assembly process is programmed (mechanically) to generate the same geometry even if the number of parts is greater than the necessary number for the structure, thus is capable of generating multiple structures simultaneously.</td>
                <td>Robot motion, Proteins, Legged locomotion, Three-dimensional displays, Self-assembly, Shape, Water heating</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342239&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342239&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Output Feedback Formation Control of a School of Robotic Fish with Artificial Lateral Line Sensing</td>
                <td>A. Wolek and D. A. Paley</td>
                <td>2023</td>
                <td>This paper presents an estimation and control framework to stabilize the parallel motion of a school of robotic fish using sensory feedback. Each robot is modeled as a constant speed, planar, self-propelled particle that produces a flowfield according to a dipole potential flow model. An artificial lateral line system senses pressure fluctuations at several locations along each robot's body. The equations of motion and measurement model are formulated in a path frame that translates and rotates with each robot's position and velocity, respectively. A particle filter estimates the relative position and heading of other nearby robots. The resulting estimate drives a Lyapunov-based formation controller to synchronize the headings of the robots and achieve a parallel formation. Numerical simulations illustrate the proposed approach.</td>
                <td>Estimation, Robot sensing systems, Fish, Numerical simulation, Mathematical models, Particle filters, Numerical models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342055&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342055&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Origami-Based Miniature Jumping Robot with Adjustable Jumping Trajectory and Enhanced Intermittent Jumps</td>
                <td>Z. Xiong et al.</td>
                <td>2023</td>
                <td>A small-scale jumping robot can reach obstacles much larger than its size. It is important for a jumping robot to perform intermittent jumps to cross through rough terrains. However, the limitations of conventional structures hinder the further integration of functions to a miniature (sub-50 g) jumping robot. No sub-50 g jumpers could perform intermittent jumps with adjustable jumping trajectories. In this work, we proposed an origami-based miniature jumper, which performed intermittent jumps with adjustable omni-directional trajectories. The intermittent jumps were achieved by the jumping and self-righting mechanisms, which were actuated by a single motor. The clockwise and counterclockwise rotation of the motor actuated the loading, self-righting and triggering process, respectively. The jumping height was adjustable by adjusting the rotation angle of the motor. Meanwhile, the take-off pitch & yaw angle adjustment methods were integrated into the robot. Therefore, we demonstrated a 9 cm, 13.5 g prototype with functions of re-loading, self-righting, jumping height adjustment and take-off pitch & yaw angle adjustment. The robot could adjust jumping height from 16 to 34 cm and self-right for the next jump. The results revealed that our robot could jump across different obstacles with different scales and directions. The mobility was greatly increased compared with other miniature jumping robots.</td>
                <td>Loading, Prototypes, Trajectory, Robots, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342330&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342330&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Finding the Goal: Insect-Inspired Spiking Neural Network for Heading Error Estimation</td>
                <td>T. Schoepe and E. Chicca</td>
                <td>2023</td>
                <td>Insects have extraordinary navigational abilities. Monarch butterflies migrate every year to the same forest over hundreds of kilometers, desert ants find their way back to the nest tens of meters away and dung beetles maintain the same heading direction over meters. The performance of these agents has been optimized by evolution over the last 500 million years leading to power-efficient, low-latency and precise sensorimotor systems. Research efforts in the field of neuroscience, biology and robotics are instrumental for uncovering the neural substrate of insect navigation abilities. The development of models of insect navigation tightly coupled with the insect connectome and neurophysiology and their embedding in closed loop systems support the understanding of embodied animal cognition and can advance robotic systems. In this work, we focus on insect navigation because of the efficient insect navigational apparatus. Furthermore, the recent discovery of the central complex, the neuronal center of insect navigation, facilitates the development of new hypotheses about insect navigation. All navigating insects need to perform some kind of goal-directed behavior during which they have to reach a specific goal location or maintain the same movement direction over long distances. Such behavior requires the agent to be aware of its current heading direction, desired heading direction, and the error between them. Building on previous research in the field, we propose a novel model for this error estimation that can in principle be generalized for all navigating insect species. We implement the model in a spiking neural network and test its capabilities on a simulated robotic platform. The precision of the network is comparable to or even better than the biological role model. Thus, our implementation serves as a working hypothesis for how the heading error might be computed in the insect brain. Our model will help to explain navigational behavior in fruit flies, orchid bees, bumble bees and some less researched insect species. Furthermore, its simplicity in comparison to other models and implementation in a spiking neural network makes it very suitable for neuromorphic systems, an emerging field of brain-inspired hardware.</td>
                <td>Meters, Navigation, Error analysis, Insects, Biological system modeling, Computational modeling, Brain modeling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342210&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342210&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Motion Control and Planning of a Bio-Inspired Aerial Vehicle with an Actively Controlled Abdomen-Like Appendage</td>
                <td>B. Güney and M. M. Ankaralı</td>
                <td>2023</td>
                <td>Animals' anatomies have control systems combined with multi-motors and high-bandwidth sensors. Their complicated mechanisms give them high maneuverability with sufficient inertial stabilization performance during walking, jumping, and flying. From the point of aerial locomotion, flying insects use abdomen reflexes to stabilize their head positions. Articulation of the thoracic-abdominal joint contributes to the reorientation of their bodies over the law of conservation of angular momentum. Since acceleration is a fundamental component of maneuverability, increasing the acceleration without destabilizing the body is achieved with additional appendages such as the tail and abdomen. Highly actuated abdominal muscles are an essential feature of these natural flyers, conspicuously missing from the current aerial vehicles regarding maneuverability. This study proposes a bio-inspired aerial vehicle morphology with an actively controlled abdomen-like appendage. We aim to investigate the advantages and disadvantages of the abdomen-like appendage mounted on a bi-rotor aerial vehicle by constructing the dynamical model and designing optimization-based controllers; Linear Quadratic Regulator (LQR), Model Predictive Control (MPC), and Adaptive Model Predictive Control (A-MPC). We complete our analysis with a motion planning algorithm by combining the sampling-based neighborhood graph approach with the A-MPC strategy. We demonstrate through simulation experiments that the appendage improves the stability and maneuverability of aerial vehicles and the resulting motion planning structure with A-MPC ensures that the state and input constraints are not violated.</td>
                <td>Costs, Three-dimensional displays, Insects, Tail, Planning, Topology, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341458&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341458&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Tailsitter UAV Based on Bioinspired, Tendon-Driven, Shape-Morphing Wings with Aerofoil-Shaped Artificial Feathers</td>
                <td>J. Liang, J. Buzzatto and M. Liarokapis</td>
                <td>2023</td>
                <td>Unmanned aerial vehicles (UAVs) have revolutionised various industries, such as agriculture, remote sensing, and infrastructure inspection. To explore new designs and improve UAV flight performance, roboticists are seeking inspiration from nature. In this paper, we present a bioinspired tailsitter UAV utilizing shape-morphing wings with aerofoil-shaped artificial feathers. The design of the UAV is inspired by the shape and motion of bird wings, which can change their shape and span to adapt to different flight conditions. The pigeon's wing skeletal structure serves as the basis for the design, and the wing was developed to be fully tendon-driven employing a single motor for each side. The wings can contract and extend, resulting in a contraction ratio of 49% of the extended wing span. In hovering flight mode, the wing contraction shows a 42% decrease in drag for improved wind disturbance rejection. Wind tunnel testing characterises the wing's aerodynamic performance, revealing significant deflection at high angles of attack due to the articulated skeletal structure. The wings demonstrate low power consumption, averaging only 5.1 W during morphing in experiments. Finally, we demonstrate the wing's robustness through outdoor flight experiments. The research findings provide insights into the potential of bioinspired designs for tailsitter UAVs and offer a promising avenue for future research in this field.</td>
                <td>Feathers, Solid modeling, Shape, Service robots, Wind tunnels, Autonomous aerial vehicles, Birds</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341538&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341538&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>How to Achieve Maneuverability and Adaptability in an Underactuated Robotic Fish by using a Bio-inspired Control Approach</td>
                <td>G. Manduca, G. Santaera, P. Dario, C. Stefanini and D. Romano</td>
                <td>2023</td>
                <td>Biomimetic robotics can help support underwater exploration and monitoring while minimizing ecosystem distur-bance. It also has potential applications in sustainable aqua-farming management, biodiversity preservation, and animal-robot interaction studies. This study proposes a bio-inspired control strategy for an underactuated robotic fish, which utilizes a single DC motor to drive a mechanism that converts the motor's oscillating motion into an oscillatory motion of the robotic fishtail through a magnetic coupling and a wire-driven system. The proposed control strategy for the robotic fish is based on central pattern generators (CPGs) and incorporates proprioceptive sensory feedback. The torque exerted on the fishtail is adjusted based on its position, allowing for increased or decreased body speed and steering with different angular speeds and radii of curvature despite the underactuated design. The robotic fish can vary the swimming speed of 0.08 body lengths per second (BL/s) with a related change in the tail-beating frequency up to 2.3 Hz, and it can vary the steering angular speed in the range of 0.08 rad/s with a relative change in the curvature radius of 0.25 m. The controller can adapt to changes in tail structure, weight, or the surrounding environment based on the proprioceptive feedback. Design changes to the modular design can improve speed and steering performances, maintaining the control strategy developed.</td>
                <td>Bio-inspired control, Torque, Propioception, Tail, Fish, Magnetosphere, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342036&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342036&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Energy-Efficient Lane-Keeping System Using 3D LiDAR Based on Spiking Neural Network</td>
                <td>G. Zhuang et al.</td>
                <td>2023</td>
                <td>Lane keeping, as a fundamental functionality of autonomous navigation, remains a challenging task for autonomous robots and vehicles. Recently, spiking neural networks (SNNs) have gained attention and research interest due to their biological plausibility and application potential on neuromorphic processors. SNNs have also been successfully deployed on robots to solve autonomous navigation problems. However, lane keeping with a LiDAR sensor is still an open problem for SNNs. In this work, we propose an end-to-end approach based on an SNN to solve the lane-keeping problem using a 3D LiDAR sensor. For the first time, we explore the capability of the proposed SNN controller to perceive the LiDAR input and exploit the features to perform reward-based feedback learning. To ensure the effectiveness of the controller, the proposed method is deployed and evaluated on two high-fidelity simulators. The experimental results demonstrate the high applicability and performance in different scenarios. Furthermore, experiments show that the SNN is capable of performing lane keeping in a simulated urban environment with only 18 control neurons and 32 synapse connections, producing on average only a 17cm deviation from lane center, which is 4.3 % of the lane width.</td>
                <td>Laser radar, Three-dimensional displays, Urban areas, Robot sensing systems, Feature extraction, Task analysis, Biological neural networks</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342044&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342044&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Approximation Algorithms for Charging Station Placement for Mobile Robots</td>
                <td>T. Kundu and I. Saha</td>
                <td>2023</td>
                <td>Optimal placement of charging stations in a workspace is a crucial problem to address, for efficient operation of battery-driven mobile robots. When the battery charge of a robot reaches a certain threshold, the robot must be able to reach a nearby charging station to recharge its battery. In this paper, we deal with two different versions of the optimization problem related to the optimal placement of charging stations in a robot workspace. The first problem is formulated to find an optimal number of charging stations given a battery threshold deciding the need to move to a charging station, and the second problem finds an optimal battery threshold for a given number of charging stations. Both the problems involve finding the locations of charging stations, such that from any obstacle-free location at least one charging station is reachable with at most threshold amount of battery charge remaining with the robot. In this paper, we prove these optimization problems to be NP-hard, i.e., computationally intractable. To handle intractability of the above minimization problems, we design two polynomial-time approximation algorithms to find near-optimal solutions. Our algorithms achieve significantly high scalability without compromising the quality of the solution beyond a certain factor of the optimal solution. Experimental results show that our algorithms run order-of-magnitude faster than a recently proposed Satisfiability Modulo Theory (SMT)-based approach and maintain solution quality within the theoretical bounds on the optimal solution.</td>
                <td>NP-hard problem, Scalability, Charging stations, Approximation algorithms, Minimization, Batteries, Mobile robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341361&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341361&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LQR-Trees with Sampling Based Exploration of the State Space</td>
                <td>J. Fejlek and S. Ratschan</td>
                <td>2023</td>
                <td>This paper introduces an extension of the LQR-tree algorithm, which is a feedback-motion-planning algorithm for stabilizing a system of ordinary differential equations from a bounded set of initial conditions to a goal. The constructed policies are represented by a tree of exemplary system trajec-tories, so called demonstrations, and linear-quadratic regulator (LQR) feedback controllers. Consequently, the crucial component of any LQR-tree algorithm is a demonstrator that provides suitable demonstrations. In previous work, such a demonstrator was given by a local trajectory optimizer. However, these require appropriate initial guesses of solutions to provide valid results, which was pointed out, but largely unresolved in previous implementations. In this paper, we augment the LQR-tree algorithm with a randomized motion-planning procedure to discover new valid demonstration candidates to initialize the demonstrator in parts of state space not yet covered by the LQR-tree. In comparison to the previous versions of the LQR-tree algorithm, the resulting exploring LQR-tree algorithm reliably synthesizes feedback control laws for a far more general set of problems.</td>
                <td>Regulators, Heuristic algorithms, Aerospace electronics, Ordinary differential equations, Trajectory, Feedback control, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341767&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341767&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Simultaneous Survey and Inspection with Autonomous Underwater Vehicles</td>
                <td>J. McMahon, R. Parker, P. Baldoni, S. Anstee and E. Plaku</td>
                <td>2023</td>
                <td>As the future of autonomous underwater vehicle (AUV) deployments tends to multi-vehicle systems, new approaches in coordination and control are needed. In this work, we consider the problem of simultaneous survey and inspection where one vehicle dynamically discovers objects while another vehicle must inspect as many of the objects as possible over the course of the mission. This requires a fully autonomous inspection vehicle, and to this end, we present a planning approach which couples sampling-based motion planning with timed roadmap constraints as well as a real-time execution framework. The methods presented address the underlying challenges that arise during simultaneous survey and inspection using AUVs, namely those of communication constraints, safety of navigation constraints, and dynamically discovered tasks. Additionally, we present field results for the simultaneous survey and inspection mission using teamed AUVs.</td>
                <td>Surveys, Autonomous underwater vehicles, Navigation, Inspection, Real-time systems, Planning, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341736&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341736&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Relative Roughness Measurement Based Real-Time Speed Planning for Autonomous Vehicles on Rugged Road</td>
                <td>L. Wang, T. Niu, S. Wang, S. Wang and J. Wang</td>
                <td>2023</td>
                <td>In order to guarantee autonomous vehicles' autonomy, mobility, and ride quality in rugged environments, a real-time speed planning method based on the time-frequency transformation of terrain characteristics is designed to achieve adaptive speed planning of autonomous vehicles in rough ground. On the one hand, the vertical profile of the lidar's point cloud data is converted from the time domain to the frequency domain in real time, and the integrated area of the sub-frequency range in the frequency domain is chosen as the relative roughness quantification value to realize the roughness quantification under various terrains. On the other hand, to model the relationship between vehicle speed and relative roughness, iterative search is utilized to create a speed and roughness model, and sliding windows are employed to update the roughness to achieve continuous mapping between speed and roughness. Ultimately, a number of tests were conducted on various rough roads using the oil exploration vehicle EV-56 as the study object. The experimental results show that the proposed method can identify the terrain roughness changes under complex terrain and change their speed within 0.2 m accuracy.</td>
                <td>Point cloud compression, Time-frequency analysis, Roads, Oils, Real-time systems, Surface roughness, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341375&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341375&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Motion Planning Framework for Autonomous Vehicles with Learned Committed Trajectory Distribution</td>
                <td>M. Kim, S. Shin, J. Ahn and J. Park</td>
                <td>2023</td>
                <td>This study proposes a realtime motion planning framework that leverages the prediction of a portion of the optimal trajectory for sampling-based anytime planning algorithms. Existing algorithms predict the entire optimal path and bias random samples toward it for fast path planning. However, these algorithms may not be suitable for realtime frameworks because the bias-sampling strategy should consider the sequential nature of realtime execution. Therefore, the proposed algorithm predicts a portion of the optimal path, known as the committed trajectory, step by step as a probability distribution using a neural network. This distribution is then used in a sampling-based anytime planning algorithm as a non-stationary way of biasing random samples. The proposed algorithm can sequentially plan the near-optimal motion, al-lowing the vehicle to reach the desired goal pose in a timely and accurate manner. In various test parking scenarios, the proposed algorithm reduces the parking time by approximately 38% compared with conventional motion planning algorithms and by 10% compared with another realtime framework that biases samples toward the entire optimal trajectory.</td>
                <td>Neural networks, Prediction algorithms, Approximation algorithms, Real-time systems, Probability distribution, Trajectory, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342292&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342292&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks</td>
                <td>Fei and J. Bohg</td>
                <td>2023</td>
                <td>Solving real-world manipulation tasks requires robots to be equipped with a repertoire of skills that can be applied to diverse scenarios. While learning-based methods can enable robots to acquire skills from interaction data, their success relies on collecting training data that covers the diverse range of tasks that the robot may encounter during the test time. However, creating diverse and feasible training tasks often requires extensive domain knowledge and non-trivial manual labor. We introduce Active Task Randomization (ATR), an approach that learns robust skills through the unsupervised generation of training tasks. ATR selects suitable training tasks-which consist of an environment configuration and manipulation goal-by actively balancing their diversity and feasibility. In doing so, ATR effectively creates a curriculum that gradually increases task diversity while maintaining a moderate level of feasibility, which leads to more complex tasks as the skills become more capable. ATR predicts task diversity and feasibility with a compact task representation that is learned concurrently with the skills. The selected tasks are then procedurally generated in simulation with a graph-based parameterization. We demonstrate that the learned skills can be composed by a task planner to solve unseen sequential manipulation problems based on visual inputs. Compared to baseline methods, ATR can achieve superior success rates in single-step and sequential manipulation tasks. Videos are available at sites.google.com/view/active-task-randomization/</td>
                <td>Training, Learning systems, Visualization, Training data, Manuals, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341727&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341727&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Self-Supervised Extrinsic Self-Calibration</td>
                <td>T. Kanai, I. Vasiljevic, V. Guizilini, A. Gaidon and R. Ambrus</td>
                <td>2023</td>
                <td>Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. The project page: https://sites.google.com/tri.global/tri-sesc</td>
                <td>Training, Visualization, Benchmark testing, Cameras, Robot sensing systems, Robustness, Calibration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341367&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341367&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Do More with Less: Single-Model, Multi-Goal Architectures for Resource-Constrained Robots</td>
                <td>Z. Wang, D. Threatt, S. B. Andersson and R. Tron</td>
                <td>2023</td>
                <td>Deep learning methods are widely used in robotic applications. By learning from prior experience, the robot can abstract knowledge of the environment, and use this knowledge to accomplish different goals, such as object search, frontier exploration, or scene understanding, with a smaller amount of resources than might be needed without that knowledge. Most existing methods typically require a significant amount of sensing, which in turn has significant costs in terms of power consumption for acquisition and processing, and typically focus on models that are tuned for each specific goal, leading to the need to train, store and run each one separately. These issues are particularly important in a resource-constrained setting, such as with small-scale robots or during long-duration missions. We propose a single, multi-task deep learning architecture that takes advantage of the structure of the partial environment to predict different abstractions of the environment (thus reducing the need for rich sensing), and to leverage these predictions to simultaneously achieve different high-level goals (thus sharing computation between goals). As an example application of the proposed architecture, we consider the specific example of a robot equipped with a 2-D laser scanner and an object detector, tasked with searching for an object (such as an exit) in a residential building while constructing a topological map that can be used for future missions. The prior knowledge of the environment is encoded using a U-Net deep network architecture. In this context, our work leads to an object search algorithm that is complete, and that outperforms a more traditional frontier-based approach. The topological map we produce uses scene trees to qualitatively represent the environment as a graph at a fraction of the cost of existing SLAM-based solutions. Our results demonstrate that it is possible to extract multi-task semantic information that is useful for navigation and mapping directly from bare-bone, non-semantic measurements.</td>
                <td>Costs, Semantics, Computer architecture, Predictive models, Robot sensing systems, Search problems, Multitasking</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341879&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341879&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhancing State Estimation in Robots: A Data-Driven Approach with Differentiable Ensemble Kalman Filters</td>
                <td>X. Liu, G. Clark, J. Campbell, Y. Zhou and H. B. Amor</td>
                <td>2023</td>
                <td>This paper introduces a novel state estimation framework for robots using differentiable ensemble Kalman filters (DEnKF). DEnKF is a reformulation of the traditional ensemble Kalman filter that employs stochastic neural networks to model the process noise implicitly. Our work is an extension of previous research on differentiable filters, which has provided a strong foundation for our modular and end-to-end differentiable framework. This framework enables each component of the system to function independently, leading to improved flexibility and versatility in implementation. Through a series of experiments, we demonstrate the flexibility of this model across a diverse set of real-world tracking tasks, including visual odometry and robot manipulation. Moreover, we show that our model effectively handles noisy observations, is robust in the absence of observations, and outperforms state-of-the-art differentiable filters in terms of error metrics. Specifically, we observe a significant improvement of at least 59% in translational error when using DEnKF with noisy observations. Our results underscore the potential of DEnKF in advancing state estimation for robotics. Code for DEnKF is available at https://github.com/ir-lab/DEnKF</td>
                <td>Visualization, Uncertainty, Perturbation methods, Robot sensing systems, Kalman filters, Noise measurement, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341617&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341617&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning a Causal Transition Model for Object Cutting</td>
                <td>Z. Zhang et al.</td>
                <td>2023</td>
                <td>Cutting objects into desired fragments is challenging for robots due to the spatially unstructured nature of fragments and the complex one-to-many object fragmentation caused by actions. We present a novel approach to model object fragmentation using an attributed stochastic grammar. This grammar abstracts fragment states as node variables and captures causal transitions in object fragmentation through production rules. We devise a probabilistic framework to learn this grammar from human demonstrations. The planning process for object cutting involves inferring an optimal parse tree of desired fragments using the learned grammar, with parse tree productions corresponding to cutting actions. We employ Monte Carlo Tree Search (MCTS) to efficiently approximate the optimal parse tree and generate a sequence of executable cutting actions. The experiments demonstrate the efficacy in planning for object-cutting tasks, both in simulation and on a physical robot. The proposed approach outperforms several baselines by demonstrating superior generalization to novel setups, thanks to the compositionality of the grammar model.</td>
                <td>Training, Monte Carlo methods, Stochastic processes, Production, Probabilistic logic, Robustness, Grammar</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341424&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341424&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Object Rearrangement Planning for Target Retrieval in a Confined Space with Lateral View</td>
                <td>M. Kang, J. Kim, H. Kee and S. Oh</td>
                <td>2023</td>
                <td>In this paper, we perform an object rearrangement task for target retrieval in an environment with a confined space and limited observation directions. The agent must create a collision-free path to bring out the target object by relocating the surrounding objects using the prehensile action, i.e., pick-and-place. Object rearrangement in a confined space is a non-monotone problem, and finding a valid plan within a reasonable time is challenging. We propose a novel algorithm that divides the target retrieval task, which requires a long sequence of actions, into sequential sub-problems and explores each solution through Monte Carlo tree search (MCTS). In the experiment, we verify that the proposed algorithm can find safe rearrangement plans with various objects efficiently compared to the existing planning methods. Furthermore, we show that the proposed method can be transferred to a real robot experiment without additional training.</td>
                <td>Training, Monte Carlo methods, Shape, Planning, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341865&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341865&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Type-Generalized Actions for Symbolic Planning</td>
                <td>D. Tanneberg and M. Gienger</td>
                <td>2023</td>
                <td>Symbolic planning is a powerful technique to solve complex tasks that require long sequences of actions and can equip an intelligent agent with complex behavior. The downside of this approach is the necessity for suitable symbolic representations describing the state of the environment as well as the actions that can change it. Traditionally such representations are carefully hand-designed by experts for distinct problem domains, which limits their transferability to different problems and environment complexities. In this paper, we propose a novel concept to generalize symbolic actions using a given entity hierarchy and observed similar behavior. In a simulated grid-based kitchen environment, we show that type-generalized actions can be learned from few observations and generalize to novel situations. Incorporating an additional on-the-fly generalization mechanism during planning, unseen task combinations, involving longer sequences, novel entities and unexpected environment behavior, can be solved.</td>
                <td>Planning, Behavioral sciences, Complexity theory, Intelligent agents, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342301&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342301&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments</td>
                <td>R. Cannizzaro and L. Kunze</td>
                <td>2023</td>
                <td>Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of “anytime regularized determinized sparse partially observable tree” (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to learn offline the partial parameterisation of the causal model for planning, from ground truth model data. We evaluate our methods on a toy problem with an unobserved confounder and show that the learned causal model is highly accurate, while our planning method is more robust to confounding and produces overall higher performing policies than AR-DESPOT.</td>
                <td>Toy manufacturing industry, Semantics, Decision making, Markov processes, Data models, Cognition, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342223&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342223&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Recurrent Macro Actions Generator for POMDP Planning</td>
                <td>Y. Liang and H. Kurniawati</td>
                <td>2023</td>
                <td>Many planning problems in robotics require long planning horizon and uncertain in nature. The Par-tially Observable Markov Descision Process (POMDP) is a mathematically principled framework for planning under uncertainty. To alleviate the difficulties of computing good approximate POMDP solutions for long horizon problems, one often plans using macro actions, where each macro action is a chain of primitive actions. Such a strategy reduces the effective planning horizon of the problem, and hence reduces the computational complexity for solving. The difficulty is in generating a set of suitable macro actions. In this paper, we present a simple recurrent neural network that learns to generate suitable sets of candidate macro actions that exploits environment information. Key to this learning method is to represent the raw partial information from the environment as a latent problem instance, and sequentially generate macro actions conditioned on the past information. We compare our proposed method with state-of-the-art [1] on four dif-ferent long horizon planning tasks with various difficulties. The results indicate the quality of the policies computed using macro actions generated by our proposed method consistently exceeds benchmarks. Our implementation can be accessed at https://github.com/YC-Liang/Recurrent-Macro-Action-Generator.</td>
                <td>Learning systems, Uncertainty, Recurrent neural networks, Scalability, Computer architecture, Markov processes, Generators</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341759&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341759&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Task Planning and Motion Control with Temporal Logic Specifications</td>
                <td>M. S. Pereira, L. C. A. Pimenta and B. V. Adorno</td>
                <td>2023</td>
                <td>This paper proposes a task planning and motion control framework that generates task plans for a linear temporal logic specification (LTL), which are then executed using a task-space constrained motion controller and a local task planner that overcomes local minima. We propose a new encoding for task specifications, directly in the task-space, as constraints of a mixed-integer linear program that can be used with off-the-shelf LTL linear encoding. We apply our framework to plan and execute trajectories for a free-flying robot and show that the task plan is accomplished without collisions, even in the presence of unexpected moving obstacles that are not considered in the planning phase, while control signal constraints are satisfied. To evaluate the local minima avoidance, we compare the local task planner with a sampling-based motion planner, and the results show a smoother trajectory with a faster execution and less total planning time when using our framework. Last, our framework scaled well with a longer LTL specification, as opposed to automata-based frameworks that usually suffer with the curse of the dimensionality.</td>
                <td>Motion planning, Navigation, Simulation, Encoding, Planning, Trajectory, Quadratic programming</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341494&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341494&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Simultaneous Action and Grasp Feasibility Prediction for Task and Motion Planning Through Multi-Task Learning</td>
                <td>S. A. Bouhsain, R. Alami and T. Siméon</td>
                <td>2023</td>
                <td>In this paper, we address task and motion plan-ning (TAMP) which is an important yet challenging robotics problem. It is known to suffer from the high combinatorial complexity of discrete search, often requiring a large number of geometric planning calls. We build upon recent works in TAMP by taking advantage of learning methods to provide action feasibility information as a heuristic to the symbolic planner, thus guiding it to a geometrically feasible solution and reducing geometric planning time. We propose AGFP-Net, a multi-task neural network predicting not only action feasibility, but also the feasibility of a set of grasp types. We also propose an improved feasibility-informed TAMP algorithm capable of solving more complex problems, and handling goals which are not fully specified. Comparative results obtained on different problems of varying complexity show that our method is able to greatly reduce task and motion planning time.</td>
                <td>Learning systems, Shape, Neural networks, Performance gain, Multitasking, Manipulators, Prediction algorithms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341257&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341257&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Differentiable Task Assignment and Motion Planning</td>
                <td>J. Envall, R. Poranne and S. Coros</td>
                <td>2023</td>
                <td>Task and motion planning is one of the key problems in robotics today. It is often formulated as a discrete task allocation problem combined with continuous motion planning. Many existing approaches to TAMP involve explicit descriptions of task primitives that cause discrete changes in the kinematic relationship between the actor and the objects. In this work we propose an alternative, fully differentiable approach which supports a large number of TAMP problem instances. Rather than explicitly enumerating task primitives, actions are instead represented implicitly as part of the solution to a nonlinear optimization problem. We focus on decision making for robotic manipulators, specifically for pick and place tasks, and explore the efficacy of the model through a number of simulated experiments including multiple robots, objects and interactions with the environment. We also show several possible extensions.</td>
                <td>Decision making, Kinematics, Manipulators, Planning, Resource management, Task analysis, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341602&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341602&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Effectively Rearranging Heterogeneous Objects on Cluttered Tabletops</td>
                <td>K. Gao, J. Yu, T. S. Punjabi and J. Yu</td>
                <td>2023</td>
                <td>Effectively rearranging heterogeneous objects constitutes a high-utility skill that an intelligent robot should master. Whereas significant work has been devoted to the grasp synthesis of heterogeneous objects, little attention has been given to the planning for sequentially manipulating such objects. In this work, we examine the long-horizon sequential rearrangement of heterogeneous objects in a tabletop setting, addressing not just generating feasible plans but near-optimal ones. Toward that end, and building on previous methods, including combinatorial algorithms and Monte Carlo tree search-based solutions, we develop state-of-the-art solvers for optimizing two practical objective functions considering key object properties such as size and weight. Thorough simulation studies show that our methods provide significant advantages in handling challenging heterogeneous object rearrangement problems, especially in cluttered settings. Real robot experiments further demonstrate and confirm these advantages. Source code and evaluation data associated with this research will be available at https//github.com/arc-l/TRLB upon the publication of this manuscript</td>
                <td>Costs, Monte Carlo methods, Shape, Heuristic algorithms, Search problems, Linear programming, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342164&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342164&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Semantics-Aware Mission Adaptation for Autonomous Exploration in Urban Environments</td>
                <td>Mohammadi</td>
                <td>2023</td>
                <td>Robust mission planning is an essential component for mission autonomy to perform complicated tasks in extreme environments. In this paper, we are interested in the role of semantic abstractions for guiding autonomous mission planning. In particular, we focus on how semantics can be leveraged to transition, at the mission level, in between individually robust task plans. We present a mission autonomy framework wherein a task plan adaptation policy leverages up-to-date semantics information in order to adapt to changes that occur during run-time, which endows the robot with better resiliency to unexpected events and improves the overall efficiency of mission operations. Under this new perspective, we provide a concrete and challenging application of autonomous exploration and radio source seeking in a complex multi-level building environment. Experimental results over simulations and real hardware tests demonstrate that the presented semantics-aware mission adaptation more effectively completes the mission with better qualitative results compared to a non-adaptive baseline.</td>
                <td>Adaptation models, Semantics, Buildings, Urban areas, Hardware, Planning, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341632&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341632&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Optimal Cost-Preference Trade-Off Planning with Multiple Temporal Tasks</td>
                <td>P. Amorese and M. Lahijanian</td>
                <td>2023</td>
                <td>Autonomous robots are increasingly utilized in realistic scenarios with multiple complex tasks. In these scenarios, there may be a preferred way of completing the tasks, but it is often in conflict with optimal execution. Recent work studies preference-based planning, however, they have yet to extend the notion of preference to the behavior of the robot with respect to each task. In this work, we introduce a novel notion of preference that provides a generalized framework to express preferences over individual tasks as well as their relations. We perform an optimal trade-off (Pareto) analysis between behaviors that adhere to the user's preference and the ones that are resource optimal. We introduce an efficient planning framework that generates Pareto-optimal plans given user's preference by extending A* search. Further, we show a method of computing the entire Pareto front (the set of all optimal trade-offs) via an adaptation of a multi-objective A* algorithm. We also present a search heuristic to enable scalability. We illustrate the power of the framework on both mobile robots and manipulators. Our benchmarks show the effectiveness of the heuristic with up to 2-orders of magnitude speedup.</td>
                <td>Costs, Heuristic algorithms, Scalability, Benchmark testing, Manipulators, Planning, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342351&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342351&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Optimal and Stable Multi-Layer Object Rearrangement on a Tabletop</td>
                <td>A. Xu, K. Gao, S. W. Feng and J. Yu</td>
                <td>2023</td>
                <td>Object rearrangement is a fundamental sub-task in accomplishing a great many physical tasks. As such, effectively executing rearrangement is an important skill for intelligent robots to master. In this study, we conduct the first algorithmic study on optimally solving the problem of Multi-layer Object Rearrangement on a Tabletop (MORT), in which one object may be relocated at a time, and an object can only be moved if other objects do not block its top surface. In addition, any intermediate structure during the reconfiguration process must be physically stable, i.e., it should stand without external support. To tackle the dual challenges of untangling the dependencies between objects and ensuring structural stability, we develop an algorithm that interleaves the computation of the optimal rearrangement plan and structural stability checking. Using a carefully constructed integer linear programming (ILP) model, our algorithm, Stability-Aware Rearrangement Programming (SARP), readily scales to optimally solve complex rearrangement problems of 3D structures with over 60 building blocks, with solution quality significantly outperforming natural greedy best-first approaches. Upon the publication of the manuscript source code and data will be available at https//github.com/arc-1/mort/.</td>
                <td>Solid modeling, Three-dimensional displays, Source coding, Computational modeling, Integer linear programming, Data models, Structural engineering</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342446&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342446&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Task and Motion Planning with Large Language Models for Object Rearrangement</td>
                <td>Y. Ding, X. Zhang, C. Paxton and S. Zhang</td>
                <td>2023</td>
                <td>Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop</td>
                <td>Geometry, Costs, Service robots, Manipulators, Planning, Task analysis, Commonsense reasoning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342169&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342169&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Improving Amputee Endurance Over Activities of Daily Living with a Robotic Knee-Ankle Prosthesis: A Case Study</td>
                <td>T. K. Best, C. A. Laubscher, R. J. Cortino, S. Cheng and R. D. Gregg</td>
                <td>2023</td>
                <td>Robotic knee-ankle prostheses have often fallen short relative to passive microprocessor prostheses in time-based clinical outcome tests. User ambulation endurance is an alternative clinical outcome metric that may better highlight the benefits of robotic prostheses. However, previous studies were unable to show endurance benefits due to inaccurate high-level classification, discretized mid-level control, and in-sufficiently difficult ambulation tasks. In this case study, we present a phase-based mid-level prosthesis controller which yields biomimetic joint kinematics and kinetics that adjust to suit a continuum of tasks. We enrolled an individual with an above-knee amputation and challenged him to perform repeated, rapid laps of a circuit comprising activities of daily living with both his passive prosthesis and a robotic prosthesis. The participant demonstrated improved endurance with the robotic prosthesis and our mid-level controller compared to his passive prosthesis, completing over twice as many total laps before fatigue and muscle discomfort required him to stop. We also show that time-based outcome metrics fail to capture this endurance improvement, suggesting that alternative metrics related to endurance and fatigue may better highlight the clinical benefits of robotic prostheses.</td>
                <td>Measurement, Kinematics, Muscles, Fatigue, Kinetic theory, Task analysis, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341643&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341643&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Controlling Powered Prosthesis Kinematics Over Continuous Transitions Between Walk and Stair Ascent</td>
                <td>S. Cheng, C. A. Laubscher and R. D. Gregg</td>
                <td>2023</td>
                <td>One of the primary benefits of emerging powered prosthetic legs is their ability to facilitate step-over-step stair ascent by providing positive mechanical work. Existing control methods typically have distinct steady-state activity modes for walking and stair ascent, where activity transitions involve discretely switching between controllers and often must be initiated with a particular leg. However, these discrete transitions do not necessarily replicate able-bodied joint biomechanics, which have been shown to continuously adjust over a transition stride. This paper presents a phase-based kinematic controller for a powered knee-ankle prosthesis that enables continuous, biomimetic transitions between walking and stair ascent. The controller tracks joint angles from a data-driven kinematic model that continuously interpolates between the steady-state kinematic models, and it allows both the prosthetic and intact leg to lead the transitions. Results from experiments with two transfemoral amputee participants indicate that knee and ankle kinematics smoothly transition between walking and stair ascent, with comparable or lower root mean square errors compared to variations from able-bodied data.</td>
                <td>Legged locomotion, Knee, Kinematics, Switches, Stairs, Control systems, Steady-state</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341457&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341457&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Calibration of a Tibia-Based Phase Variable for Control of Robotic Transtibial Prostheses</td>
                <td>R. R. Posh, J. A. Tittle, J. P. Schmiedeler and P. M. Wensing</td>
                <td>2023</td>
                <td>Phase variable control based on global tibia kinematics holds promise for predicting gait cycle progression to continuously control robotic transtibial prostheses. Calibration of the phase variable is critical to ensure its monotonic behavior, to approach a linear relationship with gait percentage, and to accurately predict the percentage of gait. This paper compares four calibration approaches using data from 22 able-bodied subjects walking at 14 speeds [1]. The typical pure centering (PC) approach employed for thigh-based phase variables is not viable, yielding monotonic phase progression in fewer than half of the cases. An optimization (OPT) approach found monotonic calibrations in 305/308 cases with high linearity (average R2 of 0.91). Critical point centering (CPC) approximates the OPT performance, with 274/308 monotonic calibrations and an average R2 of 0.85, whereas the related vertical weighted average (VWA) approach was only slightly better than PC. All four approaches are similarly accurate in predicting gait percentage, staying within 5% at least 92.7% of the time.</td>
                <td>Legged locomotion, Shape, Linearity, Optimized production technology, Kinematics, Calibration, Impedance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341724&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341724&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>On Intuitive Control of Ankle-Foot Prostheses: A Sensor Fusion-Based Algorithm for Real-Time Prediction of Transitions to Compliant Surfaces</td>
                <td>C. Angelidou and P. Artemiadis</td>
                <td>2023</td>
                <td>Substantial research and development on the design and control of robotic ankle-foot prostheses have aimed to restore normal function and movement capacity for people with gait impairments and lower limb amputations. However, prostheses controllers usually fail to incorporate information pertaining to the properties of the walking terrain, such as ground stiffness. There is therefore a need for a framework that adjusts the prostheses parameters according to the user's intent to transition to a variable impedance terrain. To achieve this, we need to incorporate the human wearer in the control loop of the prosthesis. This work proposes an advanced, high-level controller framework for powered ankle-foot prostheses that combines subject-specific pattern recognition (PR) and classification strategies to predict whether the next step will be on a rigid or compliant surface. Comparing the Support Vector Machine (SVM) and k-Nearest Neighbors (k-NN) classification algorithms for this task, we conclude that by combining a k-NN implementation with a Pattern Recognition Neural Network (PR NN), our method can accurately forecast upcoming surface stiffness transitions in time to allow for prompt adaptation to the new walking terrain. We also show that the sensor fusion of kinematic and surface electromyographic (EMG) data outperforms single-source inputs producing the best prediction results for all subjects with an accuracy of up to 87.5%.</td>
                <td>Support vector machines, Legged locomotion, Surface impedance, Kinematics, Prediction algorithms, Electromyography, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341783&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341783&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Powered Knee and Ankle Prosthesis Control for Adaptive Ambulation at Variable Speeds, Inclines, and Uneven Terrains</td>
                <td>L. M. Sullivan, S. Creveling, M. Cowan, L. Gabert and T. Lenzi</td>
                <td>2023</td>
                <td>Ambulation in everyday life requires walking at variable speeds, variable inclines, and variable terrains. Powered prostheses aim to provide this adaptability through control of the actuated joints. Some powered prosthesis controllers can adapt to discrete changes in speed and incline but require manual tuning to determine the control parameters, leading to poor clinical viability. Other data-driven controllers can continuously adapt to changes in speed and incline but do so by imposing the same non-amputee gait patterns for all amputee subjects, which does not consider subjective preferences and differing clinical needs of users. Here, we present a controller for powered knee and ankle prostheses that can continuously adapt to different walking speeds, inclines, and uneven terrains without enforcing a specific prosthesis position, impedance, or torque. A virtual biarticular muscle connection determines the knee flexion torque, which changes with both speed and slope. Adaptation to inclines and uneven terrains is based solely on the global shank orientation. Continuously variable damping allows for speed adaptation. Minimum-jerk programming defines the prosthesis swing trajectory at variable cadences. Experiments with one individual with an above-knee amputation suggest that the proposed controller can effectively adapt to different walking speeds, inclines, and rough terrains.</td>
                <td>Legged locomotion, Knee, Torque, Sociology, Trajectory, Impedance, Statistics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342504&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342504&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Motor Unit Action Potential Based Classification of Hand and Arm Motions</td>
                <td>M. D. Twardowski, M. D. Chan, Z. Li, G. De Luca, J. C. Kline and J. P. Chiodini</td>
                <td>2023</td>
                <td>While motion classification architectures have improved in accuracy and robustness in recent years, computationally expensive approaches and sophisticated hardware dependencies limit their real-world applicability. To overcome these challenges, we have designed a lightweight, realtime architecture for classifying motions of the arm & hand using features derived from motor unit action potentials within surface Electromyographic (sEMG) signals, rather than which provide direct interrogation of underlying muscle activation patterns. We tested the architecture on 6 motions performed dynamically across a range of muscle contraction intensities achieving median classification accuracies ranging from 91.3% to 93.3 % and an average processing time of approximately 40 ms across three different classifiers. Taken together, our findings demonstrate potential robustness of motor unit based neural interfaces for motion classification tasks.</td>
                <td>Action potentials, Computer architecture, Muscles, Arms, Robustness, Distance measurement, Pattern recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342444&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342444&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Adjusting the Quasi-Stiffness of an Ankle-Foot Prosthesis Improves Walking Stability during Locomotion over Compliant Terrain</td>
                <td>C. Karakasis, R. Salati and P. Artemiadis</td>
                <td>2023</td>
                <td>Despite significant advances in the design of robotic lower-limb prostheses for individuals with impaired mobility, there is a need for further progress in improving the robustness, safety, and stability of these devices in a wide range of activities of daily living. Although powered prostheses have been able to adapt to different speeds, conditions, and rigid terrains, no control strategies have been proposed for addressing walking over compliant surfaces. This work proposes a continuous admittance controller that adjusts the ankle quasistiffness of a powered ankle-foot prosthesis and improves gait stability during locomotion over compliant terrain. The proposed controller is evaluated with walking experiments on an instrumented treadmill that can accurately change the walking surface stiffness. In these experiments, the proposed controller accurately changes the prosthesis ankle quasi-stiffness across a wide range of $10-20\frac{Nm}{deg}-($ while improving local dynamic stability compared to a standard phase-variable controller. The proposed controller can significantly improve the performance of lower-limb prostheses in dynamic and compliant environments frequently encountered in daily activities, resulting in improved quality of life for people with lower-limb amputation.</td>
                <td>Legged locomotion, Instruments, Green products, Robustness, Safety, Admittance, Standards</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342344&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342344&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Unified Controller for Natural Ambulation on Stairs and Level Ground with a Powered Robotic Knee Prosthesis</td>
                <td>M. Cowan, S. Creveling, L. M. Sullivan, L. Gabert and T. Lenzi</td>
                <td>2023</td>
                <td>Powered lower-limb prostheses have the potential to improve amputee mobility by closely imitating the biomechanical function of the missing biological leg. To accomplish this goal, powered prostheses need controllers that can seamlessly adapt to the ambulation activity intended by the user. Most powered prosthesis control architectures address this issue by switching between specific controllers for each activity. This approach requires online classification of the intended ambulation activity. Unfortunately, any misclassification can cause the prosthesis to perform a different movement than the user expects, increasing the likelihood of falls and injuries. Therefore, classification approaches require near-perfect accuracy to be used safely in real life. In this paper, we propose a unified controller for powered knee prostheses which allows for walking, stair ascent, and stair descent without the need for explicit activity classification. Experiments with one individual with an above-knee amputation show that the proposed controller enables seamless transitions between activities. Moreover, transition between activities is possible while leading with either the sound-side or the prosthesis. A controller with these characteristics has the potential to improve amputee mobility.</td>
                <td>Legged locomotion, Knee, Sociology, Switches, Stairs, Control systems, Statistics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341691&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341691&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Volitional EMG Control Enables Stair Climbing with a Robotic Powered Knee Prosthesis</td>
                <td>S. Creveling, M. Cowan, L. M. Sullivan, L. Gabert and T. Lenzi</td>
                <td>2023</td>
                <td>Existing controllers for robotic powered prostheses regulate the prosthesis speed, timing, and energy generation using predefined position or torque trajectories. This approach enables climbing stairs step-over-step. However, it does not provide amputees with direct volitional control of the robotic prosthesis, a functionality necessary to restore full mobility to the user. Here we show that proportional electromyographic (EMG) control of the prosthesis knee torque enables volitional control of a powered knee prosthesis during stair climbing. The proposed EMG controller continuously regulates knee torque based on activation of the residual hamstrings, measured using a single EMG electrode located within the socket. The EMG signal is mapped to a desired knee flexion/extension torque based on the prosthesis knee position, the residual limb position, and the interaction with the ground. As a result, the proposed EMG controller enabled an above-knee amputee to climb stairs at different speeds, while carrying additional loads, and even backwards. By enabling direct, volitional control of powered robotic knee prostheses, the proposed EMG controller has the potential to improve amputee mobility in the real world.</td>
                <td>Torque, Sockets, Stairs, Electromyography, Trajectory, Timing, Torque measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341615&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341615&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Development and Online Validation of an Intrinsic Fault Detector for a Powered Robotic Knee Prosthesis</td>
                <td>C. Lee and H. H. Huang</td>
                <td>2023</td>
                <td>Robotic prosthetic legs have the potential to significantly improve the quality of life for lower limb amputees to perform locomotion in various environments and task conditions. However, these devices lack the capability to recover from internal intrinsic control faults, which can lead to harmful consequences affecting the user's gait performance and eroding trust in these robotic devices. Therefore, a reliable fault detection system is necessary to detect intrinsic faults in a timely manner and provide a compensatory response to mitigate their effects. This paper focuses on designing an active fault detector for a robotic knee prosthesis and demonstrates its effectiveness in real time. The developed system utilizes a Gaussian Process model to estimate knee angular velocity, which is sensitive to intrinsic faults and relies on the difference between estimated velocity and the actual measurement to detect internal control faults. In an offline analysis, the developed detector demonstrated a higher detection rate, lower false alarm ratio, and faster detection time compared with the two approaches reported previously. An online demonstration was also conducted with a unilateral amputee participant and showed performance similar to that of offline analysis. We expect that this detector can be integrated into a fault tolerance strategy to enhance the reliability and safety of robotic prosthetic legs, enabling users to perform their everyday tasks with greater confidence.</td>
                <td>Legged locomotion, Knee, Performance evaluation, Fault detection, Detectors, Robot sensing systems, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342433&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342433&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Collision-Free Reconfiguration Planning for Variable Topology Trusses Using a Linking Invariant</td>
                <td>A. Spinos and M. Yim</td>
                <td>2023</td>
                <td>We introduce a multi-modal reconfiguration planner for the Variable Topology Truss (VTT) modular robot system. The VTT system is a truss-architecture modular self-reconfigurable robot. When a VTT is restricted to a single topology, the collision constraints between the truss members divide the configuration space into many connected components, which makes collision-free planning difficult. This new planner leverages a mathematical invariant based on link theory to find topological reconfiguration actions that can connect these different regions and make progress towards a goal. We show that this planner is effective at finding paths between configurations with different truss topologies.</td>
                <td>Kinematics, Topology, Planning, Labeling, Collision avoidance, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341927&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341927&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hybrid Map-Based Path Planning for Robot Navigation in Unstructured Environments</td>
                <td>J. Liu, X. Chen, J. Xiao, S. Lin, Z. Zheng and H. Lu</td>
                <td>2023</td>
                <td>Fast and accurate path planning is important for ground robots to achieve safe and efficient autonomous navigation in unstructured outdoor environments. However, most existing methods exploiting either 2D or 2.5D maps struggle to balance the efficiency and safety for ground robots navigating in such challenging scenarios. In this paper, we propose a novel hybrid map representation by fusing a 2D grid and a 2.5D digital elevation map. Based on it, a novel path planning method is proposed, which considers the robot poses during traversability estimation. By doing so, our method explicitly takes safety as a planning constraint enabling robots to navigate unstructured environments smoothly. The proposed approach has been evaluated on both simulated datasets and a real robot platform. The experimental results demonstrate the efficiency and effectiveness of the proposed method. Compared to state-of-the-art baseline methods, the proposed approach consistently generates safer and easier paths for the robot in different unstructured outdoor environments. The implementation of our method is publicly available at https://github.com/nubot-nudt/T-Hybrid-planner.</td>
                <td>Point cloud compression, Navigation, Robot kinematics, Path planning, Hybrid power systems, Safety, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341666&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341666&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CDT-Dijkstra: Fast Planning of Globally Optimal Paths for All Points in 2D Continuous Space</td>
                <td>J. Liu, M. Fu, W. Zhang, B. Chen, R. Prakapovich and U. Sychou</td>
                <td>2023</td>
                <td>The Dijkstra algorithm is a classic path planning method, which in a discrete graph space, can start from a specified source node and find the shortest path between the source node and all other nodes in the graph. However, to the best of our knowledge, there is no effective method that achieves a function similar to that of the Dijkstra's algorithm in a continuous space. In this study, an optimal path planning algorithm called convex dissection topology (CDT)-Dijkstra is developed, which can quickly compute the global optimal path from one point to all other points in a 2D continuous space. CDT-Dijkstra is mainly divided into two stages: SetInit and GetGoal. In SetInit, the algorithm can quickly obtain the optimal CDT encoding set of all the cut lines based on the initial point Xinit. In GetGoal, the algorithm can return the global optimal path of any goal point at an extremely high speed. In this study, we propose and prove the planning principle of considering only the points on the cutlines, thus reducing the state space of the distance optimal path planning task from 2D to 1D. In addition, we propose a fast method to find the optimal path in a homogeneous class and theoretically prove the correctness of the method. Finally, by testing in a series of environments, the experimental results demonstrate that CDT-Dijkstra not only plans the optimal path from all points at once, but also has a significant advantage over advanced algorithms considering certain complex tasks.</td>
                <td>Path planning, Encoding, Planning, Topology, Task analysis, Optimization, Intelligent robots, Path Planning, Topology, Path Homotopy, Optimal Distance, Mobile Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341744&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341744&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Large Scale Pursuit-Evasion Under Collision Avoidance Using Deep Reinforcement Learning</td>
                <td>H. Yang, P. Ge, J. Cao, Y. Yang and Y. Liu</td>
                <td>2023</td>
                <td>This paper examines a pursuit-evasion game (PEG) involving multiple pursuers and evaders. The decentralized pursuers aim to collaborate to capture the faster evaders while avoiding collisions. The policies of all agents are learning-based and are subjected to kinematic constraints that are specific to unicycles. To address the challenge of high dimensionality encountered in large-scale scenarios, we propose a state processing method named Mix-Attention, which is based on Self-Attention. This method effectively mitigates the curse of dimensionality. The simulation results provided in this study demonstrate that the combination of Mix-Attention and Independent Proximal Policy Optimization (IPPO) surpasses alternative approaches when solving the multi-pursuer multi-evader PEG, particularly as the number of entities increases. Moreover, the trained policies showcase their ability to adapt to scenarios involving varying numbers of agents and obstacles without requiring retraining. This adaptability showcases their transferability and robustness. Finally, our proposed approach has been validated through physical experiments conducted with six robots.</td>
                <td>Deep learning, Simulation, Kinematics, Reinforcement learning, Games, Robustness, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341975&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341975&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments</td>
                <td>J. A. Ansari, S. Tourani, G. Kumar and B. Bhowmick</td>
                <td>2023</td>
                <td>This work proposes a novel approach to social robot navigation by learning to generate robot controls from a social motion latent space. By leveraging this social motion latent space, the proposed method achieves significant improvements in social navigation metrics such as success rate, navigation time, and trajectory length while producing smoother (less jerk and angular deviations) and more anticipatory trajectories. The superiority of the proposed method is demonstrated through comparison with baseline models in various scenarios. Additionally, the concept of humans' awareness towards the robot is introduced into the social robot navigation framework, showing that incorporating human awareness leads to shorter and smoother trajectories owing to humans' ability to positively interact with the robot.</td>
                <td>Measurement, Navigation, Social robots, Robot control, Aerospace electronics, Trajectory, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341721&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341721&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DS-MPEPC: Safe and Deadlock-Avoiding Robot Navigation in Cluttered Dynamic Scenes</td>
                <td>S. H. Arul, J. J. Park and D. Manocha</td>
                <td>2023</td>
                <td>We present an algorithm for safe robot navigation in complex dynamic environments using a variant of model predictive equilibrium point control. We use an optimization formulation to navigate robots gracefully in dynamic environments by optimizing over a trajectory cost function at each timestep. We present a novel trajectory cost formulation that significantly reduces conservative and deadlocking behaviors and generates smooth trajectories. In particular, we propose a new collision probability function that effectively captures the risk associated with a given configuration and the time to avoid collisions based on the velocity direction. Moreover, we propose a terminal state cost based on the expected time-to-goal and time-to-collision values that helps in avoiding trajectories that could result in deadlock. We evaluate our cost formulation in multiple simulated scenarios, including narrow corridors with dynamic obstacles, and observe significantly improved navigation behavior and reduced deadlocks as compared to prior methods.</td>
                <td>Costs, Navigation, Heuristic algorithms, System recovery, Predictive models, Prediction algorithms, Cost function</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341869&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341869&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Path-Following Control with Path and Orientation Snap-In</td>
                <td>Nesic, E. Pritzi and A. Kugi</td>
                <td>2023</td>
                <td>Robots need to be as simple to use as tools in a workshop and allow non-experts to program, modify and execute tasks. In particular for repetitive tasks in high-mix/low-volume production, robotic support and physical human-robot interaction (pHRI) help to significantly increase productivity. In path-following control (PFC), the geometric description of the path is decoupled from the time evolution of the robot's end-effector along the path. PFC is inherently suitable for pHRI since path progress can be derived from the interaction with the human. In this work, an extension to multi-path PFC is proposed, which allows smooth transitions between the paths initiated by the human. Additionally, two pHRI modes called path snap-in and orientation snap-in are proposed, which use attractive forces to snap the robot end-effector onto a path or a predefined orientation. Moreover, the stability properties of PFC are inherited and the method is applicable to linear, nonlinear and self-intersecting paths. The proposed pHRI modes are validated on an experimental drilling task for teach-in (using orientation snap-in) and execution (using path snap-in) with the kinematically redundant collaborative robot Kuka Lbr iiwa 14 R820.</td>
                <td>Drilling, Productivity, Damping, Robot kinematics, Human-robot interaction, Collaboration, End effectors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341392&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341392&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Control of a Reluctance-Based Micropositioning Stage for Scanning Motion Applications</td>
                <td>M. Al Saaideh, N. Alatawneh, K. Aljanaideh and M. Al Janaideh</td>
                <td>2023</td>
                <td>This paper presents a design and characterization of a micropositioning stage driven by a reluctance actuator. The stage is constructed with a C-core reluctance actuator and four compression springs. The design of the stage is presented using a CAD model, followed by the fabrication process of the prototype. The mathematical model is formulated to present the interaction among the stage's electrical, magnetic, and mechanical dynamic behaviour. Next, the force-current and force-gap characteristics are obtained by measuring the force under different applied currents and air gaps. After that, the system is analyzed to determine the maximum applied voltage that stabilizes the system in an open-loop configuration, followed by the time-domain and frequency-domain response. Finally, the feedforward controller is presented to linearize the dynamic behavior of the stage over a specific range of motion. The experimental results under the feedforward controller show a linear characteristic between the desired force and the output displacement.</td>
                <td>Actuators, Voltage measurement, Current measurement, Force, Dynamics, Random access memory, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341381&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341381&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Body Posture Controller for Actively Articulated Tracked Vehicles Moving Over Rough and Unknown Terrains</td>
                <td>F. Rocha et al.</td>
                <td>2023</td>
                <td>Terrestrial mobile robots face diverse topographies while in field missions. Rough terrains cause the platform to oscillate, which is undesirable for some tasks. Robotic platforms with active tracked flippers can use such mechanisms to reach and maintain a leveled configuration while halted or moving. Thus, this work presents a posture controller that regulates the robot's orientation and contact plane clearance using flippers while the robot moves over unknown, uneven ground. The method takes as input the flippers' joint position, torque, and the robot chassis orientation, outputting as the command signal the flippers' joint velocities. Based on Stewart platforms, a differential kinematics model relates desired platform's motion to flippers' frame velocities. Later, a flippers-ground interaction model transforms their frames' computed velocities to flippers' joint speed commands. The controller is based on dual-quaternion algebra for generating the error signal. The efficacy of the proposed controller is evaluated experimentally in an industrial robotic platform moving as it moves along an open field track. The method successfully regulates the robot's posture while navigating over non-modeled rough terrain.</td>
                <td>Torque, Three-dimensional displays, Tracking, Service robots, Navigation, Kinematics, Transforms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341528&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341528&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Exploring Learning-Based Control Policy for Fish-Like Robots in Altered Background Flows</td>
                <td>X. Lin, W. Song, X. Liu, X. He and Y. Wang</td>
                <td>2023</td>
                <td>The study of motion control for the fish-like robots in complex fluid fields is of great importance in improving the performance of underwater vehicles, due to its strong maneuverability, propulsion efficiency, and deceptive visual appearance. In this article, a novel learning-based control framework is first proposed to autonomously explore efficient control policies that are capable of performing motion control tasks in non-quiescent and unknown background flows. First, we utilize a high-fidelity simulation system, named FishGym, to generate various uniform flows. Next, a DRL-based algorithm is incorporated with the FishGym to train the fish-like robot to control its motion to optimally complete a delicately designed task (Approaching Target and Stay) in both quiescent and uniform flow. Then, the obtained control policy together with an online estimator is directly applied to a Path-Following Task. The proposed framework well balances the simulation accuracy and the computational efficiency, which is of crucial importance for effective coupling with the learning algorithm. The simulation results indicate that, via the proposed learning framework, the robot successfully acquired a swimming strategy that can be used to adapt to different background flows and tasks. Furthermore, we also observe some adaptation behavior of the robot, such as rheotaxis, that is similar to the fish in nature, which gains us more insight into the mechanism underlying the adaptation behavior of fish in a complex environment.</td>
                <td>Visualization, Simulation, Propulsion, Fish, Behavioral sciences, Task analysis, Motion control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342394&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342394&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>On the Design of Region-Avoiding Metrics for Collision-Safe Motion Generation on Riemannian Manifolds</td>
                <td>H. Klein, N. Jaquier, A. Meixner and T. Asfour</td>
                <td>2023</td>
                <td>The generation of energy-efficient and dynamic-aware robot motions that satisfy constraints such as joint limits, self-collisions, and collisions with the environment remains a challenge. In this context, Riemannian geometry offers promising solutions by identifying robot motions with geodesics on the so-called configuration space manifold. While this manifold naturally considers the intrinsic robot dynamics, constraints such as joint limits, self-collisions, and collisions with the environment remain overlooked. In this paper, we propose a modification of the Riemannian metric of the configuration space manifold allowing for the generation of robot motions as geodesics that efficiently avoid given regions. We introduce a class of Riemannian metrics based on barrier functions that guarantee strict region avoidance by systematically generating accelerations away from no-go regions in joint and task space. We evaluate the proposed Riemannian metric to generate energy-efficient, dynamic-aware, and collision-free motions of a humanoid robot as geodesics and sequences thereof.</td>
                <td>Measurement, Manifolds, Robot motion, Geometry, Humanoid robots, Energy efficiency, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341266&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341266&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Connecting Control to Perception: High-Performance Whole-Body Collision Avoidance Using Control-Compatible Obstacles</td>
                <td>M. Eckhoff, D. Knobbe, H. Zwirnmann, A. Swikir and S. Haddadin</td>
                <td>2023</td>
                <td>One of the most important aspects of autonomous systems is safety. This includes ensuring safe human-robot and safe robot-environment interaction when autonomously performing complex tasks or in collaborative scenarios. Al-though several methods have been introduced to tackle this, most are unsuitable for real-time applications and require carefully handcrafted obstacle descriptions. In this work, we propose a method combining high-frequency and real-time self and environment collision avoidance of a robotic manipulator with low-frequency, multimodal, and high-resolution environmental perceptions accumulated in a digital twin system. Our method is based on geometric primitives, so-called primitive skeletons. These, in turn, are information-compressed and real-time compatible digital representations of the robot's body and environment, automatically generated from ultra-realistic virtual replicas of the real world provided by the digital twin. Our approach is a key enabler for closing the loop between environment perception and robot control by providing the millisecond real-time control stage with a current and accurate world description, empowering it to react to environmental changes. We evaluate our whole-body collision avoidance on a 9-DOFs robot system through five experiments, demonstrating the functionality and efficiency of our framework.</td>
                <td>Robot control, Digital representation, Manipulators, Real-time systems, Skeleton, Digital twins, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342515&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342515&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Whole-Body Collision Avoidance and Path Following of a Snake Robot Through MPC-based Optimization Strategies</td>
                <td>L. Wang et al.</td>
                <td>2023</td>
                <td>The work in this paper delves into the challenge of whole elongated body's obstacle avoidance during path following for a class of bionic snake robots. Currently, most studies focus solely on preventing the robot's head from colliding with obstacles through designed controllers. However, due to the unique elongated structure and biomimetic locomotion modes of snake robots, it is unavoidable that the rest of the robot's body could still collide with obstacles. To resolve this problem, we propose a novel real-time optimization obstacle avoidance strategy for a class of terrestrial snake robots with multi-link elongated body using model predictive control (MPC). Moreover, by leveraging the elongated body characteristics of the robot, an improved path guidance strategy is also developed. The effectiveness of the proposed strategies is verified and validated through extensive simulations and experiments on a custom-built nine-link elongated snake robot. The results demonstrate that all links of the robot can well avoid obstacles while continuing to track the given path.</td>
                <td>Target tracking, Friction, Snake robots, Predictive models, Real-time systems, Collision avoidance, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342404&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342404&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Safety-Critical Coordination for Cooperative Legged Locomotion via Control Barrier Functions</td>
                <td>J. Kim, J. Lee and A. D. Ames</td>
                <td>2023</td>
                <td>This paper presents a safety-critical approach to the coordinated control of cooperative robots locomoting in the presence of fixed (holonomic) constraints. To this end, we leverage control barrier functions (CBFs) to ensure the safe cooperation of the robots while maintaining a desired formation and avoiding obstacles. The top-level planner generates a set of feasible trajectories, accounting for both kinematic constraints between the robots and physical constraints of the environment. This planner leverages CBFs to ensure safety-critical coordination control, i.e., guarantee safety of the collaborative robots during locomotion. The middle-level trajectory planner incorporates interconnected single rigid body (SRB) dynamics to generate optimal ground reaction forces (GRFs) to track the safety-ensured trajectories from the top-level planner while addressing the interconnection dynamics between agents. Distributed low-level controllers generate whole-body motion to follow the prescribed optimal GRFs while ensuring the friction cone condition at each end of the stance legs. The effectiveness of the approach is demonstrated through numerical simulations and experimentally on a pair of quadrupedal robots.</td>
                <td>Legged locomotion, Tracking, Robot kinematics, Dynamics, Kinematics, Numerical simulation, Reduced order systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341987&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341987&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Staged Contact Optimization: Combining Contact-Implicit and Multi-Phase Hybrid Trajectory Optimization</td>
                <td>M. R. Turski, J. Norby and A. M. Johnson</td>
                <td>2023</td>
                <td>Trajectory optimization problems for legged robots are commonly formulated with fixed contact schedules. These multi-phase Hybrid Trajectory Optimization (HTO) methods result in locally optimal trajectories, but the result depends heavily upon the predefined contact mode sequence. Contact-Implicit Optimization (CIO) offers a potential solution to this issue by allowing the contact mode to be determined throughout the trajectory by the optimization solver. However, CIO suffers from long solve times and convergence issues. This work combines the benefits of these two methods into one algorithm: Staged Contact Optimization (SCO). SCO tightens constraints on contact in stages, eventually fixing them to allow robust and fast convergence to a feasible solution. Results on a planar biped and spatial quadruped demonstrate speed and optimality improvements over CIO and HTO. These properties make SCO well suited for offline trajectory generation or as an effective tool for exploring the dynamic capabilities of a robot.</td>
                <td>Legged locomotion, Schedules, Visualization, Cost function, Finite element analysis, Behavioral sciences, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342296&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342296&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hierarchical Relaxation of Safety-critical Controllers: Mitigating Contradictory Safety Conditions with Application to Quadruped Robots</td>
                <td>J. Lee, J. Kim and A. D. Ames</td>
                <td>2023</td>
                <td>The safety-critical control of robotic systems often must account for multiple, potentially conflicting, safety constraints. This paper proposes novel relaxation techniques to address safety-critical control problems in the presence of conflicting safety conditions. In particular, Control Barrier Functions (CBFs) provide a means to encode safety as constraints in a Quadratic Program (QP), wherein multiple safety conditions yield multiple constraints. However, the QP problem becomes infeasible when the safety conditions cannot be simultaneously satisfied. To resolve this potential infeasibility, we introduce a hierarchy between the safety conditions and employ an additional variable to relax the less important safety conditions (Relaxed-CBF-QP). We also formulate a cascaded structure to achieve smaller violations of lower-priority safety conditions (Hierarchical-CBF-QP). The proposed approach, therefore, ensures the existence of at least one solution to the QP problem with the CBFs while dynamically balancing enforcement of additional safety constraints. Importantly, this paper evaluates the impact of different weighting factors in the Hierarchical-CBF-QP and, due to the sensitivity of these weightings in the observed behavior, proposes a method to determine the weighting factors via a sampling-based technique. The validity of the proposed approach is demonstrated through simulations and experiments on a quadrupedal robot navigating to a goal through regions with different levels of danger.</td>
                <td>Legged locomotion, Uncertainty, Sensitivity, Navigation, Control systems, Safety, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342094&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342094&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design of Novel Knee Joint Mechanism of Lower-Limb Exoskeleton to Realize Spatial Motion of Human Knee</td>
                <td>M. B. Hong, Y. Kim, G. T. Kim, M. Lee and S. Kim</td>
                <td>2023</td>
                <td>The rotation axis of human knee joint varies according to knee flexion angles. That is, human knee movement is spatial with dominant flexional rotation. Knee joints of most lower-limb exoskeletons were, however, realized with a simple revolute pair for design simplicity. Wearing the knee joint with a simple revolute pair constrains inevitably natural parasitic motion of human knee joint. Rigid constraints imposed due to the simple knee joint lowers wearability and comfortability. In addition, it may act as potential risks to harm the knee joint structure of the wearer. The concept of polycentric knee is a well-known approach to mimic the variation of knee rotation center. Polycentric knees, however, realize only the planar trace of projected points of rotation axis. That is, parasitic knee rotations of varus and internal rotation occuring naturally during knee flexion cannot be realized by polycentric knees. In order to resolve this, a novel spherical knee joint of exoskeleton is introduced in this paper to realize knee spherical movements. For the design, change in instantaneous rotation axes during knee flexion is formulated to a spherical trace as a function of knee flexion angles. A spherical four-bar linkage is suggested to realize the axis trajectory, not the point trace. A method to find instantaneous rotation axis and rotation matrix of coupler link is derived, when the angle of input link is given. Using the kinematic relations, kinematic parameters of the mechanism are optimized to minimize angle difference between the instantaneous axis of coupler and the required axis of knee rotation. Finally, based on the synthesized kinematic parameters, a prototype design was introduced in this paper.</td>
                <td>Industries, Couplings, Transmission line matrix methods, Exoskeletons, Government, Prototypes, Kinematics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342378&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342378&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Novel Coiled Cable-Conduit-Driven Hyper-Redundant Manipulator for Remote Operating in Narrow Spaces</td>
                <td>M. Luo et al.</td>
                <td>2023</td>
                <td>Operating in narrow spaces is an important challenge in the development of robots. Redundant manipulators are one way to solve this problem, but their mechanism design and control method still have much room for improvement. In this paper, we propose a coiled cable-conduit-driven hyper-redundant manipulator (C-CDHRM) with great slenderness and flexibility. In terms of mechanism design, it considers both compactness and operability. By imitating the structure and behavior of a constricting snake, it can be uncoiled sequentially from a coiled storage state, led by the head. In terms of control methods, we propose a multi-layer control system that can make remote operations more accurate and reliable. On the one hand, guiding, segmenting, and following the path overcome the planning ambiguity caused by redundancy. On the other hand, conduit transmission modeling and cable length correction overcome the nonlinear mapping of cable-driven joints and were verified in experiments. Through tests, the mobile integrated system composed of C-CDHRM has an excellent performance in operation precision and accuracy, ensuring safety and accessibility in narrow spaces. Finally, in field experiments, the inspection and cleaning of various types of electrical equipment have been successfully completed, showing excellent application prospects.</td>
                <td>Design methodology, Redundancy, Inspection, Manipulators, Control systems, Cleaning, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341359&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341359&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Testing of a Flexure-Based XYZ Micropositioner with High Space-Utilization Efficiency</td>
                <td>Z. Lyu and Q. Xu</td>
                <td>2023</td>
                <td>The flexure-based XYZ micropositioner with hybrid configuration has become more prevalent due to the characteristics of less mechanism decoupling and high motion precision. However, traditional mechanism design suffers from a large plane occupation with Z stage stacking, which leads to a low space-utilization efficiency. To address this issue, a novel conceptual design is proposed in this paper by integrating a spatially structured XY stage and an embedded Z stage together. After completing the design of the mechanism, the driving stiffness of the stage in three axes is evaluated by the mechanics analysis. Then, the model is verified by performing finite element analysis simulation study and experimental test. The theoretical model, simulation results, and experimental data indicate a good agreement. Experimental results show that the proposed flexure-based XYZ micropositioner can deliver a stroke of $\boldsymbol{4.15 \text{mm} \times 4.06 \text{mm} \times 0.04}$ mm with a physical size of $\boldsymbol{116 \text{mm} \times 116 \text{mm} \times 45 \text{mm}. }$ The performance comparison reveals that it has a superior space-utilization efficiency. In consideration of the feasibility of the proposed conceptual design, it provides a reference for diversified and refined design of XYZ micropositioners.</td>
                <td>Analytical models, Simulation, Stacking, Data models, Finite element analysis, Behavioral sciences, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341380&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341380&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Development of a Deformable In-Pipe Inspection Robot for Various Diameter Pipes</td>
                <td>H. Xu, J. Cao, Z. Cheng, Z. Liang and J. Chen</td>
                <td>2023</td>
                <td>Pipelines have become one of the most important infrastructures in the city. Over time, they are prone to aging, cracks, corrosion, and the demand for regular inspection is gradually increasing. Robotic solutions are effective methods for in-pipe inspection. However, existing In-pipe Inspection Robots (IPIR) require that the inner diameter of the pipe is fixed in the application scenarios, and need extra labor to control the robot and handle the cable. In this work, we design and develop a deformable robot to adapt to pipes with different inner diameters. Specifically, the passive elastic hinge is used by us to make the robot fully in contact with the pipe, generating enough friction to ensure that the robot is attached to the inner wall of the pipe. An edge device is deployed on the robot, generating velocity commands of wheels through the data from Inertial Measurement Unit (IMU), which eliminates the need for external devices. Experimental results demonstrate that the robot can move in horizontal and vertical pipelines, as well as traverse through pipe joints and scenarios where there is dirty or small obstacle.</td>
                <td>Measurement units, Friction, Pipelines, Urban areas, Wheels, Inertial navigation, Inspection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341512&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341512&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Bioinspired Underactuated Dual Tendon-Based Adaptive Gripper for Space Applications</td>
                <td>Meziani, S. Davis and H. Isakhani</td>
                <td>2023</td>
                <td>Hands are one of the most intricate elements of a humanoid due to their role as end-effectors interacting with their non-linear surrounding environment. This paper aims to present the design of a bioinspired underactuated robotic hand with an improved dexterity that is capable of adaptive grasping and manipulation of a wide-range of objects using a dual-tendon mechanism. The proposed design is focused on the key elements of scalability, modularity, ease of fabrication and cost efficiency to meet several imperative constraints of space applications. These features are achieved by introducing a novel actuation mechanism, manufacturing methods, and component design. In particular, monolithic finger modules are fabricated by fusing and integrating both hard and soft materials analogous to bones wrapped in muscles using economical and readily-available materials and machines (intermediate 3D printer). Weight-to-power ratio, actuation optimisation, design trade-offs, and various potential applications of the proposed adaptive hand is discussed in this paper. Furthermore, the prototype is subjected to evaluation of its performance in different scenarios that ultimately confirms its improved dexterity and gripping power compared to the literature.</td>
                <td>Fabrication, Three-dimensional displays, Scalability, Force, Grasping, Robustness, Topology</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342142&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342142&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Parallel-Jaw Gripper and Grasp Co-Optimization for Sets of Planar Objects</td>
                <td>R. H. Jiang, N. Doshi, R. Gondhalekar and A. Rodriguez</td>
                <td>2023</td>
                <td>We propose a framework for optimizing a planar parallel-jaw gripper for use with multiple objects. While optimizing general-purpose grippers and contact locations for grasps are both well studied, co-optimizing grasps and the gripper geometry to execute them receives less attention. As such, our framework synthesizes grippers optimized to stably grasp sets of polygonal objects. Given a fixed number of contacts and their assignments to object faces and gripper jaws, our framework optimizes contact locations along these faces, gripper pose for each grasp, and gripper shape. Our key insights are to pose shape and contact constraints in frames fixed to the gripper jaws, and to leverage the linearity of constraints in our grasp stability and gripper shape models via an augmented Lagrangian formulation. Together, these enable a tractable nonlinear program implementation. We apply our method to several examples. The first illustrative problem shows the discovery of a geometrically simple solution where possible. In another, space is constrained, forcing multiple objects to be contacted by the same features as each other. Finally a toolset-grasping example shows that our framework applies to complex, real-world objects. We provide a physical experiment of the toolset grasps.</td>
                <td>Measurement, Geometry, Shape, Linearity, Grasping, Stability analysis, Grippers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342241&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342241&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Inertial Propulsion Robot Using the Shape Characteristics of a Streamlined Body Frame</td>
                <td>M. Nishihara and F. Asano</td>
                <td>2023</td>
                <td>We have been investigating a crawling-like loco-motion robot to make it efficiently slide forward based on a simple system and control mechanisms on a slippery level surface, where the motion of the center of mass plays an important role. In this paper, we induce an effective motion of the center of mass considering a streamlined body shape of a locomotion robot in which a pendulum is installed. First, we derive the equation of motion and a control input to achieve a desired motion of the inner pendulum. Second, we formulate constraint conditions between the streamlined body and a slippery floor. Third, we demonstrate the numerical simulation, and the robot steadily slides forward by adopting the streamlined shape as the body frame. Fourth, we verify the numerical results through experiments, and the experimental results exhibit a similar tendency compared with the numerical results. Fifth, we find a local minimum value of locomotion efficiency based on Bayesian optimization which is a class of machine-learning-based optimization, and we achieve exceedingly efficient locomotion of the robot on the slippery floor at the local minimum in both the simulation and experiment.</td>
                <td>Shape, Propulsion, Numerical simulation, Control systems, Mathematical models, Bayes methods, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342508&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342508&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Two-Stage Trajectory-Tracking Control of Cable-Driven Upper-Limb Exoskeleton Robots with Series Elastic Actuators: A Simple, Accurate, and Force-Sensorless Method</td>
                <td>Y. Shu et al.</td>
                <td>2023</td>
                <td>The advantages of cable-driven exoskeleton robots with series elastic actuators can be summarized in twofold: 1) the inertia of the robot joint is relatively low, which is more friendly for human-robot interaction; 2) the elastic element is tolerant to impacts and hence provides structural safety. As trade-offs, the overall dynamic model of such a system is of high order and subject to both unmodelled disturbances (due to the cable-driven mechanism) and external torques (due to the human-robot interaction), opening up challenges for the controller development. This paper proposes a new trajectory-tracking control scheme for cable-driven upper-limb exoskeleton robots with series elastic actuators. The control objectives are achieved in two stages: Stage I is to approximate then compensate for unmodelled disturbances with iterative learning techniques; Stage II is to employ a suboptimal model predictive controller to drive the robot to track the desired trajectory. While controlling such a robot is not trivial, the proposed control scheme exhibits the advantages of force-sensorlessness, high accuracy, and low complexity compared with other methods in the real-world experiments.</td>
                <td>Training, Actuators, Exoskeletons, Human-robot interaction, Drives, Trajectory, Complexity theory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342056&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342056&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Retractable Soft Growing Robot with a Flexible Backbone</td>
                <td>X. Pi, I. A. Szczech and L. Cao</td>
                <td>2023</td>
                <td>Soft-growing robots are emerging with numerous potential applications because of their superior capability of frictionless navigation. However, their success is hindered by their tendency to buckle under the tension required to retract them via inversion. In this paper, we propose a simple and scalable tubular backbone to facilitate retracting the robot body without buckling. With this backbone, compressive forces at the robot's tip are mitigated and a limit is placed on the effective length for retraction during the application of tension. We first present the selection of the backbone and the development of such a retractable soft-growing robot. Along with the characterization of the working principles behind this buckling-free mechanism, success was observed with the use of the backbone in retraction tests. The effects of different parameters such as robot body lengths, air pressures, curvatures, and retraction modes on the performance were also investigated. This backbone approach requires no bulky or in-situ mechatronic components inside the robot body and thus may be used in medical applications which appreciate simple, compact, and in-situ electronic-free designs.</td>
                <td>Biomedical equipment, Mechatronics, Navigation, Medical services, Robots, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342214&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342214&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CurveQuad: A Centimeter-Scale Origami Quadruped that Leverages Curved Creases to Self-Fold and Crawl with One Motor</td>
                <td>D. Feshbach et al.</td>
                <td>2023</td>
                <td>We present CurveQuad, a miniature curved origami quadruped that is able to self-fold and unfold, crawl, and steer, all using a single actuator. CurveQuad is designed for planar manufacturing, with parts that attach and stack sequentially on a flat body. The design uses 4 curved creases pulled by 2 pairs of tendons from opposite ends of a link on a 270°servo. It is 8 cm in the longest direction and weighs 10.9 g. Rotating the horn pulls the tendons inwards to induce folding. Continuing to rotate the horn shears the robot, enabling the robot to shuffle forward while turning in either direction. We experimentally validate the robot's ability to fold, steer, and unfold by changing the magnitude of horn rotation. We also demonstrate basic feedback control by steering towards a light source from a variety of starting positions and orientations, and swarm aggregation by having 4 robots simultaneously steer towards the light. The results demonstrate the potential of using curved crease origami in self-assembling and deployable robots with complex motions such as locomotion.</td>
                <td>Legged locomotion, Robot sensing systems, Turning, Surface roughness, Sensor systems, Manufacturing, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342339&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342339&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Pendulum-Driven Legless Rolling Jumping Robot</td>
                <td>J. Buzhardt, P. Chivkula and P. Tallapragada</td>
                <td>2023</td>
                <td>In this paper, we present a novel rolling, jumping robot. The robot consists of a driven pendulum mounted to a wheel in a compact, lightweight, 3D printed design. We show that by driving the pendulum to shift the robot's weight distribution, the robot is able to obtain significant rolling speed, achieve jumps of up to 2.5 body lengths vertically, and clear horizontal distances of over 6 body lengths. The robot's dynamic model is derived and simulation results indicate that it is consistent with the rolling motion and jumping observed on the robot. The ability to both roll and jump effectively using a minimalistic design makes this robot unique and could inspire the use of similar mechanisms on robots intended for applications in which agile locomotion on unstructured terrain is necessary, such as disaster response or planetary exploration.</td>
                <td>Solid modeling, Three-dimensional displays, Trajectory planning, Simulation, Robot vision systems, Wheels, Angular velocity</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342513&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342513&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Vine Robot Localization Via Collision</td>
                <td>Miranda, A. Srivastava, S. Wang and L. H. Blumenschein</td>
                <td>2023</td>
                <td>Localization of robots is a complex task that is often hindered by the sensors these systems use. Due to the majority of field robots being rigid, most of these sensing modalities have the same common faults, such as performance being hindered when their camera vision is obscured. In addition, rigid systems lack flexibility when traversing multiple environments: especially when traversing uneven and unpredictable ground. Soft robots, which can adaptably interact with the environment, could serve as a solution to both problems. One specific soft robot, the Vine Robot, has exhibited excellent performance while moving through constrained, unpredictable environments. This makes the Vine Robot an ideal candidate for a novel method of sensing and localizing in environments, obstacle collision localization. We use our understanding of the nature of Vine Robot motion to be able to predict the tip position of the robot at every instant based on sensor feedback. Through the single obstacle experiments, it was found that our algorithm can provide a precise picture of the tip position of the robot in differing environments. Further, in a multi obstacle demonstration, less than 5% max error relative to the full robot length was observed on the path prediction. Our study helps lay the foundation for a new method for Vine Robot localization using contact as a new sensing modality.</td>
                <td>Location awareness, Robot vision systems, Soft robotics, Robot sensing systems, Robot localization, Sensor systems, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342238&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342238&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Mapping Unknown Environments through Passive Deformation of Soft, Growing Robots</td>
                <td>F. Fuentes and L. H. Blumenschein</td>
                <td>2023</td>
                <td>When faced with an unstructured environment filled with an unknown number and size of obstacles on a chaotic terrain, it can be a challenge to determine the best method for navigating and mapping the space. This problem, known as Simultaneous Localization and Mapping (SLAM), has typically been approached using vision-based solutions, but these solutions require clear visual conditions in order to function optimally. A different approach to sensing environments has been explored in soft robotic systems, specifically by sensing changes in the environment through sensing changes in the robot's configuration. Building on this idea, we introduce a method of mapping based on colliding with and deforming around obstacles using a soft, growing robot. Instead of avoiding obstacles, as is typically done to protect robots, we take advantage of the soft, growing robot's compliance in order to navigate through, and collect information about, the environment. Through the construction and testing of a geometry-based simulation, we analyzed the behavior and ef-fectiveness of this approach for mapping by generating random launch positions and collecting information from contacted obstacles and traversed regions. Through a myriad of randomly generated environments, we determine that: 1) the density of obstacles in an environment has minimal impact on mapping abilities and 2) at least 70% of each environment tested can be mapped by deploying 20 or fewer soft, growing robots.</td>
                <td>Location awareness, Visualization, Simultaneous localization and mapping, Navigation, Soft robotics, Sensors, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341705&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341705&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Stable Real-Time Feedback Control of a Pneumatic Soft Robot</td>
                <td>Aydin</td>
                <td>2023</td>
                <td>Soft actuators offer compliant and safe interaction with an unstructured environment compared to their rigid counterparts. However, control of these systems is often challenging because they are inherently under-actuated, have infinite degrees of freedom (DoF), and their mechanical properties can change by unknown external loads. Existing works mainly relied on discretization and reduction, suffering from either low accuracy or high computational cost for real-time control purposes. Recently, we presented an infinite-dimensional feedback controller for soft manipulators modeled by partial differential equations (PDEs) based on the Cosserat rod theory. In this study, we examine how to implement this controller in real-time using only a limited number of actuators. To do so, we formulate a convex quadratic programming problem that tunes the feedback gains of the controller in real-time such that it becomes realizable by the actuators. We evaluated the controller's performance through experiments on a physical soft robot capable of planar motions and show that the actual controller implemented by the finite-dimensional actuators still preserves the stabilizing property of the desired infinite-dimensional controller. This research fills the gap between the infinite-dimensional control design and finite-dimensional actuation in practice and suggests a promising direction for exploring PDE-based control design for soft robots.</td>
                <td>Pneumatic actuators, Shape control, Control design, Partial differential equations, Soft robotics, Real-time systems, Mechanical factors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342212&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342212&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real2Sim2Real Transfer for Control of Cable-Driven Robots Via a Differentiable Physics Engine</td>
                <td>K. Wang et al.</td>
                <td>2023</td>
                <td>Tensegrity robots, composed of rigid rods and flexible cables, exhibit high strength-to-weight ratios and significant deformations, which enable them to navigate unstructured terrains and survive harsh impacts. They are hard to control, however, due to high dimensionality, complex dynamics, and a coupled architecture. Physics-based simulation is a promising avenue for developing locomotion policies that can be transferred to real robots. Nevertheless, modeling tensegrity robots is a complex task due to a substantial sim2real gap. To address this issue, this paper describes a Real2Sim2Real (R2S2R) strategy for tensegrity robots. This strategy is based on a differentiable physics engine that can be trained given limited data from a real robot. These data include offline measurements of physical properties, such as mass and geometry for various robot components, and the observation of a trajectory using a random control policy. With the data from the real robot, the engine can be iteratively refined and used to discover locomotion policies that are directly transferable to the real robot. Beyond the R2S2R pipeline, key contributions of this work include computing non-zero gradients at contact points, a loss function for matching tensegrity locomotion gaits, and a trajectory segmentation technique that avoids conflicts in gradient evaluation during training. Multiple iterations of the R2S2R process are demonstrated and evaluated on a real 3-bar tensegrity robot.</td>
                <td>Training, Pipelines, Robot sensing systems, Turning, Trajectory, Task analysis, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341811&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341811&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Gait Locomotion Planning and Tracking for Tendon-Actuated Terrestrial Soft Robot (TerreSoRo)</td>
                <td>A. N. Mahendran, C. Freeman, A. H. Chang, M. McDougall, P. A. Vela and V. Vikas</td>
                <td>2023</td>
                <td>The adaptability of soft robots makes them ideal candidates to maneuver through unstructured environments. However, locomotion challenges arise due to complexities in modeling the body mechanics, actuation, and robot-environment dynamics. These factors contribute to the gap between their potential and actual autonomous field deployment. A closed-loop path planning framework for soft robot locomotion is critical to close the real-world realization gap. This paper presents a generic path planning framework applied to TerreSoRo (Tetra-Limb Terrestrial Soft Robot) with pose feedback. It employs a gait-based, lattice trajectory planner to facilitate navigation in the presence of obstacles. The locomotion gaits are synthesized using a data-driven optimization approach that allows for learning from the environment. The trajectory planner employs a greedy breadth-first search strategy to obtain a collision-free trajectory. The synthesized trajectory is a sequence of rotate-then-translate gait pairs. The control architecture integrates high-level and low-level controllers with real-time localization (using an overhead webcam). Terre-SoRo successfully navigates environments with obstacles where path re-planning is performed. To best of our knowledge, this is the first instance of real-time, closed-loop path planning of a non-pneumatic soft robot.</td>
                <td>Location awareness, Navigation, Webcams, Lattices, Soft robotics, Search problems, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341926&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341926&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Soft Robot Dynamics Using Differentiable Kalman Filters and Spatio-Temporal Embeddings</td>
                <td>X. Liu, S. Ikemoto, Y. Yoshimitsu and H. B. Amor</td>
                <td>2023</td>
                <td>This paper introduces a novel approach for modeling the dynamics of soft robots, utilizing a differentiable filter architecture. The proposed approach enables end-to-end training to learn system dynamics, noise characteristics, and temporal behavior of the robot. A novel spatio-temporal embedding process is discussed to handle observations with varying sensor placements and sampling frequencies. The efficacy of this approach is demonstrated on a tensegrity robot arm by learning end-effector dynamics from demonstrations with complex bending motions. The model is proven to be robust against missing modalities, diverse sensor placement, and varying sampling rates. Additionally, the proposed framework is shown to identify physical interactions with humans during motion. The utilization of a differentiable filter presents a novel solution to the difficulties of modeling soft robot dynamics. Our approach shows substantial improvement in accuracy compared to state-of-the-art filtering methods, with at least a 24% reduction in mean absolute error (MAE) observed. Furthermore, the predicted end-effector positions show an average MAE of 25.77mm from the ground truth, highlighting the advantage of our approach. The code is available at https://github.com/ir-lab/soft_robot_DEnKF.</td>
                <td>Measurement, Sensor placement, Analytical models, Deformation, Force, Dynamics, Soft robotics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341856&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341856&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Closed Loop Control of Tendon Driven Continuum Robots Using IMUs</td>
                <td>M. Srivastava, R. Groff and I. D. Walker</td>
                <td>2023</td>
                <td>In this paper, we present a new approach to the control of continuum robot sections using IMU quaternion feedback. We use a discrete time root finding algorithm to steer a continuum section in the desired shape space direction. We found that the approach lacks end effector positioning accuracy when used by itself, however, when used in conjunction with a feedforward model it actively counters the influence of unmodeled factors. The approach is implemented on a single section of a continuum hose robot developed for 3D printing of concrete in construction applications. The results demonstrate significant improvements in positioning accuracy compared to standalone kinematics/mechanics-based position control of tendon lengths. Additionally, this approach can be implemented using low cost sensing and control hardware.</td>
                <td>Hoses, Costs, Shape, Robot kinematics, Aerospace electronics, Robot sensing systems, Hardware, Continuum robot, IMU, tendons, control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342138&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342138&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Machine Learning Best Practices for Soft Robot Proprioception</td>
                <td>H. Wang, R. L. Truby, L. Chin and D. Rus</td>
                <td>2023</td>
                <td>Machine learning-based approaches for soft robot proprioception have recently gained popularity, in part due to the difficulties in modeling the relationship between sensor signals and robot shape. However, to date, there exists no systematic analysis of the required design choices to set up a machine learning pipeline for soft robot proprioception. Here, we present the first study examining how design choices on different levels of the machine learning pipeline affect the performance of a neural network for predicting the state of a soft robot. We address the most frequent questions researchers face, such as how to choose the appropriate sensor and actuator signals, process input and output data, deal with time series, and pick the best neural network architecture. By testing our hypotheses on data collected from two vastly different systems–an electrically actuated robotic platform and a pneumatically actuated soft trunk–we seek conclusions that may generalize beyond one specific type of soft robot and hope to provide insights for researchers to use machine learning for soft robot proprioception.</td>
                <td>Actuators, Systematics, Pipelines, Neural networks, Time series analysis, Machine learning, Soft robotics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342379&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342379&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Active Capsule System for Multiple Therapeutic Patch Delivery: Preclinical Evaluation</td>
                <td>J. Lee et al.</td>
                <td>2023</td>
                <td>Recently, active research has been conducted on the therapeutic functions of capsule endoscopes. Here, we propose an active capsule system that captures images of the interior of the gastrointestinal tract (GI) and actively delivers therapeutic patches. The active capsule system mainly comprises therapeutic patches, an active capsule equipped with a camera, and a robot-assisted magnetic actuator. The active capsule moves inside the GI tract via a magnetic actuator using a robot, captures pictures of the GI tract in actual time, and performs hemostatic treatment by delivering therapeutic patches to the target lesions. First, the fundamental performance of the active capsule system was verified via a hemostatic performance test of the therapeutic patch, patch contamination prevention test of the active capsule, and basic actuation test of the capsule. Second, multiple therapeutic patches were delivered to the gastric surface in an ex vivo test using an active capsule system. Finally, as a preclinical test, it was confirmed that the GI tract examination and the therapeutic patches delivery were possible using the active capsule system through an animal test using a porcine. Consequently, the proposed active capsule system represents a new paradigm for capsule endoscopy with multiple therapeutic patch delivery capabilities.</td>
                <td>Actuators, Endoscopes, Animals, Magnetic resonance imaging, Surface contamination, Robot vision systems, Medical treatment</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341491&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341491&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Parallel Cell Array Patterning and Target Cell Lysis on an Optoelectronic Micro-Well Device</td>
                <td>C. Gan et al.</td>
                <td>2023</td>
                <td>This work presents a novel electrical method, implemented in the form of a microfluidic device, for cell arraying and target cell lysis. The microfluidic device contains a micro-well array on the photoconductive layer based on the optoelectronic tweezers (OET) method, where parallel cell manipulation is performed. As cell suspension flows over the micro-wells, cells can be actively captured in the micro-wells by light-induced dielectrophoresis (DEP) forces, form the designed pattern array in less than 120 s. The single-cell capture rate is over 83 % in the patterned cell array, and about 94% of micro-wells are occupied by cells. Then, the target cell in the specific micro-well is illuminated and lysed by electroporation in 5 seconds. The micro-well barriers and DEP forces block the influence of the flow, and a relatively closed space is critical to preserve the cell lysates. Through experiments, light-induced DEP force cell capture and target cell electroporation can be modulated by changing the light patterns and the applied signal. This device, based on the OET and dynamic electroporation, allows the rapidity in the cell capture and target lysis at the single-cell level and can enable single-cell-based studies, such as molecular diagnostics and disease detection.</td>
                <td>Performance evaluation, Fluidic microsystems, Force, Dynamics, Switches, Electroporation, Genetics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341701&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341701&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Microrobot Control Method Based on Movement of Field Free Point in Gradient Magnetic Field</td>
                <td>C. Wang et al.</td>
                <td>2023</td>
                <td>The untethered microrobots driven by multiple external physics fields have promising ability in minimally invasive disease treatments. One common type of the driving fields is gradient magnetic field, which can provide microrobots with adequate driving force in complicated environment. In this study, a control method of microrobot through gradient magnetic field system is presented, which is realized by moving the field free point (FFP) to produce an alterable magnetic driving force. A confirmatory experiment of the robot reciprocating motion control is undertaken in a 1D gradient magnetic robot system. The control method could be applied to further studies on in vivo applications of targeted microrobot drug delivery system.</td>
                <td>Robot motion, In vivo, Three-dimensional displays, Minimally invasive surgery, Force, Control systems, Magnetic fields, Micro/nano robot control, Gradient magnetic field</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341642&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341642&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Helical Propulsion in Low-Re Numbers with Near-Zero Angle of Attack</td>
                <td>J. W. Ligtenberg et al.</td>
                <td>2023</td>
                <td>One approach to the wireless actuation and gravity compensation of untethered helical magnetic devices (UHMD) is through swimming with a non-zero angle of attack (AoA). This configuration allows us to counteract gravity, so that for a given desired path, we can move the UHMD controllably without drifting downward under its own weight. This study seeks to investigate the use a reduced-order model of the complex 6-degrees-of-freedom model of UHMDs in low Reynolds-number regime. A one-dimensional model representing the relative position of the UHMD with respect to an actuator rotating permanent magnet is used to predict a gap which yields bounded behavior of the open-loop system. Using geometric representation of the reduced-order model, the local bounded behavior of the UHMD with near-zero AoA is attributed to periodic active magnetic suspension, which dominates near-zero AoA. Our numerical results are verified experimentally and bounded behavior of the UHMD demonstrates the capability to swim with near-zero AoA (6.3° ± 2.2°) without drifting downward. With this actuation strategy, it is unlikely that the orientation of the UHMD will be needed during noninvasive localization, making the control system dependent on only its position with respect to a prescribed trajectory. This strategy will also provide a computational advantage in adjusting the gap between the UHMD and a robotically controlled rotating permanent magnet actuator.</td>
                <td>Wireless communication, Actuators, Magnetic liquids, Propulsion, Predictive models, Reduced order systems, Permanent magnets</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341627&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341627&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Influence of Nanoparticle Coating on the Differential Magnetometry and Wireless Actuation of Biohybrid Microrobots</td>
                <td>V. Magdanz et al.</td>
                <td>2023</td>
                <td>Magnetic nanoparticles can be electrostatically assembled around sperm cells to form biohybrid micro robots. These biohybrid microrobots possess sufficient magnetic material to potentially allow for pulse-echo localization and wireless actuation. Alternatively, magnetic excitation of these nanoparticles can be used for localization based on Faraday's law of induction using a detection coil. Here, we investigate the influence of the electrostatic attraction between positively charged nanoparticles and negatively charged sperm cells on the activation of the nanoparticles during nonlinear differential magnetometry and wireless magnetic actuation. Activation of clusters of free nanoparticles and nanoparticles bound to the body of sperm cells is achieved by a combination of a high- frequency alternating field and a pulsating static field. The nonlinear response in both cases indicates that constraining the nanoparticles is likely to yield significant decreases in the magnetometry sensitivity. While the attachment of particles to the cells enables wireless actuation (rolling locomotion), the rate of change of the magnetization of the nanoparticles decreases one order of magnitude compared to free nanoparticles.</td>
                <td>Nanoparticles, Wireless communication, Wireless sensor networks, Magnetic field measurement, Voltage measurement, Magnetometers, Magnetostatics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341258&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341258&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Using Piezoceramic-Actuated Stages in Precision Long-Stroke Motion Systems: A Design Procedure</td>
                <td>Rawashdeh, M. A. Saaideh and M. A. Janaideh</td>
                <td>2023</td>
                <td>Mainly, the integration of fine positioning piezo-actuated stages in precision motion systems is considered, which results in multi-stage configurations. Mostly, in such configurations, the fine stages are attached to the coarse positioning stages- that do not meet required precision- by mechanical means. Once the motion is synchronized, the fine stages enhance the overall precision of the multi-stage system. Undesirably, mechanical, and electromagnetic interference between the in-volved stages take place, which may limit the possible attainable precision. To control the fine stages, we propose the use of feedforward control based on the Prandtl-Ishlinskii model inverse in an attempt to accommodate related piezoceramics dynamic behavior and hysteresis. Targeting the semiconductor manufacturing, the needed multi-stage design steps according to the herein proposed approach are outlined. Also, the performance of a representative precision motion system comprising a planner coarse stage, and a uni-axial fine stage under step-and-scan trajectories is assessed. The results show that the proposed piezo-actuated fine stage improves the scanning accuracy of the overall motion system.</td>
                <td>Semiconductor device modeling, Nanopositioning, Piezoelectric materials, Dynamics, Semiconductor device manufacture, Behavioral sciences, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342314&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342314&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Buoyancy Enabled Non-Inertial Dynamic Walking</td>
                <td>M. Yim, W. Gosrich and M. Miskin</td>
                <td>2023</td>
                <td>We propose a mechanism for low Reynolds num-ber walking (e.g., legged microscale robots). Whereas locomotion for legged robots has traditionally been classified as dynamic (where inertia plays a role) or static (where the system is always statically stable), we introduce a new locomotion modality we call buoyancy enabled non-inertial dynamic walking in which inertia plays no role, yet the robot is not statically stable. Instead, falling and viscous drag play critical roles. This model assumes squeeze flow forces from fluid interactions combined with a well timed gait as the mechanism by which forward motion can be achieved from a reciprocating legged robot. Using two physical demonstrations of robots with Reynold's number ranging from 0.0001 to 0.02 (a microscale robot in water and a centimeter scale robot in glycerol) we find the model qualitatively describes the motion. This model can help understand microscale locomotion and design new microscale walking robots including controlling forward and backwards motion and potentially steering these robots.</td>
                <td>Legged locomotion, Micrometers, Actuators, Fluids, Dynamics, Buoyancy, Distance measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341582&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341582&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Ultrafast Acoustic Holography with Physics-Reinforced Self-Supervised Learning for Precise Robotic Manipulation</td>
                <td>Q. Lu, C. Zhong, Q. Liu, T. Li, H. Su and S. Liu</td>
                <td>2023</td>
                <td>Ultrafast acoustic holography (AH) enabling dynamic contactless micro-nano robotic manipulation has recently attracted wide attention. As an advanced technique, AH encodes specific three-dimensional (3D) acoustic field on a two-dimensional (2D) hologram whereby realizing holographic reconstruction with high fidelity. However, current approaches face the limitation of encoding time, accuracy and flexibility, thus, leading to inapplicability for dynamic and precise robotic manipulation. Here, we develop an approach to overcome these issues. Its basic idea is to use a convolutional neural network trained in a self-supervised manner with iterative interaction with virtual physical environment. Energy conservation is incorporated to access the physical constrain during wave propagation. The experimental results demonstrate that the proposed method circumvents laborious annotated dataset preparation and boosts the reinforcement from physics model. By the validation and comparison on distinct acoustic fields with various patterns, the accuracy and real-time performance of the proposed method are confirmed supporting dynamic and precise robotic manipulation.</td>
                <td>Three-dimensional displays, Propagation, Two dimensional displays, Self-supervised learning, Holography, Real-time systems, Acoustic field</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341483&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341483&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Surface Navigation of Alginate Artificial Cells in Mucus Solutions</td>
                <td>L. W. Rogowski, J. Wood, T. Cooke, G. Kararsiz and M. J. Kim</td>
                <td>2023</td>
                <td>Alginate hydrogels are widely researched in phar-maceutical applications for their abilities to encapsulate and dis-perse therapeutics in response to stimuli. While effective, their utility can be greatly improved once converted into artificial cell soft-microrobots, allowing them to actively navigate through complex in vivo environments and facilitate targeted drug deliv-ery. In this study, artificial cells were fabricated by crosslinking alginate with magnetic nanoparticles and then deployed within mucus solutions to characterize their propulsion capabilities. The goal of this study was to understand how variations in simplified gastrointestinal fluid, artificial cell properties, and magnetic field characteristics could affect surface locomotion. A comparison between automatic feedback control and manual “open-loop” operation was also quantitatively explored. Under feedback control, individual artificial cells were navigated with automatically generated waypoints and a PID controller. Simu-lations were used to verify controller performance and accuracy. User operation was carried out using an Xbox controller, where the joystick could directly change navigation direction. We conclude in this study that the surface navigation of artificial cells is highly predictable within mucus concentrations and that both feedback and open-loop control are equally successful in navigation.</td>
                <td>Drugs, In vivo, Fluids, Navigation, Hydrogels, Propulsion, Feedback control, microrobotics, hydrogels, feedback control, drug delivery, biofluids</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342232&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342232&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design and Control of Microscale Dual Locomotion Mode Multi-Functional Robots (μDMMFs)</td>
                <td>A. C. Davis and D. J. Cappelleri</td>
                <td>2023</td>
                <td>This paper presents the design and control of a novel microrobot that utilizes two distinct magnetic locomotion methods, a combination of rotating and gradient field control, for precise micro-object manipulation using multiple end-effectors. Rotating magnetic fields induce a tumbling locomotion mode to increase the movement speed and decrease issues associated with stiction and locomotion over rough surfaces. The gradient field control allows for precise manipulation using the end-effectors, which include a pointed tip for splitting groups of objects and a blunt end for pushing or capturing objects. The microrobot is fabricated using a two-photon polymerization 3D printer, allowing for the precise reproduction of complex geometries and designs. The potential applications of this technology in the medical field are discussed, highlighting the potential for in vitro cellular manipulation.</td>
                <td>Fabrication, Force measurement, Three-dimensional displays, Shape, Force, Transportation, End effectors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341659&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341659&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A New 1-mg Fast Unimorph SMA-Based Actuator for Microrobotics</td>
                <td>Arancibia</td>
                <td>2023</td>
                <td>We present a new unimorph actuator for micro-robotics, which is driven by thin shape-memory alloy (SMA) wires. Using a passive-capillary-alignment technique and existing SMA-microsystem fabrication methods, we developed an actuator that is 7 mm long, has a volume of 0.45 mm3, weighs 0.96 mg, and can achieve operation frequencies of up to 40 Hz as well as lift 155 times its own weight. To demonstrate the capabilities of the proposed actuator, we created an 8-mg crawler, the MiniBug, and a bioinspired 56-mg controllable water-surface-tension crawler, the WaterStrider. The MiniBug is 8.5 mm long, can locomote at speeds as high as 0.76 BL/s (body-lengths per second), and is the lightest fully-functional crawling microrobot of its type ever created. The WaterStrider is 22 mm long, and can locomote at speeds of up to 0.28 BL/s as well as execute turning maneuvers at angular rates on the order of 0.144 rad/s. The WaterStrider is the lightest controllable SMA-driven water-surface-tension crawler developed to date.</td>
                <td>Actuators, Time-frequency analysis, Crawlers, Wires, Microfabrication, Dynamics, Metals</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342518&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342518&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Toward Sub-Gram Helicopters: Designing a Miniaturized Flybar for Passive Stability</td>
                <td>K. Johnson, V. Arroyos, R. Villanueva, A. Schulz, S. Fuller and V. Iyer</td>
                <td>2023</td>
                <td>Sub-gram flying robots have transformative potential in applications from search and rescue to precision agriculture to environmental monitoring. However, a key gap in achieving autonomous flight for these applications is the low lift to weight ratio of flapping wing and quadrotor designs around 1 g or less. To close this gap, we propose a helictoper-style design that minimizes size and weight by leveraging the high lift, reliability, and low-voltage of sub-gram motors. We take an important step to enable this goal by designing a light-weight, micfrofabricated flybar mechanism to passively stabilize such a robot. Our 48 mg flybar is folded from a flat carbon fiber laminate into a 3D mechanism that couples tilting of the flybar to a change in the angle of attack of the rotors. Our design uses flexure joints instead of ball-in-socket joints common in larger flybars. To expedite the design exploration and optimization of a microfabricated flat-folded flybar, we develop a novel user-in-the-loop bi-level optimization workflow that combines Bayesian optimization design tools and expert feedback. We develop four template designs and use this method to achieve a peak damping ratio of 0.528, an 18.9x improvement from our initial design. Compared to a flybar-less rotor with a near 0 damping ratio, our flybar-rotor mechanism maintains a stable roll and pitch with relative deviations < 1°. Our results show that, if combined with a counter-torque mechanism such as a tail rotor, our miniaturized flybar could mechanically provide attitude stability for a sub-gram helicopter.</td>
                <td>Damping, Low voltage, Three-dimensional displays, Rotors, Tail, Reliability engineering, Laminates</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342256&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342256&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Manipulation of Optical Force-Induced Micro-Assemblies at the Air-Liquid Interface</td>
                <td>N. Carlisle, M. A. K. Williams, C. P. Whitby, V. Nock, J. L. Y. Chen and E. Avci</td>
                <td>2023</td>
                <td>Colloidal particles trapped by a focused laser at the air-liquid interface provide an interesting assembly dynamic. In this study, we demonstrated manipulating optical force-induced swarms via dynamic locomotion of assemblies built with holographic optical tweezers. This manipulation approach builds the foundation for autonomous control of building assemblies at the air-liquid interface, which is the first time optical micro-robots have performed this feat. Our proposed semi-autonomous control allows users to produce small dynamic secondary assemblies at the interface, which are transported to and merged with a main static assembly. This static-dynamic approach grows assemblies up to ~2.1 times larger than conventional methods. Manipulation and control of large-scale optical force-induced assemblies in real-time to create re-configurable swarms has the potential to lead the development of new technology and approaches for complex tasks, such as the development of new material, transportation of biological matter, studying biofilm formation created by bacteria colonies at the air-liquid interface, and more.</td>
                <td>Biomedical optical imaging, Dynamics, Buildings, Transportation, Holography, Holographic optical components, Optical materials</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341939&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341939&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Creating a Dynamic Quadrupedal Robotic Goalkeeper with Reinforcement Learning</td>
                <td>X. Huang et al.</td>
                <td>2023</td>
                <td>We present a reinforcement learning (RL) framework that enables quadrupedal robots to perform soccer goalkeeping tasks in the real world. Soccer goalkeeping with quadrupeds is a challenging problem, that combines highly dynamic locomotion with precise and fast non-prehensile object (ball) manipulation. The robot needs to react to and intercept a potentially flying ball using dynamic locomotion maneuvers in a very short amount of time, usually less than one second. In this paper, we propose to address this problem using a hierarchical model-free RL framework. The first component of the framework contains multiple control policies for distinct locomotion skills, which can be used to cover different regions of the goal. Each control policy enables the robot to track random parametric end-effector trajectories while performing one specific locomotion skill, such as jump, dive, and sidestep. These skills are then utilized by the second part of the framework which is a high-level planner to determine a desired skill and end-effector trajectory in order to intercept a ball flying to different regions of the goal. We deploy the proposed framework on a Mini Cheetah quadrupedal robot and demonstrate the effectiveness of our framework for various agile interceptions of a fast-moving ball in the real world.</td>
                <td>Reinforcement learning, End effectors, Trajectory, Quadrupedal robots, Task analysis, Manipulator dynamics, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341936&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341936&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Walking in Narrow Spaces: Safety-Critical Locomotion Control for Quadrupedal Robots with Duality-Based Optimization</td>
                <td>Q. Liao, Z. Li, A. Thirugnanam, J. Zeng and K. Sreenath</td>
                <td>2023</td>
                <td>This paper presents a safety-critical locomotion control framework for quadrupedal robots. Our goal is to enable quadrupedal robots to safely navigate in cluttered environments. To tackle this, we introduce exponential Discrete Control Barrier Functions (exponential DCBFs) with duality-based obstacle avoidance constraints into a Non-linear Model Predictive Control (NMPC) with Whole-Body Control (WBC) framework for quadrupedal locomotion control. This enables us to use polytopes to describe the shapes of the robot and obstacles for collision avoidance while doing locomotion control of quadrupedal robots. Compared to most prior work, especially using CBFs, that utilize spherical and conservative approximation for obstacle avoidance, this work demonstrates a quadrupedal robot autonomously and safely navigating through very tight spaces in the real world. (Our open-source code is available at https://github.com/HybridRobotics/quadruped_nmpc_dcbf_duality, and the video is available at https://youtu.be/plgSQjwXm1Q.)</td>
                <td>Legged locomotion, Codes, Navigation, Shape, Aerospace electronics, Quadrupedal robots, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341896&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341896&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ARMP: Autoregressive Motion Planning for Quadruped Locomotion and Navigation in Complex Indoor Environments</td>
                <td>J. Kim, T. Li and S. Ha</td>
                <td>2023</td>
                <td>Generating natural and physically feasible motions for legged robots has been a challenging problem due to its complex dynamics. In this work, we introduce a novel learning-based framework of autoregressive motion planner (ARMP) for quadruped locomotion and navigation. Our method can generate motion plans with an arbitrary length in an autore-gressive fashion, unlike most offline trajectory optimization algorithms for a fixed trajectory length. To this end, we first construct the motion library by solving a dense set of trajectory optimization problems for diverse scenarios and parameter settings. Then we learn the motion manifold from the dataset in a supervised learning fashion. We show that the proposed ARMP can generate physically plausible motions for various tasks and situations. We also showcase that our method can be successfully integrated with the recent robot navigation frameworks as a low-level controller and unleash the full capability of legged robots for complex indoor navigation.</td>
                <td>Legged locomotion, Manifolds, Indoor navigation, Supervised learning, Libraries, Planning, Quadrupedal robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341389&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341389&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Perceptive Hexapod Legged Locomotion for Climbing Joist Environments</td>
                <td>Beaudan, W. Yu, T. Zhang and A. Zakhor</td>
                <td>2023</td>
                <td>Attics are one of the largest sources of energy loss in residential homes, but they are uncomfortable and dangerous for human workers to conduct air sealing and insulation. Hexapod robots are potentially suitable for carrying out those tasks in tight attic spaces since they are stable, compact, and lightweight. For hexapods to succeed in these tasks, they must be able to navigate inside tight attic spaces of single-family residential homes in the U.S., which typically contain rows of approximately 6 or 8-inch tall joists placed 16 inches apart from each other. Climbing over such obstacles is challenging for autonomous robotics systems. In this work, we develop a perceptive walking model for legged hexapods that can traverse over terrain with random joist structures using egocentric vision. Our method can be used on low-cost hardware not requiring real-time joint state feedback. We train our model in a teacher-student fashion with 2 phases: In phase 1, we use reinforcement learning with access to privileged information such as local elevation maps and joint feedback. In phase 2, we use supervised learning to distill the model into one with access to only onboard observations, consisting of egocentric depth images and robot orientation captured by a tracking camera. We demonstrate zero-shot sim-to-real transfer on a Hiwonder[1] SpiderPi robot, equipped with a depth camera onboard, climbing over joist courses we construct to simulate the environment in the field. Our proposed method achieves nearly 100% success rate climbing over the test courses, significantly outperforming the model without perception and the controller provided by the manufacturer.</td>
                <td>Legged locomotion, State feedback, Navigation, Robot vision systems, Supervised learning, Reinforcement learning, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341957&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341957&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design of STARQ: A Multimodal Quadrupedal Robot for Running, Climbing, and Swimming</td>
                <td>D. A. Vasquez, D. Jay, M. Dina, M. Austin, S. McConomy and J. E. Clark</td>
                <td>2023</td>
                <td>Legged animals have developed a variety of modes of locomotion to adapt to the diverse and unknown terrain challenges posed in the natural world. Legged robots, however, have been largely limited to specializing in one domain, with few that have endeavored to bridge the gap between two. In this work we present the Scansorial, Terrestrial, and Aquatic Robot Quadruped (STARQ), a novel legged robot capable of bridging three different domains with three modes of locomotion: walking, climbing, and swimming. In this study we describe model-based design techniques as well as design innovations that have made multimodal locomotion possible including waterproof hips for 2-DOF high torque legs, legs capable of effective power transmission in three modes, and bi-directionally compliant feet for walking and attaching to vertical surfaces. To demonstrate the robot's capabilities we present locomotion test data including speed and cost of transport in each of these domains. We also demonstrate the capability to transition from walking to swimming in a natural environment.</td>
                <td>Legged locomotion, Technological innovation, Torque, Costs, Power transmission, Quadrupedal robots, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342423&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342423&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hierarchical Adaptive Control for Collaborative Manipulation of a Rigid Object by Quadrupedal Robots</td>
                <td>M. Sombolestan and Q. Nguyen</td>
                <td>2023</td>
                <td>Despite the potential benefits of collaborative robots, effective manipulation tasks with quadruped robots remain difficult to realize. In this paper, we propose a hierarchical control system that can handle real-world collaborative manipulation tasks, including uncertainties arising from object properties, shape, and terrain. Our approach consists of three levels of controllers. Firstly, an adaptive controller computes the required force and moment for object manipulation without prior knowledge of the object's properties and terrain. The computed force and moment are then optimally distributed between the team of quadruped robots using a Quadratic Programming (QP)-based controller. This QP-based controller optimizes each robot's contact point location with the object while satisfying constraints associated with robot-object contact. Finally, a decentralized loco-manipulation controller is designed for each robot to apply manipulation force while maintaining the robot's stability. We successfully validated our approach in a high-fidelity simulation environment where a team of quadruped robots manipulated an unknown object weighing up to 18 kg on different terrains while following the desired trajectory.</td>
                <td>Uncertainty, Force, Collaboration, Stability analysis, Trajectory, Quadrupedal robots, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341700&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341700&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Proprioception and Reaction for Walking Among Entanglements</td>
                <td>J. K. Yim, J. Ren, D. Ologan, S. G. Gonzalez and A. M. Johnson</td>
                <td>2023</td>
                <td>Entanglements like vines and branches in natural settings or cords and pipes in human spaces prevent mobile robots from accessing many environments. Legged robots should be effective in these settings, and more so than wheeled or tracked platforms, but naive controllers quickly become entangled and stuck. In this paper we present a method for proprioception aimed specifically at the task of sensing entanglements of a robot's legs as well as a reaction strategy to disentangle legs during their swing phase as they advance to their next foothold. We demonstrate our proprioception and reaction strategy enables traversal of entanglements of many stiffnesses and geometries succeeding in 14 out of 16 trials in laboratory tests, as well as a natural outdoor environment.</td>
                <td>Legged locomotion, Torque, Estimation, Observers, Robot sensing systems, Hardware, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341986&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341986&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning a Single Policy for Diverse Behaviors on a Quadrupedal Robot Using Scalable Motion Imitation</td>
                <td>A. Klipfel, N. Sontakke, R. Liu and S. Ha</td>
                <td>2023</td>
                <td>Learning various motor skills for quadrupedal robots is a challenging problem that requires careful design of task-specific mathematical models or reward descriptions. In this work, we propose to learn a single capable policy using deep reinforcement learning by imitating a large number of reference motions, including walking, turning, pacing, jumping, sitting, and lying. On top of the existing motion imitation framework, we first carefully design the observation space, the action space, and the reward function to improve the scalability of the learning as well as the robustness of the final policy. In addition, we adopt a novel adaptive motion sampling (AMS) method, which maintains a balance between successful and unsuccessful behaviors. This technique allows the learning algorithm to focus on challenging motor skills and avoid catastrophic forgetting. We demonstrate that the learned policy can exhibit diverse behaviors in simulation by successfully tracking both the training dataset and out-of-distribution trajectories. We also validate the importance of the proposed learning formulation and the adaptive motion sampling scheme by conducting experiments.</td>
                <td>Legged locomotion, Tracking, Databases, Scalability, Reinforcement learning, Turning, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341709&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341709&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Novel Lockable Spring-Loaded Prismatic Spine to Support Agile Quadrupedal Locomotion</td>
                <td>K. Ye, K. Chung and K. Karydis</td>
                <td>2023</td>
                <td>This paper introduces a way to systematically investigate the effect of compliant prismatic spines in quadrupedal robot locomotion. We develop a novel spring-loaded lockable spine module, together with a new Spinal Compliance-Integrated Quadruped (SCIQ) platform for both empirical and numerical research. Individual spine tests reveal beneficial spinal characteristics like a degressive spring, and validate the efficacy of a proposed compact locking/unlocking mechanism for the spine. Benchmark vertical jumping and landing tests with our robot show comparable jumping performance between the rigid and compliant spines. An observed advantage of the compliant spine module is that it can alleviate more challenging landing conditions by absorbing impact energy and dissipating the remainder via feet slipping through much in cat-like stretching fashion.</td>
                <td>Legged locomotion, Spine, Benchmark testing, Quadrupedal robots, Springs, Intelligent robots, Foot</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341427&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341427&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Path Planning In Manipulation Planning Problems by Actively Reusing Validation Effort</td>
                <td>Haro and M. Toussaint</td>
                <td>2023</td>
                <td>The path planning problems arising in manipulation planning and in task and motion planning settings are typically repetitive: the same manipulator moves in a space that only changes slightly. Despite this potential for reuse of information, few planners fully exploit the available information. To better enable this reuse, we decompose the collision checking into reusable, and non-reusable parts. We then treat the sequences of path planning problems in manipulation planning as a multiquery path planning problem. This allows the usage of planners that actively minimize planning effort over multiple queries, and by doing so, actively reuse previous knowledge. We implement this approach in EIRM* and effort ordered LazyPRM*, and benchmark it on multiple simulated robotic examples. Further, we show that the approach of decomposing collision checks additionally enables the reuse of the gained knowledge over multiple different instances of the same problem, i.e., in a multiquery manipulation planning scenario. The planners using the decomposed collision checking out-perform the other planners in initial solution time by up to a factor of two while providing a similar solution quality.</td>
                <td>Torque, Force, Benchmark testing, Manipulators, Path planning, Planning, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341264&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341264&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Improving Reliable Navigation Under Uncertainty via Predictions Informed by Non-Local Information</td>
                <td>R. I. Arnob and G. J. Stein</td>
                <td>2023</td>
                <td>We improve reliable, long-horizon, goal-directed navigation in partially-mapped environments by using nonlocally available information to predict the goodness of temporally-extended actions that enter unseen space. Making predictions about where to navigate in general requires nonlocal information: any observations the robot has seen so far may provide information about the goodness of a particular direction of travel. Building on recent work in learning-augmented model-based planning under uncertainty, we present an approach that can both rely on nonlocal information to make predictions (via a graph neural network) and is reliable by design: it will always reach its goal, even when learning does not provide accurate predictions. We conduct experiments in three simulated environments in which nonlocal information is needed to perform well. In our large scale university building environment, generated from real-world floorplans to the scale, we demonstrate a 9.3% reduction in cost-to-go compared to a non-learned baseline and a 14.9% reduction compared to a learning-informed planner that can only use local information to inform its predictions.</td>
                <td>Uncertainty, Navigation, Buildings, Predictive models, Reliability engineering, Graph neural networks, Space exploration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342276&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342276&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TOP-UAV: Open-Source Time-Optimal Trajectory Planner for Point-Masses Under Acceleration and Velocity Constraints</td>
                <td>F. Meyer, K. Glock and D. Sayah</td>
                <td>2023</td>
                <td>In the latest research for unmanned aerial vehicles (UAVs), time-optimal trajectory planning of a point-mass with acceleration as control input and constrained maximum velocity (TOT-PMAV) has proved to be very promising for UAV behavior planning. They can be calculated within microseconds and tracked with high precision by modern trajectory tracking controllers like model predictive control (MPC). However, recent research shows that the state-of-the-art (SOTA) approach to generate these time-optimal trajectories is based on an invalid method to synchronize the coordinate axes which sometimes yields trajectories that miss the desired final state by far. Hence, an alternative approach was proposed that claims to resolve this issue. However, it still needs mathematical proof of its correctness. In this work, we provide the missing proof and mathematically demonstrate the problems arising from the SOTA approach. Further, since neither the SOTA nor the alternative solution approach utilizes the full kinematic capacity of a UAV, we propose an improved solution approach to the TOT-PMAV that better exploits kinematic properties and yields, on average, up to 14% faster trajectories. We substantiate our findings with an extensive computational study, show in which situations the SOTA is likely to fail and provide metrics to measure the consequence during failure. To enable reproducibility, our code is open-source.</td>
                <td>Trajectory tracking, Trajectory planning, Kinematics, Autonomous aerial vehicles, Reproducibility of results, Trajectory, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342270&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342270&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Fast Asymptotically Optimal Path Planning in Dynamic, Uncertain Environments</td>
                <td>L. Huang and X. Jing</td>
                <td>2023</td>
                <td>This paper presents Fast Adaptive Tree (FAT), an asymptotically-optimal sampling-based path planner for dynamic and uncertain scenarios. Namely, the solution extracted converges to the optimal solution given the sensor information as the number of samples approaches infinity. The planner maintains an underlying graph, which increasingly approximates the search domain, and a dynamic spanning tree of the graph, which contains the shortest path from the start to the goal state. The planner quickly responds to the availability of new information about the environments or the robot movements by minimally repairing the spanning tree over the navigation. The simulation results show that the proposed path planner achieves higher efficiency of replanning than several state-of-the-art path planners without sacrificing solution quality.</td>
                <td>Navigation, Simulation, Heuristic algorithms, Robot sensing systems, Path planning, Fats, Data mining, Path planning, Aymptotically-optimal, Dynamic environment, Sampling-based</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341928&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341928&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Efficient Trajectory Planner for Car-Like Robots on Uneven Terrain</td>
                <td>L. Xu et al.</td>
                <td>2023</td>
                <td>Autonomous navigation of ground robots on uneven terrain is being considered in more and more tasks. However, uneven terrain will bring two problems to motion planning: how to assess the traversability of the terrain and how to cope with the dynamics model of the robot associated with the terrain. The trajectories generated by existing methods are often too conservative or cannot be tracked well by the controller since the second problem is not well solved. In this paper, we propose terrain pose mapping to describe the impact of terrain on the robot. With this mapping, we can obtain the SE(3) state of the robot on uneven terrain for a given state in SE(2). Then, based on it, we present a trajectory optimization framework for car-like robots on uneven terrain that can consider both of the above problems. The trajectories generated by our method conform to the dynamics model of the system without being overly conservative and yet able to be tracked well by the controller. We perform simulations and real-world experiments to validate the efficiency and trajectory quality of our algorithm.</td>
                <td>Tracking, Heuristic algorithms, Dynamics, Stochastic processes, Approximation algorithms, Tires, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341558&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341558&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robots as AI Double Agents: Privacy in Motion Planning</td>
                <td>R. Shome, Z. Kingston and L. E. Kavraki</td>
                <td>2023</td>
                <td>Robotics and automation are poised to change the landscape of home and work in the near future. Robots are adept at deliberately moving, sensing, and interacting with their environments. The pervasive use of robotics promises societal and economic payoffs due to its capabilities—conversely, the capabilities of robots to move within and sense the world around them is susceptible to abuse. Robots, unlike typical sensors, are inherently autonomous, active, and deliberate. Such automated agents can become AI double agents liable to violate the privacy of coworkers, privileged spaces, and other stakeholders. In this work we highlight the understudied and inevitable threats to privacy that can be posed by the autonomous, deliberate motions and sensing of robots. We frame the problem within broader sociotechnological questions alongside a comprehensive review. The privacy-aware motion planning problem is formulated in terms of cost functions that can be modified to induce privacy-aware behavior: preserving, agnostic, or violating. Simulated case studies in manipulation and navigation, with altered cost functions, are used to demonstrate how privacy-violating threats can be easily injected, sometimes with only small changes in performance (solution path lengths). Such functionality is already widely available. This preliminary work is meant to lay the foundations for near-future, holistic, interdisciplinary investigations that can address questions surrounding privacy in intelligent robotic behaviors determined by planning algorithms.</td>
                <td>Privacy, Navigation, Robot sensing systems, Cost function, Sensors, Planning, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341460&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341460&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bang-Bang Boosting of RRTs</td>
                <td>A. J. La Valle, B. Sakcak and S. M. LaValle</td>
                <td>2023</td>
                <td>This paper presents methods for dramatically improving the performance of sampling-based kinodynamic planners. The key component is a complete, exact steering method that produces a time-optimal trajectory between any states for a vector of synchronized double integrators. This method is applied in three ways: 1) to generate RRT edges that quickly solve the two-point boundary-value problems, 2) to produce a (quasi)metric for more accurate Voronoi bias in RRTs, and 3) to iteratively time-optimize a given collision-free trajectory. Experiments are performed for state spaces with up to 2000 dimensions, resulting in improved computed trajectories and orders of magnitude computation time improvements over using ordinary metrics and constant controls.</td>
                <td>Aerospace electronics, Extraterrestrial measurements, Boosting, Trajectory, Synchronization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341760&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341760&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Geometric Gait Optimization for Inertia-Dominated Systems with Nonzero Net Momentum</td>
                <td>Y. Yang and R. L. Hatton</td>
                <td>2023</td>
                <td>Inertia-dominated mechanical systems can achieve net displacement by 1) periodically changing their shape (known as kinematic gait) and 2) adjusting their inertia distribution to utilize the existing nonzero net momentum (known as momentum gait). Therefore, finding the gait that most effectively utilizes the two types of locomotion in terms of the magnitude of the net momentum is a significant topic in the study of locomotion. For kinematic locomotion with zero net momentum, the geometry of optimal gaits is expressed as the equilibria of system constraint curvature flux through the surface bounded by the gait, and the cost associated with executing the gait in the metric space. In this paper, we identify the geometry of optimal gaits with nonzero net momentum effects by lifting the gait description to a time-parameterized curve in shape-time space. We also propose the variational gait optimization algorithm corresponding to the lifted geometric structure, and identify two distinct patterns in the optimal motion, determined by whether or not the kinematic and momentum gaits are concentric. The examples of systems with and without fluid-added mass demonstrate that the proposed algorithm can efficiently solve forward and turning locomotion gaits in the presence of nonzero net momentum. At any given momentum and effort limit, the proposed optimal gait that takes into account both momentum and kinematic effects outperforms the reference gaits that each only considers one of these effects.</td>
                <td>Geometry, Measurement, Costs, Shape, Kinematics, Turning, Mechanical systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342211&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342211&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Tube-Based Non-Gaussian Risk Bounded Motion Planning for Stochastic Nonlinear Systems in Uncertain Environments via Motion Primitives</td>
                <td>W. Han, A. Jasour and B. Williams</td>
                <td>2023</td>
                <td>We consider the motion planning problem for stochastic nonlinear systems in uncertain environments. More precisely, in this problem the robot has stochastic nonlinear dynamics and uncertain initial locations, and the environment contains multiple dynamic uncertain obstacles. Obstacles can be of arbitrary shape, can deform, and can move. All uncertainties do not necessarily have Gaussian distribution. This general setting has been considered and solved in [1]. In addition to the assumptions above, in this paper, we consider long-term tasks, where the planning method in [1] would fail, as the uncertainty of the system states grows too large over a long time horizon. Unlike [1], we present a real-time online motion planning algorithm. We build discrete-time motion primitives and their corresponding continuous-time tubes offline, so that almost all system states of each motion primitive are guaranteed to stay inside the corresponding tube. We convert probabilistic safety constraints into a set of deterministic constraints called risk contours. During online execution, we verify the safety of the tubes against deterministic risk contours using sum-of-squares (SOS) programming. The provided SOS-based method verifies the safety of the tube in the presence of uncertain obstacles without the need for uncertainty samples and time discretization in real-time. By bounding the probability the system states staying inside the tube and bounding the probability of the tube colliding with obstacles, our approach guarantees bounded probability of system states colliding with obstacles. We demonstrate our approach on several long-term robotics tasks.</td>
                <td>Uncertainty, Shape, Programming, Probabilistic logic, Real-time systems, Electron tubes, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342429&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342429&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Bifunctional Push-Grasping Synergistic Strategy for Goal-Agnostic and Goal-Oriented Tasks</td>
                <td>D. Ren, S. Wu, X. Wang, Y. Peng and X. Ren</td>
                <td>2023</td>
                <td>Both goal-agnostic and goal-oriented tasks have practical value for robotic grasping: goal-agnostic tasks target all objects in the workspace, while goal-oriented tasks aim at grasping pre-assigned goal objects. However, most current grasping methods are only better at coping with one task. In this work, we propose a bifunctional push-grasping synergistic strategy for goal-agnostic and goal-oriented grasping tasks. Our method integrates pushing along with grasping to pick up all objects or pre-assigned goal objects with high action efficiency depending on the task requirement. We introduce a bifunctional network, which takes in visual observations and outputs dense pixel-wise maps of $Q$ values for pushing and grasping primitive actions, to increase the available samples in the action space. Then we propose a hierarchical reinforcement learning framework to coordinate the two tasks by considering the goal-agnostic task as a combination of multiple goal-oriented tasks. To reduce the training difficulty of the hierarchical framework, we design a two-stage training method to train the two types of tasks separately. We perform pre-training of the model in simulation, and then transfer the learned model to the real world without any additional real-world fine-tuning. Experimental results show that the proposed approach outperforms existing methods in task completion rate and grasp success rate with less motion number. Supplementary material is available at https://github.com/DafaRen/Learning_Bifunctional_Push-grasping_Synergistic_Strategy_for_Goal-agnostic_and_Goal-oriented_Tasks.</td>
                <td>Training, Visualization, Robot kinematics, Grasping, Reinforcement learning, Task analysis, Clutter</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342533&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342533&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Domain Adaptation on Point Clouds for 6D Pose Estimation in Bin-Picking Scenarios</td>
                <td>L. Zhao, M. Sun, W. J. Lv, X. Y. Zhang and L. Zeng</td>
                <td>2023</td>
                <td>Training with simulated data is a common approach in pose estimation research. However, a sim-to-real gap between clean simulated data and noisy real data will seriously weaken the generalization ability of the algorithm, especially for point clouds. To address this problem, this paper proposes a domain adaptive pose estimation network (DAPE-Net). For the feature extracted from the backbone, the network will conduct the real and simulation discrimination based on a feature discriminator, and complete the pose estimation by adversarial training. This makes the network pay more attention to the domain invariant features of simulation and real point clouds to complete domain adaptation. In our experiment, DAPE-Net improved the performance of pose estimation by 10%. To solve the problem that domain adaptation requires a small amount of real data, we propose a scheme that can semi-automatically collect real data in bin-picking scenarios for 6D pose estimation.</td>
                <td>Point cloud compression, Training, Adaptation models, Adaptive systems, Pose estimation, Prototypes, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341920&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341920&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Robotic Powder Weighing from Simulation for Laboratory Automation</td>
                <td>Y. Kadokawa, M. Hamaya and K. Tanaka</td>
                <td>2023</td>
                <td>This study focuses on a robotic powder weighing task used in laboratory automation. In this task, a robot weighs a certain amount of powder with a milligram-level target mass using a dispensing spoon. The complex dynamics of the powder, the variations in the materials being weighed, and the need to balance conservative and aggressive actions are significant challenges in the robotics field. Therefore, learning approaches are critical for this task. However, many learning interactions in real-world environments require substantial efforts to clean the spread powder. To overcome this issue, this study employs a sim-to-real transfer learning approach using a domain randomization (DR) technique. This enables the robot to weigh various powders with a small target mass and alleviates the burden of collecting data in a real-world environment. Herein, we formulated weighing manipulation as a reinforcement learning problem. Besides, we developed a powder weighing simulator and carefully selected the dynamics parameters used for DR to adapt to unseen environments. A recurrent neural network-based policy was adopted considering the balance of conservative and aggressive actions. The sim-to-real zero-shot transfer experiments demonstrated that the robot completed the weighing tasks with an average weighing error of 0.1 - 0.2 mg for different powder materials and target masses (5 - 15 mg). Overall, this approach shows promising results and can be useful for automating laboratory tasks that involve weighing powders.</td>
                <td>Powders, Automation, Recurrent neural networks, Transfer learning, Reinforcement learning, Maintenance engineering, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342463&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342463&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Constrained Generative Sampling of 6-DoF Grasps</td>
                <td>J. Lundell, F. Verdoja, T. N. Le, A. Mousavian, D. Fox and V. Kyrki</td>
                <td>2023</td>
                <td>Most state-of-the-art data-driven grasp sampling methods propose stable and collision-free grasps uniformly on the target object. For bin-picking, executing any of those reachable grasps is sufficient. However, for completing specific tasks, such as squeezing out liquid from a bottle, we want the grasp to be on a specific part of the object's body while avoiding other locations, such as the cap. This work presents a generative grasp sampling network, VCGS, capable of constrained 6-Degrees of Freedom (DoF) grasp sampling. In addition, we also curate a new dataset designed to train and evaluate methods for constrained grasping. The new dataset, called CONG, consists of over 14 million training samples of synthetically rendered point clouds and grasps at random target areas on 2889 objects. VCGS is benchmarked against GraspNet, a state-of-the-art unconstrained grasp sampler, in simulation and on a real robot. The results demonstrate that VCGS achieves a 10-15% higher grasp success rate than the baseline while being 2–3 times as sample efficient. Supplementary material is available on our project website.</td>
                <td>Training, Point cloud compression, Constraint handling, Liquids, Semantics, Grasping, Sampling methods</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341344&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341344&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RGBD Fusion Grasp Network with Large-Scale Tableware Grasp Dataset</td>
                <td>J. Yoon et al.</td>
                <td>2023</td>
                <td>This paper proposes a novel approach to address the technical challenges of stable object grasping, particularly in the context of handling tableware in a home environment. Handling tableware is particularly important, yet challenging, due to the flat nature of most tableware objects and the need to maintain a stable posture to prevent spills. To address these challenges, we present three key contributions: 1) a large-scale tableware dataset, not commonly found in the previous datasets; 2) a novel sampling method for stable grasp pose generation; and 3) a multi-modal fusion grasp network that effectively learns 6- DoF grasp pose, including flat objects. Our dataset contains over 45 million grasp poses and 1 million RGBD images captured in 800 scenes, which include randomly selected 10–18 tableware objects under 4 different lighting conditions. The grasp poses in the dataset are generated using a novel sampling method that incorporates geometric analysis to ensure stable grasping with minimal object movement. Furthermore, we design an RGBD fusion grasp network (RGBD-FGN) that can combine information from RGB and depth images considering each characteristic. Our experimental results demonstrate the superior performance of our approach over existing techniques, which is a significant contribution towards developing a multitasking home robot. Our dataset and source code can be accessed at https://github.com/SamsungLabs/RGBD-FGN.</td>
                <td>Fuses, Source coding, Lighting, Grasping, Benchmark testing, Sampling methods, Multitasking</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341357&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341357&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>One-Shot Affordance Learning (OSAL): Learning to Manipulate Articulated Objects by Observing Once</td>
                <td>R. Fan, T. Wang, M. Hirano and Y. Yamakawa</td>
                <td>2023</td>
                <td>We present One-Shot Affordance Learning (OSAL): a unified pipeline that learns manipulation for articulated objects by observing human demonstration only once. The key idea of our method is to embody affordance of articulated objects with an open-loop trajectory conditioned on a certain area of the object's surface. It serves as a simplified object-centric manipulation representation, which can be easily transferred into robot motion, while traditional methods fail to deal with the configuration difference between human hands and robot end effectors. Our system extracts the embodied affordance by focusing on hand action's effect on the object, and further grounds such affordance into object visual features through self-supervised learning for novel object configurations. We evaluated our method on a collection of real-life objects and furniture and demonstrated high success rates. With our system, humans only need to manipulate a novel object once with any gesture to transfer that manipulation skill to the robot, which we believe to be a highly efficient and user-friendly paradigm oriented for future real-life robots.</td>
                <td>Robot motion, Visualization, Service robots, Affordances, Friction, Pipelines, Self-supervised learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341421&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341421&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>EARL: Eye-on-Hand Reinforcement Learner for Dynamic Grasping with Active Pose Estimation</td>
                <td>B. Huang, J. Yu and S. Jain</td>
                <td>2023</td>
                <td>In this paper, we explore the dynamic grasping of moving objects through active pose tracking and reinforcement learning for hand-eye coordination systems. Most existing vision-based robotic grasping methods implicitly assume target objects are stationary or moving predictably. Performing grasping of unpredictably moving objects presents a unique set of challenges. For example, a pre-computed robust grasp can become unreachable or unstable as the target object moves, and motion planning must also be adaptive. In this work, we present a new approach, Eye-on-hAnd Reinforcement Learner (EARL), for enabling coupled Eye-on-Hand (EoH) robotic manipulation systems to perform real-time active pose tracking and dynamic grasping of novel objects without explicit motion prediction. EARL readily addresses many thorny issues in automated hand-eye coordination, including fast-tracking of 6D object pose from vision, learning control policy for a robotic arm to track a moving object while keeping the object in the camera's field of view, and performing dynamic grasping. We demonstrate the effectiveness of our approach in extensive experiments validated on multiple commercial robotic arms in both simulations and complex real-world tasks.</td>
                <td>Visualization, Target tracking, Robot kinematics, Pose estimation, Dynamics, Grasping, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341988&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341988&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Whole-Body Torque Control Without Joint Position Control Using Vibration-Suppressed Friction Compensation for Bipedal Locomotion of Gear-Driven Torque Sensorless Humanoid</td>
                <td>T. Hiraoka et al.</td>
                <td>2023</td>
                <td>Humanoids operate in repeated contact and non-contact with their environment and so the motion of humanoids such as walking on uneven terrain or in a narrow space requires the accurate force and position control. Joint torque control systems are suitable for position and force control, but are prone to friction and other modeling errors. To solve this problem, methods have been proposed to realize torque control in combination with joint position control systems or by improving joint structures such as sensors and actuators, but these methods have problems such as response delay and increased weight and volume. Thus, it is difficult to achieve motion of life-sized humanoids by whole-body torque control. In this paper, we solve challenges not with one specific layer, but rather with multiple layers that complement each other. We propose a hierarchical whole-body torque control method using four layers: friction compensation based on a vibration-suppressed model, whole-body resolved acceleration control using priority, center-of-gravity acceleration control based on foot-guided control, and landing position time modification based on capture point. We verify through walking experiments that the proposed methods can control the life-sized humanoid robot driven by high-reduction ratio joints by whole-body torque control without a torque sensor or joint position control, and that it enables the robot to move and even transport an object on outdoor uneven terrain.</td>
                <td>Legged locomotion, Torque, Friction, Torque control, Humanoid robots, Position control, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341698&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341698&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Approach for Generating Families of Energetically Optimal Gaits from Passive Dynamic Walking Gaits</td>
                <td>N. Rosa, B. Katamish, M. Raff and C. D. Remy</td>
                <td>2023</td>
                <td>For a class of biped robots with impulsive dynamics and a non-empty set of passive gaits (unactuated, periodic motions of the biped model), we present a method for computing continuous families of locally optimal gaits with respect to a class of commonly used energetic cost functions (e.g., the integral of torque-squared). We compute these families using only the passive gaits of the biped, which are globally optimal gaits with respect to these cost functions. Our approach fills in an important gap in the literature when computing a library of locally optimal gaits, which often do not make use of these globally optimal solutions as seed values. We demonstrate our approach on a well-studied two-link biped model.</td>
                <td>Legged locomotion, Computational modeling, Dynamics, Cost function, Libraries, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342322&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342322&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Stair Climbing Using the Angular Momentum Linear Inverted Pendulum Model and Model Predictive Control</td>
                <td>Ogunbi, A. Shrivastava, G. Gibson and J. W. Grizzle</td>
                <td>2023</td>
                <td>A new control paradigm using angular momentum and foot placement as state variables in the linear inverted pendulum model has expanded the realm of possibilities for the control of bipedal robots. This new paradigm, known as the ALIP model, has shown effectiveness in cases where a robot's center of mass height can be assumed to be constant or near constant as well as in cases where there are no non-kinematic restrictions on foot placement. Walking up and down stairs violates both of these assumptions, where center of mass height varies significantly within a step and the geometry of the stairs restrict the effectiveness of foot placement. In this paper, we explore a variation of the ALIP model that allows the length of the virtual pendulum formed by the robot's stance foot and center of mass to follow smooth trajectories during a step. We couple this model with a control strategy constructed from a novel combination of virtual constraint-based control and a model predictive control algorithm to stabilize a stair climbing gait that does not soley rely on foot placement. Simulations on a 20-degree of freedom model of the Cassie biped in the SimMechanics simulation environment show that the controller is able to achieve periodic gait.</td>
                <td>Legged locomotion, Geometry, Stairs, Predictive models, Prediction algorithms, Trajectory, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342369&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342369&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-time Dynamic Bipedal Avoidance</td>
                <td>T. Wang, J. White and C. Hubicki</td>
                <td>2023</td>
                <td>In real-world settings, bipedal robots must avoid collisions with people and their environment. Further, a biped can choose between modes of avoidance: (1) adjust its pose while standing or (2) step to gain maneuverability. We present a real-time motion planner and multibody control framework for dynamic bipedal robots that avoids multiple moving obstacles and automatically switches between standing and stepping modes as necessary. By leveraging a reduced-order model (i.e. Linear Inverted Pendulum Model) and a half-space relaxation of the safe region, the planner is formulated as a convex optimization problem (i.e. Quadratic Programming) that can be used for real-time application with Model-Predictive-Control (MPC). To facilitate mode switching, we introduce center-of-pressure related slack-variables to the convex planning optimization that both shapes the planning cost function and provides a mode switching criterion for dynamic locomotion. Finally, we implement the proposed algorithm on a 3D Cassie bipedal robot and present hardware experiments showing real-time bipedal standing avoidance, stepping avoidance, and automatic switching of avoidance modes.</td>
                <td>Three-dimensional displays, Shape, Heuristic algorithms, Dynamics, Switches, Real-time systems, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341951&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341951&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Data-Driven Adaptation for Robust Bipedal Locomotion with Step-to-Step Dynamics</td>
                <td>M. Dai, X. Xiong, J. Lee and A. D. Ames</td>
                <td>2023</td>
                <td>This paper presents an online framework for synthesizing agile locomotion for bipedal robots that adapts to unknown environments, modeling errors, and external disturbances. To this end, we leverage step-to-step (S2S) dynamics which has proven effective in realizing dynamic walking on underactuated robots-assuming known dynamics and environments. This paper considers the case of uncertain models and environments and presents a data-driven representation of the S2S dynamics that can be learned via an adaptive control approach that is both data-efficient and easy to implement. The learned S2S controller generates desired discrete foot placement, which is then realized on the full-order dynamics of the bipedal robot by tracking desired outputs synthesized from the given foot placement. The benefits of the proposed approach are twofold. First, it improves the ability of the robot to walk at a given desired velocity when compared to the non-adaptive baseline controller. Second, the data-driven approach enables stable and agile locomotion under the effect of various unknown disturbances: additional unmodeled payload, large robot model errors, external disturbance forces, biased velocity estimation, and sloped terrains. This is demonstrated through in-depth evaluation with a high-fidelity simulation of the bipedal robot Cassie subject to the aforementioned disturbances [1].</td>
                <td>Legged locomotion, Adaptation models, Dynamics, Estimation, Robustness, Behavioral sciences, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341396&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341396&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Template Model Inspired Task Space Learning for Robust Bipedal Locomotion</td>
                <td>G. A. Castillo, B. Weng, S. Yang, W. Zhang and A. Hereid</td>
                <td>2023</td>
                <td>This work presents a hierarchical framework for bipedal locomotion that combines a Reinforcement Learning (RL)-based high-level (HL) planner policy for the online generation of task space commands with a model-based low-level (LL) controller to track the desired task space trajectories. Different from traditional end-to-end learning approaches, our HL policy takes insights from the angular momentum-based linear inverted pendulum (ALIP) to carefully design the observation and action spaces of the Markov Decision Process (MDP). This simple yet effective design creates an insightful mapping between a low-dimensional state that effectively captures the complex dynamics of bipedal locomotion and a set of task space outputs that shape the walking gait of the robot. The HL policy is agnostic to the task space LL controller, which increases the flexibility of the design and generalization of the framework to other bipedal robots. This hierarchical design results in a learning-based framework with improved performance, data efficiency, and robustness compared with the ALIP model-based approach and state-of-the-art learning-based frameworks for bipedal locomotion. The proposed hierarchical controller is tested in three different robots, Rabbit, a five-link underactuated planar biped; Walker2D, a seven-link fully-actuated planar biped; and Digit, a 3D humanoid robot with 20 actuated joints. The trained policy naturally learns human-like locomotion behaviors and is able to effectively track a wide range of walking speeds while preserving the robustness and stability of the walking gait even under adversarial conditions.</td>
                <td>Legged locomotion, Torso, Three-dimensional displays, Aerospace electronics, Stairs, Robustness, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341263&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341263&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Overtaking Moving Obstacles with Digit: Path Following for Bipedal Robots via Model Predictive Contouring Control</td>
                <td>K. S. Narkhede, D. A. Thanki, A. M. Kulkarni and I. Poulakakis</td>
                <td>2023</td>
                <td>Humanoid robots are expected to navigate in changing environments and perform a variety of tasks. Frequently, these tasks require the robot to make decisions online regarding the speed and precision of following a reference path. For example, a robot may want to decide to temporarily deviate from its path to overtake a slowly moving obstacle that shares the same path and is ahead. In this case, path following performance is compromised in favor of fast path traversal. Available global trajectory tracking approaches typically assume a given-specified in advance-time parametrization of the path and seek to minimize the norm of the Cartesian error. As a result, when the robot should be where on the path is fixed and temporary deviations from the path are strongly discouraged. Given a global path, this paper presents a Model Predictive Contouring Control (MPCC) approach to selecting footsteps that maximize path traversal while simultaneously allowing the robot to decide between faithful versus fast path following. The method is evaluated in high-fidelity simulations of the bipedal robot Digit in terms of tracking performance of curved paths under disturbances and is also applied to the case where Digit overtakes a moving obstacle.</td>
                <td>Legged locomotion, Trajectory tracking, Navigation, Humanoid robots, Predictive models, Trajectory, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342209&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342209&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>STL: Surprisingly Tricky Logic (for System Validation)</td>
                <td>H. C. Siu, K. Leahy and M. Mann</td>
                <td>2023</td>
                <td>Much of the recent work developing formal methods techniques to specify or learn the behavior of autonomous systems is predicated on a belief that formal specifications are interpretable and useful for humans when checking systems. Though frequently asserted, this assumption is rarely tested. We performed a human experiment $(\mathbf{N}=62)$ with a mix of people who were and were not familiar with formal methods beforehand, asking them to validate whether a set of signal temporal logic (STL) constraints would keep an agent out of harm and allow it to complete a task in a gridworld capture-the-ftag setting. Validation accuracy was 45% $\pm$ 20% (mean $\pm$ standard deviation). The ground-truth validity of a specification, subjects' familiarity with formal methods, and subjects' level of education were found to be significant factors in determining validation correctness. Participants exhibited an affirmation bias, causing significantly increased accuracy on valid specifications, but significantly decreased accuracy on invalid specifications. Additionally, participants, particularly those familiar with formal methods, tended to be overconfident in their answers, and be similarly confident regardless of actual correctness. Our data do not support the belief that formal specifications are inherently human-interpretable to a meaningful degree for system validation. We recommend ergonomic improvements to data presentation and validation training, which should be tested before claims of interpretability make their way back into the formal methods literature.</td>
                <td>Training, Semantics, Behavioral sciences, Safety, Formal specifications, System validation, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342290&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342290&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time RRT* with Signal Temporal Logic Preferences</td>
                <td>A. Linard, I. Torre, E. Bartoli, A. Sleat, I. Leite and J. Tumova</td>
                <td>2023</td>
                <td>Signal Temporal Logic (STL) is a rigorous specification language that allows one to express various spatio-temporal requirements and preferences. Its semantics (called robustness) allows quantifying to what extent are the STL specifications met. In this work, we focus on enabling STL constraints and preferences in the Real-Time Rapidly Exploring Random Tree (RT-RRT*) motion planning algorithm in an environment with dynamic obstacles. We propose a cost function that guides the algorithm towards the asymptotically most robust solution, i.e. a plan that maximally adheres to the STL specification. In experiments, we applied our method to a social navigation case, where the STL specification captures spatio-temporal preferences on how a mobile robot should avoid an incoming human in a shared space. Our results show that our approach leads to plans adhering to the STL specification, while ensuring efficient cost computation.</td>
                <td>Navigation, Heuristic algorithms, Semantics, Dynamics, Real-time systems, Robustness, Planning, Signal Temporal Logic, Real-Time Planning, Sampling-based Motion Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341993&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341993&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sensor Selection for Fine-Grained Behavior Verification that Respects Privacy</td>
                <td>R. Phatak and D. A. Shell</td>
                <td>2023</td>
                <td>A useful capability is that of classifying some agent's behavior using data from a sequence, or trace, of sensor measurements. The sensor selection problem involves choosing a subset of available sensors to ensure that, when generated, observation traces will contain enough information to determine whether the agent's activities match some pattern. In generalizing prior work, this paper studies a formulation in which multiple behavioral itineraries may be supplied, with sensors selected to distinguish between behaviors. This allows one to pose fine-grained questions, e.g., to position the agent's activity on a spectrum. In addition, with multiple itineraries, one can also ask about choices of sensors where some behavior is always plausibly concealed by (or mistaken for) another. Using sensor ambiguity to limit the acquisition of knowledge is a strong privacy guarantee, a form of guarantee which some earlier work examined under formulations distinct from our inter-itinerary conflation approach. By concretely formulating privacy requirements for sensor selection, this paper connects both lines of work in a novel fashion: privacy-where there is a bound from above, and behavior verification-where sensors choices are bounded from below. We examine the worst-case computational complexity that results from both types of bounds, proving that upper bounds are more challenging under standard computational complexity assumptions. The problem is intractable in general, but we introduce an approach to solving this problem that can exploit interrelationships between constraints, and identify opportunities for optimizations. Case studies are presented to demonstrate the usefulness and scalability of our proposed solution, and to assess the impact of the optimizations.</td>
                <td>Privacy, Upper bound, Scalability, Robot sensing systems, Behavioral sciences, Computational complexity, Optimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341877&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341877&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Interactive System for Multiple-Task Linear Temporal Logic Path Planning</td>
                <td>Y. Chen et al.</td>
                <td>2023</td>
                <td>Beyond programming robots to accomplish a single high-level task at a time, people also hope robots follow instructions and complete a series of tasks while meeting their requirements. This paper presents an interactive software system that consists of a multiple-task linear temporal logic (LTL) path planner and a human-machine interface (HMI). The HMI transforms human oral instructions into task commands that can be understood by the machine. The planner grows a rapid random exploring tree to search for solutions for multiple tasks. When switching tasks, the search tree is re-initialized and reconnected to utilize the information gathered during the exploration of the workspace. The feasibility of the improved planner is theoretically guaranteed, and profiling in simulation shows an acceleration in planning. An experiment with a quadcopter is conducted to show that the combination of the multiple-task LTL planner and the HMI results in a synergistic effect in real-world applications.</td>
                <td>Interactive systems, Transforms, Switches, Programming, Software systems, Probabilistic logic, Path planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342309&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342309&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Temporal Logic-Based Intent Monitoring for Mobile Robots</td>
                <td>H. Yoon and S. Sankaranarayanan</td>
                <td>2023</td>
                <td>We propose a framework that uses temporal logic specifications to predict and monitor the intent of a robotic agent through passive observations of its actions over time. Our approach uses a set of possible hypothesized intents specified as Büchi automata, obtained from translating temporal logic formulae. Based on observing the actions of the robot, we update the probabilities of each hypothesis using Bayes rule. Observations of robot actions provide strong evidence for its “immediate” short-term goals, whereas temporal logic specifications describe behaviors over a “never-ending” infinite time horizon. To bridge this gap, we use a two-level hierarchical monitoring approach. At the lower level, we track the immediate short-term goals of the robot which are modeled as atomic propositions in the temporal logic formalism. We apply our approach to predicting intent of human workers and thus their movements in an indoor space based on the publicly available THOR dataset. We show how our approach correctly labels each agent with their appropriate intents after relatively few observations while predicting their future actions accurately over longer time horizons.</td>
                <td>Bridges, Tracking, Automata, Behavioral sciences, Mobile robots, Robots, Monitoring</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341623&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341623&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Evaluation Metrics of Object Detection for Quantitative System-Level Analysis of Safety-Critical Autonomous Systems</td>
                <td>A. Badithela, T. Wongpiromsarn and R. M. Murray</td>
                <td>2023</td>
                <td>This paper proposes two metrics for evaluating learned object detection models: the proposition-labeled and distance-parametrized confusion matrices. These metrics are leveraged to quantitatively analyze the system with respect to its system-level formal specifications via probabilistic model checking. In particular, we derive transition probabilities from these confusion matrices to compute the probability that the closed-loop system satisfies its system-level specifications expressed in temporal logic. Instead of using object class labels, the proposition-labeled confusion matrix uses atomic propositions relevant to the high-level control strategy. Furthermore, unlike the traditional confusion matrix, the proposed distance-parametrized confusion matrix accounts for variations in detection performance with respect to the distance between the ego and the object. Empirically, these evaluation metrics, chosen by considering system-level specifications and control module design, result in less conservative system-level evaluations than those from traditional confusion matrices. We demonstrate this framework on a car-pedestrian example by computing the satisfaction probabilities for safety requirements formalized in Linear Temporal Logic.</td>
                <td>Measurement, Autonomous systems, Computational modeling, Object detection, Model checking, Probabilistic logic, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342465&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342465&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Energy-Aware Planning of Heterogeneous Multi-Agent Systems for Serving Cooperative Tasks with Temporal Logic Specifications</td>
                <td>A. T. Buyukkocak, D. Aksaray and Y. Yazıcıoğlu</td>
                <td>2023</td>
                <td>We address a coordination problem for a team of heterogeneous and energy-limited agents to achieve cooperative tasks given as team-level spatio-temporal specifications. We assume that agents have stochastic energy dynamics and do not have identical capabilities. We define the team-level specification using Signal Temporal Logic (STL) with integral predicates, which can express tasks that can be completed collectively in an asynchronous way. We first abstract the environment as a graph using sampling-based methods. This abstraction includes the possible paths of different types of agents and ensures the availability of recharging within a certain distance. Then, we formulate a mixed-integer program over this abstraction to find the high-level paths of the agents. Finally, we steer the agents in the environment according to the nominal plan under stochastic energy consumption models and a recharging policy. Such stochastic energy dynamics cause deviations from the nominal plan and delays in completing the tasks. Accordingly, we define and evaluate the expected delay (temporal relaxation) in achieving the STL specification under the proposed solution.</td>
                <td>Energy consumption, Green products, Stochastic processes, Charging stations, Planning, Delays, State of charge</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342064&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342064&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Symbolic Approaches for Quantitative Reactive Synthesis with Finite Tasks</td>
                <td>K. Muvvala and M. Lahijanian</td>
                <td>2023</td>
                <td>This work introduces efficient symbolic algorithms for quantitative reactive synthesis. We consider resource-constrained robotic manipulators that need to interact with a human to achieve a complex task expressed in linear temporal logic. Our framework generates reactive strategies that not only guarantee task completion but also seek cooperation with the human when possible. We model the interaction as a two-player game and consider regret-minimizing strategies to encourage cooperation. We use symbolic representation of the game to enable scalability. For synthesis, we first introduce value iteration algorithms for such games with min-max objectives. Then, we extend our method to the regret-minimizing objectives. Our benchmarks reveal that our the symbolic framework not only significantly improves computation time (up to an order of magnitude) but also can scale up to much larger instances of manipulation problems with up to 2× number of objects and locations than the state of the art.</td>
                <td>Scalability, Games, Benchmark testing, Manipulators, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342496&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342496&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Minimal Path Violation Problem with Application to Fault Tolerant Motion Planning of Manipulators</td>
                <td>A. Upadhyay, M. Ghosh and C. Ekenna</td>
                <td>2023</td>
                <td>Failure of any component in a robotic system during operation is a critical concern, and it is essential to address such incidents promptly. This work investigates a novel technique to recover from failures or changes in the configuration space while avoiding expensive re-computation or re-planning. We propose the Minimal Path Violation (MPV) concept to find the best feasible path with minimal re-configurations. The algorithm ranks pathways based on visibility, expansiveness, and cost. We perform experiments with articulated 3 DOF to 28 DOF robots ranging from serial linkage robots, Kuka YouBots, and PR2 robots. Our results show that our method outperforms existing optimal planners in computation time, total nodes, and path cost while preserving path feasibility in changed configuration space.</td>
                <td>Fault tolerance, Costs, Limiting, Heuristic algorithms, Fault tolerant systems, Dynamics, Distance measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342242&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342242&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Reinforcement Learning Under Probabilistic Spatio-Temporal Constraints with Time Windows</td>
                <td>X. Lin, A. Koochakzadeh, Y. Yazıcıoğlu and D. Aksaray</td>
                <td>2023</td>
                <td>We propose an automata-theoretic approach for reinforcement learning (RL) under complex spatio-temporal constraints with time windows. The problem is formulated using a Markov decision process under a bounded temporal logic constraint. Different from existing RL methods that can eventually learn optimal policies satisfying such constraints, our proposed approach enforces a desired probability of constraint satisfaction throughout learning. This is achieved by translating the bounded temporal logic constraint into a total automaton and avoiding “unsafe” actions based on the available prior information regarding the transition probabilities, i.e., a pair of upper and lower bounds for each transition probability. We provide theoretical guarantees on the resulting probability of constraint satisfaction. We also provide numerical results in a scenario where a robot explores the environment to discover high-reward regions while fulfilling some periodic pick-up and delivery tasks that are encoded as temporal logic constraints.</td>
                <td>Learning automata, Reinforcement learning, Markov processes, Probabilistic logic, Time factors, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342259&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342259&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Unified Trajectory Generation Algorithm for Dynamic Dexterous Manipulation</td>
                <td>C. Zhou et al.</td>
                <td>2023</td>
                <td>This paper proposes a novel efficient multi-phase trajectory generation algorithm for dynamic dexterous manipulation tasks, such as throwing, catching, dynamic regrasping, and dynamic handover, which can be decomposed into multiple manipulation primitives, including sticking, rolling, approaching, separating, colliding, and grasping. Each manipulation primitive is formulate as a free-terminal optimal control problem (OCP), aimed at computing the optimal pose (position and orientation) trajectories of the object and the robot subject to the pose and force linkage constraints between them and the expected force maintenance at contact. A single-arm regrasping task and a dual-arm dynamic handover task are conducted to demonstrate the effectiveness of the proposed algorithm.</td>
                <td>Couplings, Heuristic algorithms, Dynamics, Force, Optimal control, Tactile sensors, Handover</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342095&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342095&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hybrid Learning- and Model-Based Planning and Control of In-Hand Manipulation</td>
                <td>R. S. Zarrin, R. Jitosho and K. Yamane</td>
                <td>2023</td>
                <td>This paper presents a hierarchical framework for planning and control of in-hand manipulation of a rigid object involving grasp changes using fully-actuated multifin-gered robotic hands. While the framework can be applied to the general dexterous manipulation, we focus on a more complex definition of in-hand manipulation, where at the goal pose the hand has to reach a grasp suitable for using the object as a tool. The high level planner determines the object trajectory as well as the grasp changes, i.e. adding, removing, or sliding fingers, to be executed by the low-level controller. While the grasp sequence is planned online by a learning-based policy to adapt to variations, the trajectory planner and the low-level controller for object tracking and contact force control are exclusively model-based to robustly realize the plan. By infusing the knowledge about the physics of the problem and the low-level controller into the grasp planner, it learns to successfully generate grasps similar to those generated by model-based optimization approaches, obviating the high computation cost of online running of such methods to account for variations. By performing experiments in physics simulation for realistic tool use scenarios, we show the success of our method on different tool-use tasks and dexterous hand models. Additionally, we show that this hybrid method offers more robustness to trajectory and task variations compared to a model-based method.</td>
                <td>Computational modeling, Robustness, Hardware, Trajectory, Planning, Object tracking, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342153&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342153&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Vision-Based In-Hand Manipulation of Variously Shaped Objects via Contact Point Prediction</td>
                <td>Y. Isobe, S. Kang, T. Shimamoto, Y. Matsuyama, S. Pathak and K. Umeda</td>
                <td>2023</td>
                <td>In-hand manipulation (IHM) is an important ability for robotic hands. This ability refers to changing the position and orientation of a grasped object without dropping it from the hand workspace. One major challenge of IHM is to achieve a large range of manipulation (especially rotation), regardless of the shape, size, and the orientation during manipulation of the grasped object. There are two main challenges - the manipulation range (due to the range of motion of the hand) and keeping the object grasped under all shapes and orientations. Specifically, even when the contact points between the hand and the object switch and the positions of these points change due to its shape and changing orientation, constant grasp of the object is required. This paper presents an IHM method for a robotic hand with belts, based on the prediction of the contact-point changes via image information. The focus is on a robotic hand that has a two-fingered parallel gripper with conveyor belts which can continuously manipulate an object through a large range. A stereo camera is attached to the hand. First, the contour of the grasped object is acquired from the camera. From the contour, the switching of the contact points between the surfaces of the belts and the object is predicted. Then, the positions of the contact points in the next frame are estimated by rotating the contour. The velocities of the belts are calculated based on the prediction of the switching. The fingers are controlled to follow the estimated positions of the contact points, via a feed-forward control. The effectiveness of the proposed method is verified through in-hand manipulation experiments for 22 objects of various shapes and sizes.</td>
                <td>Shape, Contacts, Robot vision systems, Switches, Grasping, Belts, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341968&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341968&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Object Manipulation Through Contact Configuration Regulation: Multiple and Intermittent Contacts</td>
                <td>O. Taylor, N. Doshi and A. Rodriguez</td>
                <td>2023</td>
                <td>In this work, we build on our method for manipulating unknown objects via contact configuration regulation: the estimation and control of the location, geometry, and mode of all contacts between the robot, object, and environment. We further develop our estimator and controller to enable manipulation through more complex contact interactions, including intermittent contact between the robot/object, and multiple contacts between the object/environment. In addition, we support a larger set of contact geometries at each interface. This is accomplished through a factor graph based estimation framework that reasons about the complementary kinematic and wrench constraints of contact to predict the current contact configuration. We are aided by the incorporation of a limited amount of visual feedback; which when combined with the available F/T sensing and robot proprioception, allows us to differentiate contact modes that were previously indistinguishable. We implement this revamped framework on our manipulation platform, and demonstrate that it allows the robot to perform a wider set of manipulation tasks. This includes, using a wall as a support to re-orient an object, or regulating the contact geometry between the object and the ground. Finally, we conduct ablation studies to understand the contributions from visual and tactile feedback in our manipulation framework. Our code can be found at: https://github.com/mcubelab/pbal.</td>
                <td>Geometry, Visualization, Estimation, Tactile sensors, Kinematics, Regulation, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341362&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341362&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Non-Parametric Self-Identification and Model Predictive Control of Dexterous In-Hand Manipulation</td>
                <td>P. Chanrungmaneekul, K. Ren, J. T. Grace, A. M. Dollar and K. Hang</td>
                <td>2023</td>
                <td>Building hand-object models for dexterous in-hand manipulation remains a crucial and open problem. Major challenges include the difficulty of obtaining the geometric and dynamical models of the hand, object, and time-varying contacts, as well as the inevitable physical and perception uncertainties. Instead of building accurate models to map between the actuation inputs and the object motions, this work proposes to enable the hand-object systems to continuously approximate their local models via a self-identification process where an underlying manipulation model is estimated through a small number of exploratory actions and non-parametric learning. With a very small number of data points, as opposed to most data-driven methods, our system self-identifies the underlying manipulation models online through exploratory actions and non-parametric learning. By integrating the self-identified hand-object model into a model predictive control framework, the proposed system closes the control loop to provide high accuracy in-hand manipulation. Furthermore, the proposed self-identification is able to adaptively trigger online updates through additional exploratory actions, as soon as the self-identified local models render large discrepancies against the observed manipulation outcomes. We implemented the proposed approach on a sensorless underactuated Yale Model O hand with a single external camera to observe the object's motion. With extensive experiments, we show that the proposed self-identification approach can enable accurate and robust dexterous manipulation without requiring an accurate system model nor a large amount of data for offline training.</td>
                <td>Training, Robust control, Adaptation models, Uncertainty, Buildings, Predictive models, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341520&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341520&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>In-Hand Cube Reconfiguration: Simplified</td>
                <td>S. Patidar, A. Sieler and O. Brock</td>
                <td>2023</td>
                <td>We present a simple approach to in-hand cube reconfiguration. By simplifying planning, control, and perception as much as possible, while maintaining robust and general performance, we gain insights into the inherent complexity of in-hand cube reconfiguration. We also demonstrate the effectiveness of combining GOFAI-based planning with the exploitation of environmental constraints and inherently compliant end-effectors in the context of dexterous manipulation. The proposed system outperforms a substantially more complex system for cube reconfiguration based on deep learning and accurate physical simulation, contributing arguments to the discussion about what the most promising approach to general manipulation might be. Project website: https://rbo.gitlab-pages.tu-berlin.de/robotics/simpleIHM/</td>
                <td>Deep learning, Dynamics, Robustness, End effectors, Planning, Complexity theory, Complex systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341521&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341521&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dexterous Soft Hands Linearize Feedback-Control for In-Hand Manipulation</td>
                <td>A. Sieler and O. Brock</td>
                <td>2023</td>
                <td>This paper presents a feedback-control framework for in-hand manipulation (IHM) with dexterous soft hands that enables the acquisition of manipulation skills in the real-world within minutes. We choose the deformation state of the soft hand as the control variable. To control for a desired deformation state, we use coarsley approximated Jacobians of the actuation-deformation dynamics. These Jacobian are obtained via explorative actions. This is enabled by the self-stabilizing properties of compliant hands, which allow us to use linear feedback control in the presence of complex contact dynamics. To evaluate the effectiveness of our approach, we show the generalization capabilities for a learned manipulation skill to variations in object size by 100 %, 360 degree changes in palm inclination and to disabling up to 50 % of the involved actuators. In addition, complex manipulations can be obtained by sequencing such feedback-skills.</td>
                <td>Jacobian matrices, Actuators, Sequential analysis, Deformation, Feedback control, Behavioral sciences, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341438&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341438&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>In-Hand Manipulation of Unknown Objects with Tactile Sensing for Insertion</td>
                <td>C. Pan, M. Lepert, S. Yuan, R. Antonova and J. Bohg</td>
                <td>2023</td>
                <td>In this paper, we present a method to manipulate unknown objects in-hand using tactile sensing without relying on a known object model. In many cases, vision-only approaches may not be feasible; for example, due to occlusion in cluttered spaces. We address this limitation by introducing a method to reorient unknown objects using tactile sensing. It incrementally builds a probabilistic estimate of the object shape and pose during task-driven manipulation. Our approach uses Bayesian optimization to balance exploration of the global object shape with efficient task completion. To demonstrate the effectiveness of our method, we apply it to a simulated Tactile-Enabled Roller Grasper, a gripper that rolls objects in hand while collecting tactile data. We evaluate our method on an insertion task with randomly generated objects and find that it reliably reorients objects while significantly reducing the exploration time.</td>
                <td>Three-dimensional displays, Shape, Robot sensing systems, Probabilistic logic, Sensors, Bayes methods, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341456&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341456&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bi-Manual Robot Shoe Lacing</td>
                <td>H. Luo and Y. Demiris</td>
                <td>2023</td>
                <td>Shoe lacing (SL) is a challenging sensorimotor task in daily life and a complex engineering problem in the shoe-making industry. In this paper, we propose a system for autonomous SL. It contains a mathematical definition of the SL task and searches for the best lacing pattern corresponding to the shoe configuration and the user preferences. We propose a set of action primitives and generate plans of action sequences according to the designed pattern. Our system plans the trajectories based on the perceived position of the eyelets and aglets with an active perception strategy, and deploys the trajectories on a bi-manual robot. Experiments demonstrate that the proposed system can successfully lace 3 different shoes in different configurations, with a completion rate of 92.0%, 91.6% and 77.5% for 6, 8 and 10-eyelet patterns respectively. To the best of our knowledge, this is the first demonstration of autonomous SL using a bi-manual robot.</td>
                <td>Industries, Active perception, Footwear, Robot sensing systems, Trajectory, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341934&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341934&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hand Design Approach for Planar Fully Actuated Manipulators</td>
                <td>K. Nave, K. DuFrene, N. Swenson, R. Balasubramanian and C. Grimm</td>
                <td>2023</td>
                <td>Robotic in-hand manipulation increases the capability of robotic hands to interact with the world. The amount of manipulation that a robot is capable of is highly dependent on the design of the robot hand, and previous works have shown success in designing hands to improve performance for different types of grasping and manipulation. In this paper we present a method for designing a fully-actuated planar manipulator that optimizes for specific in-hand motions. We demonstrate that, with the Asterisk Benchmark and a light-weight IK controller, we can translate our results from simulation to the real world with minimal effort and high-fidelity. Using the simulated data (over 4,000 simulated hand-designs) we begin to analyze which features contribute to improved planar manipulation.</td>
                <td>Grasping, Benchmark testing, Manipulators, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342077&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342077&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dynamic Finger Gaits via Pivoting and Adapting Contact Forces</td>
                <td>B. Jia</td>
                <td>2023</td>
                <td>For over three decades, finger gaiting has remained largely a subject for theoretical inquiries. Successful execution of a sequence of finger gaits does not simply reduce to planning collision-free paths for the involved fingers. A major issue is how to move the gaiting finger without losing the finger contacts with the object, which will most likely undergo a motion as the contact forces need to be adapted during the gait. This paper focuses on a single finger gait executed on a tool by an anthropomorphic hand driven by an arm. To improve stability, the tool's tip is leveraged as a pivot on the supporting plane. The gait consists of three stages: removal, during which the contact force on the gaiting finger gradually decreases to zero; relocation, during which the finger follows a pre-planned path (relative to the moving object) to establish a new contact; and addition, during which the contact force on the relocated finger increases to some desired level. Hybrid position/impedance control employs reference finger forces that satisfy the friction cone constraints and are dynamically consistent with the object's motion, which in turn provides reference poses for the fingertips to maintain their contacts during the gait. Finger gaits have been demonstrated on a kitchen knife and a screwdriver with an Adept SCARA robot and a Shadow Dexterous Hand.</td>
                <td>Friction, Force, Dynamics, Planning, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342156&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342156&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Rotating Objects via in-Hand Pivoting Using Vision, Force and Touch</td>
                <td>S. Xu, T. Liu, M. Wong, D. Kulić and A. Cosgun</td>
                <td>2023</td>
                <td>We propose a robotic manipulation method that can pivot objects on a surface using vision, wrist force and tactile sensing. We aim to control the rotation of an object around the grip point of a parallel gripper by allowing rotational slip, while maintaining a desired wrist force profile. Our approach runs an end-effector position controller and a gripper width controller concurrently in a closed loop. The position controller maintains a desired force using vision and wrist force. The gripper controller uses tactile sensing to keep the grip firm enough to prevent translational slip, but loose enough to allow rotational slip. Our sensor-based control approach relies on matching a desired force profile derived from object dimensions and weight, as well as vision-based monitoring of the object pose. The gripper controller uses tactile sensors to detect and prevent translational slip by tightening the grip when needed. Experimental results where the robot was tasked with rotating cuboid objects 90 degrees show that the multi-modal pivoting approach was able to rotate the objects without causing lift or translational slip, and was more energy-efficient compared to using a single sensor modality or pick-and-place.</td>
                <td>Wrist, Force, Tactile sensors, Robot sensing systems, Energy efficiency, Sensors, Grippers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341505&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341505&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CoFlyers: A Universal Platform for Collective Flying of Swarm Drones</td>
                <td>J. Huang, F. Wang and T. Hu</td>
                <td>2023</td>
                <td>Swarm drones flying is a very attractive field of robotics research, motivated by natural bird flocking or other animal collective behaviors. In this paper, we propose and develop an open-source11https://github.com/micros-uav/CoFlyers universal platform CoFlyers for end-to-end whole-chain development from flocking-inspired models to real-drone swarm flying. In particular, CoFlyers is more user-friendly with only a unified programming language of MATLAB&Simulink, rather than several existing platforms with mixed programming languages or more efforts on raw functional modules. The prototype simulator of CoFlyers is implemented in MATLAB, allowing users to quickly develop and prototype swarm flying algorithms, and to conduct task-oriented parameter auto-tuning and batch processing within reproducible scenarios. Moreover, a real-world verification module of swarm drones is developed in Simulink as well, which directly calls the prototype simulator modules for code reuse. It connects the external platforms via a standardized user-datagram-protocol communication in-terface. As a case study, CoFlyers is utilized into a multi-drone collective flying scenario in confined environments, by implementing ROS&PX4&Gazebo for high-fidelity simulation and Optitrack&Tello-drones for experiments. Eventually, both simulation and experimental results have demonstrated and validated the user-friendly practicability of CoFlyers.</td>
                <td>Computer languages, Codes, Software packages, Prototypes, Birds, Behavioral sciences, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342485&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342485&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Collective Decision-Making and Change Detection with Bayesian Robots in Dynamic Environments</td>
                <td>K. Pfister and H. Hamann</td>
                <td>2023</td>
                <td>Solving complex problems collectively with simple entities is a challenging task for swarm robotics. For the task of collective decision-making, robots decide based on local observations on the microscopic level to achieve consensus on the macroscopic level. We study this problem for a common benchmark of classifying distributed features in a binary dynamic environment. Our special focus is on environmental features that are dynamic as they change during the experiment. We present a control algorithm that uses sophisticated statistical change detection in combination with Bayesian robots to classify dynamic environments. The main profit is to reduce false positives allowing for improved speed and accuracy in decision-making. Supported by results from various simulated experiments, we introduce three feedback loops to balance speed and accuracy. In our benchmarks, we show the superiority of our new approach over previous works on Bayesian robots. Our approach of using change detection shows a more reliable detection of environmental changes. This enables the swarm to successfully classify even difficult environments (i.e., hard to detect differences between the binary features), while achieving faster and more accurate results in simpler environments.</td>
                <td>Heuristic algorithms, Microscopy, Decision making, Swarm robotics, Benchmark testing, Software, Bayes methods</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341649&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341649&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Autonomous Swarm Robot Coordination via Mean-Field Control Embedding Multi-Agent Reinforcement Learning</td>
                <td>P. Zhang</td>
                <td>2023</td>
                <td>The learning approaches of designing a controller to guide the collective behavior of swarm robots have gained significant attention in recent years. However, the scalability of swarm robots and their inherent stochasticity complicate the control problem due to increasing complexity, unpredictability, and non-linearity. Despite considerable progress made in swarm robotics, addressing these challenges remains a significant issue. In this work, we model the stochastic dynamics of a swarm robot system and then propose a novel control framework based on a mean-field control (MFC) embedding multi-agent reinforcement learning (MARL) approach named MF-MARL to deal with these challenges. While MARL is able to deal with stochasticity statistically, we integrate MFC, allowing MF-MARL to cope with large-scale robots. Moreover, we apply statistical moments of robots' state and control action to discretize continuous input and enable MF-MARL to be applied in continuous scenarios. To demonstrate the effectiveness of MF-MARL, we evaluate the performance of the robots on a specific swarm simulation platform. The experimental results show that our algorithm outperforms the traditional algorithms both in navigation and manipulation tasks. Finally, we demonstrate the adaptability of the proposed algorithm through the component failure test.</td>
                <td>Navigation, Robot kinematics, Scalability, Swarm robotics, Stochastic processes, Reinforcement learning, Complexity theory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341749&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341749&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Instance Task in Swarm Robotics: Sorting Groups of Robots or Objects into Clusters with Minimalist Controllers</td>
                <td>A. Krischanski, Y. K. Lopes, A. B. Leal, R. F. Martins and R. S. U. Rosso</td>
                <td>2023</td>
                <td>Relying only on behaviors that emerge from simple responsive controllers; swarms of robots have been shown capable of autonomously aggregate themselves or objects into clusters without any form of communication. We push these controllers to the limit, requiring robots to sort themselves or objects into different clusters. Based on a responsive controller that maps the current reading of a line-of-sight sensor to a pair of speeds for the robots' differential wheels, we demonstrate how multiple tasks instances can be accomplished by a robotic swarm. Using the dividing rectangles approach and physics simulation, a training step optimizes the parameters of the controller guided by a fitness function. We conducted a series of systematic trials in physics-based simulation and evaluate the performance in terms of dispersion and the ratio of clustered robots/objects. Across 20 trials where 30 robots cluster themselves into 3 groups, an average of 99.83% of them were correctly clustered into their group after 300 s. Across 50 trials where 15 robots cluster 30 objects into 3 groups, an average of 61.20%, 82.87%, and 97.73% of objects were correctly clustered into their group after 600 s, 900 s, and 1800 s, respectively. The object cluster behavior scales well while the aggregation does not, the latter due to the requirement of control tuning based on the number of robots.</td>
                <td>Training, Scalability, Line-of-sight propagation, Wheels, Robot sensing systems, Behavioral sciences, Mobile robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341775&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341775&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bio-Inspired 3D Flocking Algorithm with Minimal Information Transfer for Drones Swarms</td>
                <td>M. Verdoucq, C. Sire, R. Escobedo, G. Theraulaz and G. Hattenberger</td>
                <td>2023</td>
                <td>This article introduces a bio-inspired 3D flocking algorithm for a drone swarm, built upon a previously established 2D model, which has proven to be effective in promoting stability, alignment, and distance variation between agents within large groups of agents. The study highlights how the incorporation of a vertical interaction between agents and the acquisition by each agent of a minimal amount of information about their most influential neighbor impacts the collective behavior of the swarm. Additionally, we present a comprehensive investigation of the impacts of the intensity of alignment and attraction interactions on the collective motion patterns that emerge at the group level. These results, mostly conducted in a validated simulator, have significant implications for designing efficient UAV swarm systems and using collective patterns, or phases, in operational contexts such as corridor tracking, surveillance, and exploration. Further research will explore the effectiveness and efficiency of this UAV swarm flocking algorithm, as well as its ability to ensure safe transitions between collective phases in different operational contexts.</td>
                <td>Solid modeling, Three-dimensional displays, Tracking, Biological system modeling, Surveillance, Autonomous aerial vehicles, Stability analysis, 3D Flocking Algorithm, Collective Motion, Distributed Control, Drone Swarm, Unmanned Aerial Vehicle (UAV)</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341413&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341413&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Generic Framework for Byzantine-Tolerant Consensus Achievement in Robot Swarms</td>
                <td>H. Zhao et al.</td>
                <td>2023</td>
                <td>Recent studies show that some security features that blockchains grant to decentralized networks on the internet can be ported to swarm robotics. Although the integration of blockchain technology and swarm robotics shows great promise, thus far, research has been limited to proof-of-concept scenarios where the blockchain-based mechanisms are tailored to a particular swarm task and operating environment. In this study, we propose a generic framework based on a blockchain smart contract that enables robot swarms to achieve secure consensus in an arbitrary observation space. This means that our framework can be customized to fit different swarm robotics missions, while providing methods to identify and neutralize Byzantine robots, that is, robots which exhibit detrimental behaviours stemming from faults or malicious tampering.</td>
                <td>Measurement, Smart contracts, Swarm robotics, Robot sensing systems, Robustness, Blockchains, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341423&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341423&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sharing the Control of Robot Swarms Among Multiple Human Operators: A User Study</td>
                <td>G. Miyauchi, Y. K. Lopes and R. Groß</td>
                <td>2023</td>
                <td>Simultaneously controlling multiple robot swarms is challenging for a single human operator. When involving multiple operators, however, they can each focus on controlling a specific robot swarm, which helps distribute the cognitive workload. They could also exchange some robots with each other in response to the requirements of the tasks they discover. This paper investigates the ability of multiple operators to dynamically share the control of robot swarms and the effects of different communication types on performance and human factors. A total of 52 participants completed an experiment in which they were randomly paired to form a team. In a $2\times 2$ mixed factorial study, participants were split into two groups by communication type (direct vs. indirect). Both groups experienced different robot-sharing conditions (robot-sharing vs. no-robot-sharing). Results show that although the ability to share robots did not necessarily increase task scores, it allowed the operators to switch between working independently and collaboratively, reduced the total energy consumed by the swarm, and was considered useful by the participants.</td>
                <td>Training, Atmospheric measurements, Human factors, Switches, Particle measurements, Task analysis, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342457&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342457&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Decentralized Multi-Agent Reinforcement Learning with Global State Prediction</td>
                <td>J. Bloom, P. Paliwal, A. Mukherjee and C. Pinciroli</td>
                <td>2023</td>
                <td>Deep reinforcement learning (DRL) has seen re-markable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second approach, we introduce Global State Prediction (GSP), a network trained to form a belief over the swarm as a whole and predict its future states. We provide a comprehensive study over four well-known deep reinforcement learning algorithms in environments with obstacles, measuring performance as the successful transport of the object to a goal location within a desired time-frame. Through an ablation study, we show that including GSP boosts performance and increases robustness when compared with methods that use global knowledge.</td>
                <td>Training, Deep learning, Robot kinematics, Message passing, Reinforcement learning, Markov processes, Prediction algorithms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341563&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341563&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Minimalistic Collective Perception with Imperfect Sensors</td>
                <td>K. Y. Chin, Y. Khaluf and C. Pinciroli</td>
                <td>2023</td>
                <td>Collective perception is a foundational problem in swarm robotics, in which the swarm must reach consensus on a coherent representation of the environment. An important variant of collective perception casts it as a best-of-n decision-making process, in which the swarm must identify the most likely representation out of a set of alternatives. Past work on this variant primarily focused on characterizing how different algorithms navigate the speed-vs-accuracy tradeoff in a scenario where the swarm must decide on the most frequent environmental feature. Crucially, past work on best-of-n decision-making assumes the robot sensors to be perfect (noise- and fault-less), limiting the real-world applicability of these algorithms. In this paper, we apply optimal estimation techniques and a decentralized Kalman filter to derive, from first principles, a probabilistic framework for minimalistic swarm robots equipped with flawed sensors. Then, we validate our approach in a scenario where the swarm collectively decides the frequency of a certain environmental feature. We study the speed and accuracy of the decision-making process with respect to several parameters of interest. Our approach can provide timely and accurate frequency estimates even in presence of severe sensory noise.</td>
                <td>Time-frequency analysis, Limiting, Navigation, Decision making, Swarm robotics, Sensor phenomena and characterization, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341384&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341384&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Onboard Predictive Flocking of Quadcopter Swarm in the Presence of Obstacles and Faulty Robots</td>
                <td>G. Önür, M. Şahin, E. E. Keyvan, A. E. Turgut and E. Şahin</td>
                <td>2023</td>
                <td>Achieving fluent flocking, similar to those observed in birds and fish, on robotic swarms in a desired direction while avoiding obstacles using onboard sensing and computation remains a challenge. In a previous study (Önür et al, Proc. of ANTS'2022), we proposed a predictive flocking model as a computationally efficient method to generate smoother and more robust motion of the swarm. In this study, we extend this model to achieve safe flocking in cluttered environments in the presence of faulty robots that get immobilized during flocking. Systematical evaluation of the model in simulation with different swarm sizes and different faulty robot ratios has shown that safe flocking can be achieved even when 40% of the robots malfunction during flocking. Finally, we validate the model on a swarm of five micro quadcopters using only onboard range and bearing sensors and computation in a distributed manner without any communication11Videos of the experiments are available here: https://www.youtube.com/playlist?list=PLY04vZs6xGr8U5Y8KWMD056ZRjaUVMh63..</td>
                <td>Filtering, Computational modeling, Predictive models, Robot sensing systems, Fish, Sensor systems, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341354&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341354&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>OA-Bug: An Olfactory-Auditory Augmented Bug Algorithm for Swarm Robots in a Denied Environment</td>
                <td>S. Tan et al.</td>
                <td>2023</td>
                <td>Searching in a denied environment is challenging for swarm robots as no assistance from GNSS, mapping, data sharing, and central processing is allowed. However, using olfactory and auditory signals to cooperate like animals could be an important way to improve the collaboration of swarm robots. In this paper, an Olfactory-Auditory augmented Bug algorithm (OA-Bug) is proposed for a swarm of autonomous robots to explore a denied environment. A simulation environment is built to measure the performance of OA-Bug. The coverage of the search task can reach 96.93% using OA-Bug, which is significantly improved compared with a similar algorithm, SGBA [1]. Furthermore, experiments are conducted on real swarm robots to prove the validity of OA-Bug. Results show that OA-Bug can improve the performance of swarm robots in a denied environment. Video: https://youtu.be/vj9cRiSmgeM.</td>
                <td>Global navigation satellite system, Animals, Computer bugs, Olfactory, Swarm robotics, Collaboration, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341510&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341510&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Agent Prioritization and Virtual Drag Minimization in Dynamical System Modulation For Obstacle Avoidance of Decentralized Swarms</td>
                <td>N. Douce et al.</td>
                <td>2023</td>
                <td>Efficient and safe multi-agent swarm coordination in environments where humans operate, such as warehouses, assistive living rooms, or automated hospitals, is crucial for adopting automation. In this paper, we augment the obstacle avoidance algorithm based on dynamical system modulation for a swarm of heterogeneous holonomic mobile agents. A smooth prioritization is proposed to change the reactivity of the swarm towards the specific agents. Further, a soft decoupling of the initial agent's kinematics is used to design an independent rotation control to ensure the agent reaches the desired position and orientation simultaneously. This decoupling allowed the introduction of a novel heuristic, the virtual drag. It minimizes the disturbance influence an agent has when moving through its surrounding. Additionally, the safety module adapts the velocity commands from the dynamical system modulation to avoid colliding trajectories between agents. The evaluation was performed in simulated assisted living and hospital environments. The prioritization successfully increased the minimum distance relative to a moving agent. The safety module is observed to create collision-free dynamics where alternative methods fail. Additionally, the repulsive nature of the safety module augments the convergence rate, thus making the proposed method better applicable to dense real-world scenarios.</td>
                <td>Hospitals, Mobile agents, Modulation, Kinematics, Minimization, Safety, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341717&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341717&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>How the Fingerprint Effect Applies to Digitized Fingerprint-Like Structures</td>
                <td>R. Kovenburg, C. George, R. Gale and B. Aksak</td>
                <td>2023</td>
                <td>The fingerprint effect describes the relationship between slip speed, fingerprint ridge spacing, and the frequency of vibrations created by the movement of a fingerprint across a surface. We have previously shown that the spacing between straight, parallel, evenly spaced ridges in fingerprint-like structures, and thus the vibrations produced by the fingerprint effect, are dependent on the orientation of the ridges with respect to the direction of movement. We also showed that, when ridge orientation is known, the fingerprint effect can be used to estimate slip speed in real-time. The physical processes behind the fingerprint effect also apply to the interaction between a surface and other, non-ridge, microstructures. It is, therefore, theoretically possible to use the fingerprint effect generated by these structures to estimate slip speed. However, it is first necessary to understand the nature of the fingerprint effect generated by these non-ridge structures. In this paper, we show that digitized structures, evenly spaced in columns and rows, have a more complex relationship to the fingerprint effect than ridges do. At most orientations, these structures produce vibrations amplified around four frequencies, each determined by a set of virtual ridges defined by the digitized structures. A sensor with $\mathbf{100}\ \boldsymbol{\mu} \mathbf{m}$ tall, $\mathbf{150}\ \boldsymbol{\mu} \mathbf{m}$ wide micropillars in evenly spaced rows and columns, with a spacing of $\mathbf{300}\ \boldsymbol{\mu} \mathbf{m}$ center-to-center, is fabricated. This sensor is tested at angles between 0° and 90° by 15° increments. The results support our theoretical analysis.</td>
                <td>Vibrations, Visualization, Shape, Friction, Estimation, Fingerprint recognition, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342396&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342396&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Two-Dimensional Reticular Core Optical Waveguide Sensor for Tactile and Positioning Sensing</td>
                <td>Z. Liu, Z. Li and L. Cheng</td>
                <td>2023</td>
                <td>Tactile sensors based on optical waveguides are highly sensitive to pressure, possess good chemical inertness and electromagnetic resistance, and are unaffected by temperature changes in the surrounding environment. Researchers have developed various waveguide structures with multi-level cores to simultaneously measure tactile forces and positions. However, these designs result in thicker waveguides and reduced sensitivity in the lower levels. This study introduces a two-dimensional reticular core optical waveguide for tactile force and positioning sensing, where vertical waveguides intersect each other. The reticular core reduces waveguide thickness and simplifies fabrication processes. The simulation investigates the characteristics of light propagation and geometric parameters. Experimental results confirm the proposed reticular waveguide's force-sensing capability, with an average sensitivity of 0.36 dB/N. Compared to the split-level structure, the reticular waveguide demonstrates more consistent sensitivities along the two shear directions. Utilizing a deep neural network, the spatial resolution achieves approximately 0.72 mm along the X-axis and 1.14 mm along the Y-axis, outperforming the split-level structure.</td>
                <td>Temperature measurement, Temperature sensors, Resistance, Sensitivity, Optical device fabrication, Tactile sensors, Electromagnetic waveguides</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342366&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342366&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sliding Touch-Based Exploration for Modeling Unknown Object Shape with Multi-Fingered Hands</td>
                <td>Y. Chen, A. E. Tekden, M. P. Deisenroth and Y. Bekiroglu</td>
                <td>2023</td>
                <td>Efficient and accurate 3D object shape reconstruction contributes significantly to the success of a robot's physical interaction with its environment. Acquiring accurate shape information about unknown objects is challenging, especially in unstructured environments, e.g. the vision sensors may only be able to provide a partial view. To address this issue, tactile sensors could be employed to extract local surface information for more robust unknown object shape estimation. In this paper, we propose a novel approach for efficient unknown 3D object shape exploration and reconstruction using a multi-fingered hand equipped with tactile sensors and a depth camera only providing a partial view. We present a multi-finger sliding touch strategy for efficient shape exploration using a Bayesian Optimization approach and a single-leader-multi-follower strategy for multi-finger smooth local surface perception. We evaluate our proposed method by estimating the 3D shape of objects from the YCB and OCRTOC datasets based on simulation and real robot experiments. The proposed approach yields successful reconstruction results relying on only a few continuous sliding touches. Experimental results demonstrate that our method is able to model unknown objects in an efficient and accurate way.</td>
                <td>Surface reconstruction, Three-dimensional displays, Sensitivity, Shape, Tactile sensors, Vision sensors, Manipulators</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342303&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342303&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Re-Evaluating Parallel Finger-Tip Tactile Sensing for Inferring Object Adjectives: An Empirical Study</td>
                <td>F. Zhang and P. Corke</td>
                <td>2023</td>
                <td>Finger-tip tactile sensors are increasingly used for robotic sensing to establish stable grasps and to infer object properties. Promising performance has been shown in a number of works for inferring adjectives that describe the object, but there remains a question about how each taxel contributes to the performance. This paper explores this question with empirical experiments, leading insights for future finger-tip tactile sensor usage and design: one tactile sensor instead of a pair of sensors is sufficient for symmetric objects and interaction motions; dense taxels are beneficial for texture-related adjectives, but can be distracting to non-texture-related ones; and a frame-rate much lower than the BioTac sensor can satisfy the demand of inferring object adjectives in the PHAC-2 dataset.</td>
                <td>Tactile sensors, Sensors, Biosensors, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342262&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342262&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Content Estimation Through Tactile Interactions with Deformable Containers</td>
                <td>L. Tsao</td>
                <td>2023</td>
                <td>Pouring snacks and moving containers with beverages are challenging for a service robot. To obtain accurate content properties for planning robotic motion, tactile sensing can provide information about the pressure distribution of the contact surface, which is not obvious by visual observation. In this work, we focus on estimating the content properties of various content materials in distinct deformable containers through tactile interactions. We propose a learning-based model that can estimate content properties by using the tactile data collected by slightly squeezing a container with the content of interest. We analyzed an uncalibrated tactile sensor and collected a dataset consisting of 1125 sets of tactile sequences, which are combinations of five types of deformable containers and eleven types of content materials in different content heights. Experiments were conducted on content estimation with known contents and containers, unknown contents, and unknown containers. For unknown contents, our model can still achieve 8.5% height relative error and 79.7% state of matter accuracy. Furthermore, we analyzed that the tactile features of contents with similar content properties are close in the latent snace to show the effectiveness of our model.</td>
                <td>Deformable models, Robot motion, Visualization, Service robots, Estimation, Tactile sensors, Containers, Content Estimation, Deformable Containers, Tactile Sensing, Robotic Interaction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342436&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342436&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Placing by Touching: An Empirical Study on the Importance of Tactile Sensing for Precise Object Placing</td>
                <td>L. Lach et al.</td>
                <td>2023</td>
                <td>This work deals with a practical everyday problem: stable object placement on flat surfaces starting from unknown initial poses. Common object-placing approaches require either complete scene specifications or extrinsic sensor measurements, e.g., cameras, that occasionally suffer from occlusions. We propose a novel approach for stable object placing that combines tactile feedback and proprioceptive sensing. We devise a neural architecture called PlaceNet that estimates a rotation matrix, resulting in a corrective gripper movement that aligns the object with the placing surface for the subsequent object manipulation. We compare models with different sensing modalities, such as force-torque, an external motion capture system, and two classical baseline models in real-world object placing tasks with different objects. The experimental evaluation of our placing policies with a set of unseen everyday objects reveals significant generalization of our proposed pipeline, suggesting that tactile sensing plays a vital role in the intrinsic understanding of robotic dexterous object manipulation. Code, models, and supplementary videos are available on https://sites.google.com/view/placing-by-touching.</td>
                <td>Training, Supervised learning, Pipelines, Tactile sensors, Propioception, Robot sensing systems, Motion capture</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342340&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342340&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Incipient Slip Detection with a Biomimetic Skin Morphology</td>
                <td>Cherrier</td>
                <td>2023</td>
                <td>Incipient slip is defined as the slippage of part, but not all, of the contact surface between a sensor and an object. Reliably detecting incipient slip in artificial tactile sensors would benefit autonomous robot handling capabilities by helping prevent object slippage during manipulation. Here, we present a biomimetic skin morphology based on the human fingerprint with application to marker-based tactile sensors such as the TacTip biomimetic optical tactile sensor. We modify the 3D-printed outer membrane of the TacTip to mimic glabrous skin morphology with the inclusion of external ridges (fingerprint) and internal markers (intermediate ridges), allowing localised shear deformation of the sensor's skin prior to the onset of gross slip. To validate the performance of this skin morphology, we train a random forest classifier (RFC) to identify incipient slip based on the extracted marker displacements from the sensor when it is compressed against an acrylic plate and moved laterally. The RFC model achieves 97.46% accuracy on incipient slip prediction, and is then validated on an unseen pouring task, in which gravity-induced incipient slip is detected on average within $418\pm 753\ \text{ms}$ of its onset, and before gross slip in all trials. This accurate detection of incipient slip enables corrective actions prior to the onset of gross slip, a key capability in robotic manipulation and upper-limb prosthetics.</td>
                <td>Biomimetics, Morphology, Tactile sensors, Surface morphology, Fingerprint recognition, Skin, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341807&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341807&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>GelSight Svelte: A Human Finger-Shaped Single-Camera Tactile Robot Finger with Large Sensing Coverage and Proprioceptive Sensing</td>
                <td>J. Zhao and E. H. Adelson</td>
                <td>2023</td>
                <td>Camera-based tactile sensing is a low-cost, popular approach to obtain highly detailed contact geometry information. However, most existing camera-based tactile sensors are fingertip sensors, and longer fingers often require extraneous elements to obtain an extended sensing area similar to the full length of a human finger. Moreover, existing methods to estimate proprioceptive information such as total forces and torques applied on the finger from camera-based tactile sensors are not effective when the contact geometry is complex. We introduce GelSight Svelte, a curved, human finger-sized, single-camera tactile sensor that is capable of both tactile and proprioceptive sensing over a large area. GelSight Svelte uses curved mirrors to achieve the desired shape and sensing coverage. Proprioceptive information, such as the total bending and twisting torques applied on the finger, is reflected as deformations on the flexible backbone of GelSight Svelte, which are also captured by the camera. We train a convolutional neural network to estimate the bending and twisting torques from the captured images. We conduct gel deformation experiments at various locations of the finger to evaluate the tactile sensing capability and proprioceptive sensing accuracy. To demonstrate the capability and potential uses of GelSight Svelte, we conduct an object holding task with three different grasping modes that utilize different areas of the finger.</td>
                <td>Geometry, Deformation, Fingers, Propioception, Tactile sensors, Bending, Tools</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341646&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341646&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Estimating Properties of Solid Particles Inside Container Using Touch Sensing</td>
                <td>J. Huang and W. Yuan</td>
                <td>2023</td>
                <td>Solid particles, such as rice and coffee beans, are commonly stored in containers and are ubiquitous in our daily lives. Understanding those particles' properties could help us make later decisions or perform later manipulation tasks such as pouring. Humans typically interact with the containers to get an understanding of the particles inside them, but it is still a challenge for robots to achieve that. This work utilizes tactile sensing to estimate multiple properties of solid particles enclosed in the container, specifically, content mass, content volume, particle size, and particle shape. We design a sequence of robot actions to interact with the container. Based on physical understanding, we extract static force/torque value from the F/T sensor, vibration-related features and topple-related features from the newly designed high-speed GelSight tactile sensor to estimate those four particle properties. We test our method on 37 very different daily particles, including powder, rice, beans, tablets, etc. Experiments show that our approach is able to estimate content mass with an error of 1.8 g, content volume with an error of 6.1 ml, particle size with an error of 1.1 mm, and achieves an accuracy of 75.6% for particle shape estimation. In addition, our method can generalize to unseen particles with unknown volumes. By estimating these particle properties, our method can help robots to better perceive the granular media and help with different manipulation tasks in daily life and industry.</td>
                <td>Shape, Estimation, Tactile sensors, Containers, Robot sensing systems, Solids, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341880&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341880&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Acquisition and Prediction of High-Density Tactile Field Data for Rigid and Flexible Objects</td>
                <td>H. Xue, P. Liu, Z. Ju and F. Sun</td>
                <td>2023</td>
                <td>Obtaining high-density tactile field information is a critical aspect of research in the field of robotic haptics, as it plays a decisive role in determining the precision of robot manipulations. Vision-based tactile sensors have unique high-resolution features, which make them promising for related research. However, previous studies have mainly focused on reconstructing the shape of rigid objects or predicting the three-dimensional force of rigid objects, neglecting the analysis of flexible objects. Moreover, due to the resolution limitations of existing commercial sensors, the performance evaluation of previous force prediction models relied solely on the total force. To overcome these limitations and in order to explore the tactile field information of objects with more attributes, this paper presents a detailed high-density tactile field data acquisition method based on a mechanical simulation environment. Additionally, we constructed a network to learn the mapping relationship between tactile images and six-dimensional tactile field information. Our results demonstrate that the proposed method can predict the three-dimensional force and displacement information of the object. Notably, the prediction error is within the tolerance range for fine manipulation by robots.</td>
                <td>Performance evaluation, Mechanical sensors, Shape, Force, Data acquisition, Tactile sensors, Sensor phenomena and characterization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341734&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341734&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Calibration-Free BEV Representation for Infrastructure Perception</td>
                <td>S. Fan, Z. Wang, X. Huo, Y. Wang and J. Liu</td>
                <td>2023</td>
                <td>Effective BEV object detection on infrastructure can greatly improve traffic scene understanding and vehicle-to-infrastructure (V2I) cooperative perception. However, cameras installed on infrastructure have various postures, and previous BEV detection methods rely on accurate calibration, which is difficult for practical applications due to inevitable natural factors (e.g., wind and snow). In this paper, we propose a Calibration-free BEV Representation (CBR) network, which achieves 3D detection based on BEV representation without calibration parameters and additional depth supervision. Specifically, we utilize two multi-layer perceptrons for decoupling the features from perspective view to front view and bird-eye view under boxes-induced foreground supervision. Then, a cross-view feature fusion module matches features from orthogonal views according to similarity and conducts BEV feature enhancement with front-view features. Experimental results on DAIR-V2X demonstrate that CBR achieves acceptable performance without any camera parameters and is naturally not affected by calibration noises. We hope CBR can serve as a baseline for future research addressing practical challenges of infrastructure perception.</td>
                <td>Three-dimensional displays, Error analysis, Vehicle-to-infrastructure, Snow, Robot vision systems, Object detection, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341916&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341916&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>UVSS: Unified Video Stabilization and Stitching for Surround View of Tractor-Trailer Vehicles</td>
                <td>C. Zhu, Y. Yang, H. Liang, Z. Dong and M. Fu</td>
                <td>2023</td>
                <td>Automotive surround-view camera systems have been commonly employed in automated driving to aid in near-field sensing and other perception tasks. Due to the large size of the body and the presence of multiple blind spots, panoramic surround-view systems are particularly crucial for tractor-trailer vehicles. However, the non-rigid body of tractor-trailer vehicles introduces pose changes between cameras, rendering traditional calibration-based methods inadequate. Additionally, cameras mounted separately on the tractor and the trailer will experience independent vibrations, resulting in undesirable shakiness in captured videos. In this paper, we propose a unified video stabilization and stitching method to address these challenges, which can smooth the unsteady frames and align the images from moving cameras. Delving into video stabilization techniques, we extend mesh-based motion model for unified stitching and leverage deep-learning based modules to handle complex real-world scenarios. Moreover, we design a new optimization framework to estimate the optimal displacements of mesh vertices, enabling simultaneous stabilization and stitching of frames. The experimental results, obtained by public datasets and videos captured from a model tractor-trailer vehicle, demonstrate that our approach outperforms previous methods and is highly effective in real-world applications.</td>
                <td>Vibrations, Robot vision systems, Parallel processing, Cameras, Rendering (computer graphics), Robustness, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342264&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342264&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Falcon: A Wide-and-Deep Onboard Active Vision System</td>
                <td>M. Hirano and Y. Yamakawa</td>
                <td>2023</td>
                <td>The tradeoff between the field-of-view and resolution of conventional onboard vision systems primarily results from their fixed optical components. We propose a novel active vision system, Falcon, as an optimal solution. This system comprises an electric zoom lens connected to a high-speed camera with a pair of galvanometer mirrors, enabling high-resolution imaging of a moving object across a wide range, from near to far. To ensure accurate calibration of the Falcon system, we introduce a novel mapping-based calibration method using external cameras. We also present a robust and lightweight visual feedback method that utilizes this mapping-based calibration for effective object tracking. The effectiveness of the Falcon system is verified by constructing a prototype and conducting tracking experiments in an indoor setting, which demonstrated the superior performance of our method. Additionally, we successfully achieved continuous and high-resolution imaging of a curved mirror on public roads while the vehicle was moving.</td>
                <td>Visualization, Machine vision, Roads, Robot vision systems, High-resolution imaging, Cameras, Calibration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342192&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342192&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Driver Distraction Detection for Daytime and Nighttime with Unpaired Visible and Infrared Image Translation</td>
                <td>Y. Lin</td>
                <td>2023</td>
                <td>Driver distraction detection is an important function of driver monitoring systems and intelligent vehicles. Most previous research only focuses on the system development for daytime operations. In this paper, we propose a network model, V2IA-Net, which is able to use the daytime visible and nighttime infrared images for the driver distraction detection task. With the visible-infrared image translation, driver action recognition and head pose detection, the driver distraction behavior can be analyzed in real-time performance. To provide realistic driving scenes for network training and testing, a visible-infrared image dataset, VID, is created. The proposed V2IA-Net is trained on the unpaired images, and capable of common feature extraction for visible-infrared image conversion. In the experiments, our technique is compared with various driver distraction detection models. The results have demonstrated the effectiveness of the proposed method. Source code and datasets are available at https://github.com/kk2487/V2IA-Net.</td>
                <td>Training, Image recognition, Head, Source coding, Network architecture, Real-time systems, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342206&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342206&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Long-Short Term Policy for Visual Object Navigation</td>
                <td>Y. Bai, X. Song, W. Li, S. Zhang and S. Jiang</td>
                <td>2023</td>
                <td>The goal of visual object navigation for an agent is to find the target objects accurately. Recent works mainly focus on the feature of embedding, attempting to learn better features with different variants, such as object distribution and graph representations. However, some typical navigation problems in complex environments, such as partially known and obstacle problems, may not be effectively addressed by previous feature embedding methods. In this paper, we propose a framework with a long-short objective policy, where the hidden states are classified according to the navigation objectives at that moment and separately rewarded. Specifically, we consider two objectives: the long-term objective is to go closer to the target, and the short-term objective is for obstacle avoidance and exploration. To alleviate the effect of long-term and short-term alternation, we build a state memory and propose an adjustment gate to update the state memory. Finally, all past hidden states are reweighted and combined for action prediction with an action-boosting gate. Experimental results on RoboTHOR show that the proposed method can significantly outperform the state-of-the-art.</td>
                <td>Visualization, Navigation, Reinforcement learning, Logic gates, Boosting, Space exploration, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341652&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341652&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lidar-Based Multiple Object Tracking with Occlusion Handling</td>
                <td>C. Lin</td>
                <td>2023</td>
                <td>Occlusion remains an issue in multiple object tracking, which could cause ambiguity in object detection, such as incorrect or missing detection. Under occlusion, a track could experience an early termination, resulting in identity switches and/or fragmentation. To recover from different lengths of occlusions, the track should be maintained by considering its occlusion status. To address the issues mentioned above, we propose an indicator that can model the track's occlusion extent via geometric information provided by LiDAR data. Through incorporating the indicator into the track management and data association process, it is feasible to prevent tracks from premature termination. The proposed method is evaluated on the collected dataset which undergoes frequent and severe occlusions. Compared to the state-of-the-art probabilistic tracking approach, our approach achieves improvements of 3.26% in MOTA and 5.36% in IDF1. Additionally, we obtain 9.89% improvements in IDF1 specifically for objects experiencing severe occlusions.</td>
                <td>Laser radar, Object detection, Probabilistic logic, Data models, Trajectory, Object tracking, Intelligent robots, Object Tracking, Occlusion, Fragmentation, Autonomous Driving</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342278&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342278&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Local and Global Information in Obstacle Detection on Railway Tracks</td>
                <td>M. Brucker, A. Cramariuc, C. Von Einem, R. Siegwart and C. Cadena</td>
                <td>2023</td>
                <td>Reliable obstacle detection on railways could help prevent collisions that result in injuries and potentially damage or derail the train. Unfortunately, generic object detectors do not have enough classes to account for all possible scenarios, and datasets featuring objects on railways are challenging to obtain. We propose utilizing a shallow network to learn railway segmentation from normal railway images. The limited receptive field of the network prevents overconfident predictions and allows the network to focus on the locally very distinct and repetitive patterns of the railway environment. Additionally, we explore the controlled inclusion of global information by learning to hallucinate obstacle-free images. We evaluate our method on a custom dataset featuring railway images with artificially augmented obstacles. Our proposed method outperforms other learning-based baseline methods.</td>
                <td>Training, Image segmentation, Image color analysis, Detectors, Rail transportation, Reliability, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342174&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342174&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hybrid Object Tracking with Events and Frames</td>
                <td>Z. Li et al.</td>
                <td>2023</td>
                <td>Robust object pose tracking plays an important role in robot manipulation, but it is still an open issue for quickly moving targets as motion blur and low frequency detection can reduce pose estimation accuracy even for state-of-the-art RGB-D-based methods. An event-camera is a low-latency vision sensor that can act complementary to RGB-D. Specifically, its sub-millisecond temporal resolution can be exploited to correct for pose estimation inaccuracies due to low frequency RGB-D based detection. To do so, we propose a dual Kalman filter: the first filter estimates an object's velocity from the spatiotemporal patterns of “events”, the second filter fuses the tracked object velocity with a low-frequency object pose estimated from a deep neural network using RGB-D data. The full system outputs high frequency, accurate object poses also for fast moving objects. The proposed method works towards low-power robotics by replacing high-cost GPU-based optical flow used in prior work with event-cameras that inherently extract the required signal without costly processing. The proposed algorithm achieves comparable or better performance when compared to two state-of-the-art 6-DoF object pose estimation algorithms and one hybrid event/RGB-D algorithm on benchmarks with simulated and real data. We discuss the benefits and tradeoffs for using the event-camera and contribute algorithm, code, and datasets to the community. The code and datasets are available at https://github.com/event-driven-robotics/Hybrid-object-tracking-with-events-and-frames.</td>
                <td>Optical filters, Training, Codes, Heuristic algorithms, Pose estimation, Vision sensors, 6-DOF</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342300&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342300&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Semantic Segmentation Based on Multiple Granularity Learning</td>
                <td>K. Wu et al.</td>
                <td>2023</td>
                <td>Accurate and robust coarse semantic segmentation plays a key role in the pursuit of autonomous driving. We present an algorithm that regularizes the representation space of Semantic Segmentation by Multiple Granularity Learning (SSMGL). This approach explores multiple levels of semantic knowledge in an unified framework, where the fine-grained semantic information can be either labeled or unlabeled. In our experiments, we find that SSMGL can achieve better results (1) on both on-road and off-road benchmarks, (2) under different segmentation architectures, or (3) with different backbones. The method is plug-and-play, not specialized for autonomous driving applications, and can be easily extended to any other segmentation scenario. Moreover, our SSMGL approach does not increase the computational overhead in the inference stage.</td>
                <td>Representation learning, Semantic segmentation, Semantics, Computer architecture, Boosting, Real-time systems, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341585&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341585&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhanced Robot Navigation with Human Geometric Instruction</td>
                <td>H. Deguchi, S. Taguchi, K. Shibata and S. Koide</td>
                <td>2023</td>
                <td>Recently, robot navigation methods using human instructions have been actively studied, including visual language navigation. Although language is one of the most promising forms of instruction, words often contain ambiguities. To complement this problem, we propose to use geometric instruction as a clue to the task goal. Specifically, in our proposed system, we assume that the robot receives a rough position of the target from human gesture. The robot adaptively estimates the reliability of this geometric instruction, and switches between exploration and instruction-following modes depending on the reliability value. We conducted evaluation of our method using a 3D simulation environment, and show that the task success rate and other metrics improve compared with the baseline methods.</td>
                <td>Measurement, Visualization, Three-dimensional displays, Navigation, Search methods, Reinforcement learning, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342107&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342107&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>InterTracker: Discovering and Tracking General Objects Interacting with Hands in the Wild</td>
                <td>Y. Shao, Q. Ye, W. Luo, K. Zhang and J. Chen</td>
                <td>2023</td>
                <td>Understanding human interaction with objects is an important research topic for embodied Artificial Intelligence and identifying the objects that humans are interacting with is a primary problem for interaction understanding. Existing methods rely on frame-based detectors to locate interacting objects. However, this approach is subjected to heavy occlusions, background clutter, and distracting objects. To address the limitations, in this paper, we propose to leverage spatio-temporal information of hand-object interaction to track interactive objects under these challenging cases. Without prior knowledge of the general objects to be tracked like object tracking problems, we first utilize the spatial relation between hands and objects to adaptively discover the interacting objects from the scene. Second, the consistency and continuity of the appearance of objects between successive frames are exploited to track the objects. With this tracking formulation, our method also benefits from training on large-scale general object-tracking datasets. We further curate a video-level hand-object interaction dataset for testing and evaluation from 100DOH. The quantitative results demonstrate that our proposed method outperforms the state-of-the-art methods. Specifically, in scenes with continuous interaction with different objects, we achieve an impressive improvement of about 10% as evaluated using the Average Precision (AP) metric. Our qualitative findings also illustrate that our method can produce more continuous trajectories for interacting objects.</td>
                <td>Training, Measurement, Location awareness, Detectors, Trajectory, Object tracking, Object recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341690&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341690&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ECTLO: Effective Continuous-Time Odometry Using Range Image for LiDAR with Small FoV</td>
                <td>X. Zheng and J. Zhu</td>
                <td>2023</td>
                <td>Prism-based LiDARs are more compact and cheaper than the conventional mechanical multi-line spinning LiDARs, which have become increasingly popular in robotics, recently. However, there are several challenges for these new LiDAR sensors, including small field of view, severe motion distortions, and irregular patterns. These difficulties hinder them from being widely used in LiDAR odometry, practically. To tackle these problems, we present an effective continuous-time LiDAR odometry (ECTLO) method for the Risley-prism-based LiDARs with non-repetitive scanning patterns. A single range image covering historical points in LiDAR's small FoV is adopted for efficient map representation. To account for the noisy data from occlusions after map updating, a filter-based point-to-plane Gaussian Mixture Model is used for robust registration. Moreover, a LiDAR-only continuous-time motion model is employed to relieve the inevitable distortions. Extensive experiments have been conducted on various testbeds using the prism-based LiDARs with different scanning patterns, whose promising results demonstrate the efficacy of our proposed approach.</td>
                <td>Point cloud compression, Laser radar, Distortion, Robot sensing systems, Sensor systems, Odometry, Noise measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341592&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341592&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TwistSLAM++: Fusing Multiple Modalities for Accurate Dynamic Semantic SLAM</td>
                <td>M. Gonzalez, E. Marchand, A. Kacete and J. Royan</td>
                <td>2023</td>
                <td>Most classical SLAM systems rely on the static scene assumption, which limits their applicability in real world scenarios. Recent SLAM frameworks have been proposed to simultaneously track the camera and moving objects. However they are often unable to estimate the canonical pose of the objects and exhibit a low object tracking accuracy. To solve this problem we propose TwistSLAM++, a semantic, dynamic, SLAM system that fuses stereo images and LiDAR information. Using semantic information, we track potentially moving objects and associate them to 3D object detections in LiDAR scans to obtain their pose and size. Then, we perform registration on consecutive object scans to refine object pose estimation. Finally, object scans are used to estimate the shape of the object and constrain map points to lie on the estimated surface within the bundle adjustment. We show on classical benchmarks that this fusion approach based on multimodal information improves the accuracy of object tracking.</td>
                <td>Simultaneous localization and mapping, Laser radar, Shape, Semantics, Pose estimation, Pipelines, Object detection, Slam, Localization, Mapping</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341786&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341786&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SLAM and Shape Estimation for Soft Robots</td>
                <td>M. A. Karimi, D. C. Bonham, E. Lopez, A. Srivastava and M. Spenko</td>
                <td>2023</td>
                <td>This paper describes Simultaneous Localization and Mapping (SLAM) techniques for mobile soft robots using on-board local sensors. The paper focuses on planar boundary-constrained swarms, which are comprised of identical modular sub-units, each flexibly connected to its neighbor. The sub-units themselves are not necessarily soft, but as the robot's size increases with respect to the size of the sub-units, the robot as a whole approaches a continuous system that exhibits the characteristics and behavior of a soft robot. Previous versions of this system have demonstrated grasping, shape formation, and tunneling; however, all prior embodiments have relied on external sensing for pose estimation. This paper is the first to demonstrate a fully self-sufficient boundary constrained swarm soft robot that does not rely on external pose estimation. The robot successfully navigates a maze-like environment while localizing and mapping the environment.</td>
                <td>Simultaneous localization and mapping, Shape, Navigation, Pose estimation, Robot vision systems, Soft robotics, Tunneling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342213&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342213&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Trajectory-Based SLAM for Indoor Mobile Robots with Limited Sensing Capabilities</td>
                <td>Y. Chen et al.</td>
                <td>2023</td>
                <td>In this paper we introduce a novel SLAM system for 2-D indoor environments that relies only on limited sensing. Our fully autonomous system uses only the trajectory of the robot around walls and objects in the environment as landmarks and is capable of robust and long-term exploration and mapping of a broad range of household floor plans. Rank-deficient and full-rank factors are created when the robot observes existing trajectory-based landmarks, and they are filtered and added in a pose graph, which is optimized periodically. The mission space is mapped by efficient adaptive local mapping algorithms. The proposed SLAM system has been extensively tested in various scenarios, and experimental results show its robustness and accuracy.</td>
                <td>Location awareness, Simultaneous localization and mapping, Space missions, Robustness, Sensors, Trajectory, Indoor environment</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341518&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341518&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Graph-Based Global Robot Localization Informing Situational Graphs with Architectural Graphs</td>
                <td>Lopez, J. Civera and H. Voos</td>
                <td>2023</td>
                <td>In this paper, we propose a solution for legged robot localization using architectural plans. Our specific contributions towards this goal are several. Firstly, we develop a method for converting the plan of a building into what we denote as an architectural graph (A-Graph). When the robot starts moving in an environment, we assume it has no knowledge about it, and it estimates an online situational graph representation (S-Graph) of its surroundings. We develop a novel graph-to-graph matching method, in order to relate the S-Graph estimated online from the robot sensors and the A-Graph extracted from the building plans. Note the challenge in this, as the S-Graph may show a partial view of the full A-Graph, their nodes are heterogeneous and their reference frames are different. After the matching, both graphs are aligned and merged, resulting in what we denote as an informed Situational Graph (is-Graph), with which we achieve global robot localization and exploitation of prior knowledge from the building plans. Our experiments show that our pipeline shows a higher robustness and a significantly lower pose error than several LiDAR localization baselines. Paper Video: https://youtu.be/3Pv7y8aOsUY</td>
                <td>Location awareness, Uncertainty, Navigation, Buildings, Pipelines, Merging, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341373&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341373&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SSGM: Spatial Semantic Graph Matching for Loop Closure Detection in Indoor Environments</td>
                <td>Y. Tang, M. Wang, Y. Deng, Y. Yang and Y. Yue</td>
                <td>2023</td>
                <td>Capturing the semantics of objects and the topological relationship allows the robot to describe the scene more intelligently like a human and measure the similarity between scenes (loop closure detection) more accurately. However, many current semantic graph matching methods are based on walk descriptors, which only extract adjacency relations between objects. In such way, the comprehensive information in the semantic graph is not fully exploited, which may lead to false closed-loop detection. This paper proposes a novel spatial semantic graph matching method (SSGM) in indoor environments, which considers multifaceted information of the semantic graphs. Firstly, two semantic graphs are aligned in the same coordinate space contributed by the second-order spatial compatibility metric between objects and local graph features of objects in semantic graphs. Secondly, the similarity of the spatial distribution of overall semantic graphs is further evaluated. The proposed algorithm is validated on public datasets and compared with the latest semantic graph matching methods, demonstrating improved accuracy and efficiency in loop closure detection. The code is available at https://github.com/BIT-TYJ/SSGM.</td>
                <td>Measurement, Graphical models, Codes, Robot kinematics, Current measurement, Semantics, Indoor environment</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342317&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342317&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Training-Free Attentive-Patch Selection for Visual Place Recognition</td>
                <td>K. Lam</td>
                <td>2023</td>
                <td>Visual Place Recognition (VPR) utilizing patch descriptors from Convolutional Neural Networks (CNNs) has shown impressive performance in recent years. Existing works either perform exhaustive matching of all patch descriptors, or employ complex networks to select good candidate patches for further geometric verification. In this work, we develop a novel two-step training-free patch selection method that is fast, while being robust to large occlusions and extreme viewpoint variations. In the first step, a self-attention mechanism is used to select sparse and evenly distributed discriminative patches in the query image. Next, a novel spatial-matching method is used to rapidly select corresponding patches with high similar appearances between the query and each reference image. The proposed method is inspired by how humans perform place recognition by first identifying prominent regions in the query image, and then relying on back-and-forth visual inspection of the query and reference image to attentively identify similar regions while ignoring dissimilar ones. Extensive experiment results show that our proposed method outperforms state-of-the-art (SOTA) methods in both place recognition precision and runtime, on various challenging conditions.</td>
                <td>Visualization, Image recognition, Runtime, Complex networks, Inspection, Robustness, Convolutional neural networks</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342347&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342347&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Exact Point Cloud Downsampling for Fast and Accurate Global Trajectory Optimization</td>
                <td>K. Koide, S. Oishi, M. Yokozuka and A. Banno</td>
                <td>2023</td>
                <td>This paper presents a point cloud downsampling algorithm for fast and accurate trajectory optimization based on global registration error minimization. The proposed algorithm selects a weighted subset of residuals of the input point cloud such that the subset yields exactly the same quadratic point cloud registration error function as that of the original point cloud at the evaluation point. This method accurately approximates the original registration error function with only a small subset of input points (29 residuals at a minimum). Experimental results using the KITTI dataset demonstrate that the proposed algorithm significantly reduces processing time (by 87%) and memory consumption (by 99%) for global registration error minimization while retaining accuracy.</td>
                <td>Point cloud compression, Visualization, Simultaneous localization and mapping, Costs, Electric breakdown, Memory management, Minimization</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342403&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342403&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>GAPSLAM: Blending Gaussian Approximation and Particle Filters for Real-Time Non-Gaussian SLAM</td>
                <td>Q. Huang and J. J. Leonard</td>
                <td>2023</td>
                <td>Inferring the posterior distribution in SLAM is critical for evaluating the uncertainty in localization and mapping, as well as supporting subsequent planning tasks aiming to reduce uncertainty for safe navigation. However, real-time full posterior inference techniques, such as Gaussian approximation and particle filters, either lack expressiveness for representing non-Gaussian posteriors or suffer from performance degeneracy when estimating high-dimensional posteriors. Inspired by the complementary strengths of Gaussian approximation and particle filters-scalability and non-Gaussian estimation, respectively-we blend these two approaches to infer marginal posteriors in SLAM. Specifically, Gaussian approximation provides robot pose distributions on which particle filters are conditioned to sample landmark marginals. In return, the maximum a posteriori point among these samples can be used to reset linearization points in the nonlinear optimization solver of the Gaussian approximation, facilitating the pursuit of global optima. We demonstrate the scalability, generalizability, and accuracy of our algorithm for real-time full posterior inference on realworld range-only SLAM and object-based bearing-only SLAM datasets.</td>
                <td>Simultaneous localization and mapping, Uncertainty, Approximation algorithms, Real-time systems, Particle filters, Inference algorithms, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341889&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341889&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Scale Point Octree Encoding Network for Point Cloud Based Place Recognition</td>
                <td>Z. Tang, H. Ye and H. Zhang</td>
                <td>2023</td>
                <td>Over the past decades, point cloud-based place recognition has garnered significant attention. This research paper presents a pioneering approach, denoted as the Multi-scale Point Octree Encoding Network (MPOE-Net), designed to acquire a discriminative global descriptor for efficient retrieval of places. The key element of the MPOE-Net is the point octree encoding module, which adeptly captures local information for each point by considering its nearest and farthest neighbors. Further enhancing local relationships, a multi-transformer network is introduced, utilizing a novel grouped offset-attention mechanism. To amalgamate the multi-scale attention maps into a comprehensive global descriptor, a multi-NetVLAD layer is incorporated. Through rigorous experimentation across diverse benchmark datasets, our proposed method unequivocally outperforms existing techniques in the realm of point cloud-based place recognition tasks, achieving state-of-the-art results. Our code is released publicly at https://github.com/Zhilong-Tang/MPOE-Net.</td>
                <td>Point cloud compression, Codes, Octrees, Benchmark testing, Encoding, Data mining, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341943&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341943&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Analytical Jacobian Approximation for Direct Optimization of a Trajectory of Interpolated Poses on SE(3)</td>
                <td>K. Botashev and G. Ferrer</td>
                <td>2023</td>
                <td>This paper relates to time-continuous trajectory representation using direct linear interpolation on SE(3). Our approach focuses on a novel analytical Jacobian approximation of a sequence of linearly interpolated poses on SE(3). This paper shows a derivation of the proposed analytical Jacobian using retraction mapping and an approximation to the commutativity property of infinitesimal group elements. We provide plenty of evaluations for 3 different optimization problems. For the synthetic point cloud alignment problem, our proposed Jacobian is compared with a numerical one. For the synthetic pose graph optimization problem, the proposed Jacobian approximation allows us to reduce by x7 factor the state dimensions while keeping a similar magnitude of resulting error compared to the full discrete-time trajectory. Finally, we show the validity of our approach in a time-continuous approach for real-world LIDAR odometry problem.</td>
                <td>Jacobian matrices, Point cloud compression, Interpolation, Three-dimensional displays, Laser radar, Trajectory, Odometry</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342321&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342321&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>On Cyber-Attacks Mitigation for Distributed Trajectory Generators</td>
                <td>Rawashdeh and M. Al Janaideh</td>
                <td>2023</td>
                <td>In this paper, an immune average consensus behavior of distributed trajectory generators given in the form of a multi-agent system is presented. Starting with the well-known results of linear consensus protocols, we propose a decomposition of the invariant consensus value to enable a distributed cyber-attacks detection and mitigation mechanism among the connected agents over mainly undirected communication links. This decomposition suggests one preferred propagation of the invariant quantity along communication links of the multi-agent systems under study. Despite its simplicity, the effectiveness of this mechanism in detecting and mitigating various types of cyber-attacks is evident through a numerical simulation. Interestingly, the resulting defense mechanism will not be passive, rather it can initiate its counter-attack measures by pretending that the attack process was a success. Moreover, the trajectory generators can operate under stealth mode where the communication links get silenced or totally disconnected without affecting the intended behavior after having the consensus value locked.</td>
                <td>Numerical simulation, Generators, Trajectory, Behavioral sciences, Consensus protocol, Cyberattack, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342286&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342286&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>CAMETA: Conflict-Aware Multi-Agent Estimated Time of Arrival Prediction for Mobile Robots</td>
                <td>J. le Fevre Sejersen and E. Kayacan</td>
                <td>2023</td>
                <td>This study presents the conflict-aware multi-agent estimated time of arrival (CAMETA) framework, a novel approach for predicting the arrival times of multiple agents in unstructured environments without predefined road infrastructure. The CAMETA framework consists of three components: a path planning layer generating potential path suggestions, a multi-agent ETA prediction layer predicting the arrival times for all agents based on the paths, and lastly, a path selection layer that calculates the accumulated cost and selects the best path. The novelty of the CAMETA framework lies in the heterogeneous map representation and the heterogeneous graph neural network architecture. As a result of the proposed novel structure, CAMETA improves the generalization capability compared to the state-of-the-art methods that rely on structured road infrastructure and historical data. The simulation results demonstrate the efficiency and efficacy of the multi-agent ETA prediction layer, with a mean average percentage error improvement of 29.5% and 44% when compared to a traditional path planning method (A *) which does not consider conflicts. The performance of the CAMETA framework shows significant improvements in terms of robustness to noise and conflicts as well as determining proficient routes compared to state-of-the-art multi-agent path planners.</td>
                <td>Costs, Roads, Simulation, Path planning, Robustness, Graph neural networks, Mobile robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341937&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341937&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Non-Linear Heterogeneous Bayesian Decentralized Data Fusion</td>
                <td>O. Dagan, T. L. Cinquini and N. R. Ahmed</td>
                <td>2023</td>
                <td>The factor graph decentralized data fusion (FG-DDF) framework was developed for the analysis and exploitation of conditional independence in heterogeneous Bayesian decentralized fusion problems, in which robots update and fuse pdfs over different, but overlapping subsets of random states. This allows robots to efficiently use smaller probabilistic models and sparse message passing to accurately and scalably fuse relevant local parts of a larger global joint state pdf while accounting for data dependencies between robots. Whereas prior work required limiting assumptions about network connectivity and model linearity, this paper relaxes these to explore the applicability and robustness of FG-DDF in more general settings. We develop a new heterogeneous fusion rule which generalizes the homogeneous covariance intersection algorithm for such cases and test it in multi-robot tracking and localization scenarios with non-linear motion/observation models under communication dropouts. Simulation and hardware experiments show that, in practice, the FG-DDF continues to provide consistent filtered estimates under these more practical operating conditions, while reducing computation and communication costs by more than 99%, thus enabling the design of scalable real-world multi-robot systems.</td>
                <td>Fuses, Tracking, Computational modeling, Simulation, Data integration, Probabilistic logic, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342177&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342177&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>BRNES: Enabling Security and Privacy-Aware Experience Sharing in Multiagent Robotic and Autonomous Systems</td>
                <td>M. T. Hossain, H. M. La, S. Badsha and A. Netchaev</td>
                <td>2023</td>
                <td>Although experience sharing (ES) accelerates multiagent reinforcement learning (MARL) in an advisor-advisee framework, attempts to apply ES to decentralized multiagent systems have so far relied on trusted environments and over-looked the possibility of adversarial manipulation and inference. Nevertheless, in a real-world setting, some Byzantine attackers, disguised as advisors, may provide false advice to the advisee and catastrophically degrade the overall learning performance. Also, an inference attacker, disguised as an advisee, may conduct several queries to infer the advisors' private information and make the entire ES process questionable in terms of privacy leakage. To address and tackle these issues, we propose a novel MARL framework (BRNES) that heuristically selects a dynamic neighbor zone for each advisee at each learning step and adopts a weighted experience aggregation technique to reduce Byzantine attack impact. Furthermore, to keep the agent's private information safe from adversarial inference attacks, we leverage the local differential privacy (LDP)-induced noise during the ES process. Our experiments show that our framework outperforms the state-of-the-art in terms of the steps to goal, obtained reward, and time to goal metrics. Particularly, our evaluation shows that the proposed framework is 8.32x faster than the current non-private frameworks and 1.41x faster than the private frameworks in an adversarial setting.</td>
                <td>Measurement, Privacy, Differential privacy, Autonomous systems, Reinforcement learning, Security, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341559&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341559&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Beacon-Based Distributed Structure Formation in Multi-Agent Systems</td>
                <td>C. Min</td>
                <td>2023</td>
                <td>Autonomous shape and structure formation is an important problem in the domain of large-scale multi-agent systems. In this paper, we propose a 3D structure representation method and a distributed structure formation strategy where settled agents guide free moving agents to a prescribed location to settle in the structure. Agents at the structure formation frontier looking for neighbors to settle act as beacons, generating a surface gradient throughout the formed structure propagated by settled agents. Free-moving agents follow the surface gradient along the formed structure surface to the formation frontier, where they eventually reach the closest beacon and settle to continue the structure formation following a local bidding process. Agent behavior is governed by a finite state machine implementation, along with potential field-based motion control laws. We also discuss appropriate rules for recovering from stagnation points. Simulation experiments are presented to show planar and 3D structure formations with continuous and discontinuous boundary/surfaces, which validate the proposed strategy, followed by a scalability analysis.</td>
                <td>Location awareness, Three-dimensional displays, Shape, Simulation, Scalability, Robot sensing systems, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341782&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341782&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Decentralized Swarm Trajectory Generation for LiDAR-based Aerial Tracking in Cluttered Environments</td>
                <td>L. Yin, F. Zhu, Y. Ren, F. Kong and F. Zhang</td>
                <td>2023</td>
                <td>Aerial tracking with multiple unmanned aerial vehicles (UAVs) has wide potential in various applications. However, the existing works for swarm tracking typically lack the capability of maintaining high target visibility in cluttered environments. To address this deficiency, we present a decentralized planner that maximizes target visibility while ensuring collision-free maneuvers for swarm tracking. In this paper, each drone's tracking performance is first analyzed by a decentralized kinodynamic searching front-end, which renders an optimal guiding path to initialize safe flight corridors and visible sectors. Afterwards, a polynomial trajectory satisfying the corridor constraints is generated by a spatial-temporal optimizer. Inter-vehicle collision and occlusion avoidance are also incorporated into the optimization objectives. The advantages of our methods are verified by extensive benchmark comparisons against other cutting-edge works. Integrated with an autonomous LiDAR-based swarm system, the proposed planner demonstrates its efficiency and robustness in real-world experiments with unknown cluttered surroundings.</td>
                <td>Target tracking, Benchmark testing, Autonomous aerial vehicles, Robustness, Trajectory, Optimization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341567&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341567&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Decentralized Planning for Car-Like Robotic Swarm in Cluttered Environments</td>
                <td>C. Ma et al.</td>
                <td>2023</td>
                <td>Robot swarm is a hot spot in robotic research community. In this paper, we propose a decentralized framework for car-like robotic swarm which is capable of real-time planning in cluttered environments. In this system, path finding is guided by environmental topology information to avoid frequent topological change, and search-based speed planning is leveraged to escape from infeasible initial value's local minima. Then spatial-temporal optimization is employed to generate a safe, smooth and dynamically feasible trajectory. During optimization, the trajectory is discretized by fixed time steps. Penalty is imposed on the signed distance between agents to realize collision avoidance, and differential flatness cooperated with limitation on front steer angle satisfies the non-holonomic constraints. With trajectories broadcast to the wireless network, agents are able to check and prevent potential collisions. We validate the robustness of our system in simulation and real-world experiments. Code will be released as open-source packages.</td>
                <td>Robot kinematics, Wireless networks, System recovery, Real-time systems, Robustness, Planning, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342360&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342360&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SCRIMP: Scalable Communication for Reinforcement- and Imitation-Learning-Based Multi-Agent Pathfinding</td>
                <td>Y. Wang, B. Xiang, S. Huang and G. Sartoretti</td>
                <td>2023</td>
                <td>Trading off performance guarantees in favor of scalability, the Multi-Agent Path Finding (MAPF) community has recently started to embrace Multi-Agent Reinforcement Learning (MARL), where agents learn to collaboratively generate individual, collision-free (but often suboptimal) paths. Scalability is usually achieved by assuming a local field of view (FOV) around the agents, helping scale to arbitrary world sizes. However, this assumption significantly limits the amount of information available to the agents, making it difficult for them to enact the type of joint maneuvers needed in denser MAPF tasks. In this paper, we propose SCRIMP, where agents learn individual policies from even very small (down to $3\mathrm{x}3$) FOVs, by relying on a highly-scalable global communication mechanism based on a modified transformer. We further equip agents with a state-value-based tie-breaking strategy to further improve performance in symmetric situations, and introduce intrinsic rewards to encourage exploration while mitigating the long-term credit assignment problem. Empirical evaluations on a set of experiments indicate that SCRIMP can achieve higher performance with improved scalability compared to other state-of-the-art learning-based MAPF planners with larger FOVs, and even yields similar performance as a classical centralized planner in many cases. Ablation studies further validate the effectiveness of our proposed techniques. Finally, we show that our trained model can be directly implemented on real robots for online MAPF through high-fidelity simulations in gazebo.</td>
                <td>Learning systems, Scalability, Stochastic processes, Information sharing, Reinforcement learning, Transformers, Global communication</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342305&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342305&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Distributed Model Predictive Formation Control of Robots with Sampled Trajectory Sharing in Cluttered Environments</td>
                <td>S. Satir, Y. F. Aktaş, S. Atasoy, M. Ankarali and E. Şahin</td>
                <td>2023</td>
                <td>In this paper, we propose a Model Predictive Control (MPC) based distributed formation control method for a multi-robot system (MRS) that would move them among dynamic obstacles to a desired goal position. Specifically, after formulating the formation control, as a distributed version of MPC, we propose and evaluate three information-sharing schemes within the MRS; namely sharing (i) positions, (ii) complete predicted trajectories, and (iii) exponentially-sampled predicted trajectories. Using a simplified kinematic model for robots, we conducted systematic simulation experiments in (a) scenarios, where the robots are instructed to switch places, as one of the most challenging forms of formation changes, and in (b) scenarios where robots are instructed to reach a goal, within environments containing dynamic obstacles. In a set of systematic experiments conducted in simulation and with mini quadcopters, we have shown that sharing of exponentially-sampled trajectories (as opposed to positions, or complete trajectories) among the robots provides near-optimal paths while decreasing the required computation cost and communication bandwidth. Surprisingly, in the presence of noise, sharing exponentially-sampled trajectories among the robots decreased the variance in the final paths. The proposed method is demonstrated on a group of Crazyflie quadcopters.</td>
                <td>Systematics, Costs, Bandwidth, Switches, Predictive models, Formation control, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341414&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341414&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Control Transformer: Robot Navigation in Unknown Environments Through PRM-Guided Return-Conditioned Sequence Modeling</td>
                <td>D. Lawson and A. H. Qureshi</td>
                <td>2023</td>
                <td>Learning long-horizon tasks such as navigation has presented difficult challenges for successfully applying reinforcement learning to robotics. From another perspective, under known environments, sampling-based planning can robustly find collision-free paths in environments without learning. In this work, we propose Control Transformer that models return-conditioned sequences from low-level policies guided by a sampling-based Probabilistic Roadmap (PRM) planner. We demonstrate that our framework can solve long-horizon navigation tasks using only local information. We evaluate our approach on partially-observed maze navigation with MuJoCo robots, including Ant, Point, and Humanoid. We show that Control Transformer can successfully navigate through mazes and transfer to unknown environments. Additionally, we apply our method to a differential drive robot (Turtlebot3) and show zero-shot sim2real transfer under noisy observations.</td>
                <td>Navigation, Humanoid robots, Reinforcement learning, Transformers, Probabilistic logic, Planning, Noise measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341628&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341628&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>InteractionNet: Joint Planning and Prediction for Autonomous Driving with Transformers</td>
                <td>J. Fu, Y. Shen, Z. Jian, S. Chen, J. Xin and N. Zheng</td>
                <td>2023</td>
                <td>Planning and prediction are two important modules of autonomous driving and have experienced tremendous advancement recently. Nevertheless, most existing methods regard planning and prediction as independent and ignore the correlation between them, leading to the lack of consideration for interaction and dynamic changes of traffic scenarios. To address this challenge, we propose InteractionNet, which leverages transformer to share global contextual reasoning among all traffic participants to capture interaction and interconnect planning and prediction to achieve joint. Besides, InteractionNet deploys another transformer to help the model pay extra attention to the perceived region containing critical or unseen vehicles. InteractionNet outperforms other baselines in several benchmarks, especially in terms of safety, which benefits from the joint consideration of planning and forecasting. The code will be available at https://github.com/fujiawei0724/InteractionNet.</td>
                <td>Correlation, Collaboration, Benchmark testing, Transformers, Planning, Safety, Vehicle dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342367&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342367&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ANEC: Adaptive Neural Ensemble Controller for Mitigating Latency Problems in Vision-Based Autonomous Driving</td>
                <td>A. Khalil and J. Kwon</td>
                <td>2023</td>
                <td>Humans have latency in their visual perception system between observation and action. Any action we take is based on an earlier observation since, by the time we act, the state has already changed, and we got a new observation. In autonomous driving, this latency is also present, determined by the amount of time the control algorithm needs to process information before acting. This algorithmic perception latency can be reduced by massive computing power via GPUs and FPGAs, which is improbable in automobile platforms. Thus, it is a reasonable assumption that the algorithmic perception latency is inevitable. Many researchers have developed different neural network driving models without consideration of the algorithmic perception latency. This paper studies the latency effect on vision-based neural network autonomous driving in the lane-keeping task and proposes a vision-based novel neural network controller, the Adaptive Neural Ensemble Controller (ANEC) that is inspired by the near/far gaze distribution of human drivers during lane-keeping. ANEC was tested in Gazebo 3D simulation environment with Robot Operating System (ROS) which showed the effectiveness of ANEC in dealing with algorithmic latency. The source code is available at https://github.com/jrkwon/oscar/tree/devel_anec.</td>
                <td>Adaptive systems, Three-dimensional displays, Source coding, Operating systems, Neural networks, Process control, Task analysis, Autonomous Vehicle Navigation, Machine Learning for Robot Control, Imitation Learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342520&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342520&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Dynamic Programming Algorithm for Grid-Based Formation Planning of Multiple Vehicles</td>
                <td>C. Au</td>
                <td>2023</td>
                <td>A common operation in multirobot systems is to generate a motion plan for multiple robots such that the robots can move in formation to achieve some desired effects. For example, in autonomous parking lots, a group of vehicles can be asked to move to another location when they block another vehicle that needs to leave the parking lot. In this paper, we present a novel grid-based planning approach for motion planning that minimizes the makespan of moving multiple vehicles from one location to another in a safe manner. Unlike most existing multirobot planning algorithms, our algorithm uses dynamic programming to compute a nearly-optimal motion plan for a large group of vehicles in polynomial time with the help of a given set of intermediate vehicle patterns. Our experimental results show that our algorithm is much faster than an exact algorithm but does not increase the minimum makespans tremendously.</td>
                <td>Heuristic algorithms, Planning, Dynamic programming, Multi-robot systems, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341481&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341481&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LB-L2L-Calib 2.0: A Novel Online Extrinsic Calibration Method for Multiple Long Baseline 3D LiDARs Using Objects</td>
                <td>J. Zhang et al.</td>
                <td>2023</td>
                <td>In V2X (Vehicle-to-Everything), one important work is to extrinsically calibrate multiple 3D LiDARs, which are mounted with a long baseline and large viewpoint-difference at the road-side. Current solutions either require a specific target being set up (e.g., a sphere), or require specific features existing in the environment (e.g., mutually orthogonal planes). However, it is time-consuming, sometimes even inconvenient, to set up specific targets, e.g., at busy intersections and highways. Furthermore, specific features do not always exist in the traffic scenario. Thus, the current solutions are not feasible. To address this problem, a novel extrinsic calibration method is proposed in this paper, namely LB-L2L-Calib 2.0. It is the 2.0 version of our previous work. The novelties are: 1) We propose to use the easily accessible objects on the road as features for calibration (i.e., the vehicles). Thus, it is not necessary to set up any specific targets and we do not need to worry whether specific features exist or not. The key point is we observed that the 3D bounding box centers of the vehicles are viewpoint-invariant from different viewpoints, which makes them ideal features for long baseline and large viewpoint-difference calibration. 2) To establish correct correspondence between the bounding box centers detected from different LiDARs, we propose an exhaustive searching strategy. It can robustly output correct correspondence. Extensive experiments are performed in three scenarios (simulation: intersection, real: carpark and highway), with two types of LiDAR (Velodyne and Livox), demonstrating that LB-L2L-Calib 2.0 is robust, effective, and accurate.</td>
                <td>Laser radar, Three-dimensional displays, Roads, Feature extraction, Calibration, Vehicle-to-everything, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342245&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342245&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A GM-PHD Filter with Estimation of Probability of Detection and Survival for Individual Targets</td>
                <td>R. A. T. Perera, M. Jeong, A. Q. Li and P. Stegagno</td>
                <td>2023</td>
                <td>This paper proposes a modification of the Gaussian mixture probability hypothesis density (GM-PHD) filter to compute online the probability of detection $(P_{D})$ and probability of survival $(P_{S})$ of targets. This eliminates the need for predetermined and/or constant $P_{D}$ and $P_{S}$ values, that may degrade the estimation. The proposed filter estimates the $P_{D}$ and $P_{S}$ values for each individual target based on newly introduced parameters, which are updated during the measurement update process. The effectiveness of the proposed filter was validated through an in-lab experiment using four unmanned ground robots with varying $P_{D}$ values and a real-world lidar-based obstacle tracking system implemented on an Automated Surface Vehicle operating in a lake with real-time boat traffic. The results of the experiments demonstrate that the proposed filter outperforms the standard PHD filter with incorrect $P_{D}$ and $P_{S}$ values. These findings highlight the potential benefits of the proposed filter in improving target tracking performance in complex environments.</td>
                <td>Target tracking, Estimation, Boats, Lakes, Real-time systems, Collision avoidance, Standards</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342438&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342438&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>F2BEV: Bird's Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving</td>
                <td>E. U. Samani, F. Tao, H. R. Dasari, S. Ding and A. G. Banerjee</td>
                <td>2023</td>
                <td>Bird's Eye View (BEV) representations are tremendously useful for perception-related automated driving tasks. However, generating BEVs from surround-view fisheye camera images is challenging due to the strong distortions introduced by such wide-angle lenses. We take the first step in addressing this challenge and introduce a baseline, F2BEV, to generate discretized BEV height maps and BEV semantic segmentation maps from fisheye images. F2BEV consists of a distortion-aware spatial cross attention module for querying and consolidating spatial information from fisheye image features in a transformer-style architecture followed by a task-specific head. We evaluate single-task and multi-task variants of F2BEV on our synthetic FB-SSEM dataset, all of which generate better BEV height and segmentation maps (in terms of the IoU) than a state-of-the-art BEV generation method operating on undistorted fisheye images. We also demonstrate discretized height map generation from real-world fisheye images using F2BEV. Our dataset is publicly available at https://github.com/volvo-cars/FB-SSEM-dataset</td>
                <td>Head, Semantic segmentation, Robot vision systems, Cameras, Transformers, Multitasking, Distortion</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341862&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341862&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>One-4-All: Neural Potential Fields for Embodied Navigation</td>
                <td>Ruiz and L. Paull</td>
                <td>2023</td>
                <td>A fundamental task in robotics is to navigate between two locations. In particular, real-world navigation can require long-horizon planning using high-dimensional RGB images, which poses a substantial challenge for end-to-end learning-based approaches. Current semi-parametric methods instead achieve long-horizon navigation by combining learned modules with a topological memory of the environment, often represented as a graph over previously collected images. However, using these graphs in practice requires tuning a number of pruning heuristics. These heuristics are necessary to avoid spurious edges, limit runtime memory usage and maintain reasonably fast graph queries in large environments. In this work, we present One-4-All (O4A), a method leveraging self-supervised and manifold learning to obtain a graph-free, end-to-end navigation pipeline in which the goal is specified as an image. Navigation is achieved by greedily minimizing a potential function defined continuously over image embeddings. Our system is trained offline on non-expert exploration sequences of RGB data and controls, and does not require any depth or pose measurements. We show that 04A can reach long-range goals in 8 simulated Gibson indoor environments and that resulting embeddings are topologically similar to ground truth maps, even if no pose is observed. We further demonstrate successful real-world navigation using a Jackal UGV platform.aaProject page https://montrealrobotics.ca/o4a/.</td>
                <td>Runtime, Navigation, Image edge detection, Pipelines, Planning, Manifold learning, Indoor environment</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342302&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342302&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Communication Resources Constrained Hierarchical Federated Learning for End-to-End Autonomous Driving</td>
                <td>B. Kou et al.</td>
                <td>2023</td>
                <td>While federated learning (FL) improves the generalization of end-to-end autonomous driving by model aggregation, the conventional single-hop FL (SFL) suffers from slow convergence rate due to long-range communications among vehicles and cloud server. Hierarchical federated learning (HFL) overcomes such drawbacks via introduction of mid-point edge servers. However, the orchestration between constrained communication resources and HFL performance becomes an urgent problem. This paper proposes an optimization-based Communication Resource Constrained Hierarchical Federated Learning (CRCHFL) framework to minimize the generalization error of the autonomous driving model using hybrid data and model aggregation. The effectiveness of the proposed CRCHFL is evaluated in the Car Learning to Act (CARLA) simulation platform. Results show that the proposed CRCHFL both accelerates the convergence rate and enhances the generalization of federated learning autonomous driving model. Moreover, under the same communication resource budget, it outperforms the HFL by 10.33% and the SFL by 12.44%.</td>
                <td>Fading channels, Federated learning, Communication channels, Benchmark testing, Data models, Servers, Automobiles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342134&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342134&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Poly-MOT: A Polyhedral Framework For 3D Multi-Object Tracking</td>
                <td>X. Li et al.</td>
                <td>2023</td>
                <td>3D Multi-object tracking (MOT) empowers mobile robots to accomplish well-informed motion planning and navigation tasks by providing motion trajectories of surrounding objects. However, existing 3D MOT methods typically employ a single similarity metric and physical model to perform data association and state estimation for all objects. With large-scale modern datasets and real scenes, there are a variety of object categories that commonly exhibit distinctive geometric properties and motion patterns. In this way, such distinctions would enable various object categories to behave differently under the same standard, resulting in erroneous matches between trajectories and detections, and jeopardizing the reliability of downstream tasks (navigation, etc.). Towards this end, we propose Poly-MOT, an efficient 3D MOT method based on the Tracking-By-Detection framework that enables the tracker to choose the most appropriate tracking criteria for each object category. Specifically, Poly-MOT leverages different motion models for various object categories to characterize distinct types of motion accurately. We also introduce the constraint of the rigid structure of objects into a specific motion model to accurately describe the highly nonlinear motion of the object. Additionally, we introduce a two-stage data association strategy to ensure that objects can find the optimal similarity metric from three custom metrics for their categories and reduce missing matches. On the NuScenes dataset, our proposed method achieves state-of-the-art performance with 75.4% AMOTA. The code is available at https://github.com/lixiaoyu20001P0Iy-MOT.</td>
                <td>Measurement, Training, Three-dimensional displays, Tracking, Navigation, Detectors, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341778&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341778&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SUIT: Learning Significance-Guided Information for 3D Temporal Detection</td>
                <td>Z. Zhou, J. Lu, Y. Zeng, H. Xu and L. Zhang</td>
                <td>2023</td>
                <td>3D object detection from LiDAR point cloud is of critical importance for autonomous driving and robotics. While sequential point cloud has the potential to enhance 3D perception through temporal information, utilizing these temporal features effectively and efficiently remains a challenging problem. Based on the observation that the foreground information is sparsely distributed in LiDAR scenes, we believe sufficient knowledge can be provided by sparse format rather than dense maps. To this end, we propose to learn Significance-gUided Information for 3D Temporal detection (SUIT), which simplifies temporal information as sparse features for information fusion across frames. Specifically, we first introduce a significant sampling mechanism that extracts information-rich yet sparse features based on predicted object centroids. On top of that, we present an explicit geometric transformation learning technique, which learns the object-centric transformations among sparse features across frames. We evaluate our method on large-scale nuScenes and Waymo dataset, where our SUIT not only significantly reduces the memory and computation cost of temporal fusion, but also performs well over the state-of-the-art baselines.</td>
                <td>Point cloud compression, Three-dimensional displays, Laser radar, Costs, Object detection, Detectors, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342350&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342350&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhancing Sample Efficiency and Uncertainty Compensation in Learning-Based Model Predictive Control for Aerial Robots</td>
                <td>K. Y. Chee, T. C. Silva, M. A. Hsieh and G. J. Pappas</td>
                <td>2023</td>
                <td>The recent increase in data availability and reliability has led to a surge in the development of learning-based model predictive control (MPC) frameworks for robot systems. Despite attaining substantial performance improvements over their non-learning counterparts, many of these frameworks rely on an offline learning procedure to synthesize a dynamics model. This implies that uncertainties encountered by the robot during deployment are not accounted for in the learning process. On the other hand, learning-based MPC methods that learn dynamics models online are computationally expensive and often require a significant amount of data. To alleviate these shortcomings, we propose a novel learning-enhanced MPC framework that incorporates components from C1 adaptive control into learning-based MPC. This integration enables the accurate compensation of both matched and unmatched uncertainties in a sample-efficient way, enhancing the control performance during deployment. In our proposed framework, we present two variants and apply them to the control of a quadrotor system. Through simulations and physical experiments, we demonstrate that the proposed framework not only allows the synthesis of an accurate dynamics model on-the-fly, but also significantly improves the closed-loop control performance under a wide range of spatio-temporal uncertainties.</td>
                <td>Adaptation models, Uncertainty, Fuses, Computational modeling, Reliability, Adaptive control, Surges</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341774&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341774&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Data-Driven Modeling and Experimental Validation of Autonomous Vehicles Using Koopman Operator</td>
                <td>A. Joglekar et al.</td>
                <td>2023</td>
                <td>This paper presents a data-driven framework to discover underlying dynamics on a scaled F1TENTH vehicle using the Koopman operator linear predictor. Traditionally, a range of white, gray, or black-box models are used to develop controllers for vehicle path tracking. However, these models are constrained to either linearized operational domains, unable to handle significant variability or lose explainability through end-2-end operational settings. The Koopman Extended Dynamic Mode Decomposition (EDMD) linear predictor seeks to utilize data-driven model learning whilst providing benefits like explainability, model analysis and the ability to utilize linear model-based control techniques. Consider a trajectory-tracking problem for our scaled vehicle platform. We collect pose measurements of our F1TENTH car undergoing standard vehicle dynamics benchmark maneuvers with an OptiTrack indoor localization system. Utilizing these uniformly spaced temporal snapshots of the states and control inputs, a data-driven Koopman EDMD model is identified. This model serves as a linear predictor for state propagation, upon which an MPC feedback law is designed to enable trajectory tracking. The prediction and control capabilities of our framework are highlighted through real-time deployment on our scaled vehicle.</td>
                <td>Location awareness, Analytical models, Trajectory tracking, Computational modeling, Predictive models, Vehicle dynamics, Standards</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341797&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341797&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Adaptive Exploration-Exploitation Active Learning of Gaussian Processes</td>
                <td>G. P. Kontoudis and M. Otte</td>
                <td>2023</td>
                <td>Active Learning of Gaussian process (GP) surrogates is an efficient way to model unknown environments in various applications. In this paper, we propose an adaptive exploration-exploitation active learning method (ALX) that can be executed rapidly to facilitate real-time decision making. For the exploration phase, we formulate an acquisition function that maximizes the approximated, expected Fisher information. For the exploitation phase, we employ a closed-form acquisition function that maximizes the total expected variance reduction of the search space. The determination of each phase is established with an exploration condition that measures the predictive accuracy of GP surrogates. Extensive numerical experiments in multiple input spaces validate the efficiency of our method.</td>
                <td>Learning systems, Adaptation models, Phase measurement, Decision making, Focusing, Gaussian processes, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342130&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342130&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sample-Efficient Real-Time Planning with Curiosity Cross-Entropy Method and Contrastive Learning</td>
                <td>M. Kotb, C. Weber and S. Wermter</td>
                <td>2023</td>
                <td>Model-based reinforcement learning (MBRL) with real-time planning has shown great potential in locomotion and manipulation control tasks. However, the existing planning methods, such as the Cross-Entropy Method (CEM), do not scale well to complex high-dimensional environments. One of the key reasons for underperformance is the lack of exploration, as these planning methods only aim to maximize the cumulative extrinsic reward over the planning horizon. Furthermore, planning inside the compact latent space in the absence of observations makes it challenging to use curiosity-based intrinsic motivation. We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for encouraging exploration via curiosity. Our proposed method maximizes the sum of state-action $Q$ values over the planning horizon, in which these $Q$ values estimate the future extrinsic and intrinsic reward, hence encouraging to reach novel observations. In addition, our model uses contrastive representation learning to efficiently learn latent representations. Experiments on image-based continuous control tasks from the DeepMind Control suite show that CCEM is by a large margin more sample-efficient than previous MBRL algorithms and compares favorably with the best model-free RL methods.</td>
                <td>Representation learning, Reinforcement learning, Real-time systems, Planning, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342018&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342018&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Underactuated MIMO Airship Control Based on Online Data-Driven Reinforcement Learning</td>
                <td>D. Boase, W. Gueaieb and M. S. Miah</td>
                <td>2023</td>
                <td>In this work, a novel online model-free controller for an underactuated dirigible is developed based on reinforcement learning and optimal control theory. A reinforcement learning structure is used while overcoming the dependence of the value function on future values by introducing a neural network that is adapted using input-output data. The suboptimal critic neural network is structured such that optimality is guaranteed over the interval from which the data is valid. The system performance is validated using a highly realistic physics engine, Gazebo, with the robot operating system (ROS) interface and the results are compared to the performance of a model-based controller specifically designed to control the airship model. It is emphasized that the proposed formulation does not leverage any knowledge of vehicle dynamics and thus is considered a vehicle agnostic control strategy.</td>
                <td>Training, Atmospheric modeling, System performance, Operating systems, Neural networks, Optimal control, Reinforcement learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341752&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341752&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Grasp Stability Assessment Through Attention-Guided Cross-Modality Fusion and Transfer Learning</td>
                <td>Z. Zhang, Z. Zhou, H. Wang, Z. Zhang, H. Huang and Q. Cao</td>
                <td>2023</td>
                <td>Extensive research has been conducted on assessing grasp stability, a crucial prerequisite for achieving optimal grasping strategies, including the minimum force grasping policy. However, existing works employ basic feature-level fusion techniques to combine visual and tactile modalities, resulting in the inadequate utilization of complementary information and the inability to model interactions between unimodal features. This work proposes an attention-guided cross-modality fusion architecture to comprehensively integrate visual and tactile features. This model mainly comprises convolutional neural networks (CNNs), self-attention, and cross-attention mechanisms. In addition, most existing methods collect datasets from real-world systems, which is time-consuming and high-cost, and the datasets collected are comparatively limited in size. This work establishes a robotic grasping system through physics simulation to collect a multimodal dataset. To address the sim-to-real transfer gap, we propose a migration strategy encompassing domain randomization and domain adaptation techniques. The experimental results demonstrate that the proposed fusion framework achieves markedly enhanced prediction performance (approximately 10%) compared to other baselines. Moreover, our findings suggest that the trained model can be reliably transferred to real robotic systems, indicating its potential to address real-world challenges.</td>
                <td>Visualization, Adaptation models, Force, Transfer learning, Grasping, Reinforcement learning, Stability analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342411&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342411&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>On-Robot Bayesian Reinforcement Learning for POMDPs</td>
                <td>H. Nguyen, S. Katt, Y. Xiao and C. Amato</td>
                <td>2023</td>
                <td>Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach can, for example, utilize typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment. We empirically demonstrate its efficiency by performing on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes. A video of learned policies is at https://youtu.be/H9xp60ngOes.</td>
                <td>Uncertainty, Monte Carlo methods, Shape, Heuristic algorithms, Reinforcement learning, Robot learning, Bayes methods</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342114&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342114&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization</td>
                <td>E. Deschaud and F. Goulette</td>
                <td>2023</td>
                <td>Supervised 3D Object Detection models have been displaying increasingly better performance in single-domain cases where the training data comes from the same environment and sensor as the testing data. However, in real-world scenarios data from the target domain may not be available for finetuning or for domain adaptation methods. Indeed, 3D object detection models trained on a source dataset with a specific point distribution have shown difficulties in generalizing to unseen datasets. Therefore, we decided to leverage the information available from several annotated source datasets with our Multi-Dataset Training for 3D Object Detection (MDT3D) method to increase the robustness of 3D object detection models when tested in a new environment with a different sensor configuration. To tackle the labelling gap between datasets, we used a new label mapping based on coarse labels. Furthermore, we show how we managed the mix of datasets during training and finally introduce a new cross-dataset augmentation method: crossdataset object injection. We demonstrate that this training paradigm shows improvements for different types of 3D object detection models. The source code and additional results for this research project will be publicly available on GitHub for interested parties to access and utilize: https://github.com/LouisSF/MDT3D</td>
                <td>Training, Solid modeling, Adaptation models, Three-dimensional displays, Laser radar, Training data, Object detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341614&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341614&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Addressing the Scale Shrinkage Problem in Learning-based Binocular Depth Estimation</td>
                <td>G. Guo, Y. Song and F. Sun</td>
                <td>2023</td>
                <td>Binocular depth estimation is a fundamental problem in computer vision. Learning-based models have achieved significant performance improvements on public datasets in recent years. Our study finds that the performance of the current state-of-the-art deep learning-based models deteriorates significantly in distant areas. We point out that these deep learning-based models suffer from a scale shrinkage problem. Specifically, the predicted depth value ratio to the ground truth decreases as depth increases. Such a phenomenon is not conducive to the path planning and navigation of intelligent agents in outdoor scenes. We analyze the reasons for the scale shrinkage problem and give a simple and effective method. Our method employs a two-stage fine-tuning strategy and appropriately fuses the predictions of the two-stage models. The method does not reduce the prediction accuracy in close areas and significantly improves the accuracy of the models in distant areas. On the KITTI stereo 2015 dataset, our method can reduce the absolute relative difference by about 6% and the root-mean-square error (RMSE) by about 10%.</td>
                <td>Measurement, Computer vision, Navigation, Fuses, Computational modeling, Estimation, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341579&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341579&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Holistic Parking Slot Detection with Polygon-Shaped Representations</td>
                <td>L. Wang et al.</td>
                <td>2023</td>
                <td>Current parking slot detection in advanced driver-assistance systems (ADAS) primarily relies on ultrasonic sen-sors. This method has several limitations such as the need to scan the entire parking slot before detecting it, the incapacity of detecting multiple slots in a row, and the difficulty of classifying them. Due to the complex visual environment, vehicles are equipped with surround view camera systems to detect vacant parking slots. Previous research works in this field mostly use image-domain models to solve the problem. These two-stage approaches separate the 2D detection and 3D pose estimation steps using camera calibration. In this paper, we propose one-step Holistic Parking Slot Network (HPS-Net), a tailor-made adaptation of the You Only Look Once (YOLO)v4 algorithm. This camera-based approach directly outputs the four vertex coordinates of the parking slot in topview domain, instead of a bounding box in raw camera images. Several visible points and shapes can be proposed from different angles. A novel regression loss function named polygon-corner Generalized Intersection over Union (GIoU) for polygon vertex position optimization is also proposed to manage the slot orientation and to distinguish the entrance line. Experiments show that HPS-Net can detect various vacant parking slots with a F1-score of 0.92 on our internal Valeo Parking Slots Dataset (VPSD) and 0.99 on the public dataset PS2.0. It provides a satisfying generalization and robustness in various parking scenarios, such as indoor (F1: 0.86) or paved ground (F1: 0.91). Moreover, it achieves a realtime detection speed of 17 FPS on Nvidia Drive AGX Xavier. A demo video can be found at https://streamable.com/75j7sj.</td>
                <td>Visualization, Three-dimensional displays, Shape, Pose estimation, Streaming media, Cameras, Acoustics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342486&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342486&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>End-to-End Point Cloud Registration via Rotation Equivariant Descriptors</td>
                <td>Y. Cao, Y. Shi, Z. Cheng and H. Li</td>
                <td>2023</td>
                <td>Point cloud registration (PCR) aims to recover the rigid transformation between two noisy, unordered point sets. This task is typically tackled by establishing point-wise correspondences, and solving the rigid transformation between the two sets. Since descriptor-based methods find correspondences by matching the feature space distance, a powerful and rotation-robust point feature extractor is critical to the success of this task. Existing methods assume soft rotation invariance/equivariance through the means of training augmentation, rotational discretization or pre-alignment of patches. In contrast, this paper proposes a new method which generates fully rotation invariant and equivariant descriptors by construction. For each keypoint patch, our network extracts not only a rotation invariant descriptor for establishing corre-spondences, but also a rotation equivariant one. The rotation equivariant descriptor allows relative transformation to be directly recovered from a single correspondence pair, unlike standard methods that require three correspondences. This design significantly reduces iteration number of RANSAC and guarantees high registration recall when the inlier ratio of estimated correspondences is low. Extensive experiments have demonstrated that the proposed method outperforms state-of-art methods in the same category even after much fewer RANSAC iterations.</td>
                <td>Point cloud compression, Training, Feature extraction, Noise measurement, Task analysis, Standards, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342154&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342154&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>The Audio-Visual BatVision Dataset for Research on Sight and Sound</td>
                <td>A. Brunetto, S. Hornauer, S. X. Yu and F. Moutarde</td>
                <td>2023</td>
                <td>Vision research showed remarkable success in understanding our world, propelled by datasets of images and videos. Sensor data from radar, LiDAR and cameras supports research in robotics and autonomous driving for at least a decade. However, while visual sensors may fail in some conditions, sound has recently shown potential to complement sensor data. Simulated room impulse responses (RIR) in 3D apartment-models became a benchmark dataset for the community, fostering a range of audiovisual research. In simulation, depth is predictable from sound, by learning bat-like perception with a neural network. Concurrently, the same was achieved in reality by using RGB-D images and echoes of chirping sounds. Biomimicking bat perception is an exciting new direction but needs dedicated datasets to explore the potential. Therefore, we collected the BatVision dataset to provide large-scale echoes in complex real-world scenes to the community. We equipped a robot with a speaker to emit chirps and a binaural microphone to record their echoes. Synchronized RGB-D images from the same perspective provide visual labels of traversed spaces. We sampled modern US office spaces to historic French university grounds, indoor and outdoor with large architectural variety. This dataset will allow research on robot echolocation, general audio-visual tasks and sound phænomena unavailable in simulated data. We show promising results for audio-only depth prediction and show how state-of-the-art work developed for simulated data can also succeed on our dataset. Project page: https://amandinebtto.github.io/Batvision-Dataset/</td>
                <td>Training, Visualization, Ultrasonic imaging, Three-dimensional displays, Chirp, Robot vision systems, Radar imaging</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341715&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341715&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Long-Range UAV Thermal Geo-Localization with Satellite Imagery</td>
                <td>J. Xiao, D. Tortei, E. Roura and G. Loianno</td>
                <td>2023</td>
                <td>Onboard sensors, such as cameras and thermal sensors, have emerged as effective alternatives to Global Positioning System (GPS) for geo-Iocalization in Unmanned Aerial Vehicle (UAV) navigation. Since GPS can suffer from signal loss and spoofing problems, researchers have explored camera-based techniques such as Visual Geo-Iocalization (VG) using satellite RGB imagery. Additionally, thermal geo-Iocalization (TG) has become crucial for long-range UAV flights in low-illumination environments. This paper proposes a novel thermal geo-Iocalization framework using satellite RGB imagery, which includes multiple domain adaptation methods to address the limited availability of paired thermal and satellite images. The experimental results demonstrate the effectiveness of the proposed approach in achieving reliable thermal geo-Iocalization performance, even in thermal images with indistinct self-similar features. We evaluate our approach on real data collected onboard a UAV. We also release the code and Boson-nighttime, a dataset of paired satellite-thermal and unpaired satellite images for thermal geo-Iocalization with satellite imagery. To the best of our knowledge, this work is the first to propose a thermal geo-Iocalization method using satellite RGB imagery in long-range flights.</td>
                <td>Training, Adaptation models, Visualization, Satellites, Thermal sensors, Autonomous aerial vehicles, Sensor systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342068&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342068&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Sonar2Depth: Acoustic-Based 3D Reconstruction Using cGANs</td>
                <td>N. Jaber, B. Wehbe and F. Kirchner</td>
                <td>2023</td>
                <td>This work proposes the use of conditional Generative Adversarial Networks (cGANs) for acoustic-based 3D reconstruction. Acoustics being the most reliable sensor modality in underwater domains is accompanied with the loss of elevation angle in its images. The challenge of recovering the missing dimension in acoustic images have pushed researchers to try various methods and approaches over the past years. cGANs being an image-to-image translation method makes it possible to learn a desired style, and transforms the data from one modality to another. This was applied here as a way of transforming an acoustic image into another form which contains the elevation characteristics, such as depth images. Depth images are hard to acquire underwater, thus data was generated synthetically and used for training and testing the deep learning model. As a way of performance enhancement, real data was collected for training a Cycle-GAN network in the aim of transferring the realistic style into the synthetically generated images. Simulation experiments were conducted to evaluate the system and find out the best experimental setup, which was then used to carry out the real experiment. The system performed dense 3D reconstruction of the scanned object and proved to be applicable in real environments.</td>
                <td>Training, Meters, Three-dimensional displays, Sonar, Transforms, Acoustics, Reliability, 3D reconstruction, imaging sonar, marine robotics, deep-learning, GANs</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342251&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342251&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Depth Self-Supervision for Single Image Novel View Synthesis</td>
                <td>G. Minelli, M. Poggi and S. Salti</td>
                <td>2023</td>
                <td>In this paper, we tackle the problem of generating a novel image from an arbitrary viewpoint given a single frame as input. While existing methods operating in this setup aim at predicting the target view depth map to guide the synthesis, without explicit supervision over such a task, we jointly optimize our framework for both novel view synthesis and depth estimation to unleash the synergy between the two at its best. Specifically, a shared depth decoder is trained in a self-supervised manner to predict depth maps that are consistent across the source and target views. Our results demonstrate the effectiveness of our approach in addressing the challenges of both tasks allowing for higher-quality generated images, as well as more accurate depth for the target viewpoint.</td>
                <td>Image synthesis, Video sequences, Pipelines, Estimation, Predictive models, Decoding, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342058&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342058&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Skill Generalization with Verbs</td>
                <td>R. Ma et al.</td>
                <td>2023</td>
                <td>It is imperative that robots can understand natural language commands issued by humans. Such commands typically contain verbs that signify what action should be performed on a given object and that are applicable to many objects. We propose a method for generalizing manipulation skills to novel objects using verbs. Our method learns a probabilistic classifier that determines whether a given object trajectory can be described by a specific verb. We show that this classifier accurately generalizes to novel object categories with an average accuracy of 76.69% across 13 object categories and 14 verbs. We then perform policy search over the object kinematics to find an object trajectory that maximizes classifier prediction for a given verb. Our method allows a robot to generate a trajectory for a novel object based on a verb, which can then be used as input to a motion planner. We show that our model can generate trajectories that are usable for executing five verb commands applied to novel instances of two different object categories on a real robot.</td>
                <td>Natural languages, Kinematics, Search problems, Probabilistic logic, Trajectory, Planning, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341472&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341472&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Source Soft Pseudo-Label Learning with Domain Similarity-based Weighting for Semantic Segmentation</td>
                <td>S. Matsuzaki, H. Masuzawa and J. Miura</td>
                <td>2023</td>
                <td>This paper describes a method of domain adap-tive training for semantic segmentation using multiple source datasets that are not necessarily relevant to the target dataset. We propose a soft pseudo-label generation method by integrating predicted object probabilities from multiple source models. The prediction of each source model is weighted based on the estimated domain similarity between the source and the target datasets to emphasize contribution of a model trained on a source that is more similar to the target and generate reasonable pseudo-labels. We also propose a training method using the soft pseudo-labels considering their entropy to fully exploit information from the source datasets while suppressing the influence of possibly misclassified pixels. The experiments show comparative or better performance than our previous work and another existing multi-source domain adaptation method, and applicability to a variety of target environments.</td>
                <td>Training, Adaptation models, Semantic segmentation, Predictive models, Entropy, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342159&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342159&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>3D-Aware Object Localization using Gaussian Implicit Occupancy Function</td>
                <td>V. Gaudillière, L. Pauly, A. Rathinam, A. G. Sanchez, M. A. Musallam and D. Aouada</td>
                <td>2023</td>
                <td>To automatically localize a target object in an image is crucial for many computer vision applications. To represent the 2D object, ellipse labels have recently been identified as a promising alternative to axis-aligned bounding boxes. This paper further considers 3D-aware ellipse labels, i.e., ellipses which are projections of a 3D ellipsoidal approximation of the object, for 2D target localization. Indeed, projected ellipses carry more geometric information about the object geometry and pose (3D awareness) than traditional 3D-agnostic bounding box labels. Moreover, such a generic 3D ellipsoidal model allows for approximating known to coarsely known targets. We then propose to have a new look at ellipse regression and replace the discontinuous geometric ellipse parameters with the parameters of an implicit Gaussian distribution encoding object occupancy in the image. The models are trained to regress the values of this bivariate Gaussian distribution over the image pixels using a statistical loss function. We introduce a novel non-trainable differentiable layer, E-DSNT, to extract the distribution parameters. Also, we describe how to readily generate consistent 3D-aware Gaussian occupancy parameters using only coarse dimensions of the target and relative pose labels. We extend three existing spacecraft pose estimation datasets with 3D-aware Gaussian occupancy labels to validate our hypothesis. Labels and source code are publicly accessible here: https://cvi2.uni.lu/3d-aware-obj-loc/.</td>
                <td>Location awareness, Space vehicles, Solid modeling, Three-dimensional displays, Source coding, Pose estimation, Pipelines</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342399&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342399&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>TransUPR: A Transformer-based Plug-and-Play Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation</td>
                <td>Z. Yu et al.</td>
                <td>2023</td>
                <td>Common image-based LiDAR point cloud semantic segmentation (LiDAR PCSS) approaches have bottlenecks resulting from the boundary-blurring problem of convolution neural networks (CNNs) and quantitation loss of spherical projection. In this work, we propose a transformer-based plug-and-play uncertain point refiner, i.e., TransUPR, to refine selected uncertain points in a learnable manner, which leads to an improved segmentation performance. Uncertain points are sampled from coarse semantic segmentation results of 2D image segmentation where uncertain points are located close to the object boundaries in the 2D range image representation and 3D spherical projection background points. Following that, the geometry and coarse semantic features of uncertain points are aggregated by neighbor points in 3D space without adding expensive computation and memory footprint. Finally, the transformer-based refiner, which contains four stacked self-attention layers, along with an MLP module, is utilized for uncertain point classification on the concatenated features of self-attention layers. As the proposed refiner is independent of 2D CNNs, our TransUPR can be easily integrated into any existing image-based LiDAR PCSS approaches, e.g., CENet. Our TransUPR with the CENet achieves state-of-the-art performance, i.e., 68.2% mean Intersection over Union (mIoU) on the Semantic KITTI benchmark, which provides a performance improvement of 0.6% on the mIoU compared to the original CENet.</td>
                <td>Point cloud compression, Laser radar, Three-dimensional displays, Semantic segmentation, Semantics, Neural networks, Memory management</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342116&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342116&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Null-Space Compliance Variation for Safe Human-Robot Collaboration in Redundant Manipulators using Safety Control Barrier Functions</td>
                <td>J. M. Salt Ducaju, B. Olofsson, A. Robertsson and R. Johansson</td>
                <td>2023</td>
                <td>In this paper, Safety Control Barrier Functions (SCBFs) were used to adjust the null-space compliant behavior of a redundant robot to improve safety in Human-Robot Collaboration (HRC) without modifying the robot behavior with respect to its main Cartesian task. A Lyapunov function was included in an energy storage formulation compatible with strict passivity to provide global asymptotic stability guarantees for the null-space compliance variation, and the necessary conditions for stability were formulated as inequality constraints of the optimization problem used for the null-space compliance variation. Experimental validation was performed using a Franka Emika Panda robot for a collaborative assembly application and its results showed that safety can be improved by using SCBFs simultaneously to the optimization of the robot configuration, while employing a single degree of freedom.</td>
                <td>Asymptotic stability, Collaboration, Manipulators, Safety, Behavioral sciences, Task analysis, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342181&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342181&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Collision Isolation and Identification Using Proprioceptive Sensing for Parallel Robots to Enable Human-Robot Collaboration</td>
                <td>A. Mohammad, M. Schappler and T. Ortmaier</td>
                <td>2023</td>
                <td>Parallel robots (PRs) allow for higher speeds in human-robot collaboration due to their lower moving masses but are more prone to unintended contact. For a safe reaction, knowledge of the location and force of a collision is useful. A novel algorithm for collision isolation and identification with proprioceptive information for a real PR is the scope of this work. To classify the collided body, the effects of contact forces at the links and platform of the PR are analyzed using a kinetostatic projection. This insight enables the derivation of features from the line of action of the estimated external force. The significance of these features is confirmed in experiments for various load cases. A feedforward neural network (FNN) classifies the collided body based on these physically modeled features. Generalization with the FNN to 300k load cases on the whole robot structure in other joint angle configurations is successfully performed with a collision-body classification accuracy of 84% in the experiments. Platform collisions are isolated and identified with an explicit solution, while a particle filter estimates the location and force of a contact on a kinematic chain. Updating the particle filter with estimated external joint torques leads to an isolation error of less than 3 cm and an identification error of 4 N in a real-world experiment.</td>
                <td>Parallel robots, Simulation, Force, Propioception, Collaboration, Observers, Particle filters</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342345&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342345&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Interpretable Trajectory Prediction for Autonomous Vehicles via Counterfactual Responsibility</td>
                <td>C. Hsu, K. Leung, Y. Chen, J. F. Fisac and M. Pavone</td>
                <td>2023</td>
                <td>The ability to anticipate surrounding agents' behaviors is critical to enable safe and seamless autonomous vehicles (AVs). While phenomenological methods have successfully predicted future trajectories from scene context, these predictions lack interpretability. On the other hand, ontological approaches assume an underlying structure able to describe the interaction dynamics or agents' internal decision processes. Still, they often suffer from poor scalability or cannot reflect diverse human behaviors. This work proposes an interpretability framework for a phenomenological method through responsibility evaluations. We formulate responsibility as a measure of how much an agent takes into account the welfare of other agents through counterfactual reasoning. Additionally, this framework abstracts the computed responsibility sequences into different responsibility levels and grounds these latent levels into reward functions. The proposed responsibility-based interpretability framework is modular and easily integrated into a wide range of prediction models. To demonstrate the utility of the proposed framework in providing added interpretability, we adapt an existing AV prediction model and perform a simulation study on a real-world nuScenes traffic dataset. Experimental results show that we can perform offline ex-post traffic analysis by incorporating the responsibility signal and rendering interpretable but accurate online trajectory predictions.</td>
                <td>Adaptation models, Computational modeling, Scalability, Predictive models, Rendering (computer graphics), Cognition, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341712&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341712&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Time-Optimal Path Tracking with ISO Safety Guarantees</td>
                <td>C. Pham</td>
                <td>2023</td>
                <td>One way of ensuring operator's safety during human-robot collaboration is through Speed and Separation Monitoring (SSM), as defined in ISO standard ISO/TS 15066. In general, it is impossible to avoid all human-robot collisions: consider for instance the case when the robot does not move at all, a human operator can still collide with it by hitting it of her own voluntary motion. In the SSM framework, it is possible however to minimize harm by requiring this: if a collision ever occurs, then the robot must be in a stationary state (all links have zero velocity) at the time instant of the collision. In this paper, we propose a time-optimal control policy based on Time-Optimal Path Parameterization (TOPP) to guarantee such a behavior. Specifically, we show that: for any robot motion that is strictly faster than the motion recommended by our policy, there exists a human motion that results in a collision with the robot in a non-stationary state. Correlatively, we show, in simulation, that our policy is strictly less conservative than state-of-the-art safe robot control methods. Additionally, we propose a parallelization method to reduce the computation time of our pre-computation phase (down to about 0.5 sec, practically), which enables the whole pipeline (including the pre-computation) to be executed at runtime, nearly in real-time. Finally, we demonstrate the application of our method in a scenario: time-optimal, safe control of a 6-dof industrial robot.</td>
                <td>Runtime, Service robots, ISO Standards, Graphics processing units, Collaboration, 6-DOF, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342287&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342287&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Upper Bounds for Localization Errors in 2D Human Pose Estimation</td>
                <td>P. Schlosser, C. Ledermann and T. Asfour</td>
                <td>2023</td>
                <td>Obtaining reliable detections of a human is crucial for many safety-related robotic tasks. This can be done by human pose estimation methods, which predict the position of several different keypoints of the human body. In most cases, recent approaches based on neural networks produce ‘good’ results, i.e. predictions with small localization errors, however, large errors do also occur. For an individual keypoint prediction, the magnitude of the error is unknown, posing a risk to safety. In this work, we extend a neural network architecture for single-person 2D human pose estimation, so that it predicts not only the keypoints of the human body, but also corresponding upper bounds for their localization errors. These upper bounds correspond to the neural network's confidence in its output, and are obtained by one of two general strategies based on (i) a direct estimation of the localization error or (ii) the predicted standard deviations of a 2D Gaussian. We propose several approaches employing these strategies and evaluate them on the MPII Human Pose dataset. In addition, we consider two quality criteria for the results: closeness of the predicted keypoint position to the actual one, and closeness of the predicted upper bound to the localization error. The best results are achieved by a Gaussian-based approach, which predicted correct upper bounds in 94.7% of the cases, while also sufficiently fulfilling the quality criteria.</td>
                <td>Location awareness, Upper bound, Uncertainty, Pose estimation, Neural networks, Safety, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341572&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341572&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhanced Performance of Human-Robot Collaboration Using Braking Surfaces and Trajectory Scaling</td>
                <td>B. Lacevic, A. R. S. E. M. Newishy, A. M. Zanchettin and P. Rocco</td>
                <td>2023</td>
                <td>This paper presents an effective approach to enable performance improvement in human-robot collaboration scenarios. The problem is tackled from the perspective of speed and separation monitoring principle, which stems from the recently instituted safety standard. The proposed approach attempts to seek for performance gains, measured by the speed-up of the production cycle, without compromising the safety constraints consistent with the standard. The approach is based on the notion of braking surface - an abstraction of the swept volume described by the manipulator during braking motion. We address two types of braking behavior: general and path-consistent. In both cases, the braking surface can be evaluated in a receding horizon manner. The robot velocity is continuously scaled such that, in case of a controlled stop, the corresponding volume spanned by the robot (braking surface) does not interfere with the surrounding obstacles. The approach is entirely kinematic and does not require the knowledge of the robot's dynamic model. Simulation study indicates that the pro-posed approach offers performance improvements compared to other state of the art methods. Moreover, the experiments demonstrate the real-time applicability of the method with the real robot in human-shared environment.</td>
                <td>Collaboration, Production, Performance gain, Real-time systems, Safety, Trajectory, Collision avoidance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341408&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341408&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Active Electric Perception-Based Haptic Modality with Applications to Robotics</td>
                <td>M. Zechmair and Y. Morel</td>
                <td>2023</td>
                <td>This paper describes the hardware implementation and characterization of a capacitive sensor designed to support detection and localization of nearby objects. The sensor can be mounted on the exterior of any given robotic system. The technology is particularly well-suited to detection of capacitive material, such as living tissue. As such, it offers perspectives of facilitating human-robot interactions (cobotics). We exploit experimental data to implement a digital model of the sensor and illustrate its accuracy by emulating experimental results in simulation. The sensor is used in a number of interaction scenarii (following and avoidance), providing examples of the manner in which it can be used to support human-robot interactions.</td>
                <td>Location awareness, Shape, Human-robot interaction, Robot sensing systems, Manipulators, Capacitive sensors, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341465&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341465&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Perirobot Space Representation for HRI: Measuring and Designing Collaborative Workspace Coverage by Diverse Sensors</td>
                <td>J. Rozlivek, P. Svarny and M. Hoffmann</td>
                <td>2023</td>
                <td>Two regimes permitting safe physical human-robot interaction, speed and separation monitoring and safety-rated monitored stop, depend on reliable perception of the space surrounding the robot. This can be accomplished by visual sensors (like cameras, RGB-D cameras, LIDARs), proximity sensors, or dedicated devices used in industrial settings like pads that are activated by the presence of the operator. The deployment of a particular solution is often ad hoc and no unified representation of the interaction space or its coverage by the different sensors exists. In this work, we make first steps in this direction by defining the spaces to be monitored, representing all sensor data as information about occupancy and using occupancy-based metrics to calculate how a particular sensor covers the workspace. We demonstrate our approach in two sensor-placement experiments in three static scenes and one experiment in a dynamic scene. The occupancy representation allow the comparison of the effectiveness of various sensor setups. Therefore, this approach can serve as a prototyping tool to establish the sensor setup that provides the most efficient coverage for the given metrics and sensor representations.</td>
                <td>Visualization, Laser radar, Robot vision systems, Human-robot interaction, Cameras, Sensor systems, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341829&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341829&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Safe Collision and Clamping Reaction for Parallel Robots During Human-Robot Collaboration</td>
                <td>L. Habich and T. Ortmaier</td>
                <td>2023</td>
                <td>Parallel robots (PRs) offer the potential for safe human-robot collaboration because of their low moving masses. Due to the in-parallel kinematic chains, the risk of contact in the form of collisions and clamping at a chain increases. Ensuring safety is investigated in this work through various contact reactions on a real planar PR. External forces are estimated based on proprioceptive information and a dynamics model, which allows contact detection. Retraction along the direction of the estimated line of action provides an instantaneous response to limit the occurring contact forces within the experiment to 70 N at a maximum velocity of 0.4 m/s. A reduction in the stiffness of a Cartesian impedance control is investigated as a further strategy. For clamping, a feedforward neural network (FNN) is trained and tested in different joint angle configurations to classify whether a collision or clamping occurs with an accuracy of 80%. A second FNN classifies the clamping kinematic chain to enable a subsequent kinematic projection of the clamping joint angle onto the rotational platform coordinates. In this way, a structure opening is performed in addition to the softer retraction movement. The reaction strategies are compared in real-world experiments at different velocities and controller stiffnesses to demonstrate their effectiveness. The results show that in all collision and clamping experiments the PR terminates the contact in less than 130 ms.</td>
                <td>Parallel robots, Fuzzy control, Trajectory planning, Propioception, Collaboration, Kinematics, Switches</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341581&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341581&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>AmbiSense: Acoustic Field Based Blindspot-Free Proximity Detection and Bearing Estimation</td>
                <td>S. Rupavatharam et al.</td>
                <td>2023</td>
                <td>In this paper, we present AmbiSense, an acoustic field based sensing system that performs proximity detection and bearing estimation for safer physical human-robot interactions. A single low cost piezoelectric transducer is used to setup this novel acoustic sensing modality to create a blindspot-free sound field engulfing a robot arm. Two detection algorithms leveraging spectral information from reflected audio waves of objects entering the acoustic field are proposed to infer object presence and bearing. We also present a new receiver structure which improves signal to noise ratio (SNR). AmbiSense is paired with a collision avoidance inverse kinematic solver for real world deployment on a Kinova Gen3 robot. Validation is performed using ten test objects generating 2000 proximity and bearing estimation events in real world settings, we show that AmbiSense detects proximity with 93.8% sensitivity and 96.6 % specificity. It estimates bearing and maps it to three zones on a robot link with 100% sensitivity and specificity, while using fewer sensors than state of the art methods for similar coverage.</td>
                <td>Direction-of-arrival estimation, Sensitivity, Receivers, Sensitivity and specificity, Robot sensing systems, Sensors, Acoustic field</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341766&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341766&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Extensions to Dynamically-Consistent Collision Reaction Control for Collaborative Robots</td>
                <td>M. Harder, M. Iskandar, J. Lee and A. Dietrich</td>
                <td>2023</td>
                <td>Since modern robots are supposed to work closely together with humans, physical human-robot interaction is gaining importance. One crucial aspect for safe collaboration is a robust collision reaction strategy that is triggered after an unintentional physical contact. In this work, we propose a dynamically-consistent collision reaction controller, where the reactive motion is performed in one particular desired direction in Cartesian space, without disturbing the remaining ones. This results in more intuitive and more predictable behavior of the end-effector. In addition, the proposed reaction control law is independent of contact and internal observer dynamics used for collision detection. The theoretical claims are validated in simulation and experiments. The proposed reaction controller is experimentally compared with a conventional approach for collision reaction. All experiments have been conducted on a torque controlled KUKA LWR IV + lightweight robot.</td>
                <td>Torque, Uncertainty, Dynamics, Collaboration, Observers, Robustness, End effectors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341950&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341950&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>All Aware Robot Navigation in Human Environments Using Deep Reinforcement Learning</td>
                <td>X. Lu, A. Faragasso, A. Yamashita and H. Asama</td>
                <td>2023</td>
                <td>Mobile robots functioning in human environments should behave with a secure and socially-compliant manner. Although many studies have revealed the effectiveness of Deep Reinforcement Learning (DRL) in robot navigation, most of them can only handle the presence of human as independent individuals. Failing to consider groups may lead to the robot getting stuck or behaving rudely, and omitting to separately handle obstacles from pedestrians will cause low efficiency. In this work, we present a novel all-aware neural network that utilizes DRL to process groups, obstacles, and individuals simultaneously. The proposed solution employs a new Group–Robot Interaction (GRI) subnetwork to encode the mutual effects between groups and the robot, and a modified Obstacle–Robot Unilateral interaction (ORU) subnetwork is presented to avoid obstacle collisions caused by sensing noises or motion uncertainties. In addition, the influences of a pedestrian, obstacle, and group on other pedestrians or groups, that indirectly affect the robot, are also integrated into the Human–Robot Interaction (HRI) subnetwork or GRI subnetwork respectively by using map tensors. Finally, the GRI, ORU, and HRI subnetworks are aggregated into a planning subnetwork to train and derive an all-aware robot navigation policy based on DRL. Evaluation results in both real-world and simulation experiments show that the proposed approach outperforms the current cutting-edge methods.</td>
                <td>Deep learning, Pedestrians, Uncertainty, Navigation, Human-robot interaction, Reinforcement learning, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341477&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341477&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Relationship Between Ankle Assistive Torque and Biomechanical Gait Metrics in Individuals After Stroke</td>
                <td>Fernández et al.</td>
                <td>2023</td>
                <td>The effectiveness of robotic exoskeletons for post-stroke gait rehabilitation might be limited as the control parameters of these devices do not adapt to key biomechanical descriptors. The main contribution of this study is to examine post-stroke gait with the aim of finding relationships between exoskeleton control parameters and a comprehensive set of biomechanical metrics. Five stroke survivors walked with the assistance of a wearable ankle exoskeleton (ABLE-S) using different levels of plantarflexion (PF) and dorsiflexion (DF) peak torque, as well as different timings of PF peak torque. We found that DF peak magnitude had significant negative relations with the temporal symmetry index ($\mathrm{p}=0.033$) and the paretic foot absolute angle at heel strike ($\mathrm{p}=0.019$). Changes in the applied PF assistance parameters were significantly correlated with a high variety of temporal and spatial parameters, e.g., walking speed ($\mathrm{p}=0.009$), stride length ($\mathrm{p}=0.011$), non-paretic step length ($\mathrm{p}=0.024$), foot clearance ($\mathrm{p}=0.003$) and hip hiking ($\mathrm{p}=0.038$), and the muscle activation for the non-paretic side, e.g., Tibialis Anterior ($\mathrm{p}=0.049$) and Gastrocnemius Medialis ($\mathrm{p}=0.049$). Based on our results, we propose a set of control laws for adapting the assistance of ankle exoskeletons that will be evaluated in future work.</td>
                <td>Measurement, Biomechanics, Legged locomotion, Torque, Exoskeletons, Timing, Indexes</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341864&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341864&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Experimental Evaluation of a Transparent Operation Mode for a Lower-Limb Exoskeleton Designed for Children with Cerebral Palsy</td>
                <td>R. M. Andrade, S. Sapienza, A. Mohebbi, E. E. Fabara and P. Bonato</td>
                <td>2023</td>
                <td>Robot-assisted rehabilitation is expected to reduce locomotor limitations of children and young adults with Cerebral Palsy (CP). However, to achieve this result, it is essential that the robot is transparent, allowing the user to move freely, and generate joint torques only when the exoskeleton joints significantly deviate from physiological gait patterns. Nevertheless, the development of transparent operation in lower-limb exoskeletons is still an open problem with several implementation challenges. In this study, we implemented a transparent operation strategy on the ExoRoboWalker, an overground exoskeleton designed for children and young adults with CP. The approach employs a feedback zero-torque controller with feedforward compensations for the exoskeleton's dynamics and actuators' impedance. We experimented the proposed system in five healthy subjects walking overground with the exoskeleton in transparent mode (ExoTransp) and non-transparent mode (ExoOff). The proposed transparent controller reduced the user-robot interaction torque and improved user gait kinematics relative to ExoOff. This is a significant step toward an overground gait training exoskeleton for CP population.</td>
                <td>Legged locomotion, Actuators, Pediatrics, Torque, Exoskeletons, Sociology, Kinematics, Lower-limb exoskeleton, cerebral palsy, gait training, transparent control, gait kinematics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342182&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342182&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Effects of Personalization on Gait-State Tracking Performance Using Extended Kalman Filters</td>
                <td>Pérez, G. C. Thomas and R. D. Gregg</td>
                <td>2023</td>
                <td>Emerging partial-assistance exoskeletons can enhance able-bodied performance and aid people with patho-logical gait or age-related immobility. However, every person walks differently, which makes it difficult to directly compute assistance torques from joint kinematics. Gait-state estimation-based controllers use phase (normalized stride time) and task variables (e.g., stride length and ground inclination) to parameterize the joint torques. Using kinematic models that depend on the gait-state, prior work has used an Extended Kalman filter (EKF) to estimate the gait-state online. However, this EKF suffered from kinematic errors since it used a subject-independent measurement model, and it is still unknown how personalization of this measurement model would reduce gait-state tracking error. This paper quantifies how much gait-state tracking improvement a personalized measurement model can have over a subject-independent measurement model when using an EKF-based gait-state estimator. Since the EKF performance depends on the measurement model covariance matrix, we tested on multiple different tuning parameters. Across reasonable values of tuning parameters that resulted in good performance, personalization improved estimation error on average by 8.5 ± 13.8% for phase (mean ± standard deviation), 27.2 $\pm \ 8.1{\%}$ for stride length, and 10.5 $\pm \ 13.5{\%}$ for ground inclination. These findings support the hypothesis that personalization of the measurement model significantly improves gait-state estimation performance in EKF based gait-state tracking $(P \ll 0.05)$, which could ultimately enable reliable responses to faster human gait changes.</td>
                <td>Measurement uncertainty, Sociology, Kinematics, Robot sensing systems, Kalman filters, Reliability, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342498&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342498&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>State- Based Control for an Actuated Reciprocal Gait Orthosis</td>
                <td>S. Eckstein, B. Leudesdorff, C. Maufroy, U. Schneider and C. D. Remy</td>
                <td>2023</td>
                <td>Upright locomotion has many health benefits for patients with spinal cord injury. Passive gait orthoses, such as an isocentric reciprocal gait orthosis (IRGO), allow patients to walk by pushing themselves forward with forearm supports. To move the legs, the IRGO physically couples the motion of stance and swing leg through a linkage. Unfortunately, this locomotion is associated with high metabolic effort. To reduce the metabolic demand while maintaining the advantages of simplicity and low weight of an IRGO, we propose the extension of an IRGO with a single actuator directly integrated into the rocking mechanism and investigate the use of a Hybrid Zero Dynamics based controller to support the user. This is a time-invariant feedback controller, in which a stable reciprocal gait is designed by means of numerically optimized virtual constraints. We evaluate the feasibility of this approach in a sagittal plane model of the dynamic system. For a range of walking speeds and step lengths, we are able to show that the chosen approach has the potential to safely support impaired users walking with IRGOs. The obtained gaits were periodic, stable, maintained a minimal ground clearance and could be implemented with a driving torque of 50 Nm.</td>
                <td>Legged locomotion, Couplings, Actuators, Torque, Numerical models, Spinal cord injury, Dynamical systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342037&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342037&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Energetic Approach to Task-Invariant Ankle Exoskeleton Control</td>
                <td>K. Walters, G. C. Thomas, J. Lin and R. D. Gregg</td>
                <td>2023</td>
                <td>Robotic ankle exoskeletons have been shown to reduce human effort during walking. However, existing ankle exoskeleton control approaches are limited in their ability to apply biomimetic torque across diverse tasks outside of the controlled lab environment. Energy shaping control can provide task-invariant assistance without estimating the user's state, classifying task, or reproducing pre-defined torque trajectories. In previous work, we showed that an optimally task-invariant energy shaping controller implemented on a knee-ankle ex-oskeleton reduced the effort of certain muscles for a range of tasks. In this paper, we extend this approach to the sensor suite available at the ankle and present its implementation on a commercially-available, bilateral ankle exoskeleton. An experiment with three healthy subjects walking on a circuit and on a treadmill showed that the controller can approximate biomimetic profiles for varying terrains and task transitions without classifying tasks or switching control modes.</td>
                <td>Legged locomotion, Training, Torque, Biomimetics, Exoskeletons, Stairs, Control systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342136&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342136&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Human-in-the-Loop Optimization of Active Back-Support Exoskeleton Assistance Via Lumbosacral Joint Torque Estimation</td>
                <td>A. Sochopoulos, T. Poliero, D. Caldwell, J. Ortiz and C. Di Natali</td>
                <td>2023</td>
                <td>The assistive profile of an active back support exoskeleton is strongly dependent on the manual tuning of controller gains based on previous experience and trial-and-error. Human-in-the-loop (HIL) optimization allows for automatic tuning of assistive profiles to different subjects. Most HIL methods make use of intrusive sensors that could affect out-of-the-lab exoskeleton adoption. Therefore, we propose a HIL-based assistive controller architecture using only one single IMU that can be easily embedded in any exoskeleton system. To validate our algorithm we recruited 3 subjects and asked them to perform a series of successive load liftings. Meanwhile, we analysed the back-muscles activations focusing on cumulative activation (iEMG), and median activation. We also monitored the total torque generated by the exoskeleton. With respect to an assistance-less condition, the proposed controller resulted in up to 19% reduction of the back-muscles activity. Moreover, compared to a state-of-the-art controller that produced up to 15% reduction of the back-muscles activity, the new controller also required generation of 4% less exoskeleton torque.</td>
                <td>Torque, Exoskeletons, Sociology, Estimation, Human in the loop, User experience, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341810&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341810&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Closed-Loop Feedback Control of Human Step Width During Walking by Mediolaterally Acting Robotic Hip Exoskeleton</td>
                <td>A. Alili, V. Nalam, A. Fleming, M. Liu, J. Dean and H. He Huang</td>
                <td>2023</td>
                <td>Maintaining balance during gait in the mediolateral direction requires more active motor control than in the anteroposterior direction. Step width modulation is a key strategy used by healthy individuals to achieve mediolateral walking balance, but it can be disrupted in populations with poor sensorimotor integration and weak hip abductors, such as the elderly, stroke patients, and people with lower limb amputation. Wearable hip exoskeletons have the potential to serve as assistive or rehabilitation devices for these populations, but there has been limited research on their appropriate usage. In this study, we successfully demonstrated the feasibility of controlling step width using a mediolaterally acting robotic hip exoskeleton. We were able to effectively adjust the user's step width by increasing or decreasing it to predefined targets through the regulation of admittance control parameters governing the device. The maximum average error to increase or decrease the step width was 1.2 cm. This research has the potential to facilitate the development of assistive and rehabilitation applications focused on enhancing the mediolateral gait balance of individuals with neurological impairments, elderly individuals, and amputees via the control of step width.</td>
                <td>Legged locomotion, Motor drives, Exoskeletons, Sociology, Stroke (medical condition), Robot sensing systems, Regulation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342127&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342127&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Comparing the Effectiveness of Control Methodologies of a Hip-Knee-Ankle Exoskeleton During Squatting</td>
                <td>J. M. Li et al.</td>
                <td>2023</td>
                <td>Manual materials handling occupations often involve repetitive lifting, lowering, and carrying motions, which can lead to muscular fatigue and/or injury. The risk increases when loads must be worn on the body for the entirety of a job shift. Exoskeletons have been developed to assist these types of motions, but require the user to bear the weight of a load through their body. Load carriage exoskeletons have been developed to offload worn mass from the user to the ground through the device structure, but they have had limited success and have not been well studied in manual materials handling tasks. In this paper, we introduce a hip-knee-ankle exoskeleton and two control methods: virtual model control and gravity compensation. We compared the ability of each controller to reduce lower-limb muscle activity during squatting. Because the virtual model controller is tailored to squatting, we hypothesized that it would outperform gravity compensation. Both controllers were able to reduce the activity of major lower-limb muscle groups during squatting when compared to squatting with the exoskeleton turned off. Contrary to our original hypothesis, the gravity compensation controller generally outperformed the virtual model controller, which may have been caused by the gravity compensation controller having more consistent knee torque application and the virtual model controller requiring better per-user tuning and familiarization. These results indicate the efficacy of both controllers in reducing injury risk in the lower limbs during squatting.</td>
                <td>Knee, Torque, Exoskeletons, Dynamics, Materials handling, Manuals, Muscles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341374&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341374&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Novel Control Law for Multi-Joint Human-Robot Interaction Tasks While Maintaining Postural Coordination</td>
                <td>K. Ghonasgi, R. Mirsky, A. M. Haith, P. Stone and A. D. Deshpande</td>
                <td>2023</td>
                <td>Exoskeleton robots are capable of safe torque-controlled interactions with a wearer while moving their limbs through predefined trajectories. However, affecting and assisting the wearer's movements while incorporating their inputs (effort and movements) effectively during an interaction re-mains an open problem due to the complex and variable nature of human motion. In this paper, we present a control algorithm that leverages task-specific movement behaviors to control robot torques during unstructured interactions by implementing a force field that imposes a desired joint angle coordination behavior. This control law, built by using principal component analysis (PCA), is implemented and tested with the Harmony exoskeleton. We show that the proposed control law is versatile enough to allow for the imposition of different coordination behaviors with varying levels of impedance stiffness. We also test the feasibility of our method for unstructured human-robot interaction. Specifically, we demonstrate that participants in a human-subject experiment are able to effectively perform reaching tasks while the exoskeleton imposes the desired joint coordination under different movement speeds and interaction modes. Survey results further suggest that the proposed control law may offer a reduction in cognitive or motor effort. This control law opens up the possibility of using the exoskeleton for training the participating in accomplishing complex multi-joint motor tasks while maintaining postural coordination.</td>
                <td>Training, Surveys, Robot kinematics, Exoskeletons, Human-robot interaction, Behavioral sciences, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342501&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342501&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Investigations into Customizing Bilateral Ankle Exoskeletons to Increase Vertical Jumping Performance</td>
                <td>E. A. Bywater, R. Leo and E. J. Rouse</td>
                <td>2023</td>
                <td>Exoskeletons have shown great potential to enhance locomotion by augmenting the lower limb. While most research has focused on steady-state ambulatory activities, the ability to assist transient, ballistic tasks is also important for understanding the potential of exoskeletons in mobility enhancement. In this preliminary study (N = 5), we developed an individually-customized control strategy to assist vertical jumping. The control strategy was deployed on bilateral ankle exoskeletons (ExoBoot, Dephy Inc.). We structured the control strategy as a work loop that parameterized the assistance provided during the jump. We show that configuring the controller based on individual biomechanics and user preferences facilitates increased vertical jump height when using exoskeleton assistance. In addition, we demonstrate that a user's squat depth can have a significant (p < 0.05) impact on height achieved, but that this depth does not need to be optimized; rather, the exoskeleton provides the maximum performance assistance from both preferred- and deep-squat conditions. Jump height increased by 7.2% with the exoskeleton at its maximum assistance setting, which is comparable to or greater than previous systems.</td>
                <td>Biomechanics, Exoskeletons, Steady-state, Transient analysis, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341974&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341974&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Robotic Assistance Personalization Control Approach of Hip Exoskeletons for Gait Symmetry Improvement</td>
                <td>Q. Zhang, X. Tu, J. Si, M. D. Lewek and H. Huang</td>
                <td>2023</td>
                <td>Healthy human locomotion functions with good gait symmetry depend on rhythmic coordination of the left and right legs, which can be deteriorated by neurological disorders like stroke and spinal cord injury. Powered exoskeletons are promising devices to improve impaired people's locomotion functions, like gait symmetry. However, given higher uncertainties and the time-varying nature of human-robot interaction, providing personalized robotic assistance from exoskeletons to achieve the best gait symmetry is challenging, especially for people with neurological disorders. In this paper, we propose a hierarchical control framework for a bilateral hip exoskeleton to provide the adaptive optimal hip joint assistance with a control objective of imposing the desired gait symmetry during walking. Three control levels are included in the hierarchical framework, including the high-level control to tune three control parameters based on a policy iteration reinforcement learning approach, the middle-level control to define the desired assistive torque profile based on a delayed output feedback control method, and the low-level control to achieve a good torque trajectory tracking performance. To evaluate the feasibility of the proposed control framework, five healthy young participants are recruited for treadmill walking experiments, where an artificial gait asymmetry is imitated as the hemiparesis post-stroke, and only the ‘paretic’ hip joint is controlled with the proposed framework. The pilot experimental studies demonstrate that the hierarchical control framework for the hip exoskeleton successfully (asymmetry index from 8.8% to − 0.5%) and efficiently (less than 4 minutes) achieved the desired gait symmetry by providing adaptive optimal assistance on the ‘paretic’ hip joint.</td>
                <td>Legged locomotion, Neurological diseases, Torque, Uncertainty, Trajectory tracking, Robot kinematics, Exoskeletons</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341440&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341440&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Motion Degeneracy in Self-supervised Learning of Elevation Angle Estimation for 2D Forward-Looking Sonar</td>
                <td>Y. Wang, Y. Ji, C. Wu, H. Tsuchiya, H. Asama and A. Yamashita</td>
                <td>2023</td>
                <td>2D forward-looking sonar is a crucial sensor for underwater robotic perception. A well-known problem in this field is estimating missing information in the elevation direction during sonar imaging. There are demands to estimate 3D information per image for 3D mapping and robot navigation during fly-through missions. Recent learning-based methods have demonstrated their strengths, but there are still drawbacks. Supervised learning methods have achieved high-quality results but may require further efforts to acquire 3D ground-truth labels. The existing self-supervised method requires pretraining using synthetic images with 3D supervision. This study aims to realize stable self-supervised learning of elevation angle estimation without pretraining using synthetic images. Failures during self-supervised learning may be caused by motion degeneracy problems. We first analyze the motion field of 2D forward-looking sonar, which is related to the main supervision signal. We utilize a modern learning framework and prove that if the training dataset is built with effective motions, the network can be trained in a self-supervised manner without the knowledge of synthetic data. Both simulation and real experiments validate the proposed method.</td>
                <td>Training, Point cloud compression, Three-dimensional displays, Simultaneous localization and mapping, Supervised learning, Sonar, Estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341601&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341601&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MIMIR-UW: A Multipurpose Synthetic Dataset for Underwater Navigation and Inspection</td>
                <td>Tuñón et al.</td>
                <td>2023</td>
                <td>This paper presents MIMIR-UW, a multipurpose underwater synthetic dataset for SLAM, depth estimation, and object segmentation to bridge the gap between theory and application in underwater environments. MIMIR-UW integrates three camera sensors, inertial measurements, and ground truth for robot pose, image depth, and object segmentation. The underwater robot is deployed within a pipe exploration scenario, carrying artificial lights that create uneven lighting, in addition to natural artefacts such as reflections from natural light and backscattering effects. Four environments totalling eleven tracks are provided, with various difficulties regarding light conditions or dynamic elements. Two metrics for dataset evaluation are proposed, allowing MIMIR-UW to be compared with other datasets. State-of-art methods on SLAM, segmentation and depth estimation are deployed and benchmarked on MIMIR-UW. Moreover, the dataset's potential for sim-to-real transfer is demonstrated by leveraging the segmentation and depth estimation models trained on MIMIR-UW in a real pipeline inspection scenario. To the best of the authors' knowledge, this is the first underwater dataset targeted for such a variety of methods. The dataset is publicly available online. https://github.com/remaro-network/MIMIR-UW/</td>
                <td>Measurement, Simultaneous localization and mapping, Robot vision systems, Pipelines, Estimation, Object segmentation, Inspection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341436&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341436&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Marine Vessel Attitude Estimation from Coastline and Horizon</td>
                <td>S. Singhal, Y. Ao, D. Maas, B. Arsenali and S. Maranò</td>
                <td>2023</td>
                <td>Reliable monitoring of vessel motions is crucial for safe and efficient operation of marine vessels. Pitching and rolling motions are commonly monitored using high-grade inertial measurement units (IMUs). However, such sensors become unreliable in presence of long-lasting accelerations. In this work, we propose a method for attitude estimation of marine vessels relying on an image stream and known world features. Our focus is on the estimation of pitch and roll angles. We employ a semantic segmentation network and process its output for robust extraction of coastlines and horizon. The image features are matched with known world features to estimate the attitude. The proposed method is validated using different metrics on data acquired from a small passenger ferry. The proposed method achieves more than 60% reduction in vertical reprojection error compared to IMU. We show that the proposed method outperforms IMU and can be used to replace it whenever horizon or coastline is visible.</td>
                <td>Coastlines, Measurement units, Semantic segmentation, Estimation, Inertial navigation, Streaming media, Sensors, Attitude estimation, semantic segmentation, marine navigation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341522&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341522&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DANDELION: An ASV Deployed Micro-Profiler Array for Air-Sea Observation</td>
                <td>Z. Fan, C. Lyu and Z. Zeng</td>
                <td>2023</td>
                <td>The air-sea interface is vital in studying heat and energy exchange between the sea and air. The field observation technology of the air-sea interface is an effective way to explore the nature of the air-sea interface. This paper presents an observation system called DANDELION for the air-sea interface environment. The system includes an automatic surface vehicle (ASV), a launching device using a pair of high-speed rotating friction wheels, and eight dandelion-like micro profilers. This system can implement the environmental observation of the air-sea interface in an extensive range through micro profilers' rapid and multi-point placement. The DANDELION system was characterized by establishing the friction wheel launching mechanism model and summarizing the effects of different wings on the profiler performance. A series of experiments were conducted in Qiandao Lake, China, to characterize the DANDELION system. We demonstrate the developed system with data from field experiments, which show very high flexibility and feasibility to observe the air-sea interface, implying potential applications in ocean transient phenomena observation.</td>
                <td>Sea surface, Friction, Atmospheric modeling, Wheels, Lakes, Arrays, Vehicle dynamics, Marine Robotics, Environment Monitoring and Management, Air-sea Interface, Micro-profiler</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342079&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342079&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Long-Endurance Optical Seafloor Imaging Using Underwater Gliders: Concept, Development and Initial Trials</td>
                <td>D. Gregorek, A. Tibebu, E. Caudet, C. Barrera and R. Bachmayer</td>
                <td>2023</td>
                <td>In-situ observation of the deep-sea floor is a fundamental need for marine sciences and ecosystem monitoring. This work proposes a novel robotic approach for benthic observations in the deep sea using underwater gliders. The glider is equipped with a downward looking camera system to acquire high resolution optical images of the seafloor. The system works fully autonomous and has the potential for long-endurance missions at low deployment costs. Key factors for long battery lifetime are a low power consumption during idle phases, a timely activation of the main imaging system and an efficient lighting setup. The results of our initial trials at sea using a 1000m glider show the applicability of the approach for marine science applications. We achieved optical image quality utilizable for seafloor classification and 3D reconstruction of underwater objects. Keeping the vertical zig-zag motion of underwater gliders in mind, our findings substantiate the feasibility of multi-week seafloor observation missions at long operating ranges.</td>
                <td>Three-dimensional displays, Power demand, Sea floor, Robot vision systems, Lasers, Lighting, Seaports</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341259&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341259&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Disturbance Preview for Non-Linear Model Predictive Trajectory Tracking of Underwater Vehicles in Wave Dominated Environments</td>
                <td>Serchi</td>
                <td>2023</td>
                <td>Operating in the near-vicinity of marine energy devices poses significant challenges to the control of underwater vehicles, predominantly due to the presence of large magnitude wave disturbances causing hazardous state perturbations. Approaches to tackle this problem have varied, but one promising solution is to adopt predictive control methods. Given the predictable nature of ocean waves, the potential exists to incorporate disturbance estimations directly within the plant model; this requires inclusion of a wave predictor to provide online preview information. To this end, this paper presents a Non-linear Model Predictive Controller with an integrated Deterministic Sea Wave Predictor for trajectory tracking of underwater vehicles. State information is obtained through an Extended Kalman Filter, forming a complete closed-loop strategy and facilitating online wave load estimations. The strategy is compared to a similar feed-forward disturbance mitigation scheme, showing mean performance improvements of 51% in positional error and 44.5% in attitude error. The preliminary results presented here provide strong evidence of the proposed method's high potential to effectively mitigate disturbances, facilitating accurate tracking performance even in the presence of high wave loading.</td>
                <td>Power demand, Trajectory tracking, Perturbation methods, Estimation, Predictive models, Regulation, Underwater vehicles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341695&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341695&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Ensemble of Online Estimation Methods for One Degree-of-Freedom Models of Unmanned Surface Vehicles: Applied Theory and Preliminary Field Results with Eight Vehicles</td>
                <td>T. M. Paine and M. R. Benjamin</td>
                <td>2023</td>
                <td>In this paper we report an experimental evaluation of three popular methods for online system identification of unmanned surface vehicles (USVs) which were implemented as an ensemble: certifiably stable shallow recurrent neural network (RNN), adaptive identification (AID), and recursive least squares (RLS). The algorithms were deployed on eight USVs for a total of 30 hours of online estimation. During online training the loss function for the RNN was augmented to include a cost for violating a sufficient condition for the RNN to be stable in the sense of contraction stability. Additionally we described an efficient method to calculate the equilibrium points of the RNN and classify the associated stability properties about these points. We found the AID method had lowest mean absolute error in the online prediction setting, but a weighted ensemble had lower error in offline processing.</td>
                <td>Training, Sufficient conditions, Recurrent neural networks, Linear regression, Estimation, Stability analysis, System identification</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341433&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341433&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Unmanned Surface Vehicle Navigation with Distributional Reinforcement Learning</td>
                <td>X. Lin, J. McConnell and B. Englot</td>
                <td>2023</td>
                <td>Autonomous navigation of Unmanned Surface Vehicles (USV) in marine environments with current flows is challenging, and few prior works have addressed the sensor-based navigation problem in such environments under no prior knowledge of the current flow and obstacles. We propose a Distributional Reinforcement Learning (RL) based local path planner that learns return distributions which capture the uncertainty of action outcomes, and an adaptive algorithm that automatically tunes the level of sensitivity to the risk in the environment. The proposed planner achieves a more stable learning performance and converges to safer policies than a traditional RL based planner. Computational experiments demonstrate that comparing to a traditional RL based planner and classical local planning methods such as Artificial Potential Fields and the Bug Algorithm, the proposed planner is robust against environmental flows, and is able to plan trajectories that are superior in safety, time and energy consumption.</td>
                <td>Training, Energy consumption, Uncertainty, Sensitivity, Navigation, Computer bugs, Reinforcement learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342389&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342389&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Full Actuation: Reconfigurable Micro Underwater Robots</td>
                <td>N. Bauschmann, D. A. Duecker, T. L. Alff, R. C. Hochdahl and R. Seifried</td>
                <td>2023</td>
                <td>Exploration and monitoring of hazardous environments, such as legacy nuclear storage ponds, constitute safety-critical missions to be performed by small-scale underwater robots. These monitoring tasks require fully actuated robot platforms in order to allow for hovering while inspecting objects of interest in detail. A severe bottleneck arises from the restricted access points commonly encountered in these surveillance missions that pose strict limitations on the vehicle dimensions. However, small-scale underwater robots usually possess underactuated propulsion systems and are, thus, only partially suitable for these missions. In this work, we investigate and exploit the idea of reconfigurability. Following the idea of the whole is more than the sum of its parts, we daisy-chain multiple small-scale underwater robots with revolute joints to enable shape reconfigurations. In combination with a centralized sliding mode control scheme, the robot platform is able to change its shape depending on the current task, see Fig. 1. While the straight configuration fits well through tight passages such as inspection holes, the robot can reconfigure itself towards a triangular shape that enables the neutrally buoyant robot to hover at areas of interest, e. g. for inspection tasks. Finally, we examine our proposed concept in a series of simulations and experiments. Moreover, we demonstrate the performance of key elements such as reconfiguration and navigation, discuss their limitations, and point out future directions.</td>
                <td>Autonomous underwater vehicles, Three-dimensional displays, Shape, Trajectory tracking, Surveillance, Prototypes, Inspection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341621&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341621&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Water Surface Walking of Six-Legged Robot by Controlling Attitude of Feet When It Enter Water</td>
                <td>Y. He, Y. Zheng and F. Asano</td>
                <td>2023</td>
                <td>This paper presents a water walking robot with 6 feet which consists of a rimless wheel and a flywheel, and has a foot attached to the tip of each leg. First, in order to make the robot walk on water, we propose a foot control method by imitating the legs of some animals that can walk on water. Second, in order to increase the robot's forward speed, we improved the control method. In addition we have analysed the effect of some different physical parameters on the motion of the robot. This research is aimed at building a versatile water walking model for such applications as marine exploration.</td>
                <td>Legged locomotion, Attitude control, Animals, Buildings, Wheels, Flywheels, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342494&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342494&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Perseus AUV: Towards Linear Convoying of Agile A-Sized AUVs Through Acoustic Track-and-Trail</td>
                <td>N. R. Rypkema, S. Randeni, M. Sacarny, M. Benjamin and M. Triantafyllou</td>
                <td>2023</td>
                <td>We present the Perseus autonomous underwater vehicle (AUV) - an A-sized aaA-size stands for the standard sonobuoy [2] form factor, with a maximum diameter of 124 mm and a length of around 0.9 m, ensuring the ability to launch from standard sonobuoy launchers onboard a wide array of fixed wing and rotary wing air crafts, surface ships and submarines [3] micro AUV, outfitted with a low-cost passive inverted ultra-short baseline (piUSBL) acoustic reception system, which allows it to acoustically track-and-trail a leader vehicle that carries an acoustic transmission source. With a long-term goal of linear convoying of multiple A-sized AUVs, in this work, we used an unmanned surface vehicle (USV) towing an acoustic source, as a proxy for a lead AUV, demonstrating that the Perseus AUV is able to successfully track-and-trail a leader vehicle. The AUV was also outfitted with a tuna-inspired morphing fin mechanism that allowed the vehicle to achieve good directional stability as well as good maneuverability; properties that are useful for linear convoying AUVs, but are presently difficult to achieve because they impose contradictory requirements. We demonstrated this system with real-world, in-water experiments in the Charles river, Massachusetts, USA.</td>
                <td>Meters, Autonomous underwater vehicles, Receivers, Acoustics, Rivers, Reliability, Vehicle dynamics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341835&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341835&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Open-Source Robotic Chinese Chess Player</td>
                <td>S. An et al.</td>
                <td>2023</td>
                <td>Consumer robots can accompany children growing up, improving their abilities while playing and entertaining. This paper presents an open-source, practical, low-cost robotic Chinese chess player. The proposed system includes an elaborate mechanical structure, a simple kinematic solution, a novel robot operating system, real-time and accurate chess recognition. Regarding its mechanical design, it combines a magnetism structure and mechanical cam drive, while the overall system has just three servo motors. At the same time, its control strategy is simple and effective. Furthermore, a lightweight robot message communication mechanism, entitled TinyROS, is developed for computing resource-limited embedded chips. Concerning the recognition process, our CNNbased object detector determines chess and achieves accurate identification. As a result, our robotic Chinese chess player is exquisite and easy for large-scale promotion while improving users' chess skills. Aiming to facilitate future consumer robot research and popularize customer robots, the model's mechanical and software design and the TinyROS protocol are open-sourced at https://github.com/Star-Robot/chinese-chess-robot.</td>
                <td>Visualization, Protocols, Software design, Operating systems, Detectors, Real-time systems, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341697&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341697&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>ETAUS: An Edge and Trustworthy AI UAV System with Self-Adaptivity for Air Quality Monitoring</td>
                <td>H. Wang</td>
                <td>2023</td>
                <td>This work presents the ETAUS, an Edge and Trustworthy AI UAV System, as a mobile sensing platform for air quality monitoring. ETAUS employs an FPGA device as the main hardware computing architecture rather than relying solely on a microprocessor or integrating with GPUs to meet real-time processing demands and achieve adaptivity and scalability. ETAUS contains a neural engine that can execute our customized AI model for air quality index (AQI) level classification and a pre-trained model for detecting objects containing private information. ETAUS also incorporates a de-identification process, cryptographic functions, and protection matrices to safeguard information and individuals' privacy. Furthermore, cryptographic functions and protection matrices are implemented as reconfigurable modules, which can accelerate processing, protect data privacy, and be reconfigured as needed. Experiments have demonstrated ETAUS can achieve a speedup of 3.15x to 72.46x for AQI level classification compared to microprocessor-based and GPU-based designs. ETAUS can also enhance energy efficiency by 5.02x compared to embedded GPU solutions such as NVIDIA Jetson Nano. To support all the cryptographic functions and protection matrices, system adaptivity in ETAUS can significantly increase resource utilization while decreasing power consumption by up to 2.79%.</td>
                <td>Privacy, Scalability, Computational modeling, Air quality, Autonomous aerial vehicles, Energy efficiency, Cryptography</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342087&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342087&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FPGADDS: An Intra-FPGA Data Distribution Service for ROS 2 Robotics Applications</td>
                <td>C. Lienen, S. H. Middeke and M. Platzner</td>
                <td>2023</td>
                <td>Modern computing platforms for robotics applications comprise a set of heterogeneous elements, e.g., multi-core CPUs, embedded GPUs, and FPGAs. FPGAs are reprogrammable hardware devices that allow for fast and energy-efficient computation of many relevant tasks in robotics. ROS is the de-facto programming standard for robotics and decomposes an application into a set of communicating nodes. ReconROS is a previous approach that can map complete ROS nodes into hardware for acceleration. Since ReconROS relies on standard ROS communication layers, exchanging data between hardware-mapped nodes can lead to a performance bottleneck. This paper presents fpgaDDS, a lean data distribution service for hardware-mapped ROS 2 nodes. fpgaDDS relies on a customized and statically generated streaming-based communication architecture. We detail this communication architecture with its components and outline its benefits. We evaluate fpgaDDS on a test example and a larger autonomous vehicle case study. Compared to a ROS 2 application in software, we achieve speedups of up to 13.34 and reduce jitter by two orders of magnitude.</td>
                <td>Computer architecture, Programming, Jitter, Hardware, Software, Energy efficiency, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341921&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341921&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Enhancing 5G-Enabled Robots Autonomy by Radio-Aware Semantic Maps</td>
                <td>A. Lendinez et al.</td>
                <td>2023</td>
                <td>Future robotics systems aiming for true autonomy must be robust against dynamic and unstructured environments. The 5th generation (5G) mobile network is expected to provide ubiquitous, reliable and low-latency wireless communications to ground robots, especially in outdoor scenarios. Empowered by 5G, the digital transformation of robotics is emerging, enabled by the cloud-native paradigm and the adoption of edge-computing principles for heavy computational task offloading. However, wireless link quality fluctuates due to multiple aspects such as the topography of the deployment area, the presence of obstacles, robots' movement and the configuration of the serving base stations. This directly impacts not only the connectivity to the robots but also the performance of robot operations, resulting in severe challenges when targeting full robot autonomy. To address such challenges, in this paper, we propose a framework to build a semantic map based on radio quality. By means of our proposed approach, mobile robots can gain knowledge on up-to-date radio context map information of the surrounding environment, hence enabling reliable and efficient robotics operations.</td>
                <td>Wireless communication, 5G mobile communication, Semantics, Surfaces, Reliability, Mobile robots, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342279&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342279&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SkiROS2: A Skill-Based Robot Control Platform for ROS</td>
                <td>M. Mayr, F. Rovida and V. Krueger</td>
                <td>2023</td>
                <td>The need for autonomous robot systems in both the service and the industrial domain is larger than ever. In the latter, the transition to small batches or even “batch size 1” in production created a need for robot control system architectures that can provide the required flexibility. Such architectures must not only have a sufficient knowledge integration framework. It must also support autonomous mission execution and allow for interchangeability and interoperability between different tasks and robot systems. We introduce SkiROS2, a skill-based robot control platform on top of ROS. SkiROS2 proposes a layered, hybrid control structure for automated task planning, and reactive execution, supported by a knowledge base for reasoning about the world state and entities. The scheduling formulation builds on the extended behavior tree model that merges task-level planning and execution. This allows for a high degree of modularity and a fast reaction to changes in the environment. The skill formulation based on pre-, hold-and post-conditions allows to organize robot programs and to compose diverse skills reaching from perception to low-level control and the incorporation of external tools. We relate SkiROS2 to the field and outline three example use cases that cover task planning, reasoning, multisensory input, integration in a manufacturing execution system and reinforcement learning.</td>
                <td>Robot control, Systems architecture, Tutorials, Reinforcement learning, Production, Organizations, Cognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342216&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342216&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Soy: An Efficient MILP Solver for Piecewise-Affine Systems</td>
                <td>H. Wu, M. Wu, D. Sadigh and C. Barrett</td>
                <td>2023</td>
                <td>Piecewise-affine (PWA) systems are widely used for modeling and control of robotics problems including modeling contact dynamics. A common approach is to encode the control problem of the PWA system as a Mixed-Integer Convex Program (MICP), which can be solved by general-purpose off-the-shelf MICP solvers. To mitigate the scalability challenge of solving these MICP problems, existing work focuses on devising efficient and strong formulations of the problems, while less effort has been spent on exploiting their specific structure to develop specialized solvers. The latter is the theme of our work. We focus on efficiently handling one-hot constraints, which are particularly relevant when encoding PWA dynamics. We have implemented our techniques in a tool, Soy, which organically integrates logical reasoning, arithmetic reasoning, and stochastic local search. For a set of PWA control benchmarks, Soy solves more problems, faster, than two state-of-the-art MICP solvers.</td>
                <td>Scalability, Stochastic processes, Benchmark testing, Cognition, Encoding, Intelligent robots, Arithmetic</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342011&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342011&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Small Form Factor Aerial Research Vehicle for Pick-and-Place Tasks with Onboard Real-Time Object Detection and Visual Odometry</td>
                <td>C. A. Dimmig, A. Goodridge, G. Baraban, P. Zhu, J. Bhowmick and M. Kobilarov</td>
                <td>2023</td>
                <td>This paper introduces a novel, small form-factor, aerial vehicle research platform for agile object detection, classification, tracking, and interaction tasks. General-purpose hardware components were designed to augment a given aerial vehicle and enable it to perform safe and reliable grasping. These components include a custom collision tolerant cage and low-cost Gripper Extension Package, which we call GREP, for object grasping. Small vehicles enable applications in highly constrained environments, but are often limited by computational resources. This work evaluates the challenges of pick-and-place tasks, with entirely onboard computation of object pose and visual odometry based state estimation on a small platform, and demonstrates experiments with enough accuracy to reliably grasp objects. In a total of 70 trials across challenging cases such as cluttered environments, obstructed targets, and multiple instances of the same target, we demonstrated successfully grasping the target in 93 % of trials. Both the hardware component designs and software framework are released as open-source, since our intention is to enable easy reproduction and application on a wide range of small vehicles.</td>
                <td>Grasping, Object detection, Hardware, Visual servoing, Software, Software reliability, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342295&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342295&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Ungar - A C++ Framework for Real-Time Optimal Control Using Template Metaprogramming</td>
                <td>F. De Vincenti and S. Coros</td>
                <td>2023</td>
                <td>We present Ungar, an open-source library to aid the implementation of high-dimensional optimal control problems (OCPs). We adopt modern template metaprogramming techniques to enable the compile-time modeling of complex systems while retaining maximum runtime efficiency. Our framework provides syntactic sugar to allow for expressive formulations of a rich set of structured dynamical systems. While the core modules depend only on the header-only Eigen and Boost.Hana libraries, we bundle our codebase with optional packages and custom wrappers for automatic differentiation, code generation, and nonlinear programming. Finally, we demonstrate the versatility of Ungar in various model predictive control applications, namely, four-legged locomotion and collaborative loco-manipulation with multiple one-armed quadruped robots. Ungar is available under the Apache License 2.0 at https://github.com/fdevinc/ungar.</td>
                <td>Codes, Optimal control, Collaboration, C++ languages, Syntactics, Libraries, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341365&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341365&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>VaPr: Variable-Precision Tensors to Accelerate Robot Motion Planning</td>
                <td>S. Hsiao et al.</td>
                <td>2023</td>
                <td>High-dimensional motion generation requires nu-merical precision for smooth, collision-free solutions. Typically, double-precision or single-precision floating-point (FP) formats are utilized. Using these for big tensors imposes a strain on the memory bandwidth provided by the devices and alters the memory footprint, hence limiting their applicability to low-power edge devices needed for mobile robots. The uniform application of reduced precision can be advantageous but severely degrades solutions. Using decreased precision data types for important tensors, we propose to accelerate motion generation by removing memory bottlenecks. We propose variable-precision (VaPr) search optimization to determine the appropriate precision for large tensors from a vast search space of approximately 4 million unique combinations for FP data types across the tensors. To obtain the efficiency gains, we exploit existing platform support for an out-of-the-box GPU speedup and evaluate prospective precision converter units for GPU types that are not currently supported. Our experimental results on 800 planning problems for the Franka Panda robot on the MotionBenchmaker dataset across 8 environments show that a 4-bit FP format is sufficient for the largest set of tensors in the motion generation stack. With the software-only solution, VaPr achieves 6.3% and 6.3% speedups on average for a significant portion of motion generation over the SOTA solution (CuRobo) on Jetson Orin and RTX2080 Ti GPU, respectively, and 9.9%, 17.7% speedups with the FP converter. More details are available at sites.google.com/nvidia.com/vapr.</td>
                <td>Robot motion, Tensors, Limiting, Pipelines, Graphics processing units, Manipulators, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342109&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342109&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Scalable. Intuitive Human to Robot Skill Transfer with Wearable Human Machine Interfaces: On Complex, Dexterous Tasks</td>
                <td>F. Sanches et al.</td>
                <td>2023</td>
                <td>The advent of collaborative industrial and house-hold robotics has blurred the demarcation between the human and robot workspace. The capability of robots to function efficiently alongside humans requires new research to be conducted in dynamic environments as opposed to the traditional well-structured laboratory. In this work, we propose an efficient skill transfer methodology comprising intuitive interfaces, efficient optical tracking systems, and compliant control of robotic arm-hand systems. The lightweight wearable interfaces mounted with robotic grippers and hands allow the execution of dexterous activities in dynamic environments without restricting human dexterity. The fiducial and reflective markers mounted on the interfaces facilitate the extraction of positional and rotational information allowing efficient trajectory tracking. As the tasks are performed using the mounted grippers and hands, gripper state information can be directly transferred. The hardware-agnostic nature and efficiency of the proposed interfaces and skill transfer methodology are demonstrated through the execution of complex tasks that require increased dexterity, writing and drawing.</td>
                <td>Visualization, Service robots, Optical recording, End effectors, Trajectory, Optical reflection, Optical sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341661&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341661&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Towards Cooperative Flight Control Using Visual-Attention</td>
                <td>L. Yin et al.</td>
                <td>2023</td>
                <td>The cooperation of a human pilot with an autonomous agent during flight control realizes parallel autonomy. We propose an air-guardian system that facilitates cooperation between a pilot with eye tracking and a parallel end-to-end neural control system. Our vision-based air-guardian system combines a causal continuous-depth neural network model with a cooperation layer to enable parallel autonomy between a pilot and a control system based on perceived differences in their attention profiles. The attention profiles for neural networks are obtained by computing the networks' saliency maps (feature importance) through the VisualBackProp algorithm, while the attention profiles for humans are either obtained by eye tracking of human pilots or saliency maps of networks trained to imitate human pilots. When the attention profile of the pilot and guardian agents align, the pilot makes control decisions. Otherwise, the air-guardian makes interventions and takes over the control of the aircraft. We show that our attention-based air-guardian system can balance the trade-off between its level of involvement in the flight and the pilot's expertise and attention. The guardian system is particularly effective in situations where the pilot was distracted due to information overload. We demonstrate the effectiveness of our method for navigating flight scenarios in simulation with a fixed-wing aircraft and on hardware with a quadrotor platform.</td>
                <td>Visualization, Neural networks, Gaze tracking, Aircraft navigation, Hardware, Aircraft, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342229&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342229&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Cyber-Attacks on Wheeled Mobile Robotic Systems with Visual Servoing Control</td>
                <td>A. Jokic, A. Khazraei, M. Petrovic, Z. Jakovljevic and M. Pajic</td>
                <td>2023</td>
                <td>Visual servoing represents a control strategy capable of driving dynamical systems from the current to the desired pose, when the only available information is the images generated at both poses. In this work, we analyze vulnerability of such systems and introduce two types of attacks to deceive visual servoing controller within a wheeled mobile robotic system. The attack goal is to alter the visual servoing procedure in such a way that mobile robot achieves the pose defined by an attacker instead of the desired one. Specifically, the attacks exploit image transformations developed using a methodology based on simulated annealing. The main difference between the attacks is the considered threat model - i.e., how the attacker has infiltrated the system. The first attack assumes the real-time camera feed has been compromised and thus, the images from the current pose are modified (e.g., during the acquisition or communication); for the second, only the desired destination image is potentially altered. Finally, in 3D simulations and real- world experiments, we show the effectiveness of cyber-attacks.</td>
                <td>Threat modeling, Image transformation, Solid modeling, Three-dimensional displays, Runtime, Simulated annealing, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341376&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341376&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>An Event-Based Tracking Control Framework for Multirotor Aerial Vehicles Using a Dynamic Vision Sensor and Neuromorphic Hardware</td>
                <td>Gotarredona and K. J. Kyriakopoulos</td>
                <td>2023</td>
                <td>In this paper, we present an event-based control framework for the efficient tracking of contour-based areas, such as road pavements, using a multirotor aerial vehicle equipped with a bio-inspired Dynamic Vision Sensor (DVS). Concerning the detection part, the DVS camera captures events, which are asynchronously fed into a Neuromorphic Hough Transform algorithm running on a SpiNN-3 board and implemented as a Spiking Neural Network (SNN). Next, the asynchronous output of the detection module is fed into an analytically formulated event-based Partitioned Visual Servoing (PVS) algorithm, running on conventional processing hardware, which allows the multirotor to autonomously track and navigate along the detected contour. The proposed architecture achieves efficient tracking of contour-based areas, while constantly maintaining the latter inside the DVS camera's field of view. A set of real-time experiments in various settings employing an octorotor equipped with a downward-looking DVS and a SpiNN-3 board demonstrate the effectiveness of the suggested framework.</td>
                <td>Neuromorphics, Transforms, Vision sensors, Cameras, Hardware, Real-time systems, Visual servoing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342437&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342437&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Shape Servoing of a Soft Object Using Fourier Series and a Physics-Based Model</td>
                <td>F. Makiyeh, F. Chaumette, M. Marchal and A. Krupa</td>
                <td>2023</td>
                <td>In this paper, we propose a physics-based robot controller to deform a soft object toward a desired 3D shape using a limited number of handling points. For this purpose, the shape of the deformable object is represented using Fourier descriptors. We derive the analytical relation that provides the variation of the Fourier coefficients as a function of the movements of the handling points by considering a mass-spring model (MSM). A control law is then designed from this relation. Since the MSM provides an approximation of the object behavior, which in practice can lead to a drift between the object and its model, an online realignment of the model with the real object is performed by tracking its surface from data provided by a remote RGB-D camera. Simulation results validate the approach for the case where many points interact on a 2D soft object while experimental results obtained with two robotic arms demonstrate the autonomous shaping of a 3D soft object.</td>
                <td>Three-dimensional displays, Shape, Tracking, Simulation, Robot vision systems, Manipulators, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342354&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342354&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Visual Servoing on Wheels: Robust Robot Orientation Estimation in Remote Viewpoint Control</td>
                <td>L. Robinson, D. De Martini, M. Gadd and P. Newman</td>
                <td>2023</td>
                <td>This work proposes a fast deployment pipeline for visually-servoed robots which does not assume anything about either the robot - e.g. sizes, colour or the presence of markers - or the deployment environment. Specifically, we apply a learning based approach to reliably estimate the pose of a robot in the image frame of a 2D camera upon which a visual servoing control system can be deployed. To alleviate the time-consuming process of labelling image data, we propose a weakly supervised pipeline that can produce a vast amount of data in a small amount of time. We evaluate our approach on a dataset of remote camera images captured in various indoor environments demonstrating high tracking performances when integrated into a fully-autonomous pipeline with a simple controller. With this, we then analyse the data requirement of our approach, showing how it is possible to deploy a new robot in a new environment in fewer than 30.00 min.</td>
                <td>Training, Visualization, Pipelines, Wheels, Cameras, Visual servoing, Mobile robots, Cloud Robotics, Visual Servoing, Deep Learning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341260&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341260&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Maintaining Visibility of Dynamic Objects in Cluttered Environments Using Mobile Manipulators and Vector Field Inequalities</td>
                <td>F. Dursun, B. V. Adorno, S. Watson and W. Pan</td>
                <td>2023</td>
                <td>Vision-based perception has become prevalent in robotic applications, especially in those where the control loop relies on visual data, such as visual servoing. For those applications, ensuring that the features or target object remain visible to the camera is critical, necessitating visibility-aware control. In this paper, we propose a method to guarantee the visibility of a dynamic object using a constrained kinematic controller and Vector Field Inequalities (VFIs) to include a linear visibility constraint. Unlike existing methods, we introduce constraints into the kinematic controller to ensure the target's visibility without needing a trajectory optimizer or local planner. Our method maintains the target object in the camera field of view (FoV) by representing the FoV with four infinite planes and maintaining the distance between the target object and each plane higher than a predefined distance. We evaluated the proposed approach using a mobile manipulator in two simulations involving cluttered environments: the first scenario involves a stationary target object, whereas the second scenario presents a more challenging workspace involving a moving target. Our results demonstrate that the proposed approach successfully maintains the target within the FoV while avoiding obstacles in the workspace, showing the potential of our method to improve the safety and reliability of visual-servoing-based robotic systems.</td>
                <td>Visualization, Target tracking, Kinematics, Cameras, End effectors, Visual servoing, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341763&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341763&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Shape Control of Variable Length Continuum Robots Using Clothoid-Based Visual Servoing</td>
                <td>S. Chiang, C. D. Onal and B. Calli</td>
                <td>2023</td>
                <td>In this paper, we present a novel clothoid-based visual servoing method for controlling the shape of a variable length continuum manipulator. Clothoids are curves with linearly changing curvature. They allow us to obtain a smooth representation of a continuum manipulator's shape in a compact form with few parameters. Using this curve model, we generate image features that are used in an adaptive visual servoing method to drive the robot to a desired shape. The adaptive algorithm estimates and updates a local interaction matrix that maps the rate of change in clothoid features to actuator velocities of the continuum manipulator. As such, the method does not require any robot model or even actuator encoder measurements and only uses the visual clothoid features to control the robot shape. A unique advantage of using our clothoid representation is being able to generate reference shape curves without the need for taking images of the robot at the desired shapes. Experiments demonstrate successful shape and end effector pose convergence for a diverse set of references. Our repeatability tests demonstrate that the system performance is consistent. Notably, we also present the first results in the literature for the vision-based shape control of a variable length continuum robot, extending and contracting to achieve the desired shape.</td>
                <td>Adaptation models, Actuators, Visualization, Shape control, Shape, System performance, Shape measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342057&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342057&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Keypoints-Based Adaptive Visual Servoing for Control of Robotic Manipulators in Configuration Space</td>
                <td>S. Chatterjee, A. C. Karade, A. Gandhi and B. Calli</td>
                <td>2023</td>
                <td>This paper presents a visual servoing method for controlling a robot in the configuration space by purely using its natural features. We first created a data collection pipeline that uses camera intrinsics, extrinsics, and forward kinematics to generate 2D projections of a robot's joint locations (keypoints) in image space. Using this pipeline, we are able to collect large sets of real-robot data, which we use to train realtime keypoint detectors. The inferred keypoints from the trained model are used as control features in an adaptive visual servoing scheme that estimates, in runtime, the Jacobian relating the changes of the keypoints and joint velocities. We compared the 2D configuration control performance of this method to the skeleton-based visual servoing method (the only other algorithm for purely vision-based configuration space visual servoing), and demonstrated that the keypoints provide more robust and less noisy features, which result in better transient response. We also demonstrate the first vision-based 3D configuration space control results in the literature, and discuss its limitations. Our data collection pipeline is available at https://github.com/JaniC-WPI/KPDataGenerator.git which can be utilized to collect image datasets and train realtime keypoint detectors for various robots and environments.</td>
                <td>Transient response, Three-dimensional displays, Runtime, Pipelines, Detectors, Aerospace electronics, Data collection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342503&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342503&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Autonomous Marker-Less Rapid Aerial Grasping</td>
                <td>E. Bauer, B. G. Cangan and R. K. Katzschmann</td>
                <td>2023</td>
                <td>In a future with autonomous robots, visual and spatial perception is of utmost importance for robotic systems. Particularly for aerial robotics, there are many applications where utilizing visual perception is necessary for any real-world scenarios. Robotic aerial grasping using drones promises fast pick-and-place solutions with a large increase in mobility over other robotic solutions. Utilizing Mask R-CNN scene segmentation (detectron2), we propose a vision-based system for autonomous rapid aerial grasping which does not rely on mark-ers for object localization and does not require the appearance of the object to be previously known. Combining segmented images with spatial information from a depth camera, we generate a dense point cloud of the detected objects and perform geometry - based grasp planning to determine grasping points on the objects. In real-world experiments on a dynamically grasping aerial platform, we show that our system can replicate the performance of a motion capture system for object local-ization up to 94.5 % of the baseline grasping success rate. With our results, we show the first use of geometry-based grasping techniques with a flying platform and aim to increase the autonomy of existing aerial manipulation platforms, bringing them further towards real-world applications in warehouses and similar environments.††Code: https://github.com/srl-ethz/detectron-realsense</td>
                <td>Location awareness, Point cloud compression, Image segmentation, Visualization, Shape, Grasping, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342033&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342033&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Vision-Based Oxy-Fuel Torch Control for Robotic Metal Cutting</td>
                <td>J. Akl, Y. Patil, C. Todankar and B. Calli</td>
                <td>2023</td>
                <td>The automation of key processes in metal cutting would substantially benefit many industries such as manufacturing and metal recycling. We present a vision-based control scheme for automated metal cutting with oxy-fuel torches, an established cutting medium in industry. The system consists of a robot equipped with a cutting torch and an eye-in-hand camera observing the scene behind a tinted visor. We develop a vision-based control algorithm to servo the torch's motion by visually observing its effects on the metal surface. As such, the vision system processes the metal surface's heat pool and computes its associated features, specifically pool convexity and intensity, which are then used for control. The operating conditions of the control problem are defined within which the stability is proven. In addition, metal cutting experiments are performed using a physical 1-DOF robot and oxy-fuel cutting equipment. Our results demonstrate the successful cutting of metal plates across three different plate thicknesses, relying purely on visual information without a priori knowledge of the thicknesses.</td>
                <td>Industries, Heating systems, Visualization, Service robots, Metals, Combustion, Steel</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341532&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341532&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators</td>
                <td>R. Parosi, M. Risiglione, D. G. Caldwell, C. Semini and V. Barasuol</td>
                <td>2023</td>
                <td>We propose a control pipeline for SAG (Searching, Approaching, and Grasping) of objects, based on a decoupled arm kinematic chain and impedance control, which integrates image-based visual servoing (IBVS). The kinematic decoupling allows for fast end-effector motions and recovery that leads to robust visual servoing. The whole approach and pipeline can be generalized for any mobile platform (wheeled or tracked vehicles), but is most suitable for dynamically moving quadruped manipulators thanks to their reactivity against disturbances. The compliance of the impedance controller makes the robot safer for interactions with humans and the environment. We demonstrate the performance and robustness of the proposed approach with various experiments on our 140 kg HyQReal quadruped robot equipped with a 7-DoF manipulator arm. The experiments consider dynamic locomotion, tracking under external disturbances, and fast motions of the target object.</td>
                <td>Wrist, Pipelines, Kinematics, Grasping, Search problems, Visual servoing, Impedance</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341714&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341714&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lyapunov Constrained Safe Reinforcement Learning for Multicopter Visual Servoing</td>
                <td>D. Yu, M. Xu, Z. Liu and H. Wang</td>
                <td>2023</td>
                <td>Traditional methods based on Lyapunov analysis and learning-based approaches such as reinforcement learning (RL) are two powerful tools in visual servo tasks. Traditional methods are interpretable and their stability can be guar-anteed by Lyapunov analysis. However, they tend to have a high dependency on an accurate system dynamic model. RL approaches learn to act based on past experiences and thus have higher adaptability on disturbances and errors. However, the training process is long and its safety or stability is generally hard to guarantee, making real-world training risky. In this paper, we propose a residual RL framework for training a multicopter to finish visual servo tasks under disturbances, guided by the system safety in terms of system Lyapunov function. Such an approach compensates for the lack of disturbance-rejection ability of the traditional method, and optimizes stability explicitly so the RL agent makes safer actions both during the training and in the final policy. A comparison between our approach and the baselines is provided in simulation, and real-world experiments on a multicopter are also carried out to show our effectiveness. We believe that this work moves one step toward achieving RL applications on real-world robotic systems.</td>
                <td>Training, Visualization, System dynamics, Reinforcement learning, Stability analysis, Visual servoing, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341561&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341561&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Employing Multi-Layer, Sensorised Kirigami Grippers for Single-Grasp Based Identification of Objects and Force Exertion Estimation</td>
                <td>J. Liang et al.</td>
                <td>2023</td>
                <td>Soft robotic devices have been popular in handling intricate grasping and dexterous manipulation tasks, serving as an alternative to conventional, rigid end-effectors. These devices are relatively simple, lightweight, and cost-effective. Recently, kirigami based structures have been used to create low-cost and disposable soft robotic grippers and hands. These grippers undergo a complex post-contact reconfiguration and conform to an object's shape and size during grasping. In this paper, we explore this new class of soft robotic grippers by utilising them for single-grasp object classification and grasping force estimation. We install simplistic sensors on both the gripper and the actuation system to estimate the state of the kirigami gripper, and the collected data features are employed to train Random Forest models for identifying the grasped object. The classifier trained exhibits a high accuracy of 98 % in discriminating objects of various shapes. When handling food items, the classifier achieves an accuracy of 94 %, while in classifying transparent objects, the classifier obtained again a high accuracy of 97 %. Finally, object-specific force estimation models are triggered based on the classification decision of the Random Forest model to estimate the grasping force exerted by the gripper. These positive outcomes demonstrate the kirigami based robotic gripper's potential for object classification in a variety of circumstances, particularly where vision systems are not available or not reliable.</td>
                <td>Shape, Force, Estimation, Grasping, Soft robotics, Robot sensing systems, Reliability</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341390&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341390&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Finite-Time State-Dependent Differential Riccati Equation Control Design for Closed-Loop SMA-Actuated Hip Joint</td>
                <td>Sanchez, S. R. Nekoo, B. Arrue and A. Ollero</td>
                <td>2023</td>
                <td>This paper presents the modeling and closed loop control of the shape-memory-alloy (SMA)-actuated hip joint of a flapping-wing flying robot (FWFR). Despite the lightweight legs/claw mechanism, a strong force of grasping is needed. The SMAs show high force delivery; however, it is difficult to control (position and temperature) the actuation due to the necessity of high currents for warming up, and time for cooling down process. This paper presents a state-dependent differential Riccati equation (SDDRE) controller taking into account the SMA dynamic and the actuator limits to control the leg/claw system. The use of nonlinear optimal control, specifically, the SDDRE, has been reported for the first time for bio-inspired leg/claw control of FWFR. The dynamics of the SMA actuators and on-off switching of the MOSFETs to provide current for the system demands switching in the design of the controller as a constraint for inputs which was considered in the design. Simulation and experimental results and analysis of different phases of heating of SMAs were discussed and resulted in satisfactory control performance.</td>
                <td>Actuators, Shape memory alloys, Force, Riccati equations, Switches, Nonlinear dynamical systems, Task analysis, SDRE, SMA, Closed-loop control, Bio-inspired claw, Fkapping-wing robots, UAV, Aerial Robot</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341562&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341562&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Rapid Grasping of Fabric Using Bionic Soft Grippers with Elastic Instability</td>
                <td>Z. Xiong, Z. Guo, L. Yuan, Y. Su, Y. Liu and H. Lipson</td>
                <td>2023</td>
                <td>Robot grasping is subject to an inherent tradeoff: Grippers with a large span typically take a longer time to close, and fast grippers usually cover a small span. However, many practical applications of grippers require the ability to close a large distance rapidly. For example, grasping cloth typically requires pressing a wide span of fabric into a graspable cusp. Besides, the ability to perform human-like grasping and ease offabrication are also very important for new soft grippers. Here, we demonstrate a human-finger-inspired snapping gripper that exploits elastic instability to achieve rapid and reversible closing over a wide span. Using prestressed semi-rigid material as the skeleton, the gripper fingers can widely open (86 mm) and rapidly close (46 ms) following a trajectory similar to that of a thumb-index finger pinching, and is 2.7 times and 10.9 times better than the reference gripper in terms of span and speed, respectively. We theoretically give the design principle, simulatively verify the method, and experimentally test this gripper on a variety of rigid, flexible, and limp objects and achieve good mechanical performance.</td>
                <td>Thumb, Grasping, Pressing, Soft robotics, Fabrics, Software, Skeleton</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341428&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341428&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design of a Cable Driven Wearable Fitness Device for Upper Limb Exercise</td>
                <td>J. Park et al.</td>
                <td>2023</td>
                <td>To provide an alternative to conventional large-scale fitness equipment, we previously developed a soft passive wearable device for upper limb resistance exercises that utilized elastic exercise bands. However, the user was required to manually adjust the level of strength. In this paper, we introduce a novel wearable fitness device for upper limb exercise that constitutes cable-driven actuation to control the resistance profiles. Our proposed device allows for the generation of isotonic force trajectories, similar to those produced during dumbbell lifting, utilizing custom cable-driven actuators. The cable path of the device was determined based on the results of user testing aimed at optimizing muscle stimulation levels. In addition, linear resonant actuators were installed at customized haptic handles and upper arm modules to enhance proprioceptive sensitivity and exercise efficacy. The immersive “exer-tainment” user display interface was also redesigned to increase user motivation. The efficacy of our device was assessed by comparing the surface electromyography (sEMG) activities of upper limb muscles during chest press and triceps extension exercises performed with our device versus those using traditional weight training machines and rubber bands. It was found that our device could effectively strengthen during arm exercise, such as triceps extension.</td>
                <td>Resistance, Performance evaluation, Training, Actuators, Presses, Force, Entertainment industry</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342373&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342373&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Soft Cap for Vine Robots</td>
                <td>C. Suulker et al.</td>
                <td>2023</td>
                <td>Growing robots based on the eversion principle are known for their ability to extend rapidly, from within, along their longitudinal axis, and, in doing so, reach deep into hitherto inaccessible, remote spaces. Despite many advantages, vine robots also present significant challenges, one of which is maintaining sensory payload at the tip without restricting the eversion process. A variety of tip mechanisms have been proposed by the robotics community, among them rounded caps of relatively complex construction that are not always compatible with functional hardware, such as sensors or navigation pouches, integrated with the main eversion structure. Moreover, many tip designs incorporate rigid materials, reducing the robot's flexibility and consequent ability to navigate through narrow openings. Here, we address these shortcomings and propose a design to overcome them: a soft, entirely fabric based, cylindrical cap that can easily be slipped onto the tip of vine robots. Having created a series of caps of different sizes and materials, an experimental study was conducted to evaluate our new design in terms of four key aspects: vine robot made from multiple layers of everting material, solid objects protruding from the vine robot, squeezability, and navigability. In all scenarios, we can show that our soft, flexible cap is robust in its ability to maintain its position and is capable of transporting payloads such as a camera across long distances. We also demonstrate that the robot's ability to move through restricted aperture openings and indeed its overall flexibility is virtually unhindered by the addition of our cap. The paper discusses the advantages of this design and gives further recommendations in relation to aspects of its engineering.</td>
                <td>Navigation, Robot vision systems, Solids, Cameras, Hardware, Fabrics, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341377&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341377&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning Robotic Assembly by Leveraging Physical Softness and Tactile Sensing</td>
                <td>Hernandez and K. Tanaka</td>
                <td>2023</td>
                <td>This study aims to achieve autonomous robotic assembly under uncertain conditions arising from imprecise goal positioning and variations in the angle of the grasped part. Soft robots are suitable for such uncertain and contact-rich environments and are capable of insertion tasks with imprecise goal positions. However, we may also struggle to handle further uncertainty, such as variations in grasping pose. To address the challenge posed by multiple sources of uncertainty, we equipped the soft robot with a tactile sensor. Our key insight is that tactile signal patterns are closely linked to the subtask transitions in an assembly process, specifically from the search to insertion subtasks. We hypothesize soft robots could complete the task by exploring the transition via tactile signals, even in scenarios with imprecise goal positions and grasp misalignment. To this end, we develop an anomaly detection model using a Variational Autoencoder to identify the timing of these transitions. We then employ learning and heuristic-based controllers to navigate the peg tip to the hole and perform the insertion. Our method was validated through real-robot experiments using a soft wrist and a vision-based tactile sensor. The results demonstrate that our method achieves a 100% success rate in scenarios with less uncertain goal pose ($\sigma=2\text{mm}$) and grasp misalignment (up to 5°) and a 70% success rate in scenarios with uncertain goal pose ($\sigma=10\text{mm}$) and grasp misalignment (up to 20°). Moreover, our anomaly detection model can generalize to different peg diameters without additional training.</td>
                <td>Wrist, Robotic assembly, Training, Uncertainty, Tactile sensors, Soft robotics, Sensors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341471&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341471&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>High-Curvature Consecutive Tip Steering of a Soft Growing Robot for Improved Target Reachability</td>
                <td>H. Ryu</td>
                <td>2023</td>
                <td>Over the years, soft growing robots that allow the feeding of new materials at their tips have attracted considerable attention owing to their unique locomotion characteristics. However, accessing targets over highly curved passages by steering compliant continuum bodies remains challenging. To this end, this study proposes a new tip steering mechanism that imparts soft growing robots with consecutive high-curvature steering ability. We place a hyper-redundant rolling contact joint and twisted string actuator (TSA) at the tip of a soft growing robot to provide high-curvature continuous steering with a smaller scale factor, allowing consecutive turns. The proposed small form factor continuous curvature tip steering mechanism allows the steering mechanism to stay at the tip during steering, improving accessibility by enabling consecutive turns. The improved accessibility of the proposed mechanism is demonstrated based on better confined space reachability compared with that of conventional whole-body steering soft growing robots, along with a consecutive high-curvature pipe navigation demonstration.</td>
                <td>Actuators, Navigation, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341407&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341407&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Single Channel Soft Robotic Actuator Leveraging Switchable Strain-Limiting Structures for Deep-Sea Suction Sampling</td>
                <td>J. Peters, C. M. Sourkounis, M. Wiese, T. Kwasnitschka and A. Raatz</td>
                <td>2023</td>
                <td>Soft Robotics has established itself as an integral field in the broader discipline of general robotics through multiple advantages like inherent safety, adaptable morphology, and energy- and weight efficiency. Especially in environments hostile to humans and classical robots like the deep sea, soft robotic structures made out of silicone and actuated by seawater have numerous advantages. An application with a huge scientific and commercial potential for soft robotic solutions is suction sampling for marine geology in depths of up to 6000 m. In this paper, we propose a single channel soft robotic actuator that is able to bend into six directions while absorbing process forces. By embedding a low melting point alloy (LMPA) acting as switchable strain-limiting structures, the actuator is capable of hexa-planar bending of up to 40° and elongation of 30 % with only one valve used for actuation. In addition, the LMPA chambers enable a stiffening factor of 4.1 and locking the actuator in its bending state for energy efficient usage in robotic deep-sea suction sampling.</td>
                <td>Actuators, Wires, Water heating, Switches, Soft robotics, Bending, Valves</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341262&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341262&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Locomotion and Obstacle Avoidance of a Worm-Like Soft Robot</td>
                <td>Aydin</td>
                <td>2023</td>
                <td>This paper presents a soft earthworm robot that is capable of both efficient locomotion and obstacle avoidance. The robot is designed to replicate the unique locomotion mechanisms of earthworms, which enable them to move through narrow and complex environments with ease. The robot consists of multiple segments, each with its own set of actuators, that are connected through rigid plastic joints, allowing for increased adaptability and flexibility in navigating different environments. The robot utilizes proprioceptive sensing and control algorithms to detect and avoid obstacles in real-time while maintaining efficient locomotion. The robot uses a pneumatic actuation system to mimic the circumnutation behavior exhibited by plant roots in order to navigate through complex environments. The results demonstrate the capabilities of the robot for navigating through cluttered environments, making this development significant for various fields of robotics, including search and rescue, environmental monitoring, and medical procedures.</td>
                <td>Navigation, Propioception, Pneumatic systems, Soft robotics, Real-time systems, Sensors, Plastics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341586&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341586&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>EasyGaze3D: Towards Effective and Flexible 3D Gaze Estimation from a Single RGB Camera</td>
                <td>Z. Yang and Y. Guo</td>
                <td>2023</td>
                <td>Eye gaze can convey rich information of human intentions, which enables the social robots to comprehend the cognition and behavior of human targets. However, the existing 3D gaze estimation methods generally have high requirements either on the dedicated hardware or the quantity and quality of training databases, which largely limits their practical application values. This paper proposes EasyGaze3D, an effective 3D gaze estimation framework using a single RGB camera. First, the framework detects the 2D facial landmarks and recovers the 3D facial shape from the input image, and derives the required camera parameters with these features. Then, without loss of generality, the gaze direction can be regarded as the vector pointing from the eyeball center to the pupil center, which are derived respectively from the detected facial landmarks and the spherical fitting performed on the recovered 3D facial shape. Besides, we propose a flexible yet efficient calibration module, namely Easy-Cali, for deriving the subject-specific 3D facial shape and eyeball centers. The features calibrated by Easy-Cali can further boost the performance of EasyGaze3D. Experimental results show that our proposed method, being plug-and-play and without the need of training on large-scale dataset, can achieve superior performance against the existing methods based on deep models.</td>
                <td>Training, Three-dimensional displays, Shape, Fitting, Social robots, Estimation, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342361&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342361&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Visual Perception of Human-Robot Walking Environments Using Semi-Supervised Learning</td>
                <td>D. Kuzmenko, O. Tsepa, A. G. Kurbis, A. Mihailidis and B. Laschowski</td>
                <td>2023</td>
                <td>Convolutional neural networks trained using supervised learning can improve visual perception for human-robot walking. These advances have been possible due to largescale datasets like ExoNet and StairNet - the largest open-source image datasets of real-world walking environments. However, these datasets require vast amounts of manually annotated data, the development of which is time consuming and labor intensive. Here we present a novel semi-supervised learning system (ExoNet-SSL) that uses over 1.2 million unlabelled images from ExoNet to improve training efficiency. We developed a deep learning model based on mobile vision transformers and trained the model using semi-supervised learning for image classification. Compared to standard supervised learning (98.4%), our ExoNet-SSL system was able to maintain high prediction accuracy (98.8%) when tested on previously unseen environments, while requiring 35% fewer labelled images during training. These results show that semi-supervised learning can improve training efficiency by leveraging large amounts of unlabelled data and minimize the size requirements for manually annotated images. Future research $\text{will}$ focus on model deployment for onboard real-time inference and control of human-robot walking.</td>
                <td>Legged locomotion, Training, Supervised learning, Semisupervised learning, Transformers, Real-time systems, Standards</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341654&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341654&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Contactless Weight Estimation of Human Body and Body Parts for Safe Robotics-Assisted Casualty Extraction</td>
                <td>J. Lee, E. Quist, J. Chambers, M. Yip and N. Fisher</td>
                <td>2023</td>
                <td>Deploying humans in a high-risk environment to extract casualties in order to provide medical attention is an inherently dangerous endeavor. To minimize this risk, Robotics and Autonomous Systems can be deployed in hazardous areas in place of human personnel to limit the exposure of first responders to various life-threatening conditions. The success of robotic extraction of injured persons depends heavily on how safely the human subject is handled. Therefore, the integration of intelligent technologies for secure control and motion planning is crucial in overcoming the dynamic and complex challenges of robotic grasping and manipulation. In this regard, the measurement of the target human subject's weight is an essential factor for safe grasping and maneuvering during robotic interactions with humans. This paper presents a contactless vision-based approach for estimating the weight of the human body. This approach employs visual body perception, 3D body point cloud representation, and a deep learning network for body segmentation to measure specific body parameters. Next, the body parameters are fed into a neural network model to predict the total body weight. This prediction then enables an approximation of the weight of individual body segments to be obtained.</td>
                <td>Weight measurement, Point cloud compression, Visualization, Biological system modeling, Estimation, Grasping, Data collection, Robotic casualty extraction, Human robot interaction, Point cloud segmentation, Vision-based body perception, parameter measurement, weight estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342473&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342473&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Feature-based Visual Odometry for Bronchoscopy: A Dataset and Benchmark</td>
                <td>J. Deng, P. Li, K. Dhaliwal, C. X. Lu and M. Khadem</td>
                <td>2023</td>
                <td>Bronchoscopy is a medical procedure that involves the insertion of a flexible tube with a camera into the airways to survey, diagnose and treat lung diseases. Due to the complex branching anatomical structure of the bronchial tree and the similarity of the inner surfaces of the segmental airways, navigation systems are now being routinely used to guide the operator during procedures to access the lung periphery. Current navigation systems rely on sensor-integrated bronchoscopes to track the position of the bronchoscope in real-time. This approach has limitations, including increased cost and limited use in non-specialized settings. To address this issue, researchers have proposed visual odometry algorithms to track the bronchoscope camera without the need for external sensors. However, due to the lack of publicly available datasets, limited progress is made. To this end, we have developed a database of bronchoscopy videos in a phantom lung model and ex-vivo human lungs. The dataset contains 34 video sequences with over 23,000 frames with odometry ground truth data collected using electromagnetic tracking sensors. With our dataset, we empower the robotics and machine learning community to advance the field. We share our insights on challenges in endoscopic visual odometry. Furthermore, we provide benchmark results for this dataset. State-of-the-art feature extraction algorithms including SIFT, ORB, Superpoint, Shi- Tomasi, and LoFTR are tested on this dataset. The benchmark results demonstrate that the LoFTR algorithm outperforms other approaches, but still has significant errors in the presence of rapid movements and occlusions.</td>
                <td>Bronchoscopy, Machine learning algorithms, Navigation, Lung, Phantoms, Benchmark testing, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342034&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342034&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Motion Magnification in Robotic Sonography: Enabling Pulsation-Aware Artery Segmentation</td>
                <td>D. Huang, Y. Bi, N. Navab and Z. Jiang</td>
                <td>2023</td>
                <td>Ultrasound (US) imaging is widely used for diagnosing and monitoring arterial diseases, mainly due to the advantages of being non-invasive, radiation-free, and real-time. In order to provide additional information to assist clinicians in diagnosis, the tubular structures are often segmented from US images. To improve the artery segmentation accuracy and stability during scans, this work presents a novel pulsation-assisted segmentation neural network (PAS-NN) by explicitly taking advantage of the cardiac-induced motions. Motion magnification techniques are employed to amplify the subtle motion within the frequency band of interest to extract the pulsation signals from sequential US images. The extracted real-time pulsation information can help to locate the arteries on cross-section US images; therefore, we explicitly integrated the pulsation into the proposed PAS-NN as attention guidance. Notably, a robotic arm is necessary to provide stable movement during US imaging since magnifying the target motions from the US images captured along a scan path is not manually feasible due to the hand tremor. To validate the proposed robotic US system for imaging arteries, experiments are carried out on volunteers' carotid and radial arteries. The results demonstrated that the PAS-NN could achieve comparable results as state-of-the-art on carotid and can effectively improve the segmentation performance for small vessels (radial artery). The code11Code: https://qithub.com/dianveHuanq/RobPMEPASNN and demonstration video22Video: https://youtu.belc9AM042_lUQ can be publicly accessed.</td>
                <td>Image segmentation, Ultrasonic imaging, Motion segmentation, Neural networks, Imaging, Manipulators, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342220&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342220&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Compact and Accurate Adaptive Width Radial Basis Function Neural Network with Discriminative Features for Thyroid Gland Segmentation</td>
                <td>H. You and J. Chen</td>
                <td>2023</td>
                <td>The goal of this work is to develop a fast, compact, accurate, and robust neural network for automatic thyroid gland segmentation in medical ultrasound images when the computational resources are limited, such as portable intelligent medical diagnostic devices or robots. An adaptive width radial basis function neural network model with discriminative features is proposed in this work. The model combines the local characteristics and global context of the entire feature space, which improves the performance of the automatic thyroid gland segmentation, and enhances the data fitting capability and robustness of the neural network. A new feature selection method combining the measurements of area under the curve values and Pearson correlation coefficients is proposed to select six discriminative and low correlated features from a set of handcrafted features, which helps to construct a compact yet accurate radial basis function neural network. The proposed method is experimented on thyroid ultrasound images using five-fold cross validation. The proposed method achieves an average accuracy of 0.9770 and an average IoU of 0.7841, achieving competitive segmentation performance compared to convolutional neural networks, with thousands of times faster training speed on a CPU.</td>
                <td>Training, Performance evaluation, Image segmentation, Ultrasonic imaging, Computational modeling, Fitting, Radial basis function networks</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342090&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342090&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Unsupervised Deformable Ultrasound Image Registration and Its Application for Vessel Segmentation</td>
                <td>F. Abhimanyu, A. L. Orekhov, A. Bal, J. Galeotti and H. Choset</td>
                <td>2023</td>
                <td>This paper presents a deep-learning model for deformable registration of ultrasound images at online rates, which we call U-RAFT. As its name suggests, U-RAFT is based on RAFT, a convolutional neural network for estimating optical flow. U-RAFT, however, can be trained in an unsupervised manner and can generate synthetic images for training vessel segmentation models. We propose and compare the registration quality of different loss functions for training U-RAFT. We also show how our approach, together with a robot performing force-controlled scans, can be used to generate synthetic deformed images to significantly expand the size of a femoral vessel segmentation training dataset without the need for additional manual labeling. We validate our approach on both a silicone human tissue phantom as well as on in-vivo porcine images. We show that U-RAFT generates synthetic ultrasound images with 98% and 81% structural similarity index measure (SSIM) to the real ultrasound images for the phantom and porcine datasets, respectively. We also demonstrate that synthetic deformed images from U-RAFT can be used as a data augmentation technique for vessel segmentation models to improve intersection-over-union (IoU) segmentation performance.</td>
                <td>Training, Deformable models, Optical losses, Image segmentation, Ultrasonic imaging, Ultrasonic variables measurement, Imaging phantoms</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341870&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341870&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Thoracic Cartilage Ultrasound-CT Registration Using Dense Skeleton Graph</td>
                <td>Z. Jiang, C. Li, X. Lil and N. Navab</td>
                <td>2023</td>
                <td>Autonomous ultrasound (US) imaging has gained increased interest recently, and it has been seen as a potential solution to overcome the limitations of free-hand US exami-nations, such as inter-operator variations. However, it is still challenging to accurately map planned paths from a generic atlas to individual patients, particularly for thoracic applications with high acoustic-impedance bone structures below the skin. To address this challenge, a dense graph-based non-rigid registration is proposed to transfer planned paths from the atlas to the current setup by explicitly considering subcutaneous bone surface. To this end, the sternum and cartilage branches are segmented using a template matching to assist coarse alignment of US and CT point clouds. Afterward, a directed graph is generated based on the CT template. Then, the self-organizing map using geographical distance is successively performed twice to extract the optimal graph representations for CT and US point clouds, individually. To evaluate the proposed approach, five cartilage point clouds from distinct patients are employed. The results demonstrate that the proposed graph-based registration can effectively map trajectories from CT to the current setup to do US examination through limited intercostal space. The non-rigid registration results in terms of Hausdorff distance (Mean±SD) is $9.48 \pm 0.27$ mm and the path transferring error in terms of Euclidean distance is $2.21\pm 1.11\ mm$. The code11https://github.com/marslicy/Cartilage-graph-based-US-CT-Registration and video22Video: https://www.youtube.com/watch?v=QJz2fkwgbP8 can be publicly accessed.</td>
                <td>Point cloud compression, Self-organizing feature maps, Ultrasonic imaging, Computed tomography, Sternum, Euclidean distance, Bones</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341575&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341575&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Landmark Based Bronchoscope Localization for Needle Insertion Under Respiratory Deformation</td>
                <td>I. Fried, J. Hoelscher, J. A. Akulian, S. Pizer and R. Alterovitz</td>
                <td>2023</td>
                <td>Bronchoscopy is currently the least invasive method for definitively diagnosing lung cancer, which kills more people in the United States than any other form of cancer. Successfully diagnosing suspicious lung nodules requires accurate localization of the bronchoscope relative to a planned biopsy site in the airways. This task is challenging because the lung deforms intraoperatively due to respiratory motion, the airways lack photometric features, and the anatomy's appearance is repetitive. In this paper, we introduce a real-time camera-based method for accurately localizing a bronchoscope with respect to a planned needle insertion pose. Our approach uses deep learning and accounts for deformations and overcomes limitations of global pose estimation by estimating pose relative to anatomical landmarks. Specifically, our learned model considers airway bifurcations along the airway wall as landmarks because they are distinct geometric features that do not vary significantly with respiratory motion. We evaluate our method in a simulated dataset of lungs undergoing respiratory motion. The results show that our method generalizes across patients and localizes the bronchoscope with accuracy sufficient to access the smallest clinically-relevant nodules across all levels of respiratory deformation, even in challenging distal airways. Our method could enable physicians to perform more accurate biopsies and serve as a key building block toward accurate autonomous robotic bronchoscopy.</td>
                <td>Location awareness, Bronchoscopy, Deformation, Robot kinematics, Pose estimation, Lung, Medical services</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342115&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342115&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference</td>
                <td>Z. Li, I. Reyes and H. Alemzadeh</td>
                <td>2023</td>
                <td>Surgical context inference has recently garnered significant attention in robot-assisted surgery as it can facilitate workflow analysis, skill assessment, and error detection. However, runtime context inference is challenging since it requires timely and accurate detection of the interactions among the tools and objects in the surgical scene based on the segmentation of video data. On the other hand, existing state-of-the-art video segmentation methods are often biased against infrequent classes and fail to provide temporal consistency for segmented masks. This can negatively impact the context inference and accurate detection of critical states. In this study, we propose a solution to these challenges using a Space-Time Correspondence Network (STCN). STCN is a memory network that performs binary segmentation and minimizes the effects of class imbalance. The use of a memory bank in STCN allows for the utilization of past image and segmentation information, thereby ensuring consistency of the masks. Our experiments using the publicly-available JIGSAWS dataset demonstrate that STCN achieves superior segmentation performance for objects that are difficult to segment, such as needle and thread, and improves context inference compared to the state-of-the-art. We also demonstrate that segmentation and context inference can be performed at runtime without compromising performance.</td>
                <td>Context, Training, Image segmentation, Runtime, Instruction sets, Surgery, Needles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342013&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342013&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Modeling, Characterization, and Control of Bacteria-Inspired Bi-Flagellated Mechanism with Tumbling</td>
                <td>Z. Hao, S. Lim and M. K. Jawed</td>
                <td>2023</td>
                <td>Multi-flagellated bacteria utilize the hydrodynamic interaction between their filamentary tails, known as flagella, to swim and change their swimming direction in low Reynolds number flow. Simplified hydrodynamics model, like Resistive Force Theories (RFT), lacks the capability to capture the dynamics of certain interactions known as bundling and tumbling. However, for the development of efficient and steerable robots inspired by bacteria, it becomes crucial to exploit this interaction. In this paper, we present the construction of a macroscopic bio-inspired robot featuring two rigid flagella arranged as right-handed helices, along with a cylindrical head. By rotating the flagella in opposite directions, the robot's body can reorient itself through repeatable and controllable tumbling. To accurately model this bi-flagellated mechanism in low Reynolds flow, we employ a coupling of rigid body dynamics and the method of Regularized Stokeslet Segments (RSS). Unlike RFT, RSS takes into account the hydrodynamic interaction between distant filamentary structures. Furthermore, we delve into the exploration of the parameter space in terms of the flagellum geometry to optimize the propulsion and torque of the system. To achieve the desired reorientation of the robot, we propose a tumble control scheme that involves modulating the rotation direction and speed of the two flagella. The scheme enhance the steerability by enabling the robot to attain the desired heading angle with high accuracy. Notably, the overall scheme boasts a simplified design and control as it only requires two control inputs. With our macroscopic framework serving as a foundation, we envision the eventual miniaturization of this technology to construct mobile and controllable micro-scale bacterial robots.</td>
                <td>Geometry, Microorganisms, Torque, Biological system modeling, Tail, Propulsion, Hydrodynamics, bio-inspired robot, tumbling</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341992&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341992&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Evolving Physical Instinct for Morphology and Control Co-Adaption</td>
                <td>X. Chen et al.</td>
                <td>2023</td>
                <td>The capability of a robot to perform tasks depends not only on precise motion control, but also on a well-suited body morphology. Adapting both morphology and control of robots to improve their task performance has been a widely studied and long-standing issue. While the bio-inspired bi-level optimization framework has gained popularity in recent years, it suffers from high computation complexity due to the time-consuming and inefficient learning process for each morphology. In fact, in nature, besides the adaptive morphology and the intelligent brain, animals also possess an important gift, which is physical instinct. These instincts allow animals to respond quickly to their surroundings in the neonatal period, facilitating skills acquisition. Inspired by this, we propose an evolvable instinct controller to enhance the morphology-control co-adaption. The instinct controller suggests rough motion inclinations, which require minimal domain knowledge and entail less sophisticated design. Its purpose is to assist the main controller in learning fine-grained and robust control efficiently. We implemented this idea in the context of legged locomotion and designed the instinct controller using phase-based FSMs. We propose the instinct-based co-adaption algorithm and construct GPU parallel simulation experiments on different morphology prototypes. The results indicate that combining the co-adaption process with instinct evolution leads to the development of superior morphologies and robust controllers compared with the conventional co-adaption approach, with minimal additional time cost.</td>
                <td>Robust control, Legged locomotion, Pediatrics, Animals, Morphology, Prototypes, Process control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342243&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342243&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Development of a Five-Fingerd Biomimetic Soft Robotic Hand by 3D Printing the Skin and Skeleton as One Unit</td>
                <td>K. Miyama, K. Kawaharazuka, K. Okada and M. Inaba</td>
                <td>2023</td>
                <td>Robot hands that imitate the shape of the human body have been actively studied, and various materials and mechanisms have been proposed to imitate the human body. Although the use of soft materials is advantageous in that it can imitate the characteristics of the human body's epidermis, it increases the number of parts and makes assembly difficult in order to perform complex movements. In this study, we propose a skin-skeleton integrated robot hand that has 15 degrees of freedom and consists of four parts. The developed robotic hand is mostly composed of a single flexible part produced by a 3D printer, and while it can be easily assembled, it can perform adduction, flexion, and opposition of the thumb, as well as flexion of four fingers.</td>
                <td>Three-dimensional displays, Shape, Thumb, Grasping, Muscles, Soft robotics, Three-dimensional printing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341570&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341570&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Design Method of a Kangaroo Robot with High Power Legs and an Articulated Soft Tail</td>
                <td>S. Yoshimura et al.</td>
                <td>2023</td>
                <td>In this paper, we focus on the kangaroo, which has powerful legs capable of jumping and a soft and strong tail. To incorporate these unique structure into a robot for utilization, we propose a design method that takes into account both the feasibility as a robot and the kangaroo-mimetic structure. Based on the kangaroo's musculoskeletal structure, we determine the structure of the robot that enables it to jump by analyzing the muscle arrangement and prior verification in simulation. Also, to realize a tail capable of body support, we use an articulated, elastic structure as a tail. In order to achieve both softness and high power output, the robot is driven by a direct-drive, high-power wire-winding mechanism, and weight of legs and the tail is reduced by placing motors in the torso. The developed kangaroo robot can jump with its hind legs, moving its tail, and supporting its body using its hind legs and tail.</td>
                <td>Legged locomotion, Torso, Design methodology, Wires, Tail, Muscles, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341756&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341756&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hovering Control of Flapping Wings in Tandem with Multi-Rotors</td>
                <td>A. Dhole et al.</td>
                <td>2023</td>
                <td>This work briefly covers our efforts to stabilize the flight dynamics of Northeatern's tailless bat-inspired micro aerial vehicle, Aerobat. Flapping robots are not new. A plethora of examples is mainly dominated by insect-style design paradigms that are passively stable. However, Aerobat, in addition for being tailless, possesses morphing wings that add to the inherent complexity of flight control. The robot can dynamically adjust its wing platform configurations during gaitcycles, increasing its efficiency and agility. We employ a guard design with manifold small thrusters to stabilize Aerobat's position and orientation in hovering, a flapping system in tandem with a multi-rotor. For flight control purposes, we take an approach based on assuming the guard cannot observe Aeroat's states. Then, we propose an observer to estimate the unknown states of the guard which are then used for closed-loop hovering control of the Guard-Aerobat platform.</td>
                <td>Manifolds, Attitude control, Observers, Aerodynamics, Complexity theory, Vehicle dynamics, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341523&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341523&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Development of an Autonomous Modular Swimming Robot with Disturbance Rejection and Path Tracking</td>
                <td>H. Deng, C. Nitroy, K. Panta, D. Li, S. Priya and B. Cheng</td>
                <td>2023</td>
                <td>Here we present the development of an autonomous modular swimming robot. This robot, named µBot 2.0, was upgraded from our previous robot platform µBot and features onboard computing, sensing, and power. Its compact size and modularity render the robot an ideal platform for studying bio-inspired robot swimming. The robot is equipped with a micro controller in its head that communicates with external computers through Bluetooth Low Energy (BLE) and sends motor commands to the body segments via Inter-Integrated Circuit (I2C) protocol. Each body segment has a customized printed circuit board (PCB) that receives commands and controls the electromagnetic actuator for generating body movements. The robot head is also equipped with an Inertial Measurement Unit (IMU) to measure its heading and a battery for power. In this work, a µBot 2.0 with three actuators was assembled and the swimming performance was tested. The robot actuators were activated via rhythmic motor input from a central pattern generator (CPG). Experimental results showed that the swimming speed was highly sensitive to the frequency of the motor input, with a maximum swimming speed of 130 mm/s (equivalent to 0.7 body length per second) at 6 Hz. The robot also had the capability to correct its heading with IMU feedback and follow desired paths using a line-of-sight (LOS) guidance law with an overhead camera. Our results demonstrate the effectiveness of the robot's design and its potential in a variety of aquatic applications.</td>
                <td>Actuators, Measurement units, Aquatic robots, Protocols, Robot vision systems, Printed circuits, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341571&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341571&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Highly Maneuverable Flying Squirrel Drone with Controllable Foldable Wings</td>
                <td>G. Kang, D. Lee and S. Han</td>
                <td>2023</td>
                <td>Typical drones with multi rotors are generally less maneuverable due to unidirectional thrust, which may be unfavorable to agile flight in very narrow and confined spaces. This paper suggests a new bio-inspired drone that is empowered with high maneuverability in a lightweight and easy-to-carry way. The proposed flying squirrel inspired drone has controllable foldable wings to cover a wider range of flight attitudes and provide more maneuverable flight capability with stable tracking performance. The wings of a drone are fabricated with silicone membranes and sophisticatedly controlled by reinforcement learning based on human-demonstrated data. Specially, such learning based wing control serves to capture even the complex aerodynamics that are often impossible to model mathematically. It is shown through experiment that the proposed flying squirrel drone intentionally induces aerodynamic drag and hence provides the desired additional repulsive force even under saturated mechanical thrust. This work is very meaningful in demonstrating the potential of biomimicry and machine learning for realizing an animal-like agile drone.</td>
                <td>Drag, Biomimetics, Force, Rotors, Reinforcement learning, Aerodynamics, Mathematical models, Flying squirrel, quadrotor, drone, biomimetics, reinforcement learning, learning from demonstration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341386&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341386&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Programable On-Chip Fabrication of Magnetic Soft Micro-Robot</td>
                <td>Y. Li et al.</td>
                <td>2023</td>
                <td>In the last decade, researchers have been trying to develop many microrobots that mimic the extraordinary abilities of bionts in complex environments. How to fabricate the biomimetic microrobot with satisfying deformability and complex shapes to realize desired precise motion is the key issue. In this paper, we proposed an efficient programable fabrication method of the magnetic soft micro-robot through an on-chip photopolymerization system. The superparamagnetic nanoparticles were compiled according to the magnetic anisotropy and assembled in the micro-robot. Then these nanoparticles were immobilized by photopolymerization of the hydrogel polymer. With this fabrication method, a joint rotation mechanism was first fabricated to characterize the deformation performance under the magnetic field control. Besides, the snake-like micro-robot were also fabricated, and the desired motions were achieved. The experimental results show that the proposed programable on-chip fabrication of magnetic soft micro-robot has the potential to facilitate the development of magnetic microrobots and their applications in the biomedical field.</td>
                <td>Fabrication, Nanoparticles, Three-dimensional displays, Deformation, Hydrogels, System-on-chip, Soft magnetic materials</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342184&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342184&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Forward/Inverse Kinematics Modeling for Tensegrity Manipulator Based on Goal-Conditioned Variational Autoencoder</td>
                <td>Y. Yoshimitsu, T. Osa and S. Ikemoto</td>
                <td>2023</td>
                <td>This paper uses a data-driven approach to model a highly redundantly driven tensegrity manipulator's forward and inverse kinematics. The tensegrity manipulator is based on a class-1 tensegrity with 20 struts and bends by 40 pneumatic actuators whose internal pressures are independently controlled. Based on the data obtained through random trials with the robot, a VAE-based kinematics model is trained. The forward model, inverse model, and null space of kinematics are simultaneously acquired as subnetworks of the VAE-based kinematics model. Experiments confirmed that the subnetworks representing forward and inverse kinematics could be used for the end position estimation and control, respectively. In addition, the subnetwork representing null space can generate different target pressures that achieve the same end position, which was confirmed to mean variable stiffness properties similar to musculoskeletal robots.</td>
                <td>Training, Pneumatic actuators, Musculoskeletal system, Loading, Null space, Estimation, Kinematics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341525&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341525&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Nematode-Inspired Cable Routing Method for Cable Driven Redundant Manipulator*</td>
                <td>H. Kim, H. Lee and J. Yoon</td>
                <td>2023</td>
                <td>Cable driven redundant manipulator (CDRM) can provide complex movements with high dexterity and singularity reduction. However, traditional CDRMs with universal joints have the disadvantages of requiring a high number of motors and having a narrow joint workspace. Furthermore, there is a limitation in terms of stiffness and payload. Recently, CDRMs composed of Quaternion joints have been developed to address these disadvantages. They require fewer motors and have larger joint workspace due to the Quaternion joints. Yet, their cable routing method is the same as the traditional CDRMs. In this paper, we propose a novel nematode-inspired cable routing method to achieve complex movements and stiffness increase. To achieve the stiffness increase of CDRM, the proposed cable routing method was inspired by the alternately arranged muscle structure of nematodes. Moreover, moving pulley structure was selected to amplify the stiffness and force of CDRM. An 8-DOF CDRM prototype composed of four Quaternion joints was developed to show the effectiveness of the cable routing method. Kinematics simulation was conducted and then, verified by trajectory through experiments. Finally, a joint stiffness simulation was conducted and verified with the developed prototype by stiffness experiments.</td>
                <td>Quaternions, Simulation, Pulleys, Prototypes, Muscles, Routing, Mathematical models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341593&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341593&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Spiking Reinforcement Learning with Memory Ability for Mapless Navigation</td>
                <td>B. Yang, M. Yuan, C. Zhang, C. Hong, G. Pan and H. Tang</td>
                <td>2023</td>
                <td>Our study focuses on mapless navigation in robotics, which involves navigating without an established obstacle map of the environment. Spiking Neural Networks (SNNs) have recently been applied to this task using Deep Reinforcement Learning (DRL), but face challenges in dynamic and partially observable environments, as well as inaccuracies in transmitted data. To overcome these issues, we propose a Multi-Critic DDPG with Spiking Memory (MC-DDPGSM) framework. Our approach introduces a spiking Gate Recurrent Unit layer (Spiking-GRU) to provide memory function and evaluates the state-action value with multi-critic networks. The experimental results demonstrate that our method achieves better performance (success rate, navigation distance, navigation time spent, and power consumption) in complex navigation tasks compared to the state-of-the-art approaches. Furthermore, our model can be transferred to unseen environments without the need for fine-tuning.</td>
                <td>Deep learning, Power demand, Navigation, Neural networks, Reinforcement learning, Logic gates, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341738&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341738&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Lightweight Neural Path Planning</td>
                <td>J. Li, S. Wang, Z. Chen, Z. Kan and J. Yu</td>
                <td>2023</td>
                <td>Learning-based path planning is becoming a promising robot navigation methodology due to its adaptability to various environments. However, the expensive computing and storage associated with networks impose significant challenges for their deployment on low-cost robots. Motivated by this practical challenge, we develop a lightweight neural path planning architecture with a dual input network and a hybrid sampler for resource-constrained robotic systems. Our architecture is designed with efficient task feature extraction and fusion modules to translate the given planning instance into a guidance map. The hybrid sampler is then applied to restrict the planning within the prospective regions indicated by the guide map. To enable the network training, we further construct a publicly available dataset with various successful planning instances. Numerical simulations and physical experiments demonstrate that, compared with baseline approaches, our approach has nearly an order of magnitude fewer model size and five times lower computational while achieving promising performance. Besides, our approach can also accelerate the planning convergence process with fewer planning iterations compared to sample-based methods.</td>
                <td>Training, Navigation, Computer architecture, Numerical simulation, Path planning, Planning, Numerical models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342133&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342133&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Modeling Action Spatiotemporal Relationships Using Graph-Based Class-Level Attention Network for Long-Term Action Detection</td>
                <td>Y. Wu, X. Su, D. Salihu, H. Xing, M. Zakour and C. Patsch</td>
                <td>2023</td>
                <td>In recent years, Action Detection has become an active research topic in various fields such as human-robot interaction and assistive robots. Most of the previous methods in this field focus on temporally processing the action representation, without considering the dependencies among the action classes. However, actions that occur in a video are constantly related, and this correlation could offer effective clues for detection tasks. In this work, we propose to exploit the information of related action classes with the help of a graph neural network in conjunction with temporal modeling. We introduce the attention-based temporal class module (ATC), which models the inherent action dependencies on the graph and learns action-specific features among temporal dimensions with a dual-branch attention mechanism. Further, we present the Graph-based Class-level Attention Network (GCAN), which is built upon ATC modules with increasing temporal receptive fields to handle actions instances in complex untrimmed videos. Our network is evaluated on two challenging benchmark datasets with dense annotations: Charades and MultiTHUMOS. Experimental results show that our approach demonstrates highly competitive results with a significantly reduced model complexity.</td>
                <td>Correlation, Human-robot interaction, Benchmark testing, Assistive robots, Multitasking, Spatiotemporal phenomena, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341409&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341409&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>GraNet: A Multi-Level Graph Network for 6-DoF Grasp Pose Generation in Cluttered Scenes</td>
                <td>H. Wang, W. Niu and C. Zhuang</td>
                <td>2023</td>
                <td>6-DoF object-agnostic grasping in unstructured environments is a critical yet challenging task in robotics. Most current works use non-optimized approaches to sample grasp locations and learn spatial features without concerning the grasping task. This paper proposes GraNet, a graph-based grasp pose generation framework that translates a point cloud scene into multi-level graphs and propagates features through graph neural networks. By building graphs at the scene level, object level, and grasp point level, GraNet enhances feature embedding at multiple scales while progressively converging to the ideal grasping locations by learning. Our pipeline can thus characterize the spatial distribution of grasps in cluttered scenes, leading to a higher rate of effective grasping. Furthermore, we enhance the representation ability of scalable graph networks by a structure-aware attention mechanism to exploit local relations in graphs. Our method achieves state-of-the-art performance on the large-scale GraspNet-1Billion benchmark, especially in grasping unseen objects (+11.62 AP). The real robot experiment shows a high success rate in grasping scattered objects, verifying the effectiveness of the proposed approach in unstructured environments.</td>
                <td>Point cloud compression, Representation learning, Graphical models, Pipelines, Training data, Grasping, 6-DOF</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341549&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341549&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Modular Neural Network Policies for Learning In-Flight Object Catching with a Robot Hand-Arm System</td>
                <td>W. Hu, F. Acero, E. Triantafyllidis, Z. Liu and Z. Li</td>
                <td>2023</td>
                <td>We present a modular framework designed to enable a robot hand-arm system to learn how to catch flying objects, a task that requires fast, reactive, and accurately-timed robot motions. Our framework consists of five core modules: (i) an object state estimator that learns object trajectory prediction, (ii) a catching pose quality network that learns to score and rank object poses for catching, (iii) a reaching control policy trained to move the robot hand to pre-catch poses, (iv) a grasping control policy trained to perform soft catching motions for safe and robust grasping, and (v) a gating network trained to synthesize the actions given by the reaching and grasping policy. The former two modules are trained via supervised learning and the latter three use deep reinforcement learning in a simulated environment. We conduct extensive evaluations of our framework in simulation for each module and the integrated system, to demonstrate high success rates of in-flight catching and robustness to perturbations and sensory noise. Whilst only simple cylindrical and spherical objects are used for training, the integrated system shows successful generalization to a variety of household objects that are not used in training.</td>
                <td>Training, Deep learning, Perturbation methods, Supervised learning, Grasping, Reinforcement learning, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341463&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341463&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation</td>
                <td>T. Zhang</td>
                <td>2023</td>
                <td>Language-Guided Robotic Manipulation (LGRM) is a challenging task as it requires a robot to understand human instructions to manipulate everyday objects. Recent approaches in LGRM rely on pre-trained Visual Grounding (VG) models to detect objects without adapting to manipulation environments. This results in a performance drop due to a substantial domain gap between the pre-training and real-world data. A straight-forward solution is to collect additional training data, but the cost of human-annotation is extortionate. In this paper, we propose Grounding Vision to Ceaselessly Created Instructions (GVCCI), a lifelong learning framework for LGRM, which continuously learns VG without human supervision. GVCCI iteratively generates synthetic instruction via object detection and trains the VG model with the generated data. We validate our framework in offline and online settings across diverse environments on different VG models. Experimental results show that accumulating synthetic data from GVCCI leads to a steady improvement in VG by up to 56.7% and improves resultant LGRM by up to 29.4%. Furthermore, the qualitative analysis shows that the unadapted VG model often fails to find correct objects due to a strong bias learned from the pre-training data. Finally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k triplets of image-object-instruction from diverse manipulation environments.</td>
                <td>Visualization, Costs, Grounding, Natural languages, Training data, Object detection, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342021&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342021&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bag All You Need: Learning a Generalizable Bagging Strategy for Heterogeneous Objects</td>
                <td>A. Bahety et al.</td>
                <td>2023</td>
                <td>We introduce a practical robotics solution for the task of heterogeneous bagging, requiring the placement of multiple rigid and deformable objects into a deformable bag. This is a difficult task as it features complex interactions between multiple highly deformable objects under limited observability. To tackle these challenges, we propose a robotic system consisting of two learned policies: a rearrangement policy that learns to place multiple rigid objects and fold deformable objects in order to achieve desirable pre-bagging conditions, and a lifting policy to infer suitable grasp points for bi-manual bag lifting. We evaluate these learned policies on a real-world three-arm robot platform that achieves a 70% heterogeneous bagging success rate with novel objects. To facilitate future research and comparison, we also develop a novel heterogeneous bagging simulation benchmark that will be made publicly available.</td>
                <td>Benchmark testing, Task analysis, Observability, Bagging, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341841&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341841&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Source Fusion for Voxel-Based 7-DoF Grasping Pose Estimation</td>
                <td>J. Qiu, F. Wang and Z. Dang</td>
                <td>2023</td>
                <td>In this work, we tackle the problem of 7-DoF grasping pose estimation(6-DoF with the opening width of parallel-jaw gripper) from point cloud data, which is a fundamental task in robotic manipulation. Most existing methods adopt 3D voxel CNNs as the backbone for their efficiency in handling unordered point cloud data. However, we found that these approaches overlook detailed information of the point clouds, resulting in decreased performance. Through our analysis, we identified quantization loss and boundary information loss within 3D convolutional layers as the primary causes of this issue. To address these challenges, we introduced two novel branches: one adds an extra positional encoding operation to preserve details and unique features for each point, and the other uses a 2D CNN to operate on the range-based image, which better aggregates boundary information on a continuous 2D domain. To integrate these branches with the original branch, we introduced a novel multi-source fusion gated mechanism to aggregate features. Our approach achieved state-of-the-art performance on the Graspnet-1Billion benchmark and demonstrated high success rates in real robotic experiments across different scenes. Our work has the potential to improve the performance of robotic grasping systems and contribute to the field of robotics.</td>
                <td>Point cloud compression, Three-dimensional displays, Quantization (signal), Aggregates, Pose estimation, Grasping, Logic gates</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341840&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341840&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>VL-Grasp: a 6-Dof Interactive Grasp Policy for Language-Oriented Objects in Cluttered Indoor Scenes</td>
                <td>Y. Lu, Y. Fan, B. Deng, F. Liu, Y. Li and S. Wang</td>
                <td>2023</td>
                <td>Robotic grasping faces new challenges in human-robot-interaction scenarios. We consider the task that the robot grasps a target object designated by human's language directives. The robot not only needs to locate a target based on vision-and-language information, but also needs to predict the reasonable grasp pose candidate at various views and postures. In this work, we propose a novel interactive grasp policy, named Visual-Lingual-Grasp (VL-Grasp), to grasp the target specified by human language. First, we build a new challenging visual grounding dataset to provide functional training data for robotic interactive perception in indoor environments. Second, we propose a 6- Dof interactive grasp policy combined with visual grounding and 6- Dof grasp pose detection to extend the universality of interactive grasping. Third, we design a grasp pose filter module to enhance the performance of the policy. Experiments demonstrate the effectiveness and extendibility of the VL-Grasp in real world. The VL-Grasp achieves a success rate of 72.5 % in different indoor scenes. The code and dataset is available at https://github.com/luyh20/VL-Grasp.</td>
                <td>Point cloud compression, Visualization, Grounding, Training data, Grasping, Indoor environment, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341379&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341379&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>QDP: Learning to Sequentially Optimise Quasi-Static and Dynamic Manipulation Primitives for Robotic Cloth Manipulation</td>
                <td>Dakka and V. Kyrki</td>
                <td>2023</td>
                <td>Pre-defined manipulation primitives are widely used for cloth manipulation. However, cloth properties such as its stiffness or density can highly impact the performance of these primitives. Although existing solutions have tackled the parameterisation of pick and place locations, the effect of factors such as the velocity or trajectory of quasi-static and dynamic manipulation primitives has been neglected. Choosing appropriate values for these parameters is crucial to cope with the range of materials present in house-hold cloth objects. To address this challenge, we introduce the Quasi-Dynamic Parameterisable (QDP) method, which optimises parameters such as the motion velocity in addition to the pick and place positions of quasi-static and dynamic manipulation primitives. In this work, we leverage the framework of Sequential Reinforcement Learning to decouple sequentially the parameters that compose the primitives. To evaluate the effectiveness of the method, we focus on the task of cloth unfolding with a robotic arm in simulation and real-world experiments. Our results in simulation show that by deciding the optimal parameters for the primitives the performance can improve by 20% compared to sub-optimal ones. Real-world results demonstrate the advantage of modifying the velocity and height of manipulation primitives for cloths with different mass, stiffness, shape, and size. Supplementary material, videos, and code, can be found at https://sites.google.com/view/qdp-srl.</td>
                <td>Shape, Dynamics, Reinforcement learning, Real-time systems, Fabrics, Trajectory, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342002&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342002&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robust Visual Sim-to-Real Transfer for Robotic Manipulation</td>
                <td>R. Garcia, R. Strudel, S. Chen, E. Arlaud, I. Laptev and C. Schmid</td>
                <td>2023</td>
                <td>Learning visuomotor policies in simulation is much safer and cheaper than in the real world. However, due to discrepancies between the simulated and real data, simulator-trained policies often fail when transferred to real robots. One common approach to bridge the visual sim-to-real domain gap is domain randomization (DR). While previous work mainly evaluates DR for disembodied tasks, such as pose estimation and object detection, here we systematically explore visual domain randomization methods and benchmark them on a rich set of challenging robotic manipulation tasks. In particular, we propose an offline proxy task of cube localization to select DR parameters for texture randomization, lighting randomization, variations of object colors and camera parameters. Notably, we demonstrate that DR parameters have similar impact on our offline proxy task and online policies. We, hence, use offline optimized DR parameters to train visuomotor policies in simulation and directly apply such policies to a real robot. Our approach achieves 93% success rate on average when tested on a diverse set of challenging manipulation tasks. Moreover, we evaluate the robustness of policies to visual variations in real scenes and show that our simulator-trained policies outperform policies learned using real but limited data. Code, simulation environment, real robot datasets and trained models are available at https://www.di.ens.fr/willow/research/robust_s2r/.</td>
                <td>Location awareness, Visualization, Robot vision systems, Pose estimation, Lighting, Object detection, Benchmark testing</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342471&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342471&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multi-Dimensional Deformable Object Manipulation Using Equivariant Models</td>
                <td>T. Fu, Y. Tang, T. Wu, X. Xia, J. Wang and C. Zhao</td>
                <td>2023</td>
                <td>Manipulating deformable objects, such as ropes (1D), fabrics (2D), and bags (3D), poses a significant challenge in robotics research due to their high degree of freedom in physical state and nonlinear dynamics. Compared with single-dimensional deformable objects, multi-dimensional object manipulation suffers from the difficulty in recognizing the characteristics of the object correctly and making an accurate action decision on the deformable object of various dimensions. Some methods are proposed to use neural networks to rearrange deformable objects in all dimensions, but their approaches are not accurate in predicting the motion of the robot as they just consider the equivariance in the picking objects. To address this problem, we present a novel Transporter Network encoded and decoded with equivariance to generalize to different picking and placing positions. Additionally, we propose an equivariant goal-conditioned model to enable the robot to manipulate deformable objects into flexible configurations without relying on artificially marked visual anchors for the target position. Finally, experiments conducted in both Deformable-Ravens and the real world demonstrate that our equivariant models are more sample efficient than the traditional Transporter Network. The video is available at https://youtu.be/5_q5ff9c9FU.</td>
                <td>Deformable models, Visualization, Three-dimensional displays, Neural networks, Fabrics, Nonlinear dynamical systems, Character recognition</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341618&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341618&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Adversarial Object Rearrangement in Constrained Environments with Heterogeneous Graph Neural Networks</td>
                <td>X. Lou, H. Yu, R. Worobel, Y. Yang and C. Choi</td>
                <td>2023</td>
                <td>Adversarial object rearrangement in the real world (e.g., previously unseen or oversized items in kitchens and stores) could benefit from understanding task scenes, which inherently entail heterogeneous components such as current objects, goal objects, and environmental constraints. The semantic relationships among these components are distinct from each other and crucial for multi-skilled robots to perform efficiently in everyday scenarios. We propose a hierarchical robotic manipulation system that learns the underlying relationships and maximizes the collaborative power of its diverse skills (e.g., PICK-PLACE, PUSH) for rearranging adversarial objects in constrained environments. The high-level coordinator employs a heterogeneous graph neural network (HetGNN), which reasons about the current objects, goal objects, and environmental constraints; the low-level 3D Convolutional Neural Network-based actors execute the action primitives. Our approach is trained entirely in simulation, and achieved an average success rate of 87.88% and a planning cost of 12.82 in real-world experiments, surpassing all baseline methods. Supplementary material is available at https://sites.google.com/umn.edu/versatile-rearrangement.</td>
                <td>Three-dimensional displays, Costs, Robot kinematics, Semantics, Focusing, Grasping, Graph neural networks, Deep Learning in Grasping and Manipulation, Perception for Grasping and Manipulation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342412&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342412&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Probabilistic Slide-support Manipulation Planning in Clutter</td>
                <td>S. Nagato et al.</td>
                <td>2023</td>
                <td>To safely and efficiently extract an object from the clutter, this paper presents a bimanual manipulation planner in which one hand of the robot is used to slide the target object out of the clutter while the other hand is used to support the surrounding objects to prevent the clutter from collapsing. Our method uses a neural network to predict the physical phenomena of the clutter when the target object is moved. We generate the most efficient action based on the Monte Carlo tree search. The grasping and sliding actions are planned to minimize the number of motion sequences to pick the target object. In addition, the object to be supported is determined to minimize the position change of surrounding objects. Experiments with a real bimanual robot confirmed that the robot could retrieve the target object, reducing the total number of motion sequences and improving safety.</td>
                <td>Monte Carlo methods, Shape, Stacking, Neural networks, Probabilistic logic, Safety, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342030&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342030&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning</td>
                <td>Y. Niu, S. Jin, Z. Zhang, J. Zhu, D. Zhao and L. Zhang</td>
                <td>2023</td>
                <td>In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in the tank and a large goal state space. Besides being effective in simulation environments, our method can efficiently adapt to noisy real-robot water-scooping scenarios with diverse physical configurations and unseen settings, demonstrating superior efficacy and generalizability. The videos of this work are available on our project page: https://sites.google.com/view/goatscooping.</td>
                <td>Fluid dynamics, Reinforcement learning, Trajectory, Noise measurement, Task analysis, Intelligent robots, Videos</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342221&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342221&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Tight Collision Probability for UAV Motion Planning in Uncertain Environment</td>
                <td>T. Liu, F. Zhang, F. Gao and J. Pan</td>
                <td>2023</td>
                <td>Operating unmanned aerial vehicles (UAVs) in complex environments that feature dynamic obstacles and external disturbances poses significant challenges, primarily due to the inherent uncertainty in such scenarios. Additionally, inaccurate robot localization and modeling errors further exacerbate these challenges. Recent research on UAV motion planning in static environments has been unable to cope with the rapidly changing surroundings, resulting in trajectories that may not be feasible. Moreover, previous approaches that have addressed dynamic obstacles or external disturbances in isolation are insufficient to handle the complexities of such environments. This paper proposes a reliable motion planning framework for UAVs, integrating various uncertainties into a chance constraint that characterizes the uncertainty in a probabilistic manner. The chance constraint provides a probabilistic safety certificate by calculating the collision probability between the robot's Gaussian-distributed forward reachable set and states of obstacles. To reduce the conservatism of the planned trajectory, we propose a tight upper bound of the collision probability and evaluate it both exactly and approximately. The approximated solution is used to generate motion primitives as a reference trajectory, while the exact solution is leveraged to iteratively optimize the trajectory for better results. Our method is thoroughly tested in simulation and real-world experiments, verifying its reliability and effectiveness in uncertain environments.</td>
                <td>Uncertainty, Upper bound, Dynamics, Autonomous aerial vehicles, Probabilistic logic, Planning, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342141&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342141&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Dodging Like A Bird: An Inverted Dive Maneuver Taking by Lifting-Wing Multicopters</td>
                <td>W. Gao, S. Wang and Q. Quan</td>
                <td>2023</td>
                <td>It is crucial for hybrid unmanned aerial vehicles, such as lifting-wing multicopters, to plan a continuous, smooth, and collision-free trajectory to avoid obstacles. Unlike quad-copters, which typically work in indoor environments, lifting-wing multicopters typically fly at a high altitude with a high cruising speed, requiring higher maneuverability in the vertical direction. Inspired by birds, lifting-wing multicopters can take an inverted flight maneuver to gain more maneuverability than the corresponding multicopter owing to the additional lifting wing. In this paper, a rotation-aware collision-free motion planning strategy is proposed that takes aerodynamics into consideration and allows lifting-wing multicopters to fly at large rotation angles, even in inverted postures. Specifically, a collision-free state sequence is found using rotation-aware primitives by solving a graph search problem. The sequence is then refined with B-spline into smooth trajectories to be tracked by the differential flatness-based controller for lifting-wing multicopters. We analyze the proposed motion planning algorithm in different scenarios and demonstrate the feasibility of the generated trajectories in simulation and real-world experiments. Video: https://youtu.be/n87jK81zg_I</td>
                <td>Costs, Tracking, Search problems, Birds, Autonomous aerial vehicles, Trajectory, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341551&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341551&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Model-Based Planning and Control for Terrestrial-Aerial Bimodal Vehicles with Passive Wheels</td>
                <td>R. Zhang et al.</td>
                <td>2023</td>
                <td>Terrestrial and aerial bimodal vehicles have gained widespread attention due to their cross-domain maneuverability. Nevertheless, their bimodal dynamics significantly increase the complexity of motion planning and control, thus hindering robust and efficient autonomous navigation in unknown environments. To resolve this issue, we develop a model-based planning and control framework for terrestrial aerial bi-modal vehicles. This work begins by deriving a unified dynamic model and the corresponding differential flatness. Leveraging differential flatness, an optimization-based trajectory planner is proposed, which takes into account both solution quality and computational efficiency. Moreover, we design a tracking controller using nonlinear model predictive control based on the proposed unified dynamic model to achieve accurate trajectory tracking and smooth mode transition. We validate our framework through extensive benchmark comparisons and experiments, demonstrating its effectiveness in terms of planning quality and control performance.</td>
                <td>Trajectory tracking, Computational modeling, Dynamics, Wheels, Benchmark testing, Predictive models, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342188&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342188&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Polynomial-Based Online Planning for Autonomous Drone Racing in Dynamic Environments</td>
                <td>Q. Wang, D. Wang, C. Xu, A. Gao and F. Gao</td>
                <td>2023</td>
                <td>In recent years, there is a noteworthy advance-ment in autonomous drone racing. However, the primary focus is on attaining execution times, while scant attention is given to the challenges of dynamic environments. The high-speed nature of racing scenarios, coupled with the potential for unforeseeable environmental alterations, present stringent requirements for online replanning and its timeliness. For racing in dynamic environments, we propose an online replanning framework with an efficient polynomial trajectory representation. We trade off between aggressive speed and flexible obstacle avoidance based on an optimization approach. Additionally, to ensure safety and precision when crossing intermediate racing waypoints, we formulate the demand as hard constraints during planning. For dynamic obstacles, parallel multi-topology trajectory planning is designed based on engineering considerations to prevent racing time loss due to local optimums. The framework is integrated into a quadrotor system and successfully demonstrated at the DJI Robomaster Intelligent UAV Championship, where it successfully complete the racing track and placed first, finishing in less than half the time of the second-place11https://pro-robomasters-hz-n5i3.oss-cn-hangzhou.aliyuncs.com/sass/event-list.html.</td>
                <td>Trajectory planning, Trajectory, Planning, Topology, Safety, Optimization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342456&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342456&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Autonomous Power Line Inspection with Drones via Perception-Aware MPC</td>
                <td>Carrió and D. Scaramuzza</td>
                <td>2023</td>
                <td>Drones have the potential to revolutionize power line inspection by increasing productivity, reducing inspection time, improving data quality, and eliminating the risks for human operators. Current state-of-the-art systems for power line inspection have two shortcomings: (i) control is decoupled from perception and needs accurate information about the location of the power lines and masts; (ii) obstacle avoidance is decoupled from the power line tracking, which results in poor tracking in the vicinity of the power masts, and, consequently, in decreased data quality for visual inspection. In this work, we propose a model predictive controller (MPC) that overcomes these limitations by tightly coupling perception and action. Our controller generates commands that maximize the visibility of the power lines while, at the same time, safely avoiding the power masts. For power line detection, we propose a lightweight learning-based detector that is trained only on synthetic data and is able to transfer zero-shot to real-world power line images. We validate our system in simulation and real-world experiments on a mock-up power line infrastructure. We release our code and datasets to the public.</td>
                <td>Productivity, Visualization, Data integrity, Detectors, Inspection, Robot sensing systems, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341871&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341871&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Perching and Tilting Aerial Robot for Precise and Versatile Power Tool Work on Vertical Walls</td>
                <td>R. Dautzenberg et al.</td>
                <td>2023</td>
                <td>Drilling, grinding, and setting anchors on vertical walls are fundamental processes in everyday construction work. Manually doing these works is error-prone, potentially dangerous, and elaborate at height. Today, heavy mobile ground robots can perform automatic power tool work. However, aerial vehicles could be deployed in untraversable environments and reach inaccessible places. Existing drone designs do not provide the large forces, payload, and high precision required for using power tools. This work presents the first aerial robot design to perform versatile manipulation tasks on vertical concrete walls with continuous forces of up to 150 N. The platform combines a quadrotor with active suction cups for perching on walls and a lightweight, tiltable linear tool table. This combination minimizes weight using the propulsion system for flying, surface alignment, and feed during manipulation and allows precise positioning of the power tool. We evaluate our design in a concrete drilling application - a challenging construction process that requires high forces, accuracy, and precision. In 30 trials, our design can accurately pinpoint a target position despite perching imprecision. Nine visually guided drilling experiments demonstrate a drilling precision of 6 mm without further automation. Aside from drilling, we also demonstrate the versatility of the design by setting an anchor into concrete.</td>
                <td>Drilling, Sensor placement, Uncertainty, Propulsion, Autonomous aerial vehicles, Visual servoing, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342274&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342274&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Resource-Constrained Station-Keeping for Latex Balloons Using Reinforcement Learning</td>
                <td>J. Saunders, L. Prenevost, Ö. Şimşek, A. Hunter and W. Li</td>
                <td>2023</td>
                <td>High altitude balloons have proved useful for ecological aerial surveys, atmospheric monitoring, and communication relays. However, due to weight and power constraints, there is a need to investigate alternate modes of propulsion to navigate in the stratosphere. Very recently, reinforcement learning has been proposed as a control scheme to maintain balloons in the region of a fixed location, facilitated through diverse opposing wind-fields at different altitudes. Although air-pump based station keeping has been explored, there is no research on the control problem for venting and ballasting actuated balloons, which is commonly used as a low-cost alternative. We show how reinforcement learning can be used for this type of balloon. Specifically, we use the soft actor-critic algorithm, which on average is able to station-keep within 50 km for on average 25% of the flight, consistent with state-of-the-art. Furthermore, we show that the proposed controller effectively minimises the consumption of resources, thereby supporting long duration flights. We frame the controller as a continuous control reinforcement learning problem, which allows for a more diverse range of trajectories, as opposed to current state-of-the-art work, which uses discrete action spaces. Furthermore, through continuous control, we can make use of larger ascent rates which are not possible using air-pumps. The desired ascent-rate is decoupled into desired altitude and time-factor to provide a more transparent policy, compared to low-level control commands used in previous works. Finally, by applying the equations of motion, we establish appropriate thresholds for venting and ballasting to prevent the agent from exploiting the environment. More specifically, we ensure actions are physically feasible by enforcing constraints on venting and ballasting.</td>
                <td>Surveys, Electronic ballasts, Green products, Terrestrial atmosphere, Reinforcement learning, Aerospace electronics, Propulsion</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341711&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341711&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Light-Weight, Low-Cost, and Sustainable Planning System for UAVs Using a Local Map Origin Update Approach</td>
                <td>D. Lee, J. La and S. Joo</td>
                <td>2023</td>
                <td>This paper proposes a sustainable planning system for small-sized unmanned aerial vehicles (UAVs). Our mapping module of the system uses a voxel array as data structure with an introduced feature which is local map origin update. This approach has clear advantages that the planning system can sustainably plan trajectories regardless of operating radius and flight distance, and it shows fastest invariant time complexity $\mathcal{O}(1)$ unlike other representation methods. Also, we propose an efficient configuration space (C-space) construction algorithm using incremental voxel inflation, and extend state-of-the-art Euclidean signed distance field (ESDF) algorithm, FIESTA by applying the local map origin update feature. The proposed planning system requires single depth camera only as a sensor, and can operate in realtime on embedded computing platforms. We have verified the planning system through real-world flight tests in dense environments using a lightweight quadrotor plat-form under 300 mm size equipped with low-cost components only.</td>
                <td>Embedded computing, Robot sensing systems, Cameras, Autonomous aerial vehicles, Planning, Trajectory, Arrays</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342455&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342455&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Bubble Explorer: Fast UAV Exploration in Large-Scale and Cluttered 3D-Environments Using Occlusion-Free Spheres</td>
                <td>B. Tang et al.</td>
                <td>2023</td>
                <td>Autonomous exploration is a crucial aspect of robotics that has numerous applications. Most of the existing methods greedily choose goals that maximize immediate reward. This strategy is computationally efficient but insufficient for overall exploration efficiency. In recent years, some state-of-the-art methods are proposed, which generate a global coverage path and significantly improve overall exploration efficiency. However, global optimization produces high computational overhead, leading to low-frequency planner updates and inconsistent planning motion. In this work, we propose a novel method to support fast UAV exploration in large-scale and cluttered 3-D environments. We introduce a computationally low-cost viewpoints generation method using occlusion-free spheres. Additionally, we combine greedy strategy with global optimization, which considers both computational and exploration efficiency. We benchmark our method against state-of-the-art methods to showcase its superiority in terms of exploration efficiency and computational time. We conduct various real-world experiments to demonstrate the excellent performance of our method in large-scale and cluttered environments.</td>
                <td>Benchmark testing, Autonomous aerial vehicles, Computational efficiency, Planning, Optimization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342348&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342348&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>UPPLIED: UAV Path Planning for Inspection Through Demonstration</td>
                <td>C. Min</td>
                <td>2023</td>
                <td>In this paper, a new demonstration-based path-planning framework for the visual inspection of large structures using UAVs is proposed. We introduce UPPLIED: UAV Path PLanning for InspEction through Demonstration, which utilizes a demonstrated trajectory to generate a new trajectory to inspect other structures of the same kind. The demonstrated trajectory can inspect specific regions of the structure and the new trajectory generated by UPPLIED inspects similar regions in the other structure. The proposed method generates inspection points from the demonstrated trajectory and uses standardization to translate those inspection points to inspect the new structure. Finally, the position of these inspection points is optimized to refine their view. Numerous experiments were conducted with various structures and the proposed framework was able to generate inspection trajectories of various kinds for different structures based on the demonstration. The trajectories generated match with the demonstrated trajectory in geometry and at the same time inspect the regions inspected by the demonstration trajectory with minimum deviation. The experimental video of the work can be found at https://youtu.be/YqPx-cLkv04.</td>
                <td>Geometry, Visualization, Solid modeling, Three-dimensional displays, Databases, Standardization, Inspection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342478&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342478&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Self-Supervised Instance Segmentation by Grasping</td>
                <td>Y. Liu, X. Chen and P. Abbeel</td>
                <td>2023</td>
                <td>Instance segmentation is a fundamental skill for many robotic applications. We propose a self-supervised method that uses grasp interactions to collect segmentation supervision for an instance segmentation model. When a robot grasps an item, the mask of that grasped item can be inferred from the images of the scene before and after the grasp. Leveraging this insight, we learn a grasp segmentation model from a small dataset of labelled images to segment the grasped object from before and after grasp images. Such a model can segment grasped objects from thousands of grasp interactions without costly human annotation. Using the segmented grasped objects, we can “cut” objects from their original scenes and “paste” them into new scenes to generate instance supervision. We show that our grasp segmentation model provides a 5x error reduction when segmenting grasped objects compared with traditional image subtraction approaches. Combined with our “cut-and-paste” generation method, instance segmentation models trained with our method achieve better performance than a model trained with 10x the amount of labeled data. On a real robotic grasping system, our instance segmentation model reduces the rate of grasp errors by over 3x compared to an image subtraction baseline.</td>
                <td>Instance segmentation, Training, Error analysis, Annotations, Grasping, Data models, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342432&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342432&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Fusing Visual Appearance and Geometry for Multi-Modality 6DoF Object Tracking</td>
                <td>M. Stoiber, M. Elsayed, A. E. Reichert, F. Steidle, D. Lee and R. Triebel</td>
                <td>2023</td>
                <td>In many applications of advanced robotic manipulation, six degrees of freedom (6DoF) object pose estimates are continuously required. In this work, we develop a multi-modality tracker that fuses information from visual appearance and geometry to estimate object poses. The algorithm extends our previous method ICG, which uses geometry, to additionally consider surface appearance. In general, object surfaces contain local characteristics from text, graphics, and patterns, as well as global differences from distinct materials and colors. To incorporate this visual information, two modalities are developed. For local characteristics, keypoint features are used to minimize distances between points from keyframes and the current image. For global differences, a novel region approach is developed that considers multiple regions on the object surface. In addition, it allows the modeling of external geometries. Experiments on the YCB-Video and OPT datasets demonstrate that our approach ICG+ performs best on both datasets, outperforming both conventional and deep learning-based methods. At the same time, the algorithm is highly efficient and runs at more than 300 Hz. The source code of our tracker is publicly available.</td>
                <td>Geometry, Learning systems, Visualization, Image color analysis, Source coding, Robot vision systems, Optimized production technology</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341961&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341961&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Viewpoint Push Planning for Mapping of Unknown Confined Spaces</td>
                <td>N. Dengler, S. Pan, V. Kalagaturu, R. Menon, M. Dawood and M. Bennewitz</td>
                <td>2023</td>
                <td>Viewpoint planning is an important task in any application where objects or scenes need to be viewed from different angles to achieve sufficient coverage. The mapping of confined spaces such as shelves is an especially challenging task since objects occlude each other and the scene can only be observed from the front, posing limitations on the possible viewpoints. In this paper, we propose a deep reinforcement learning framework that generates promising views aiming at reducing the map entropy. Additionally, the pipeline extends standard viewpoint planning by predicting adequate minimally invasive push actions to uncover occluded objects and increase the visible space. Using a 2.5D occupancy height map as state representation that can be efficiently updated, our system decides whether to plan a new viewpoint or perform a push. To learn feasible pushes, we use a neural network to sample push candidates on the map based on training data provided by human experts. As simulated and real-world experimental results with a robotic arm show, our system is able to significantly increase the mapped space compared to different baselines, while the executed push actions highly benefit the viewpoint planner with only minor changes to the object configuration.</td>
                <td>Minimally invasive surgery, Runtime, Pipelines, Robot vision systems, Training data, Reinforcement learning, Real-time systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341809&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341809&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Depth-Based 6DoF Object Pose Estimation Using Swin Transformer</td>
                <td>Z. Li and I. Stamos</td>
                <td>2023</td>
                <td>Accurately estimating the 6D pose of objects is crucial for many applications, such as robotic grasping, autonomous driving, and augmented reality. However, this task becomes more challenging in poor lighting conditions or when dealing with textureless objects. To address this issue, depth images are becoming an increasingly popular choice due to their invariance to a scene's appearance and the implicit incorporation of essential geometric characteristics. However, fully leveraging depth information to improve the performance of pose estimation remains a difficult and under-investigated problem. To tackle this challenge, we propose a novel framework called SwinDePose, that uses only geometric information from depth images to achieve accurate 6D pose estimation. SwinDePose first calculates the angles between each normal vector defined in a depth image and the three coordinate axes in the camera coordinate system. The resulting angles are then formed into an image, which is encoded using Swin Transformer. Additionally, we apply RandLA-Net to learn the representations from point clouds. The resulting image and point clouds embeddings are concatenated and fed into a semantic segmentation module and a 3D keypoints localization module. Finally, we estimate 6D poses using a least-square fitting approach based on the target object's predicted semantic mask and 3D keypoints. In experiments on the LineMod and Occlusion LineMod, SwinDePose outperforms existing state-of-the-art methods for 6D object pose estimation using depth images. We also provide competitive results on the YCB-Video dataset even without post-processing. This demonstrates the effectiveness of our approach and highlights its potential for improving performance in real-world scenarios. Our code is at https://github.com/zhujunli1993/SwinDePose.</td>
                <td>Point cloud compression, Location awareness, Three-dimensional displays, Semantic segmentation, Robot kinematics, Pose estimation, Semantics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342215&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342215&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DR-Pose: A Two-Stage Deformation-and-Registration Pipeline for Category-Level 6D Object Pose Estimation</td>
                <td>L. Zhou, Z. Liu, R. Gan, H. Wang and M. H. Ang</td>
                <td>2023</td>
                <td>Category-level object pose estimation involves estimating the 6D pose and the 3D metric size of objects from predetermined categories. While recent approaches take categorical shape prior information as reference to improve pose estimation accuracy, the single-stage network design and training manner lead to sub-optimal performance since there are two distinct tasks in the pipeline. In this paper, the advantage of two-stage pipeline over single-stage design is discussed. To this end, we propose a two-stage deformation-and-registration pipeline called DR-Pose, which consists of completion-aided deformation stage and scaled registration stage. The first stage uses a point cloud completion method to generate unseen parts of target object, guiding subsequent deformation on the shape prior. In the second stage, a novel registration network is designed to extract pose-sensitive features and predict the representation of object partial point cloud in canonical space based on the deformation results from the first stage. DR-Pose produces superior results to the state-of-the-art shape prior-based methods on both CAMERA25 and REAL275 benchmarks. Codes are available at https://github.com/Zray26/DR-Pose.git.</td>
                <td>Point cloud compression, Training, Three-dimensional displays, Shape, Deformation, Pipelines, Pose estimation</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341552&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341552&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning from Pixels with Expert Observations</td>
                <td>H. Hoang, L. Dinh and H. Nguyen</td>
                <td>2023</td>
                <td>In reinforcement learning (RL), sparse rewards can present a significant challenge. Fortunately, expert actions can be utilized to overcome this issue. However, acquiring explicit expert actions can be costly, and expert observations are often more readily available. This paper presents a new approach that uses expert observations for learning in robot manipulation tasks with sparse rewards from pixel observations. Specifically, our technique involves using expert observations as intermediate visual goals for a goal-conditioned RL agent, enabling it to complete a task by successively reaching a series of goals. We demonstrate the efficacy of our method in five challenging block construction tasks in simulation and show that when combined with two state-of-the-art agents, our approach can significantly improve their performance while requiring 4–20 times fewer expert actions during training. Moreover, our method is also superior to a hierarchical baseline.</td>
                <td>Training, Visualization, Neural networks, Reinforcement learning, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342043&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342043&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>RMBench: Benchmarking Deep Reinforcement Learning for Robotic Manipulator Control</td>
                <td>Y. Xiang et al.</td>
                <td>2023</td>
                <td>Reinforcement learning is used to tackle complex tasks with high-dimensional sensory inputs. Over the past decade, a wide range of reinforcement learning algorithms have been developed, with recent progress benefiting from deep learning for raw sensory signal representation. This raises a natural question: how well do these algorithms perform across different robotic manipulation tasks? To objectively compare algorithms, benchmarks use performance metrics. Benchmarks use objective performance metrics to offer a scientific way to compare algorithms. In this paper, we introduce RMBench, the first benchmark for robotic manipulations with high-dimensional continuous action and state spaces. We implement and evaluate reinforcement learning algorithms that take observed pixels as inputs and report their average performance and learning curves to demonstrate their performance and training stability. Our study concludes that none of the evaluated algorithms can handle all tasks well, with soft Actor-Critic outperforming most algorithms in terms of average reward and stability, and an algorithm combined with data augmentation potentially facilitating learning policies. Our code is publicly available at https://github.com/xiangyanfei212/RMBench-2022.git, including all benchmark tasks and studied algorithms.</td>
                <td>Measurement, Deep learning, Training, Reinforcement learning, Benchmark testing, Robot sensing systems, Manipulators</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342479&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342479&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Shape Completion with Prediction of Uncertain Regions</td>
                <td>M. Humt, D. Winkelbauer and U. Hillenbrand</td>
                <td>2023</td>
                <td>Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet [1], of realistically rendered depth images of object views with ground-truth annotations for the uncertain regions. We train on this dataset and test each method in shape completion and prediction of uncertain regions for known and novel object instances and on synthetic and real data. While direct uncertainty prediction is by far the most accurate in the segmentation of uncertain regions, both novel methods outperform the two baselines in shape completion and uncertain region prediction, and avoiding the predicted uncertain regions increases the quality of grasps for all tested methods. Web: https://github.com/DLR-RM/shape-completion</td>
                <td>Geometry, Uncertainty, Shape, Grasping, Probabilistic logic, Safety, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342487&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342487&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Structure from Action: Learning Interactions for 3D Articulated Object Structure Discovery</td>
                <td>N. Nie, S. Y. Gadre, K. Ehsani and S. Song</td>
                <td>2023</td>
                <td>We introduce Structure from Action (SfA), a framework to discover 3D part geometry and joint parameters of unseen articulated objects via a sequence of inferred interactions. Our key insight is that 3D interaction and perception should be considered in conjunction to construct 3D articulated CAD models, especially for categories not seen during training. By selecting informative interactions, Sf A discovers parts and reveals occluded surfaces, like the inside of a closed drawer. By aggregating visual observations in 3D, Sf A accurately segments multiple parts, reconstructs part geometry, and infers all joint parameters in a canonical coordinate frame. Our experiments demonstrate that a Sf A model trained in simulation can generalize to many unseen object categories with diverse structures and to real-world objects. Empirically, Sf A outperforms a pipeline of state-of-the-art components by 25.4 3D IoU percentage points on unseen categories, while matching already performant joint estimation baselines.11For code, data, and videos, see sfa.cs.columbia.edu/</td>
                <td>Geometry, Training, Solid modeling, Surface reconstruction, Three-dimensional displays, Robot kinematics, Pipelines</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342135&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342135&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Object-Oriented Option Framework for Robotics Manipulation in Clutter</td>
                <td>C. Pang et al.</td>
                <td>2023</td>
                <td>Domestic service robots are becoming increasingly popular due to their ability to help people with household tasks. These robots often encounter the challenge of manipulating objects in cluttered environments (MoC), which is difficult due to the complexity of effective planning and control. Previous solutions involved designing specific action primitives and planning paradigms. However, the pre-coded action primitives can limit the agility and task-solving scope of robots. In this paper, we propose a general approach for MoC called the Object-Oriented Option Framework (O3F), which uses the option framework (OF) to learn planning and control. The standard OF discovers options from scratch based on reinforcement learning, which can lead to collapsed options and hurt learning. To address this limitation, O3F introduces the concept of an object-oriented option space for OF, which focuses specifically on object movement and overcomes the challenges associated with collapsed options. Based on this, we train an object-oriented option planner to determine the option to execute and a universal object-oriented option executor to complete the option. Simulation experiments on the Ginger XR1 robot and robot arm show that O3F is generally applicable to various types of robot and manipulation tasks. Furthermore, O3F achieves success rates of 72.4% and 90% in grasping and object collecting tasks, respectively, significantly outperforming baseline methods.</td>
                <td>Service robots, Reinforcement learning, Grasping, Manipulators, Planning, Complexity theory, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342335&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342335&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Weakly Supervised Referring Expression Grounding via Dynamic Self-Knowledge Distillation</td>
                <td>J. Mi, Z. Chen and J. Zhang</td>
                <td>2023</td>
                <td>Weakly supervised referring expression grounding (WREG) is an attractive and challenging task for grounding target regions in images by understanding given referring expressions. WREG learns to ground target objects without the manual annotations between image regions and referring expressions during the model training phase. Different from the predominant grounding pattern of existing models, which locates target objects by reconstructing the region-expression correspondence, we investigate WREG from a novel perspective and enrich the prevailing pattern with self-knowledge distillation. Specifically, we propose a target-guided self-knowledge distillation approach that adopts the target prediction knowledge learned from the previous training iterations as the teacher to guide the subsequent training procedure. In order to avoid the misleading caused by the teacher knowledge with low prediction confidence, we present an uncertaintyaware knowledge refinement strategy to adaptively rectify the teacher knowledge by learning dynamic threshold values based on the model prediction uncertainty. To validate the proposed approach, we implement extensive experiments on three benchmark datasets, i.e., Ref Coco, RefCOCO+, and RefCOCOg. Our approach achieves new state-of-the-art results on several splits of the benchmark datasets, showcasing the advantage of the proposed framework for WREG. The implementation codes and trained models are available at: https://github.com/dami23IWREG.sar_KD.</td>
                <td>Training, Uncertainty, Codes, Grounding, Manuals, Benchmark testing, Predictive models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341909&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341909&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>EventTransAct: A Video Transformer-Based Framework for Event-Camera Based Action Recognition</td>
                <td>T. d. Blegiers, I. R. Dave, A. Yousaf and M. Shah</td>
                <td>2023</td>
                <td>Recognizing and comprehending human actions and gestures is a crucial perception requirement for robots to interact with humans and carry out tasks in diverse domains, including service robotics, healthcare, and manufacturing. Event cameras, with their ability to capture fast-moving objects at a high temporal resolution, offer new opportunities compared to standard action recognition in RGB videos. However, previous research on event camera action recognition has primarily focused on sensor-specific network architectures and image encoding, which may not be suitable for new sensors and limit the use of recent advancement in transformer-based architectures. In this study, we employ using a computationally efficient model, namely the video transformer network (VTN), which initially acquires spatial embeddings per event-frame and then utilizes a temporal self-attention mechanism. This approach separates the spatial and temporal operations, resulting in VTN being more computationally efficient than other video transformers that process spatio-temporal volumes directly. In order to better adopt the VTN for the sparse and finegrained nature of event data, we design Event-Contrastive Loss $\left(\mathscr{L}_{E C}\right)$ and event specific augmentations. Proposed $\left(\mathscr{L}_{E C}\right)$ promotes learning fine-grained spatial cues in the spatial backbone of VTN by contrasting temporally misaligned frames. We evaluate our method on real-world action recognition of N-EPIC Kitchens dataset, and achieve state-of-the-art results on both protocols - testing in seen kitchen (74.9% accuracy) and testing in unseen kitchens (42.43% and 46.66% Accuracy). Our approach also takes less computation time compared to competitive prior approaches. We also evaluate our method on the standard DVS Gesture recognition dataset, achieving a competitive accuracy of 97.9% compared to prior work that uses dedicated architectures and image-encoding for the DVS dataset. These results demonstrate the potential of our framework EventTransAct for real-world applications of event-camera based action recognition. Project Page: https://tristandb8.github.io/EventTransAct_webpage/</td>
                <td>Robot vision systems, Gesture recognition, Transformers, Cameras, Data models, Voltage control, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341740&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341740&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Virtual Ski Training System that Allows Beginners to Acquire Ski Skills Based on Physical and Visual Feedbacks</td>
                <td>Y. Okada et al.</td>
                <td>2023</td>
                <td>This paper proposes a ski training system using VR (Virtual Reality) that enables beginners to acquire skiing skills without going to an actual ski ground. The proposed system obtains the speed of skiing based on the center of pressure (COP) of each player's foot. The first-person perspective of skiing at the obtained speed down a ski slope is fed back to the player as a VR image. Experiments were conducted to evaluate the effectiveness of the proposed system and the VR interface. Specifically, beginner skiers were categorized into three groups: “a group trained with the proposed VR system”, “a group trained with a system that provides feedback of the skiing speed calculated from the COP by increasing or decreasing the gauge (a bar-shaped graph representing changes in numerical values), instead of VR”, and “a group that does not train with the system”. After training under each of these conditions, a sliding test was conducted on an actual ski slope to check the degree of skill acquisition. The results show that subjects trained with the proposed system acquired more skiing skills than subjects who did not use the system on actual ski slopes. Furthermore, there was no clear difference in the result of the sliding test between subjects trained by the VR interface and those trained by the gauge interface, but the VR interface yields better deceleration postures.</td>
                <td>Training, Visualization, Virtual reality, Intelligent robots, Sports</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342020&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342020&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Attention-Based VR Facial Animation with Visual Mouth Camera Guidance for Immersive Telepresence Avatars</td>
                <td>A. Rochow, M. Schwarz and S. Behnke</td>
                <td>2023</td>
                <td>Facial animation in virtual reality environments is essential for applications that necessitate clear visibility of the user's face and the ability to convey emotional signals. In our scenario, we animate the face of an operator who controls a robotic Avatar system. The use of facial animation is particularly valuable when the perception of interacting with a specific individual, rather than just a robot, is intended. Purely keypoint-driven animation approaches struggle with the complexity of facial movements. We present a hybrid method that uses both keypoints and direct visual guidance from a mouth camera. Our method generalizes to unseen operators and requires only a quick enrolment step with capture of two short videos. Multiple source images are selected with the intention to cover different facial expressions. Given a mouth camera frame from the HMD, we dynamically construct the target keypoints and apply an attention mechanism to determine the importance of each source image. To resolve keypoint ambiguities and animate a broader range of mouth expressions, we propose to inject visual mouth camera information into the latent space. We enable training on large-scale speaking head datasets by simulating the mouth camera input with its perspective differences and facial deformations. Our method outperforms a baseline in quality, capability, and temporal consistency. In addition, we highlight how the facial animation contributed to our victory at the ANA Avatar XPRIZE Finals.</td>
                <td>Training, Visualization, Tongue, Telepresence, Avatars, Mouth, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342522&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342522&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning</td>
                <td>A. Hatem, Y. Qian and Y. Wang</td>
                <td>2023</td>
                <td>Affordable 3D scanners often produce sparse and non-uniform point clouds that negatively impact downstream applications in robotic systems. While existing point cloud upsampling architectures have demonstrated promising results on standard benchmarks, they tend to experience significant performance drops when the test data have different distributions from the training data. To address this issue, this paper proposes a test-time adaption approach to enhance model generality of point cloud upsampling. The proposed approach leverages meta-learning to explicitly learn network parameters for test-time adaption. Our method does not require any prior information about the test data. During meta-training, the model parameters are learned from a collection of instance-level tasks, each of which consists of a sparse-dense pair of point clouds from the training data. During meta-testing, the trained model is fine-tuned with a few gradient updates to produce a unique set of network parameters for each test instance. The updated model is then used for the final prediction. Our framework is generic and can be applied in a plug-and-play manner with existing backbone networks in point cloud upsampling. Extensive experiments demonstrate that our approach improves the performance of state-of-the-art models.</td>
                <td>Point cloud compression, Metalearning, Adaptation models, Three-dimensional displays, Training data, Predictive models, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341345&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341345&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Revisiting Event-Based Video Frame Interpolation</td>
                <td>J. Chen et al.</td>
                <td>2023</td>
                <td>Dynamic vision sensors or event cameras provide rich complementary information for video frame interpolation. Existing state-of-the-art methods follow the paradigm of combining both synthesis-based and warping networks. However, few of those methods fully respect the intrinsic characteristics of events streams. Given that event cameras only encode intensity changes and polarity rather than color intensities, estimating optical flow from events is arguably more difficult than from RGB information. We therefore propose to incorporate RGB information in an event-guided optical flow refinement strategy. Moreover, in light of the quasi-continuous nature of the time signals provided by event cameras, we propose a divide-and-conquer strategy in which event-based intermediate frame synthesis happens incrementally in multiple simplified stages rather than in a single, long stage. Extensive experiments on both synthetic and real-world datasets show that these modifications lead to more reliable and realistic intermediate frame results than previous video frame interpolation methods. Our findings underline that a careful consideration of event characteristics such as high temporal density and elevated noise benefits interpolation accuracy.</td>
                <td>Interpolation, Color, Vision sensors, Cameras, Reliability, Task analysis, Optical flow</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341804&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341804&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Revisiting Deformable Convolution for Depth Completion</td>
                <td>X. Wang</td>
                <td>2023</td>
                <td>Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convolution needs to be applied on an estimated depth map with a relatively high density for better performance. We evaluate our model on the large-scale KITTI dataset and achieve state-of-the-art level performance in both accuracy and inference speed. Our code is available at https://github.com/AlexSunNiklReDC.</td>
                <td>Deformable models, Codes, Convolution, Iterative methods, Kernel, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342026&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342026&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Long-Distance Gesture Recognition Using Dynamic Neural Networks</td>
                <td>S. Bhatnagar, S. Gopal, N. Ahuja and L. Ren</td>
                <td>2023</td>
                <td>Gestures form an important medium of communication between humans and machines. An overwhelming majority of existing gesture recognition methods are tailored to a scenario where humans and machines are located very close to each other. This short-distance assumption does not hold true for several types of interactions, for example gesture-based interactions with a floor cleaning robot or with a drone. Methods made for short-distance recognition are unable to perform well on long-distance recognition due to gestures occupying only a small portion of the input data. Their performance is especially worse in resource constrained settings where they are not able to effectively focus their limited compute on the gesturing subject. We propose a novel, accurate and efficient method for the recognition of gestures from longer distances. It uses a dynamic neural network to select features from gesture-containing spatial regions of the input sensor data for further processing. This helps the network focus on features important for gesture recognition while discarding background features early on, thus making it more compute efficient compared to other techniques. We demonstrate the performance of our method on the LD-ConGR long-distance dataset where it outperforms previous state-of-the-art methods on recognition accuracy and compute efficiency.</td>
                <td>Performance evaluation, Multimodal sensors, Neural networks, Gesture recognition, Computer architecture, Robot sensing systems, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342147&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342147&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Neural Implicit Vision-Language Feature Fields</td>
                <td>K. Blomqvist, F. Milano, J. J. Chung, L. Ott and R. Siegwart</td>
                <td>2023</td>
                <td>Recently, groundbreaking results have been presented on open-vocabulary semantic image segmentation. Such methods segment each pixel in an image into arbitrary categories provided at run-time in the form of text prompts, as opposed to a fixed set of classes defined at training time. In this work, we present a zero-shot volumetric open-vocabulary semantic scene segmentation method. Our method builds on the insight that we can fuse image features from a vision-language model into a neural implicit representation. We show that the resulting feature field can be segmented into different classes by assigning points to natural language text prompts. The implicit volumetric representation enables us to segment the scene both in 3D and 2D by rendering feature maps from any given viewpoint of the scene. We show that our method works on noisy real-world data and can run in real-time on live sensor data dynamically adjusting to text prompts. We also present quantitative comparisons on the ScanNet dataset.</td>
                <td>Training, Image segmentation, Three-dimensional displays, Semantics, Natural languages, Robot sensing systems, Rendering (computer graphics)</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342275&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342275&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Language Guided Robotic Grasping with Fine-Grained Instructions</td>
                <td>Q. Sun, H. Lin, Y. Fu, Y. Fu and X. Xue</td>
                <td>2023</td>
                <td>Given a single RGB image and the attribute-rich language instructions, this paper investigates the novel problem of using Fine-grained instructions for the Language guided robotic Grasping (FLarG). This problem is made challenging by learning fine-grained language descriptions to ground target objects. Recent advances have been made in visually grounding the objects simply by several coarse attributes [1]. However, these methods have poor performance as they cannot well align the multi-modal features, and do not make the best of recent powerful large pre-trained vision and language models, e.g., CLIP. To this end, this paper proposes a FLarG pipeline including stages of CLIP-guided object localization, and 6-DoF category-level object pose estimation for grasping. Specially, we first take the CLIP-based segmentation model CRIS as the backbone and propose an end-to-end DyCRIS model that uses a novel dynamic mask strategy to well fuse the multi-level language and vision features. Then, the well-trained instance segmentation backbone Mask R-CNN is adopted to further improve the predicted mask of our DyCRIS. Finally, the target object pose is inferred for the robotics grasping by using the recent 6-DoF object pose estimation method. To validate our CLIP-enhanced pipeline, we also construct a validation dataset for our FLarG task and name it RefNOCS. Extensive results on RefNOCS have shown the utility and effectiveness of our proposed method. The project homepage is available at https://sunqiang85.github.ioIFLarG/.</td>
                <td>Location awareness, Instance segmentation, Point cloud compression, Grounding, Pipelines, Pose estimation, Grasping</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342331&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342331&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Whole Shape Estimation of Transparent Object from Its Contour using Statistical Shape Model</td>
                <td>K. Okada et al.</td>
                <td>2023</td>
                <td>This study presents a method for estimating the three-dimensional (3D) shapes of transparent objects from an RGB-D image using a statistical shape model. Statistical shape models compress the dimensions of multiple shapes to represent shape variations using fewer parameters. It is difficult to measure the depth of a transparent object using sensors. Therefore, the statistical shape model is deformed to fit the contour extracted from an RGB image and estimate the shape of the object. The depth image is used only to detect the plane on which the transparent objects are placed. Unlike other estimation methods, the proposed method estimates the whole shape of a transparent object. To validate the proposed method, the obtained estimation accuracy is compared with that of a machine-learning-based method. In addition, the estimated whole shape is compared with the measured data from a 3D scanner.</td>
                <td>Solid modeling, Three-dimensional displays, Image coding, Shape, Shape measurement, Estimation, Position measurement</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342400&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342400&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Off the Radar: Uncertainty-Aware Radar Place Recognition with Introspective Querying and Map Maintenance</td>
                <td>J. Yuan, P. Newman and M. Gadd</td>
                <td>2023</td>
                <td>Localisation with Frequency-Modulated Continuous-Wave (FMCW) radar has gained increasing interest due to its inherent resistance to challenging environments. However, complex artefacts of the radar measurement process require appropriate uncertainty estimation - to ensure the safe and reliable application of this promising sensor modality. In this work, we propose a multi-session map management system which constructs the “best” maps for further localisation based on learned variance properties in an embedding space. Using the same variance properties, we also propose a new way to introspectively reject localisation queries that are likely to be incorrect. For this, we apply robust noise-aware metric learning, which both leverages the short-timescale variability of radar data along a driven path (for data augmentation) and predicts the downstream uncertainty in metric-space-based place recognition. We prove the effectiveness of our method over extensive cross-validated tests of the Oxford Radar RobotCar and MulRan dataset. In this, we outperform the current state-of-the-art in radar place recognition and other uncertainty-aware methods when using only single nearest-neighbour queries. We also show consistent performance increases when rejecting queries based on uncertainty over a difficult test environment, which we did not observe for a competing uncertainty-aware place recognition system.</td>
                <td>Resistance, Representation learning, Measurement, Uncertainty, Radar measurements, Estimation, Maintenance engineering, Radar, Place Recognition, Deep Learning, Uncertainty Estimation, Autonomous Vehicles, Robotics</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341965&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341965&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Global Localization in Unstructured Environments Using Semantic Object Maps Built from Various Viewpoints</td>
                <td>J. Ankenbauer, P. C. Lusk, A. Thomas and J. P. How</td>
                <td>2023</td>
                <td>We present a novel framework for global localization and guided relocalization of a vehicle in an unstructured environment. Compared to existing methods, our pipeline does not rely on cues from urban fixtures (e.g., lane markings, buildings), nor does it make assumptions that require the vehicle to be navigating on a road network. Instead, we achieve localization in both urban and non-urban environments by robustly associating and registering the vehicle's local semantic object map with a compact semantic reference map, potentially built from other viewpoints, time periods, and/or modalities. Robustness to noise, outliers, and missing objects is achieved through our graph-based data association algorithm. Further, the guided relocalization capability of our pipeline mitigates drift inherent in odometry-based localization after the initial global localization. We evaluate our pipeline on two publicly-available, real-world datasets to demonstrate its effectiveness at global localization in both non-urban and urban environments. The Katwijk Beach Planetary Rover dataset [1] is used to show our pipeline's ability to perform accurate global localization in unstructured environments. Demonstrations on the KITTI dataset [2] achieve an average pose error of 3.8 m across all 35 localization events on Sequence 00 when localizing in a reference map created from aerial images. Compared to existing works, our pipeline is more general because it can perform global localization in unstructured environments using maps built from different viewpoints.</td>
                <td>Location awareness, Space vehicles, Navigation, Roads, Pipelines, Semantics, Urban areas</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342267&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342267&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Constructing Metric-Semantic Maps Using Floor Plan Priors for Long-Term Indoor Localization</td>
                <td>N. Zimmerman, M. Sodano, E. Marks, J. Behley and C. Stachniss</td>
                <td>2023</td>
                <td>Object-based maps are relevant for scene under-standing since they integrate geometric and semantic information of the environment, allowing autonomous robots to robustly localize and interact with on objects. In this paper, we address the task of constructing a metric-semantic map for the purpose of long-term object-based localization. We exploit 3D object detections from monocular RGB frames for both, the object-based map construction, and for globally localizing in the constructed map. To tailor the approach to a target environment, we propose an efficient way of generating 3D annotations to finetune the 3D object detection model. We evaluate our map construction in an office building, and test our long-term localization approach on challenging sequences recorded in the same environment over nine months. The experiments suggest that our approach is suitable for constructing metric-semantic maps, and that our localization approach is robust to long-term changes. Both, the mapping algorithm and the localization pipeline can run online on an onboard computer. We release an open-source C++/ros implementation of our approach.</td>
                <td>Location awareness, Measurement, Solid modeling, Three-dimensional displays, Semantics, Pipelines, Object detection</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341595&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341595&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DisPlacing Objects: Improving Dynamic Vehicle Detection via Visual Place Recognition under Adverse Conditions</td>
                <td>S. Hausler, S. Garg, P. Chakravarty, S. Shrivastava, A. Vora and M. Milford</td>
                <td>2023</td>
                <td>Can knowing where you are assist in perceiving objects in your surroundings, especially under adverse weather and lighting conditions? In this work we investigate whether a prior map can be leveraged to aid in the detection of dynamic objects in a scene without the need for a 3D map or pixel-level map-query correspondences. We contribute an algorithm which refines an initial set of candidate object detections and produces a refined subset of highly accurate detections using a prior map. We begin by using visual place recognition (VPR) to retrieve a prior map image for a given query image, then use a binary classification neural network that compares the query and prior map image regions to validate the query detection. Once our classification network is trained, on approximately 1000 query-map image pairs, it is able to improve the performance of vehicle detection when combined with an existing off-the-shelf vehicle detector. We demonstrate our approach using standard datasets across two cities (Oxford and Zurich) under different settings of train-test separation of map-query traverse pairs. We further emphasize the performance gains of our approach against alternative design choices and show that VPR suffices for the task, eliminating the need for precise ground truth localization.</td>
                <td>Location awareness, Visualization, Image recognition, Vehicle detection, Urban areas, Object detection, Detectors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341550&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341550&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FM-Loc: Using Foundation Models for Improved Vision-Based Localization</td>
                <td>R. Mirjalili, M. Krawez and W. Burgard</td>
                <td>2023</td>
                <td>Visual place recognition is essential for vision-based robot localization and SLAM. Despite the tremendous progress made in recent years, place recognition in changing environments remains challenging. A promising approach to cope with appearance variations is to leverage high-level semantic features like objects or place categories. In this paper, we propose FM-Loc which is a novel image-based localization approach based on Foundation Models. Our approach uses the Large Language Model GPT-3 in combination with the Visual-Language Model CLIP to construct a semantic image descriptor that is robust to severe changes in scene geometry and camera viewpoint. We deploy CLIP to detect objects in an image, GPT-3 to suggest potential room labels based on the detected objects, and CLIP again to propose the most likely location label. The object labels and the scene label constitute an image descriptor that we use to calculate a similarity score between the query and database images. We validate our approach on real-world data that exhibit significant changes in camera viewpoints and object placement between the database and query trajectories. The experimental results demonstrate that our method is applicable to a wide range of indoor scenarios without the need for training or fine-tuning.</td>
                <td>Location awareness, Training, Visualization, Simultaneous localization and mapping, Databases, Semantics, Robot vision systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342439&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342439&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Joint On-Manifold Gravity and Accelerometer Intrinsics Estimation for Inertially Aligned Mapping</td>
                <td>R. Nemiroff, K. Chen and B. T. Lopez</td>
                <td>2023</td>
                <td>Aligning a robot's trajectory or map to the inertial frame is a critical capability that is often difficult to do accurately even though inertial measurement units (IMUs) can observe absolute roll and pitch with respect to gravity. Accelerometer biases and scale factor errors from the IMU's initial calibration are often the major source of inaccuracies when aligning the robot's odometry frame with the inertial frame, especially for low-grade IMUs. Practically, one would simultaneously estimate the true gravity vector, accelerometer biases, and scale factor to improve measurement quality but these quantities are not observable unless the IMU is sufficiently excited. While several methods estimate accelerometer bias and gravity, they do not explicitly address the observability issue nor do they estimate scale factor. We present a fixed-lag factor-graph-based estimator to address both of these issues. In addition to estimating accelerometer scale factor, our method mitigates limited observability by optimizing over a time window an order of magnitude larger than existing methods with significantly lower computational burden. The proposed method, which estimates accelerometer intrinsics and gravity separately from the other states, is enabled by a novel, velocity-agnostic measurement model for intrinsics and gravity, as well as a new method for gravity vector optimization on $S^{2}$. Accurate IMU state prediction, gravity-alignment, and roll/pitch drift correction are experimentally demonstrated on public and self-collected datasets in diverse environments.</td>
                <td>Accelerometers, Measurement units, Inertial navigation, Pareto optimization, Trajectory, Velocity measurement, Odometry</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342424&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342424&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>I2P-Rec: Recognizing Images on Large-Scale Point Cloud Maps Through Bird's Eye View Projections</td>
                <td>S. Zheng et al.</td>
                <td>2023</td>
                <td>Place recognition is an important technique for autonomous cars to achieve full autonomy since it can provide an initial guess to online localization algorithms. Although current methods based on images or point clouds have achieved satisfactory performance, localizing the images on a large-scale point cloud map remains a fairly unexplored problem. This cross-modal matching task is challenging due to the difficulty in extracting consistent descriptors from images and point clouds. In this paper, we propose the I2P-Rec method to solve the problem by transforming the cross-modal data into the same modality. Specifically, we leverage on the recent success of depth estimation networks to recover point clouds from images. We then project the point clouds into Bird's Eye View (BEV) images. Using the BEV image as an intermediate representation, we extract global features with a Convolutional Neural Network followed by a NetVLAD layer to perform matching. The experimental results evaluated on the KITTI dataset show that, with only a small set of training data, I2P-Rec achieves recall rates at Top-l % over 80% and 90%, when localizing monocular and stereo images on point cloud maps, respectively. We further evaluate I2P-Rec on a 1 km trajectory dataset collected by an autonomous logistics car and show that I2P- Rec can generalize well to previously unseen environments.</td>
                <td>Point cloud compression, Location awareness, Visualization, Image recognition, Estimation, Training data, Feature extraction</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341907&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341907&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LIO-PPF: Fast LiDAR-Inertial Odometry via Incremental Plane Pre-Fitting and Skeleton Tracking</td>
                <td>X. Chen, P. Wu, G. Li and T. H. Li</td>
                <td>2023</td>
                <td>As a crucial infrastructure of intelligent mobile robots, LiDAR-Inertial odometry (LIO) provides the basic capability of state estimation by tracking LiDAR scans. The high-accuracy tracking generally involves the $k\text{NN}$ search, which is used with minimizing the point-to-plane distance. The cost for this, however, is maintaining a large local map and performing $k\text{NN}$ plane fit for each point. In this work, we reduce both time and space complexity of LIO by saving these unnecessary costs. Technically, we design a plane pre-fitting (PPF) pipeline to track the basic skeleton of the 3D scene. In PPF, planes are not fitted individually for each scan, let alone for each point, but are updated incrementally as the scene ‘flows’. Unlike $k\text{NN}$, the PPF is more robust to noisy and non-strict planes with our iterative Principal Component Analyse (iPCA) refinement. Moreover, a simple yet effective sandwich layer is introduced to eliminate false point-to-plane matches. Our method was extensively tested on a total number of 22 sequences across 5 open datasets, and evaluated in 3 existing state-of-the-art LIO systems. By contrast, LIO-PPF can consume only 36% of the original local map size to achieve up to 4x faster residual computing and 1.92x overall FPS, while maintaining the same level of accuracy. We fully open source our implementation at https://github.com/xingyuuchen/LIO-PPF.</td>
                <td>Laser radar, Costs, Three-dimensional displays, Simultaneous localization and mapping, Pipelines, Redundancy, Skeleton</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341524&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341524&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>EDI: ESKF-based Disjoint Initialization for Visual-Inertial SLAM Systems</td>
                <td>W. Wang, J. Li, Y. Ming and P. Mordohai</td>
                <td>2023</td>
                <td>Visual-inertial initialization can be classified into joint and disjoint approaches. Joint approaches tackle both the visual and the inertial parameters together by aligning observations from feature-bearing points based on IMU integration then use a closed-form solution with visual and acceleration observations to find initial velocity and gravity. In contrast, disjoint approaches independently solve the Structure from Motion (SFM) problem and determine inertial parameters from up-to-scale camera poses obtained from pure monocular SLAM. However, previous disjoint methods have limitations, like assuming negligible acceleration bias impact or accurate rotation estimation by pure monocular SLAM. To address these issues, we propose EDI, a novel approach for fast, accurate, and robust visual-inertial initialization. Our method incorporates an Error-state Kalman Filter (ESKF) to estimate gyroscope bias and correct rotation estimates from monocular SLAM, overcoming dependence on pure monocular SLAM for rotation estimation. To estimate the scale factor without prior information, we offer a closed-form solution for initial velocity, scale, gravity, and acceleration bias estimation. To address gravity and acceleration bias coupling, we introduce weights in the linear least-squares equations, ensuring acceleration bias observability and handling outliers. Extensive evaluation on the EuRoC dataset shows that our method achieves an average scale error of 5.8% in less than 3 seconds, outperforming other state-of-the-art disjoint visual-inertial initialization approaches, even in challenging environments and with artificial noise corruption.</td>
                <td>Visualization, Simultaneous localization and mapping, Closed-form solutions, Structure from motion, Estimation, Sensor systems and applications, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342106&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342106&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>SELVO: A Semantic-Enhanced Lidar-Visual Odometry</td>
                <td>K. Jiang et al.</td>
                <td>2023</td>
                <td>In the face of complex external environment, single sensor information can no longer meet the accuracy requirements of low-drift SLAM. In this paper, we focus on the fusion scheme of cameras and lidar, and explore the gain of semantic information to SLAM system. A Semantic-Enhanced Lidar-Visual Odometry (SELVO) is proposed to achieve pose estimation with high accuracy and robustness by applying semantics and utilizing strategies of initialization and sensor fusion. In loop closure detection thread, we propose a novel place recognition method based on semantic information to maintain the global consistency of the map. In the back-end, we design a joint optimization framework including visual odometry, lidar odometry and loop closure detection, and innovatively propose to recognize degraded scenes with semantic information. We have conducted a large number of experiments on KITTI [1] and KITTI-360 [2] dataset, and the results show that our system can achieve the high accuracy and competitive performance in comparison with state-of-the-art methods.</td>
                <td>Point cloud compression, Laser radar, Simultaneous localization and mapping, Semantics, Pose estimation, Sensor fusion, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341419&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341419&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LIWO: LiDAR-Inertial-Wheel Odometry</td>
                <td>Z. Yuan, F. Lang, T. Xu and X. Yang</td>
                <td>2023</td>
                <td>LiDAR-inertial odometry (LIO), which fuses complementary information of a LiDAR and an Inertial Measurement Unit (IMU), is an attractive solution for state estimation. In LIO, both pose and velocity are regarded as state variables that need to be solved. However, the widely-used Iterative Closest Point (ICP) algorithm can only provide constraint for pose, while the velocity can only be constrained by IMU pre-integration. As a result, the velocity estimates inclined to be updated accordingly with the pose results. In this paper, we propose LIWO, an accurate and robust LiDAR-inertial-wheel (LIW) odometry, which fuses the measurements from LiDAR, IMU and wheel encoder in a bundle adjustment (BA) based optimization framework. The involvement of a wheel encoder could provide velocity measurement as an important observation, which assists LIO to provide a more accurate state prediction. In addition, con-straining the velocity variable by the observation from wheel encoder in optimization can further improve the accuracy of state estimation. Experiment results on two public datasets demonstrate that our system outperforms all state-of-the-art LIO systems in terms of smaller absolute trajectory error (ATE), and embedding a wheel encoder can greatly improve the performance of LIO based on the BA framework.</td>
                <td>Laser radar, Fuses, Wheels, Robustness, Real-time systems, Trajectory, Odometry</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342258&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342258&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>VIW-Fusion: Extrinsic Calibration and Pose Estimation for Visual-IMU-Wheel Encoder System</td>
                <td>C. Qiao, S. Zhao, Y. Zhang, Y. Wang and D. Zhang</td>
                <td>2023</td>
                <td>The data fusion of camera, IMU, and wheel encoder measurements has proved its effectiveness in localizing ground robots, and obtaining accurate sensor extrinsic parameters is its premise. We propose an extrinsic parameter calibration algorithm and a multi-sensor-based pose estimation algorithm for the camera-IMU-wheel encoder system. First, we propose a joint calibration algorithm for the extrinsic parameters of the camera-IMU-wheel encoder system, which improves the accuracy and robustness of the camera-wheel encoder calibration. We then extend the visual-inertial odometry (VIO) to incorporate the measurements from the wheel encoder and weight the wheel encoder measurements according to angular velocity in global optimization to improve the performance. We further propose a novel method for VIO initialization by integrating wheel encoder information, which significantly reduces the scale error in initialization. We conduct extrinsic parameter calibration experiments on a real self-driving car and validate the performance of our multi-sensor-based localization system on the KAIST dataset and a dataset collected by our self-driving vehicles by performing an exhaust comparison with the state-of-the-art algorithms. Our implementations are open source11https://github.com/chunxiaoqiao/VIW-Fusion.git.</td>
                <td>Weight measurement, Simultaneous localization and mapping, Pose estimation, Robot vision systems, Wheels, Data integration, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341453&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341453&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LiDAR-Inertial SLAM with Efficiently Extracted Planes</td>
                <td>C. Chen, H. Wu, Y. Ma, J. Lv, L. Li and Y. Liu</td>
                <td>2023</td>
                <td>This paper proposes a LiDAR-Inertial SLAM with efficiently extracted planes, which couples explicit planes in the odometry to improve accuracy and in the mapping for consistency. The proposed method consists of three parts: an efficient Point $\boldsymbol{\rightarrow\text{Line}\rightarrow \text{Plane}}$ extraction algorithm, a LiDAR-Inertial-Plane tightly coupled odometry, and a global plane-aided mapping. Specifically, we leverage the ring field of the LiDAR point cloud to accelerate the region-growing-based plane extraction algorithm. Then we tightly coupled IMU pre-integration factors, LiDAR odometry factors, and explicit plane factors in the sliding window to obtain a more accurate initial pose for mapping. Finally, we maintain explicit planes in the global map, and enhance system consistency by optimizing the factor graph of optimized odometry factors and plane observation factors. Experimental results show that our plane extraction method is efficient, and the proposed plane-aided LiDAR-Inertial SLAM significantly improves the accuracy and consistency compared to the other state-of-the-art algorithms with only a small increase in time consumption.</td>
                <td>Point cloud compression, Simultaneous localization and mapping, Laser radar, Odometry, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342325&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342325&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Learning to Map Efficiently by Active Echolocation</td>
                <td>X. Hu, S. Purushwalkam, D. Harwath and K. Grauman</td>
                <td>2023</td>
                <td>Using visual SLAM to map new environments requires time-consuming visits to all regions for data collection. We propose an approach to estimate maps of areas beyond the visible regions using a cheap and readily available modality of data-sound. We introduce the idea of an active audio-visual mapping agent. Besides collecting visual data, the proposed agent emits sounds during navigation, captures the echoes, and uses them to accurately map unknown areas. We propose a reinforcement learning based method that simultaneously trains models to 1) estimate a map from the visual data, 2) output navigation actions, 3) output the decision to emit a sound and 4) refine estimated mans using the cantured audio. Our agent is trained and tested on 85 real-world homes from the Matterport3D dataset using the Habitat and SoundSpaces simulators for visual and audio data. Our method, unlike visual-data reliant approaches, yields more accurate maps with broader environmental coverage. In addition, compared to an agent that continually emits sounds, we observe that intelligently choosing when to emit sounds leads to accurate maps obatined with greater efficiency.</td>
                <td>Visualization, Simultaneous localization and mapping, Navigation, Habitats, Reinforcement learning, Data collection, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341664&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341664&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Visual-LiDAR-Inertial Odometry: A New Visual-Inertial SLAM Method Based on an iPhone 12 Pro</td>
                <td>L. Jin and C. Ye</td>
                <td>2023</td>
                <td>As today's smartphone integrates various imaging sensors and Inertial Measurement Units (IMU) and becomes computationally powerful, there is a growing interest in developing smartphone-based visual-inertial (VI) SLAM methods for robotics and computer vision applications. In this paper, we introduce a new SLAM method, called Visual-LiDAR-Inertial Odometry (VLIO), based on an iPhone 12 Pro. VLIO formulates device pose estimation as an optimization problem that minimizes a cost function based on the residuals of the inertial, visual, and depth measurements. We present the first work that 1) characterizes the iPhone's LiDAR in depth measurement and identifies the models for the measurement error and standard deviation, and 2) characterizes pose change estimation with LiDAR data. The measurement models are then used to compute the depth-related and visual-feature-related residuals for the cost function. Also, VLIO tracks varying camera intrinsic parameters (CIP) in real-time and uses them in computing these residuals. Both approaches result in more accurate residual terms and thus more accurate pose estimation. The CIP tracking method eliminates the need of a sophisticated model-fitting process that includes camera calibration and paring of the CIPs and IMU measurements with various phone orientations. Experimental results validate the efficacy of VLIO.</td>
                <td>Visualization, Laser radar, Simultaneous localization and mapping, Computational modeling, Pose estimation, Cost function, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341536&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341536&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Optimization-Based VINS: Consistency, Marginalization, and FEJ</td>
                <td>C. Chen, P. Geneva, Y. Peng, W. Lee and G. Huang</td>
                <td>2023</td>
                <td>In this work, we present a comprehensive analysis of the application of the First-estimates Jacobian (FEJ) design methodology in nonlinear optimization-based Visual-Inertial Navigation Systems (VINS). The FEJ approach fixes system linearization points to preserve proper observability properties of VINS and has been shown to significantly improve the estimation performance of state-of-the-art filtering-based methods. However, its direct application to optimization-based estimators holds challenges and pitfalls, which we addressed in this paper. Specifically, we carefully examine the observability and its relation to inconsistency and FEJ, based on this, we explain how to properly apply and implement FEJ within four marginalization archetypes commonly used in non-linear optimizationbased frameworks. FEJ's effectiveness and applications to VINS are investigated and demonstrate significant performance improvements. Additionally, we offer a detailed discussion of results and guidelines on how to properly implement FEJ in optimization-based estimators.</td>
                <td>Jacobian matrices, Navigation, Design methodology, Estimation, Observability, Intelligent robots, Guidelines</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341637&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341637&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Visual-Inertial-Laser-Lidar (VILL) SLAM: Real-Time Dense RGB-D Mapping for Pipe Environments</td>
                <td>T. Tian et al.</td>
                <td>2023</td>
                <td>Robotic solutions for pipeline inspection promise enhancement of human labor by automating data acquisition for pipe condition assessments, which are vital for the early detection of pipe anomalies and the prevention of hazardous leakages and explosions. Through simultaneous localization and mapping (SLAM), colorized 3D reconstructions of the pipe's inner surface can be generated, providing a more comprehensive digital record of the pipes compared to conventional vision-only inspection. Designed for generic environments, most SLAM methods suffer limited accuracy and substantial accumulative drift in confined and featureless spaces such as pipelines, due to a lack of suitable sensor hardware and state estimation techniques. In this research, we present VILL-SLAM: a dense RGB-D SLAM algorithm that combines a monocular camera (V), an inertial sensor (I), a ring-shaped laser profiler (L), and a Lidar (L) into a compact sensor package optimized for in-pipe operations. By fusing complementary visual and depth information from the color camera, laser profiling, and Lidar measurement, our method overcomes the challenges of metric scale mapping in conventional SLAM methods, despite its monocular configuration. To further improve localization accuracy, we utilize the pipe geometry to formulate two unique optimization factors that effectively constrain odometer drift. To validate our method, we conducted real-world experiments in physical pipes, comparing the performance of our approach against other state-of-the-art algorithms. The proposed SLAM framework achieved 6.6 times drift improvement with 0.84% mean odometry drift over 22 meters and a mean pointwise 3D scanning error of 0.88mm in 12-inch diameter pipes. This research represents a significant advancement in miniature in-pipe inspection, localization, and mapping sensing techniques. It has the potential to become a core enabling technology for the next generation of highly capable in-pipe robots, capable of reconstructing photo-realistic 3D pipe scans and providing disruptive pipe locating and georeferencing capabilities.</td>
                <td>Location awareness, Surface reconstruction, Simultaneous localization and mapping, Three-dimensional displays, Laser radar, Pipelines, Measurement by laser beam</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341761&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341761&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Joint Imitation Learning of Behavior Decision and Control for Autonomous Intersection Navigation</td>
                <td>Z. Zhu and H. Zhao</td>
                <td>2023</td>
                <td>Modern autonomous driving systems face substantial challenges when navigating dense intersections due to the high uncertainty introduced by other road users. Due to the complexity of the task, the autonomous vehicle needs to generate policies at multiple levels of abstraction. However, previous deep imitation learning methods focused on learning control policies while using simple rule-based behavior models. To bridge this gap and achieve human-like driving, we develop a hierarchy of high-level behavior decision and low-level control, where both policies are jointly learned from human demonstrations based on imitation learning. Over 60 hours of driving data from 10 drivers at six intersections was collected. The proposed method is extensively evaluated in challenging intersection scenarios. Empirical results demonstrate the method's superior performance over baselines in terms of task completion and control quality. We demonstrate the importance of learning human-like behavior decisions as well as joint learning of behavior and control policies. The capability of imitating different driving styles is also illustrated.</td>
                <td>Learning systems, Uncertainty, Navigation, Roads, Behavioral sciences, Task analysis, Autonomous vehicles</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342405&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342405&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Improving the Performance of Backward Chained Behavior Trees that use Reinforcement Learning</td>
                <td>M. Kartasev, J. Salér and P. Ögren</td>
                <td>2023</td>
                <td>In this paper we show how to improve the performance of backward chained behavior trees (BTs) that include policies trained with reinforcement learning (RL). BTs represent a hierarchical and modular way of combining control policies into higher level control policies. Backward chaining is a design principle for the construction of BTs that combines reactivity with goal directed actions in a structured way. The backward chained structure has also enabled convergence proofs for BTs, identifying a set of local conditions to be satisfied for the convergence of all trajectories to a set of desired goal states. The key idea of this paper is to improve performance of backward chained BTs by using the conditions identified in a theoretical convergence proof to configure the RL problems for individual controllers. Specifically, previous analysis identified so-called active constraint conditions (ACCs), that should not be violated in order to avoid having to return to work on previously achieved subgoals. We propose a way to set up the RL problems, such that they do not only achieve each immediate subgoal, but also avoid violating the identified ACCs. The resulting performance improvement depends on how often ACC violations occurred before the change, and how much effort, in terms of execution time, was needed to re-achieve them. The proposed approach is illustrated in a dynamic simulation environment.</td>
                <td>Training, Reinforcement learning, Behavioral sciences, Trajectory, Level control, Intelligent robots, Convergence, Behavior trees, Reinforcement learning, Autonomous systems, Artificial Intelligence</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342319&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342319&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports Using Graph Learning</td>
                <td>P. KrisshnaKumar, J. Witter, S. Paul, H. Cho, K. Dantu and S. Chowdhury</td>
                <td>2023</td>
                <td>Urban Air Mobility (UAM) promises a new dimension to decongested, safe, and fast travel in urban and suburban hubs. These UAM aircraft are conceived to operate from small airports called vertiports each comprising multiple take-offllanding and battery-recharging spots. Since they might be situated in dense urban areas and need to handle many aircraft landings and take-offs each hour, managing this schedule in real-time becomes challenging for a traditional air-traffic controller but instead calls for an automated solution. This paper provides a novel approach to this problem of Urban Air Mobility - Vertiport Schedule Management (UAM-VSM), which leverages graph reinforcement learning to generate decision-support policies. Here the designated physical spots within the vertiport's airspace and the vehicles being managed are represented as two separate graphs, with feature extraction performed through a graph convolutional network (GCN). Extracted features are passed onto perceptron layers to decide actions such as continue to hover or cruise, continue idling or take-off, or land on an allocated vertiport spot. Performance is measured based on delays, safety (no. of collisions) and battery consumption. Through realistic simulations in AirSim applied to scaled down multi-rotor vehicles, our results demonstrate the suitability of using graph reinforcement learning to solve the UAM-VSM problem and its superiority to basic reinforcement learning (with graph embed dings) or random choice baselines.</td>
                <td>Schedules, Urban areas, Reinforcement learning, Feature extraction, Real-time systems, Delays, Batteries</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341398&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341398&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Scaling Vision-Based End-to-End Autonomous Driving with Multi-View Attention Learning</td>
                <td>Y. Xiao, F. Codevilla, D. Porres and A. M. López</td>
                <td>2023</td>
                <td>On end-to-end driving, human driving demonstrations are used to train perception-based driving models by imitation learning. This process is supervised on vehicle signals (e.g., steering angle, acceleration) but does not require extra costly supervision (human labeling of sensor data). As a representative of such vision-based end-to-end driving models, CILRS is commonly used as a baseline to compare with new driving models. So far, some latest models achieve better performance than CILRS by using expensive sensor suites and/or by using large amounts of human-labeled data for training. Given the difference in performance, one may think that it is not worth pursuing vision-based pure end-to-end driving. However, we argue that this approach still has great value and potential considering cost and maintenance. In this paper, we present CIL++, which improves on CILRS by both processing higher-resolution images using a human-inspired HFOV as an inductive bias and incorporating a proper attention mechanism. CIL++ achieves competitive performance compared to models which are more costly to develop. We propose to replace CILRS with CIL++ as a strong vision-based pure end-to-end driving baseline supervised by only vehicle signals and trained by conditional imitation learning.</td>
                <td>Training, Measurement, Visualization, Robot vision systems, Maintenance engineering, Cameras, Transformers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341506&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341506&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Value of Assistance for Mobile Agents</td>
                <td>A. Amuzig, D. Dovrat and S. Keren</td>
                <td>2023</td>
                <td>Mobile robotic agents often suffer from localization uncertainty which grows with time and with the agents' movement. This can hinder their ability to accomplish their task. In some settings, it may be possible to perform assistive actions that reduce uncertainty about a robot's location. For example, in a collaborative multi-robot system, a wheeled robot can request assistance from a drone that can fly to its estimated location and reveal its exact location on the map or accompany it to its intended location. Since assistance may be costly and limited, and may be requested by different members of a team, there is a need for principled ways to support the decision of which assistance to provide to an agent and when, as well as to decide which agent to help within a team. For this purpose, we propose Value of Assistance (VOA) to represent the expected cost reduction that assistance will yield at a given point of execution. We offer ways to compute VOA based on estimations of the robot's future uncertainty, modeled as a Gaussian process. We specify conditions under which our VOA measures are valid and empirically demonstrate the ability of our measures to predict the agent's average cost reduction when receiving assistance in both simulated and real-world robotic settings.</td>
                <td>Location awareness, Uncertainty, Costs, Mobile agents, Mobile robots, Multi-robot systems, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342313&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342313&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Feature Explanation for Robust Trajectory Prediction</td>
                <td>X. Zhai, R. Hu and Z. Yin</td>
                <td>2023</td>
                <td>Trajectory prediction of neighboring agents is a critical task for high-speed robotics such as autonomous vehicles. In order to obtain fine-grained and robust scene representations, existing works attempt to consider abundant information that is deemed relevant. The cost, however, is the heavy computational burden and more importantly the inevitable interference brought by redundant information. In this paper, we exploit the explainable AI (XAI) techniques and propose a model in the framework of “Encoder-Decoder” named parallel explainable Transformer (PXT) to identify the contributive features for robust trajectory prediction. A two-branch encoder is designed to disentangle the roadway information and agents' historical trajectories for better feature explanation. Two stages of feature explanation are incorporated into the encoder. In the first stage, an explainable Transformer (XT) comprising a Layer-wise Relevance Propagation (LRP)-based interpretation module is designed and implemented in both branches to score and filter the contextual and motion features. In the second stage of interpretation, the ProbSparse attention mechanism is innovatively adopted to measure the level of interactivity with sparsity, so that the relationships among highly interactive agents are focused on. The results on the Argoverse Benchmark show that our proposal achieves state-of-the-art (SOTA) performance without delicate and tedious network design, demonstrating the effectiveness of tracing and retaining contributive features in enhancing the performance of trajectory prediction.</td>
                <td>Interference, Benchmark testing, Predictive models, Transformers, Information filters, Encoding, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341825&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341825&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation</td>
                <td>Z. Liu et al.</td>
                <td>2023</td>
                <td>Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.</td>
                <td>Training, Industries, Reinforcement learning, Cognition, Task analysis, Autonomous vehicles, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341750&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341750&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Predicting Center of Mass by Iterative Pushing for Object Transportation and Manipulation</td>
                <td>S. M. Hyland, J. Xiao and C. D. Onal</td>
                <td>2023</td>
                <td>Robotic manipulation tasks rely on a plethora of environmental and payload information. One critical piece of information for accurate manipulation is the center of mass (CoM) of the object, which is essential for estimating the dynamic response of the system and determining the payload placement. Traditionally, the CoM of a payload is provided prior to manipulation. In order to create a more robust and comprehensive system, this information should be collected by the robotic agent before or during the task run time. This paper presents a method for approximating the CoM of a planar object using a small-scale mobile robot to inform manipulation tasks. On average, our system is able to converge on a CoM estimate in under 30 seconds in simulation and 20 seconds in experiment, with a relative error of 4.95% and 5.46%, respectively.</td>
                <td>Transportation, Robot sensing systems, Motion capture, Recording, Mobile robots, Iterative methods, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341534&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341534&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>The Impact of Overall Optimization on Warehouse Automation</td>
                <td>H. Yoshitake and P. Abbeel</td>
                <td>2023</td>
                <td>In this study, we propose a novel approach for investigating optimization performance by flexible robot co-ordination in automated warehouses with multi-agent rein-forcement learning (MARL)-based control. Automated systems using robots are expected to achieve efficient operations compared with manual systems in terms of overall optimization performance. However, the impact of overall optimization on performance remains unclear in most automated systems due to a lack of suitable control methods. Thus, we proposed a centralized training-and-decentralized execution MARL frame-work as a practical overall optimization control method. In the proposed framework, we also proposed a single shared critic, trained with global states and rewards, applicable to a case in which heterogeneous agents make decisions asynchronously. Our proposed MARL framework was applied to the task selection of material handling equipment through automated order picking simulation, and its performance was evaluated to determine how far overall optimization outperforms partial optimization by comparing it with other MARL frameworks and rule-based control methods.</td>
                <td>Automation, Service robots, Robot kinematics, Layout, Process control, Materials handling equipment, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342333&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342333&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Kinematics-Only Differential Flatness Based Trajectory Tracking for Autonomous Racing</td>
                <td>Y. Dighe, Y. Kim, S. Rajguru, Y. Turkar, T. Singh and K. Dantu</td>
                <td>2023</td>
                <td>In autonomous racing, accurately tracking the race line at the limits of handling is essential to guarantee competitiveness. In this study, we show the effectiveness of Differential Flatness based control for high-speed trajectory tracking for car-like robots. We compare the tracking performance of our controller against Nonlinear Model Predictive Control and resource use while running on embedded hardware and show that on average KFC reduces the computation resource usage by 50 % while performing on par with NMPC. Our implementation of the proposed controller, the simulation environment and detailed results is open-sourced on https://github.com/droneslab/.</td>
                <td>Trajectory tracking, Hardware, Intelligent robots, Predictive control</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341603&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341603&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>LEF: Late-to-Early Temporal Fusion for LiDAR 3D Object Detection</td>
                <td>T. He, P. Sun, Z. Leng, C. Liu, D. Anguelov and M. Tan</td>
                <td>2023</td>
                <td>We propose a late-to-early recurrent feature fusion scheme for 3D object detection using temporal LiDAR point clouds. Our main motivation is fusing object-aware latent embeddings into the early stages of a 3D object detector. This feature fusion strategy enables the model to better capture the shapes and poses for challenging objects, compared with learning from raw points directly. Our method conducts late-to-early feature fusion in a recurrent manner. This is achieved by enforcing window-based attention blocks upon temporally calibrated and aligned sparse pillar tokens. Leveraging bird's eye view foreground pillar segmentation, we reduce the number of sparse history features that our model needs to fuse into its current frame by 10x. We also propose a stochastic-length FrameDrop training technique, which generalizes the model to variable frame lengths at inference for improved performance without retraining. We evaluate our method on the widely adopted Waymo Open Dataset and demonstrate improvement on 3D object detection against the baseline model, especially for the challenging category of large objects.</td>
                <td>Training, Point cloud compression, Solid modeling, Three-dimensional displays, Laser radar, Shape, Fuses</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341958&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341958&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hierarchical Decision Transformer</td>
                <td>A. Correia and L. A. Alexandre</td>
                <td>2023</td>
                <td>Sequence models in reinforcement learning require task knowledge to estimate the task policy. This paper presents the hierarchical decision transformer (HDT). HDT is a hierarchical behavior cloning algorithm that improves the performance of transformer methods in imitation learning, improving their robustness to tasks with longer episodes and/or sparse rewards, without requiring task knowledge or user interaction currently present in the state-of-the-art. The high-level mechanism guides the low-level controller through the task by selecting sub-goals for the latter to reach. This sequence replaces the returns-to-go of previous methods, improving its performance overall, especially in tasks with longer episodes and scarcer rewards. We validate our method in multiple tasks of OpenAI Gym, D4RL, and RoboMimic benchmarks. Our method outperforms the baselines in twenty three out of thirty one settings of varied horizons and reward frequencies without prior task knowledge, showing the advantages of the hierarchical model approach for learning from demonstrations using a sequence model. We also evaluate the method on a reaching task on a physical robot.</td>
                <td>Learning systems, Cloning, Reinforcement learning, Benchmark testing, Transformers, Robustness, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342230&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342230&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Imitation-Guided Multimodal Policy Generation from Behaviourally Diverse Demonstrations</td>
                <td>S. Zhu, R. Kaushik, S. Kaski and V. Kyrki</td>
                <td>2023</td>
                <td>Learning policies from multiple demonstrators is often difficult because different individuals perform the same task differently due to hidden factors such as preferences. In the context of policy learning, this leads to multimodal policies. Existing policy learning methods often converge to a single solution mode, failing to capture the diversity in the solution space. In this paper, we introduce an imitation-guided reinforcement learning framework to solve the multimodal policy learning problem from a limited number of state-only demonstrations. Then, we propose LfBD (Learning from Behaviourally diverse Demonstration), an algorithm that builds a parameterised solution space to capture the variability in the behaviour space defined by demonstrations. To this end, we define a projection function based on the state density distributions from demonstrations to define such space. Our goal is not only to learn how to solve the task as the human demonstrator but also to extrapolate beyond the provided demonstrations. In addition, we show that with our method, we can perform a post-hoc policy search in the built solution space to recover policies that satisfy specific constraints or to find a policy that matches a given (state-only) behaviour.</td>
                <td>Learning systems, Reinforcement learning, Encoding, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341403&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341403&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Model-based Adversarial Imitation Learning from Demonstrations and Human Reward</td>
                <td>J. Huang, J. Hao, R. Juan, R. Gomez, K. Nakarnura and G. Li</td>
                <td>2023</td>
                <td>Reinforcement learning (RL) can potentially be applied to real-world robot control in complex and uncertain environments. However, it is difficult or even unpractical to design an efficient reward function for various tasks, especially those large and high-dimensional environments. Generative adversarial imitation learning (GAIL) - a general model-free imitation learning method, allows robots to directly learn policies from expert trajectories in large and high-dimensional environments. However, GAIL is still sample inefficient in terms of environmental interaction. In this paper, to solve this problem, we propose a model-based adversarial imitation learning from demonstrations and human reward (MAILDH), a novel model-based interactive imitation framework combining the advantages of GAIL, interactive RL and model-based RL. We tested our method in eight physics-based discrete and continuous control tasks for RL. Our results show that MAILDH can greatly improve the sample efficiency and robustness compared to the original GAIL.</td>
                <td>Learning systems, Robot control, Reinforcement learning, Robustness, Trajectory, Task analysis, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341411&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341411&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Interpretable Motion Planner for Urban Driving via Hierarchical Imitation Learning</td>
                <td>B. Wang et al.</td>
                <td>2023</td>
                <td>Learning-based approaches have achieved remarkable performance in the domain of autonomous driving. Leveraging the impressive ability of neural networks and large amounts of human driving data, complex patterns and rules of driving behavior can be encoded as a model to benefit the autonomous driving system. Besides, an increasing number of data-driven works have been studied in the decision-making and motion planning module. However, the reliability and the stability of the neural network is still full of uncertainty. In this paper, we introduce a hierarchical planning architecture including a high-level grid-based behavior planner and a low-level trajectory planner, which is highly interpretable and controllable. As the high-level planner is responsible for finding a consistent route, the low-level planner generates a feasible trajectory. We evaluate our method both in closed-loop simulation and real world driving, and demonstrate the neural network planner has outstanding performance in complex urban autonomous driving scenarios.</td>
                <td>Training, Uncertainty, Neural networks, Decision making, Stability analysis, Trajectory, Planning</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342448&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342448&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Hierarchical Imitation Learning for Stochastic Environments</td>
                <td>M. Igl et al.</td>
                <td>2023</td>
                <td>Many applications of imitation learning require the agent to generate the full distribution of behaviour observed in the training data. For example, to evaluate the safety of autonomous vehicles in simulation, accurate and diverse behaviour models of other road users are paramount. Existing methods that improve this distributional realism typically rely on hierarchical policies. These condition the policy on types such as goals or personas that give rise to multi-modal behaviour. However, such methods are often inappropriate for stochastic environments where the agent must also react to external factors: because agent types are inferred from the observed future trajectory during training, these environments require that the contributions of internal and external factors to the agent behaviour are disentangled and only internal factors, i.e., those under the agent's control, are encoded in the type. Encoding future information about external factors leads to inappropriate agent reactions during testing, when the future is unknown and types must be drawn independently from the actual future. We formalize this challenge as distribution shift in the conditional distribution of agent types under environmental stochasticity. We propose Robust Type Conditioning (RTC), which eliminates this shift with adversarial training under randomly sampled types. Experiments on two domains, including the large-scale Waymo Open Motion Dataset, show improved distributional realism while maintaining or improving task performance compared to state-of-the-art baselines.</td>
                <td>Training, Roads, Stochastic processes, Training data, Encoding, Trajectory, Safety</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341451&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341451&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Deep Learning of Robust, Adaptive Policies using Tube MPC-Guided Data Augmentation</td>
                <td>T. Zhao, A. Tagliabue and J. P. How</td>
                <td>2023</td>
                <td>The deployment of agile autonomous systems in challenging, unstructured environments requires adaptation capabilities and robustness to uncertainties. Existing robust and adaptive controllers, such as those based on model predictive control (MPC), can achieve impressive performance at the cost of heavy online onboard computations. Strategies that efficiently learn robust and onboard-deployable policies from MPC have emerged, but they still lack fundamental adaptation capabilities. In this work, we extend an existing efficient Imitation Learning (IL) algorithm for robust policy learning from MPC with the ability to learn policies that adapt to challenging model/environment uncertainties. The key idea of our approach consists in modifying the IL procedure by conditioning the policy on a learned lower-dimensional model/environment representation that can be efficiently estimated online. We tailor our approach to the task of learning an adaptive position and attitude control policy to track trajectories under challenging disturbances on a multirotor. Evaluations in simulation show that a high-quality adaptive policy can be obtained in about 1.3 hours. We additionally empirically demonstrate rapid adaptation to in- and out-of-training-distribution uncertainties, achieving a 6.1 cm average position error under wind disturbances that correspond to about 50% of the weight of the robot, and that are 36% larger than the maximum wind seen during training.</td>
                <td>Training, Adaptation models, Uncertainty, Attitude control, Data augmentation, Robustness, Electron tubes</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341998&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341998&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Masked Imitation Learning: Discovering Environment-Invariant Modalities in Multimodal Demonstrations</td>
                <td>Y. Hao, R. Wang, Z. Cao, Z. Wang, Y. Cui and D. Sadigh</td>
                <td>2023</td>
                <td>Multimodal demonstrations provide robots with an abundance of information to make sense of the world. However, such abundance may not always lead to good performance when it comes to learning sensorimotor control policies from human demonstrations. Extraneous data modalities can lead to state over-specification, where the state contains modalities that are not only useless for decision-making but also can change data distribution across environments. State over-specification leads to issues such as the learned policy not generalizing outside of the training data distribution. In this work, we propose Masked Imitation Learning (MIL) to address state over-specification by selectively using informative modalities. Specifically, we design a masked policy network with a binary mask to block certain modalities. We develop a bi-level optimization algorithm that learns this mask to accurately filter over-specified modalities. We demonstrate empirically that MIL outperforms baseline algorithms in simulated domains and effectively recovers the environment-invariant modalities on a multimodal dataset collected on a real robot. Videos and supplemental details are at: https://tinyurl.com/masked-il</td>
                <td>Decision making, Training data, Filtering algorithms, Robot sensing systems, Optimization, Intelligent robots, Videos</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341728&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341728&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Does Unpredictability Influence Driving Behavior?</td>
                <td>S. Samavi, F. Shkurti and A. P. Schoellig</td>
                <td>2023</td>
                <td>In this paper we investigate the effect of the unpredictability of surrounding cars on an ego-car performing a driving maneuver. We use Maximum Entropy Inverse reinforcement Learning to model reward functions for an ego-car conducting a lane change in a highway setting. We define a new feature based on the unpredictability of surrounding cars and use it in the reward function. We learn two reward functions from human data: a baseline and one that incorporates our defined unpredictability feature, then compare their performance with a quantitative and qualitative evaluation. Our evaluation demonstrates that incorporating the unpredictability feature leads to a better fit of human-generated test data. These results encourage further investigation of the effect of unpredictability on driving behavior.</td>
                <td>Road transportation, Measurement, Reinforcement learning, Predictive models, Entropy, Trajectory, Behavioral sciences</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342534&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342534&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>From Temporal-Evolving to Spatial-Fixing: A Keypoints-Based Learning Paradigm for Visual Robotic Manipulation</td>
                <td>K. Riou, K. Dong, K. Subrin, Y. Sun and P. L. Callet</td>
                <td>2023</td>
                <td>The current learning pipelines for robotics manipulation infer movement primitives sequentially along the temporal-evolving axis, which can result in an accumulation of prediction errors and subsequently cause the visual observations to fall out of the training distribution. This paper proposes a novel hierarchical behavior cloning approach which tries to dissociate standard behaviour cloning (BC) pipeline to two stages. The intuition of this approach is to eliminate accumu-lation errors using a fixed spatial representation. At first stage, a high-level planner will be employed to translate the initial observation of the scene into task-specific spatial waypoints. Then, a low-level robotic path planner takes over the task of guiding the robot by executing a set of pre-defined elementary movements or actions known as primitives, with the goal of reaching the previously predicted waypoints. Our hierarchical keypoints-based paradigm aims to simplify existing temporal-evolving approach to a more simple way: directly spatialize the whole sequential primitives as a set of 8D waypoints only from the very first observation. Plentiful experiments demon-strate that our paradigm can achieve comparable results with Reinforcement Learning (RL) and outperforms existing offline BC approaches, with only a single-shot inference from the initial observation. Code and models are available at: https://github.com/KevinRiou22/spatial-fixing-il</td>
                <td>Training, Visualization, Pipelines, Cloning, Reinforcement learning, Behavioral sciences, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341397&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341397&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Accurate and Interactive Visual-Inertial Sensor Calibration with Next-Best-View and Next-Best-Trajectory Suggestion</td>
                <td>C. L. Choi, B. Xu and S. Leutenegger</td>
                <td>2023</td>
                <td>Visual-Inertial (VI) sensors are popular in robotics, self-driving vehicles, and augmented and virtual reality applications. In order to use them for any computer vision or state-estimation task, a good calibration is essential. However, collecting informative calibration data in order to render the calibration parameters observable is not trivial for a non-expert. In this work, we introduce a novel VI calibration pipeline that guides a non-expert with the use of a graphical user interface and information theory in collecting informative calibration data with Next-Best-View and Next-Best-Trajectory suggestions to calibrate the intrinsics, extrinsics, and temporal misalignment of a VI sensor. We show through experiments that our method is faster, more accurate, and more consistent than state-of-the-art alternatives. Specifically, we show how calibrations with our proposed method achieve higher accuracy estimation results when used by state-of-the-art VI Odometry as well as VI-SLAM approaches. The source code of our software can be found on: https://github.com/chutsu/yac.</td>
                <td>Visualization, Simultaneous localization and mapping, Source coding, Software algorithms, Virtual reality, Software, Calibration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341815&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341815&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A ROS-Based Kinematic Calibration Tool for Serial Robots</td>
                <td>C. Pascal, O. Doaré and A. Chapoutot</td>
                <td>2023</td>
                <td>The use of serial robots for industrial and research purposes is often limited by a flawed positioning accuracy, caused by the differences between the robot nominal model, and the real one. Such an issue can be solved by means of kinematic calibration, which is usually a tedious and intricate task. In this paper, we propose a complete kinematic calibration procedure relying on established geometric modeling, measurements design and parameters identification methods, as well as multiple integration tools, to provide a high adaptability and a simplified handling. The overall process was bundled up in a ROS-based modular and user-friendly package, whose main objective is to offer a smooth and fully integrated framework for the kinematic calibration of serial robots. Our solution was successfully tested using a motion tracking device, and allowed to increase the overall positioning accuracy of two different serial robots by 75% in a matter of hours.</td>
                <td>Parameter estimation, Service robots, Tracking, Geometric modeling, Kinematics, Calibration, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341692&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341692&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>FUSE-D: Framework for UAV System-Parameter Estimation with Disturbance Detection</td>
                <td>C. Böhm and S. Weiss</td>
                <td>2023</td>
                <td>Modern unmanned aerial vehicles (UAVs) with sophisticated mechanics ask for extended online system identification to aid model-based controls in task execution. In addition, UAVs in adverse environmental conditions require a more detailed environmental disturbance understanding. The necessary combination of online system identification, sensor suite self-calibration, and external disturbance analysis to tackle these issues holistically is currently an open issue. Our proposed FUSE-D approach combines these elements based on a system model at the rotor-speed level and a single global pose sensor (e.g., a tracking system like Optitrack). Besides sensor intrinsics and extrinsics, the framework allows estimating the UAV's rotor geometry, mass, moments of inertia, and the rotors' aerodynamic properties, as well as an external force and where it acts on the UAV. The general formulation allows us to extend the approach to an N-rotor (multi-rotor) UAV and classify the type of external disturbance. We perform a detailed non-linear observability analysis for the 43 + 7N states and do a statistically relevant embedded hardware-in-the-loop performance analysis in the realistic simulation environment Gazebo with RotorS.</td>
                <td>Analytical models, Three-dimensional displays, Force, Rotors, Autonomous aerial vehicles, Robot sensing systems, System identification</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341818&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341818&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Multiplanar Self-Calibration for Mobile Cobot 3D Object Manipulation Using 2D Detectors and Depth Estimation</td>
                <td>T. Dang, K. Nguyen and M. Huber</td>
                <td>2023</td>
                <td>Calibration is the first and foremost step in dealing with sensor displacement errors that can appear during extended operation and off-time periods to enable robot object manipulation with precision. In this paper, we present a novel multiplanar self-calibration between the camera system and the robot's end-effector for 3D object manipulation. Our approach first takes the robot end-effector as ground truth to calibrate the camera's position and orientation while the robot arm moves the object in multiple planes in 3D space, and a 2D state-of-the-art vision detector identifies the object's center in the image coordinates system. The transformation between world coordinates and image coordinates is then computed using 2D pixels from the detector and 3D known points obtained by robot kinematics. Next, an integrated stereo-vision system estimates the distance between the camera and the object, resulting in 3D object localization. We test our proposed method on the Baxter robot with two 7-DOF arms and a 2D detector that can run in real time on an onboard GPU. After self-calibrating, our robot can localize objects in 3D using an RGB camera and depth image. The source code is available at https://github.com/tuantdang/calib_cobot.</td>
                <td>Three-dimensional displays, Robot kinematics, Source coding, Robot vision systems, Detectors, Cameras, End effectors</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341911&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341911&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Labelling Lightweight Robot Energy Consumption: A Mechatronics-Based Benchmarking Metric Set</td>
                <td>J. Heredia, R. J. Kirschner, C. Schlette, S. Abdolshah, S. Haddadin and M. B. Kjærgaard</td>
                <td>2023</td>
                <td>Compliance with global guidelines for sustainable and responsible production in modern industry requires a comparative analysis of consumer devices' energy consumption (EC). This also holds true for the newly established generation of lightweight industrial robots (LIRs). To identify potential strategies for energy optimization, standardized benchmarking procedures are required. However, to the best of the authors' knowledge, there is currently no standardized method for benchmarking the EC of manipulators. In response to this need, we have developed a comprehensive benchmarking framework to evaluate the EC of various LIR designs, delving into the theoretical power consumption under both static and dynamic conditions. Our analysis has led to the proposal of seven proposed metrics—three static and four dynamic. The static metrics—controller consumption, joint electronics consumption, and mechanical brakes' consumption—evaluate the maintenance EC of the robot. Meanwhile, we suggest three dynamic metrics that gauge the system's energy efficiency during motion, with or without payload. We extend this metrics selection by introducing the cost of transportation map for manipulators. For each of the metrics, we suggest a standardized measurement procedure based on state-of-the-art norms and literature. The metric set and experimental procedures are demonstrated using five manipulators (UR3e, UR5e, FR3, M0609, Gen3). Among the results, we can see interesting trends for future optimization of the electronic components and their architecture, e.g., reducing the robot's EC by decentralizing computation via low-consumption onboard controllers for basic tasks and external servers for complex ones.</td>
                <td>Measurement, Costs, Dynamics, Transportation, Electronic components, Benchmark testing, Robot sensing systems</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341484&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341484&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>The Role of Absolute Positioning Error in Hand-Eye Calibration and Robotic Guidance Systems: An Analysis</td>
                <td>M. Chaluš, O. Vaníček and J. Liška</td>
                <td>2023</td>
                <td>Robotic manipulators deal with serious issues due to their absolute positioning error. This error is usually compensated by an operator in classical robot programming using the teach-and-play method. However, it has a significant effect on accuracy of robotic guidance systems (RGS) that automatically generate process tool trajectory based on the measured data from a sensor. In this paper, we firstly describe the various components of an RGS that affect its overall accuracy. We then introduce a proposed model for the calibration process (MCP) that can be used to analyze the effect of absolute positioning errors on the accuracy of hand-eye calibration, six-point calibration of a process tool and mutual transformation between these tools. Simulations were used to evaluate the proposed MCP model. The results of this analysis are crucial for the practical use of RGS.</td>
                <td>Analytical models, Robot kinematics, Welding, Vision sensors, Robot sensing systems, Calibration, Trajectory</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342337&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342337&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Robotic Kinematic Calibration with Only Position Data and Consideration of Non-Geometric Errors Using POE-Based Model and Gaussian Mixture Models</td>
                <td>X. Luo et al.</td>
                <td>2023</td>
                <td>Kinematic calibration is crucial to improve the positioning accuracy of serial robots. This paper proposes a novel algorithm for robotic kinematic calibration based on an augmented product of exponentials (POE)-based kinematic model using Gaussian mixture models (GMMs) with only position data. In this algorithm, non-geometric errors that cannot be fitted by varying the parameters within the traditional robot model are also considered and compensated. This approach involving a three-stage calibration process which is used to identify the kinematic model parameters and to train the GMMs will be presented in this paper. Finally, this algorithm will be applied to two serial robots for simulation and experimental validation. The effectiveness of the proposed algorithm is verified from both results and significant improvement on error reduction from 26 % to 96% can be observed through the comparison with other existing approaches.</td>
                <td>Robot motion, Kinematics, Position measurement, Data models, Calibration, Noise measurement, Robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341731&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341731&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal Calibration</td>
                <td>Q. Herau et al.</td>
                <td>2023</td>
                <td>With the recent advances in autonomous driving and the decreasing cost of LiDARs, the use of multimodal sensor systems is on the rise. However, in order to make use of the information provided by a variety of complimentary sensors, it is necessary to accurately calibrate them. We take advantage of recent advances in computer graphics and implicit volumetric scene representation to tackle the problem of multi-sensor spatial and temporal calibration. Thanks to a new formulation of the Neural Radiance Field (NeRF) optimization, we are able to jointly optimize calibration parameters along with scene representation based on radiometric and geometric measurements. Our method enables accurate and robust calibration from data captured in uncontrolled and unstructured urban environments, making our solution more scalable than existing calibration solutions. We demonstrate the accuracy and robustness of our method in urban scenes typically encountered in autonomous driving scenarios.</td>
                <td>Laser radar, Multimodal sensors, Urban areas, Radiometry, Robustness, Calibration, Spatiotemporal phenomena</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342427&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342427&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Automatic Spatial Radar Camera Calibration via Geometric Constraints with Doppler-Optical Flow Fusion</td>
                <td>J. Ge, Y. Zhou, B. Lou and C. Lv</td>
                <td>2023</td>
                <td>Many intelligent robots use a combination of radar and camera sensors to capture environmental information. Robust and accurate perception highly relies on the result of multi-sensor calibration. Most current spatial calibration methods require a calibration board or a special marker as the target. In this paper, we provide a novel calibration method for RGBD camera and millimeter-wave radar, which automatically estimates the extrinsic parameters. Our proposed method includes the following two stages: rough extrinsic parameters are estimated by using object contours as geometric constraints, and meanwhile, the optimum is reached via optimizing based on the difference of velocity obtained from camera and radar. It only needs an object moving past sensors, but does not require for a calibration board. We validate our method through simulation experiments and real-world experiments. We construct a simulation environment in CARLA to verify the performance of our proposed method against different angles. Furthermore, different levels of zero mean Gaussian noise are added to evaluate the stability of our method. In addition, real-world experiments with different hardware setups are taken to verify the feasibility of our method in real-world conditions.</td>
                <td>Gaussian noise, Robot vision systems, Millimeter wave radar, Streaming media, Cameras, Stability analysis, Calibration</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342101&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342101&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Extrinsic Calibration of Camera to LIDAR Using a Differentiable Checkerboard Model</td>
                <td>L. F. T. Fu, N. Chebrolu and M. Fallon</td>
                <td>2023</td>
                <td>Multi-modal sensing often involves determining correspondences between each domain's signals, which in turn depends on the accurate extrinsic calibration of the sensors. Challengingly, the camera-LIDAR sensor modalities are quite dissimilar and the narrow field of view of most commercial LIDARs means that they observe only a partial view of the camera frustum. We present a framework for extrinsic calibration of a camera and a LIDAR using only a simple off-the-shelf checkerboard. It is designed to operate even when the LIDAR observes a significantly truncated portion of the checkerboard. Current state-of-the-art methods often require bespoke manufactured markers or full observation of the entire checkerboard in both camera and LIDAR data which is prohibitive. By contrast, our novel algorithm directly aligns the LIDAR intensity pattern to the camera-detected checkerboard pattern using our differentiable formulation. The key step for achieving accurate extrinsics estimation is the use of the spatial derivatives provided by the differentiable checkerboard pattern, and jointly optimizing over all views. In our experiments, we achieve calibration accuracy in the order of 2–4 mm and demonstrate a 30% error reduction compared to state-of-the-art approaches. We are able to achieve this improvement while using only partial LIDAR views of the checkerboard that allows for a simpler data capture process. We also demonstrate the generalizability of our approach to different combinations of LIDARs and cameras with varying sparsity patterns and noise levels.</td>
                <td>Laser radar, Robot vision systems, Estimation, Cameras, Calibration, Sensors, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341781&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341781&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Graph-Based Visual-Kinematic Fusion and Monte Carlo Initialization for Fast-Deployable Cable-Driven Robots</td>
                <td>R. Khorrambakht et al.</td>
                <td>2023</td>
                <td>Ease of calibration and high-accuracy task-space state-estimation purely based on onboard sensors is a key requirement for enabling easily deployable cable robots in real-world applications. In this work, we incorporate the onboard camera and kinematic sensors to drive a statistical fusion framework that presents a unified localization and calibration system which requires no initial values for the kinematic parameters. This is achieved by formulating a Monte-Carlo algorithm that initializes a factor-graph representation of the calibration and localization problem. With this, we are able to jointly identify both the kinematic parameters and the visual odometry scale alongside their corresponding uncertainties. We demonstrate the practical applicability of the framework using our state-estimation dataset recorded with the ARAS-CAM suspended cable driven parallel robot, and published as part of this manuscript.</td>
                <td>Location awareness, Monte Carlo methods, Uncertainty, Robot vision systems, Kinematics, Sensor fusion, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342316&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342316&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>P2O-Calib: Camera-LiDAR Calibration Using Point-Pair Spatial Occlusion Relationship</td>
                <td>S. Wang, S. Zhang and X. Qiu</td>
                <td>2023</td>
                <td>The accurate and robust calibration result of sensors is considered as an important building block to the follow-up research in the autonomous driving and robotics domain. The current works involving extrinsic calibration between 3D LiDARs and monocular cameras mainly focus on target-based and target-less methods. The target-based methods are often utilized offline because of restrictions, such as additional target design and target placement limits. The current target-less methods suffer from feature indeterminacy and feature mismatching in various environments. To alleviate these limitations, we propose a novel target-less calibration approach which is based on the 2D-3D edge point extraction using the occlusion relationship in 3D space. Based on the extracted 2D-3D point pairs, we further propose an occlusion-guided point-matching method that improves the calibration accuracy and reduces computation costs. To validate the effectiveness of our approach, we evaluate the method performance qualitatively and quantitatively on real images from the KITTI dataset. The results demonstrate that our method outperforms the existing target-less methods and achieves low error and high robustness that can contribute to the practical applications relying on high-quality Camera-LiDAR calibration.</td>
                <td>Point cloud compression, Three-dimensional displays, Laser radar, Image edge detection, Robot vision systems, Feature extraction, Robustness</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341416&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341416&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Wrench Estimation of Modular Manipulator with External Actuation and Joint Locking</td>
                <td>Y. Kim, H. Lee, J. Lee and D. Lee</td>
                <td>2023</td>
                <td>This paper proposes an external wrench estimation method for modular manipulators, where each link module is driven with external actuation (e.g., rotors, thrusters) and inter-module joints can be locked to increase end-effector stiffness or workforce of the manipulator. For such systems, the commonly-used momentum-based observer (MBO [1]) is not suitable due to the presence of unknown joint locking (JL) torque and also the degeneracy of Jacobian transpose relation with the system degree-of-freedom (DOF) becoming less than six with the joint locking. To overcome this, we propose two novel external wrench estimation algorithms: a distributed algorithm based on recursive Newton-Euler dynamics and a centralized algorithm based on D'Alembert's principle, both using an F/T (force/torque) sensor at the base. Experiments are conducted to demonstrate the effectiveness of the proposed algorithms.</td>
                <td>Jacobian matrices, Torque, Heuristic algorithms, Dynamics, Estimation, Rotors, Observers</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341887&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341887&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>AV-PedAware: Self-Supervised Audio-Visual Fusion for Dynamic Pedestrian Awareness</td>
                <td>Y. Yang, S. Yuan, M. Cao, J. Yang and L. Xie</td>
                <td>2023</td>
                <td>In this study, we introduce AV-PedAware, a self-supervised audio-visual fusion system designed to improve dynamic pedestrian awareness for robotics applications. Pedestrian awareness is a critical requirement in many robotics applications. However, traditional approaches that rely on cameras and LIDARs to cover multiple views can be expensive and susceptible to issues such as changes in illumination, occlusion, and weather conditions. Our proposed solution replicates human perception for 3D pedestrian detection using low-cost audio and visual fusion. This study represents the first attempt to employ audio-visual fusion to monitor footstep sounds for the purpose of predicting the movements of pedestrians in the vicinity. The system is trained through self-supervised learning based on LIDAR-generated labels, making it a cost-effective alternative to LIDAR-based pedestrian awareness. AV-PedAware achieves comparable results to LIDAR-based systems at a fraction of the cost. By utilizing an attention mechanism, it can handle dynamic lighting and occlusions, overcoming the limitations of traditional LIDAR and camera-based systems. To evaluate our approach's effectiveness, we collected a new multimodal pedestrian detection dataset and conducted experiments that demonstrate the system's ability to provide reliable 3D detection results using only audio and visual data, even in extreme visual conditions. We will make our collected dataset and source code available online for the community to encourage further development in the field of robotics perception systems.</td>
                <td>Training, Visualization, Pedestrians, Three-dimensional displays, Laser radar, Source coding, Semantic segmentation, Pedestrian Awareness, LIDAR, Audio-video fusion, Self-supervise</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342257&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342257&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>A Multitask and Kernel Approach for Learning to Push Objects with a Target-Parameterized Deep Q-Network</td>
                <td>M. Odobez</td>
                <td>2023</td>
                <td>Pushing is an essential motor skill involved in several manipulation tasks, and has been an important research topic in robotics. Recent works have shown that Deep Q-Networks (DQNs) can learn pushing policies (when, where to push, and how) to solve manipulation tasks, potentially in synergy with other skills (e.g. grasping). Nevertheless, DQNs often assume a fixed setting and task, which may limit their deployment in practice. Furthermore, they suffer from sparse-gradient backpropagation when the action space is very large, a problem exacerbated by the fact that they are trained to predict state-action values based on a single reward function aggregating several facets of the task, rendering the model training challenging. To address these issues, we propose a multi-head target-parameterized DQN to learn robotic manipulation tasks, in particular pushing policies, and make the following contributions: i) we show that learning to predict different reward and task aspects can be beneficial compared to predicting a single value function where reward factors are not disentangled; ii) we study several alternatives to generalize a policy by encoding the target parameters either into the network layers or visually in the input; iii) we propose a kernelized version of the loss function, allowing to obtain better, faster and more stable training performance. Extensive experiments on simulations validate our design choices, and we show that our architecture learned on simulated data can achieve high performance in a real-robot setup involving a Franka Emika robot arm and unseen objects.</td>
                <td>Training, Grasping, Predictive models, Rendering (computer graphics), Manipulators, Encoding, Data models</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341729&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341729&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>DRKF: Distilled Rotated Kernel Fusion for Efficient Rotation Invariant Descriptors in Local Feature Matching</td>
                <td>R. Huang, J. Cai, C. Li, Z. Wu, X. Liu and Z. Chai</td>
                <td>2023</td>
                <td>The performance of local feature descriptors degrades in the presence of large rotation variations. To address this issue, we present an efficient approach to learning rotation invariant descriptors. Specifically, we propose Rotated Kernel Fusion (RKF) which imposes rotations on the convolution kernel to improve the inherent nature of CNN. Since RKF can be processed by the subsequent re-parameterization, no extra computational costs will be introduced in the inference stage. Moreover, we present Multi-oriented Feature Aggregation (MOFA) which aggregates features extracted from multiple rotated versions of the input image and can provide auxiliary knowledge for the training of RKF by leveraging the distillation strategy. We refer to the distilled RKF model as DRKF. Besides the evaluation on a rotation-augmented version of the public dataset HPatches, we also contribute a new dataset named DiverseBEV which is collected during the drone's flight and consists of bird's eye view images with large viewpoint changes and camera rotations. Extensive experiments show that our method can outperform other state-of-the-art techniques when exposed to large rotation variations.</td>
                <td>Training, Convolution, Shape, Computational modeling, Aggregates, Feature extraction, Cameras</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341994&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341994&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Efficient Q-Learning over Visit Frequency Maps for Multi-Agent Exploration of Unknown Environments</td>
                <td>X. Chen, A. N. Iyer, Z. Wang and A. H. Qureshi</td>
                <td>2023</td>
                <td>The robot exploration task has been widely studied with applications spanning from novel environment mapping to item delivery. For some time-critical tasks, such as rescue catastrophes, the agent is required to explore as efficiently as possible. Recently, Visit Frequency-based map representation achieved great success in such scenarios by discouraging repetitive visits with a frequency-based penalty. However, its relatively large size and single-agent settings hinder its further development. In this context, we propose Integrated Visit Frequency Map, which encodes identical information as Visit Frequency Map with a more compact size, and a visit frequency-based multi-agent information exchange and control scheme that is able to accommodate both representations. Through tests in diverse settings, the results indicate our proposed methods can achieve a comparable level of performance of VFM with lower bandwidth requirements and generalize well to different multi-agent setups including real-world environments.</td>
                <td>Solid modeling, Three-dimensional displays, Q-learning, Scalability, Encoding, Time factors, Task analysis</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341899&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341899&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Real-Time Trajectory-Based Social Group Detection</td>
                <td>S. Jahangard, M. Hayat and H. Rezatofighi</td>
                <td>2023</td>
                <td>Social group detection is a crucial aspect of various robotic applications, including robot navigation and human-robot interactions. To date, a range of model-based techniques have been employed to address this challenge, such as the F-formation and trajectory similarity frameworks. However, these approaches often fail to provide reliable results in crowded and dynamic scenarios. Recent advancements in this area have mainly focused on learning-based methods, such as deep neural networks that use visual content or human pose. Although visual content based methods have demonstrated promising performance on large-scale datasets, their computational complexity poses a significant barrier to their practical use in real-time applications. To address these issues, we propose a simple and efficient framework for social group detection. Our approach explores the impact of motion trajectory on social grouping and utilizes a novel, reliable, and fast data-driven method. We formulate the individuals in a scene as a graph, where the nodes are represented by LSTM-encoded trajectories and the edges are defined by the distances between each pair of tracks. Our framework employs a modified graph transformer module and graph clustering losses to detect social groups. Our experiments on the popular JRDB-Act dataset reveal noticeable improvements in performance, with relative improvements ranging from 2% to 11%. Furthermore, our framework is significantly faster, with up to 12x faster inference times compared to state-of-the-art methods under the same computation resources. These results demonstrate that our proposed method is suitable for real-time robotic applications. .</td>
                <td>Visualization, Social groups, Tracking, Navigation, Image edge detection, Robot kinematics, Transformers, Social grouping, Graph transformers, Motion behaviour, Robot perception</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342121&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342121&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Point2Point: A Framework for Efficient Deep Learning on Hilbert Sorted Point Clouds with Applications in Spatio-Temporal Occupancy Prediction</td>
                <td>A. A. Pandhare</td>
                <td>2023</td>
                <td>The irregularity and permutation invariance of point cloud data pose challenges for effective learning. Conventional methods for addressing this issue involve converting raw point clouds to intermediate representations such as 3D voxel grids or range images. While such intermediate representations solve the problem of permutation invariance, they can result in significant loss of information. Approaches that do learn on raw point clouds either have trouble in resolving neighborhood relationships between points or are too complicated in their formulation. In this paper, we propose a novel approach to representing point clouds as a locality preserving 1D ordering induced by the Hilbert space-filling curve. We also introduce Point2Point, a neural architecture that can effectively learn on Hilbert-sorted point clouds. We show that Point2Point shows competitive performance on point cloud segmentation and generation tasks. Finally, we show the performance of Point2Point on Spatio-temporal Occupancy prediction from Point clouds.</td>
                <td>Point cloud compression, Deep learning, Three-dimensional displays, Image resolution, Computer architecture, Real-time systems, Hardware</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341640&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10341640&isnumber=10341342</a></td>
            </tr>
        
            <tr>
                <td>Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models</td>
                <td>J. Carvalho, A. T. Le, M. Baierl, D. Koert and J. Peters</td>
                <td>2023</td>
                <td>Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multi-modality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several baselines in simulated planar robot and 7-dof robot arm manipulator environments. To assess the generalization capabilities of our method, we test it in environments with previously unseen obstacles. Our experiments show that diffusion models are strong priors to encode high-dimensional trajectory distributions of robot motions. https://sites.google.com/view/mp-diffusion</td>
                <td>Robot motion, Noise reduction, Manipulators, Planning, Task analysis, Trajectory optimization, Intelligent robots</td>
                <td><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342382&isnumber=10341342">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10342382&isnumber=10341342</a></td>
            </tr>
        
        </table>
    </body>
    </html>
    