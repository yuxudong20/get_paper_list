M. F. Ginting et al., "Safe and Efficient Navigation in Extreme Environments using Semantic Belief Graphs," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5653-5658, doi: 10.1109/ICRA48891.2023.10161056.Abstract: To achieve autonomy in unknown and unstruc-tured environments, we propose a method for semantic-based planning under perceptual uncertainty. This capability is cru-cial for safe and efficient robot navigation in environment with mobility-stressing elements that require terrain-specific locomotion policies. We propose the Semantic Belief Graph (SBG), a geometric- and semantic-based representation of a robot's probabilistic roadmap in the environment. The SBG nodes comprise of the robot geometric state and the semantic-knowledge of the terrains in the environment. The SBG edges represent local semantic-based controllers that drive the robot between the nodes or invoke an information gathering action to reduce semantic belief uncertainty. We formulate a semantic-based planning problem on SBG that produces a policy for the robot to safely navigate to the target location with min-imal traversal time. We analyze our method in simulation and present real-world results with a legged robotic platform navigating multi-level outdoor environments. keywords: {Legged locomotion;Analytical models;Uncertainty;Automation;Navigation;Semantics;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161056&isnumber=10160212

S. Jiwani, X. Li, S. Karaman and D. Rus, "Risk-Aware Neural Navigation From BEV Input for Interactive Driving," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5659-5665, doi: 10.1109/ICRA48891.2023.10161473.Abstract: Safety has been a key goal for autonomous driving since its inception, and we believe recognizing and responding to risk is a key component of safety. In this work, we aim to answer the question, “How can explainable risk representations be generated and used to produce risk-averse trajectories?” To answer this question, previous work uses risk metrics to formulate an optimization problem. In contrast, our work is based on research showing the usefulness of grids as a representation to generate image-based risk maps through a trained neural network. We propose a method of determining risk from a bird's eye view (BEV) of an autonomous vehicle's surroundings. Our method consists of (1) a risk map generator, which is trained to recognize risk associated with nearby agents and the map, (2) differentiable value iteration using the risk map to learn a policy, and (3) a trajectory sampler, which samples from this policy to generate a trajectory. We evaluate our planner in a close-loop manner and find improvements in its overall ability to mimic human driving while maintaining comparable safety statistics. Self-ablation also reveals the potential for fine-tuning the behavior of the planner given a designer's needs. keywords: {Training;Measurement;Uncertainty;Navigation;Neural networks;Robot sensing systems;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161473&isnumber=10160212

J. -K. Huang, Y. Tan, D. Lee, V. R. Desaraju and J. W. Grizzle, "Informable Multi-Objective and Multi-Directional RRT* System for Robot Path Planning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5666-5673, doi: 10.1109/ICRA48891.2023.10160838.Abstract: Multi-objective or multi-destination path planning is crucial for mobile robotics applications such as mobility as a service, robotics inspection, and electric vehicle charging for long trips. This work proposes an anytime iterative system to concurrently solve the multi-objective path planning problem and determine the visiting order of destinations. The system is comprised of an anytime informable multi-objective and multi-directional RRT* algorithm to form a simple connected graph, and a solver that consists of an enhanced cheapest insertion algorithm and a genetic algorithm to solve approximately the relaxed traveling salesman problem in polynomial time. Moreover, a list of waypoints is often provided for robotics inspection and vehicle routing so that the robot can preferentially visit certain equipment or areas of interest. We show that the proposed system can inherently incorporate such knowledge to navigate challenging topology. The proposed anytime system is evaluated on large and complex graphs built for real-world driving applications. C++ implementations are available at: https://github.com/UMich-BipedLab/IMOMD-RRTStar. keywords: {Mobility as a service;Navigation;Vehicle routing;Inspection;Traveling salesman problems;Approximation algorithms;Path planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160838&isnumber=10160212

J. Yamada, C. -M. Hung, J. Collins, I. Havoutis and I. Posner, "Leveraging Scene Embeddings for Gradient-Based Motion Planning in Latent Space," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5674-5680, doi: 10.1109/ICRA48891.2023.10161427.Abstract: Motion planning framed as optimisation in structured latent spaces has recently emerged as competitive with traditional methods in terms of planning success while significantly outperforming them in terms of computational speed. However, the real-world applicability of recent work in this domain remains limited by the need to express obstacle information directly in state-space, involving simple geometric primitives. In this work we address this challenge by leveraging learned scene embeddings together with a generative model of the robot manipulator to drive the optimisation process. In addition, we introduce an approach for efficient collision checking which directly regularises the optimisation undertaken for planning. Using simulated as well as real-world experiments, we demonstrate that our approach, AMP-LS, is able to successfully plan in novel, complex scenes while outperforming traditional planning baselines in terms of computation speed by an order of magnitude. We show that the resulting system is fast enough to enable closed-loop planning in real-world dynamic scenes. keywords: {Automation;Computational modeling;Dynamics;Manipulators;Planning;Collision avoidance;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161427&isnumber=10160212

S. Li and N. T. Dantam, "Sample-Driven Connectivity Learning for Motion Planning in Narrow Passages," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5681-5687, doi: 10.1109/ICRA48891.2023.10161339.Abstract: Sampling-based motion planning works well in many cases but is less effective if the configuration space has narrow passages. In this paper, we propose a learning-based strategy to sample in these narrow passages, which improves overall planning time. Our algorithm first learns from the configuration space planning graphs and then uses the learned information to effectively generate narrow passage samples. We perform experiments in various 6D and 7D scenes. The algorithm offers one order of magnitude speed-up compared to baseline planners in some of these scenes. keywords: {Three-dimensional displays;Automation;Navigation;Kinematics;Manipulators;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161339&isnumber=10160212

M. A. V. J. Muthugala, S. M. B. P. Samarakoon and M. R. Elara, "Online Coverage Path Planning Scheme for a Size-Variable Robot," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5688-5694, doi: 10.1109/ICRA48891.2023.10160733.Abstract: Coverage Path Planning (CPP) is an essential feature of robots deployed for applications such as lawn mowing, cleaning, painting, and exploration. However, most of the state-of-the-art CPP methods are proposed for fixed-morphology robots, and the coverage performance is limited by physical constraints such as the inaccessibility of narrow spaces. Apart from area coverage, productivity depends on coverage time and energy usage. A robot capable of varying its footprint size could be a solution for improving productivity in these aspects. In addition to that, the environments, where robots are deployed for coverage, are often subjected to changes causing uncertainties. Therefore, this paper proposes an online CPP scheme for a size-variable robot to improve coverage productivity. The navigation planning of the proposed Size-Variable CPP (VSCPP) scheme has been implemented by adapting a Glasius bio-inspired neural network that guides a robot in an efficient path for coverage while coping with dynamic changes. The size variation required for a situation is determined by analyzing a set of occupancy grid maps corresponding to the size steps of the robot. According to the results, the proposed VSCPP can ascertain coverage while coping with dynamic changes in an environment. The reduction of the coverage time due to the size variability is significant compared to a robot with no VSCPP scheme. keywords: {Productivity;Uncertainty;Navigation;Neural networks;Path planning;Cleaning;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160733&isnumber=10160212

N. T. Nguyen, P. T. Gangavarapu, A. Sahrhage, G. Schildbach and F. Ernst, "Navigation with polytopes and B-spline path planner," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5695-5701, doi: 10.1109/ICRA48891.2023.10160561.Abstract: This paper firstly presents our optimal path planning algorithm within a $2\mathrm{D}$ non-convex, polytopic region defined as a sequence of connected convex polytopes. The path is a B-spline curve but being parametrized with its equivalent Bézier representation. By doing this, the local convexity bound of each curve's interval is significantly tighter. Thus, it allows many more possibilities for constraining the entire curve to remain inside the region by using only linear constraints on the control points of the curve. We further guarantee the existence of the valid path by pointing out an algebraic solution. We integrate the algorithm, together with our previously published results, into the Navigation with polytopes toolbox which can be used as a global path planner, compatible with ROS navigation tools. It provides a framework for constructing a polytope map from a standard occupancy gridmap, searching for an appropriate sequence of connected polytopes and finally, planning a minimal-length path with different options on B-spline or Bézier parametrizations. The validation and comparison with existing methods are done using gridmaps collected under Gazebo simulations and real experiments. keywords: {Automation;Navigation;Path planning;Planning;Splines (mathematics);Standards;Path planner;B-spline;Bézier;Polytopes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160561&isnumber=10160212

H. Rahmani, A. N. Kulkarni and J. Fu, "Probabilistic Planning with Partially Ordered Preferences over Temporal Goals," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5702-5708, doi: 10.1109/ICRA48891.2023.10160678.Abstract: In this paper, we study planning in stochastic systems, modeled as Markov decision processes (MDPs), with preferences over temporally extended goals. Prior work on temporal planning with preferences assumes that the user preferences form a total order, meaning that every pair of outcomes are comparable with each other. In this work, we consider the case where the preferences over possible outcomes are a partial order rather than a total order. We first introduce a variant of deterministic finite automaton, referred to as a preference DFA, for specifying the user's preferences over temporally extended goals. Based on the order theory, we translate the preference DFA to a preference relation over policies for probabilistic planning in a labeled MDP. In this treatment, a most preferred policy induces a weak-stochastic nondominated probability distribution over the finite paths in the MDP. The proposed planning algorithm hinges on the construction of a multi-objective MDP. We prove that a weak-stochastic nondominated policy given the preference specification is Pareto-optimal in the constructed multi-objective MDP, and vice versa. Throughout the paper, we employ a running example to demonstrate the proposed preference specification and solution approaches. We show the efficacy of our algorithm using the example with detailed analysis, and then discuss possible future directions. keywords: {Automation;Stochastic systems;Automata;Markov processes;Fasteners;Probabilistic logic;Probability distribution},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160678&isnumber=10160212

D. Chaudhuri and D. A. Shell, "A causal decoupling approach to efficient planning for logistics problems with stateful stochastic demand," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5709-5715, doi: 10.1109/ICRA48891.2023.10160544.Abstract: Future conceptions of agile, just-in-time fabrication, lean and “smart” manufacturing, and a host of allied processes that exploit advanced automation, depend in part on realizing improvements in logistics planning. The present paper hypothesizes that the key to improving flexibility will be the inclusion of sophisticated, time-correlated stochastic models of demand—whether that be demand by end-user consumers directly, or by other down-stream processes. Such dynamic models of demand, unfortunately, can greatly increase the space in which planning occurs when treated, as is common for planning under uncertainty, via the Markov Decision Processes formulation. To tackle this challenge, we identify three aspects that we postulate appear as commonalities in many logistics settings. They lead to an approach for approximate reduction of the planning problem via causal decoupling, which gives a spectrum of solutions where weakening time correlations affords faster optimization. Empirical results on small case studies —in lean manufacturing and commodity routing—show that retaining some limited (but non-zero) amount of temporal structure can provide a useful compromise between quality of the solution obtained and computation required. keywords: {Fabrication;Automation;Uncertainty;Correlation;Markov processes;Routing;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160544&isnumber=10160212

R. B. Ilyes, Q. H. Ho and M. Lahijanian, "Stochastic Robustness Interval for Motion Planning with Signal Temporal Logic," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5716-5722, doi: 10.1109/ICRA48891.2023.10161409.Abstract: In this work, we present a novel robustness measure for continuous-time stochastic trajectories with respect to Signal Temporal Logic (STL) specifications. We show the soundness of the measure and develop a monitor for reasoning about partial trajectories. Using this monitor, we introduce an STL sampling-based motion planning algorithm for robots under uncertainty. Given a minimum robustness requirement, this algorithm finds satisfying motion plans; alternatively, the algorithm also optimizes for the measure. We prove probabilistic completeness and asymptotic optimality of the motion planner with respect to the measure, and demonstrate the effectiveness of our approach on several case studies. keywords: {Uncertainty;Automation;Probabilistic logic;Robustness;Cognition;Trajectory;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161409&isnumber=10160212

Q. H. Ho, Z. N. Sunberg and M. Lahijanian, "Planning with SiMBA: Motion Planning under Uncertainty for Temporal Goals using Simplified Belief Guides," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5723-5729, doi: 10.1109/ICRA48891.2023.10160897.Abstract: This paper presents a new multi-layered algorithm for motion planning under motion and sensing uncertainties for Linear Temporal Logic specifications. We propose a technique to guide a sampling-based search tree in the combined task and belief space using trajectories from a simplified model of the system, to make the problem computationally tractable. Our method eliminates the need to construct fine and accurate finite abstractions. We prove correctness and probabilistic completeness of our algorithm, and illustrate the benefits of our approach on several case studies. Our results show that guidance with a simplified belief space model allows for significant speed-up in planning for complex specifications. keywords: {Uncertainty;Automation;Computational modeling;Search problems;Robot sensing systems;Probabilistic logic;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160897&isnumber=10160212

L. Sharma, M. Everett, D. Lee, X. Cai, P. Osteen and J. P. How, "RAMP: A Risk-Aware Mapping and Planning Pipeline for Fast Off-Road Ground Robot Navigation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5730-5736, doi: 10.1109/ICRA48891.2023.10160602.Abstract: A key challenge in fast ground robot navigation in 3D terrain is balancing robot speed and safety. Recent work has shown that 2.5D maps (2D representations with additional 3D information) are ideal for real-time safe and fast planning. However, the prevalent approach of generating 2D occupancy grids through raytracing makes the generated map unsafe to plan in, due to inaccurate representation of unknown space. Additionally, existing planners such as MPPI do not consider speeds in known free and unknown space separately, leading to slower overall plans. The RAMP pipeline proposed here solves these issues using new mapping and planning methods. This work first presents ground point inflation with persistent spatial memory as a way to generate accurate occupancy grid maps from classified pointclouds. Then we present an MPPI-based planner with embedded variability in horizon, to maximize speed in known free space while retaining cautionary penetration into unknown space. Finally, we integrate this mapping and planning pipeline with risk constraints arising from 3D terrain, and verify that it enables fast and safe navigation using simulations and hardware demonstrations. keywords: {Solid modeling;Three-dimensional displays;Navigation;Pipelines;Random access memory;Real-time systems;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160602&isnumber=10160212

S. Datta and S. Akella, "Prioritized Robotic Exploration with Deadlines: A Comparison of Greedy, Orienteering, and Profitable Tour Approaches," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5737-5743, doi: 10.1109/ICRA48891.2023.10161118.Abstract: This paper addresses the problem of robotic exploration of unknown indoor environments with deadlines. Indoor exploration using mobile robots has typically focused on exploring the entire environment without considering deadlines. The objective of the prioritized exploration in this paper is to rapidly compute the geometric layout of an initially unknown environment by exploring key regions of the environment and returning to the home location within a deadline. This prioritized exploration is useful for time-critical and dangerous environments where rapid robot exploration can provide vital information for subsequent operations. For example, firefighters, for whom time is of the essence, can utilize the map generated by this robotic exploration to navigate a building on fire. In our previous work, we showed that a priority-based greedy algorithm can outperform a cost-based greedy algorithm for exploration under deadlines. This paper models the prioritized exploration problem as an Orienteering Problem (OP) and a Profitable Tour Problem (PTP) in an attempt to generate exploration strategies that can explore a greater percentage of the environment in a given amount of time. The paper presents simulation results on multiple graph-based and Gazebo environments. We found that in many cases the priority-based greedy algorithm performs on par or better than the OP and PTP-based algorithms. We analyze the potential reasons for this counterintuitive result. keywords: {Greedy algorithms;Navigation;Simulation;Layout;Buildings;Semantics;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161118&isnumber=10160212

L. Bramblett, S. Gao and N. Bezzo, "Epistemic Prediction and Planning with Implicit Coordination for Multi-Robot Teams in Communication Restricted Environments," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5744-5750, doi: 10.1109/ICRA48891.2023.10161553.Abstract: In communication restricted environments, a multi-robot system can be deployed to either: i) maintain constant communication but potentially sacrifice operational efficiency due to proximity constraints or ii) allow disconnections to increase environmental coverage efficiency, challenges on how, when, and where to reconnect (rendezvous problem). In this work we tackle the latter problem and notice that most state-of-the-art methods assume that robots will be able to execute a predetermined plan; however system failures and changes in environmental conditions can cause the robots to deviate from the plan with cascading effects across the multi-robot system. This paper proposes a coordinated epistemic prediction and planning framework to achieve consensus without communicating for exploration and coverage, task discovery and completion, and rendezvous applications. Dynamic epistemic logic is the principal component implemented to allow robots to propagate belief states and empathize with other agents. Propagation of belief states and subsequent coverage of the environment is achieved via a frontier-based method within an artificial physics-based framework. The proposed framework is validated with both simulations and experiments with unmanned ground vehicles in various cluttered environments. keywords: {Automation;Robot kinematics;Power system protection;Land vehicles;Planning;Multi-robot systems;Vehicle dynamics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161553&isnumber=10160212

X. Wu, M. El-Shamouty, C. Nitsche and M. F. Huber, "Uncertainty-Guided Active Reinforcement Learning with Bayesian Neural Networks," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5751-5757, doi: 10.1109/ICRA48891.2023.10160686.Abstract: Recent advances in Reinforcement Learning (RL) have made significant contributions in past years by offering intelligent solutions to solve robotic tasks. However, most RL algorithms, especially the model-free RL, are plagued by low learning efficiency and safety problems. In this paper, we propose using the Bayesian Neural Networks (BNNs) to guide the agent exploring actively to enhance the learning efficiency in RL and investigate the potential of recognizing safety risks in working environments with uncertainty information. We compare two types of uncertainty quantification methods in both action and state spaces. To validate our method, we visualize the quantified uncertainty in robot environments with or without safety hazards. Moreover, we evaluate the learning efficiency and safety performance of the RL agents learned with BNNs on different robotic tasks. keywords: {Industries;Visualization;Uncertainty;Service robots;Neural networks;Reinforcement learning;Hazards},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160686&isnumber=10160212

D. Jin, J. Park and K. Lee, "Perturbation-Based Best Arm Identification for Efficient Task Planning with Monte-Carlo Tree Search," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5758-5764, doi: 10.1109/ICRA48891.2023.10161169.Abstract: Combining task and motion planning (TAMP) is crucial for intelligent robots to perform complex and long-horizon tasks. In TAMP, many approaches generally employ Monte-Carlo tree search (MCTS) with upper confidence bound (UCB) for task planning to handle exploration-exploitation trade-off and find globally optimal solutions. However, since UCB basically considers the estimation error caused by noise, the error caused by insufficient optimization of the sub-tree is not represented. Hence, UCB-based approaches have the disadvantage of not exploring underestimated sub-trees. To alleviate this issue, we propose a novel tree search method using perturbation-based best-arm identification (PBAI). We theoretically prove the bound of the simple regret of our method and empirically verify that PBAI finds the optimal task plans faster and more efficiently than the existing algorithms. The source code of our proposed algorithm is available at https://github.com/jdj2261/pytamp. keywords: {Estimation error;Monte Carlo methods;Automation;Source coding;Search methods;Perturbation methods;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161169&isnumber=10160212

N. Dhanaraj, S. V. Narayan, S. Nikolaidis and S. K. Gupta, "Contingency-Aware Task Assignment and Scheduling for Human-Robot Teams," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5765-5771, doi: 10.1109/ICRA48891.2023.10160806.Abstract: We consider the problem of task assignment and scheduling for human-robot teams to enable the efficient completion of complex problems, such as satellite assembly. In high-mix, low volume settings, we must enable the human-robot team to handle uncertainty due to changing task requirements, potential failures, and delays to maintain task completion efficiency. We make two contributions: (1) we account for the complex interaction of uncertainty that stems from the tasks and the agents using a multi-agent concurrent MDP framework, and (2) we use Mixed Integer Linear Programs and contingency sampling to approximate action values for task assignment. Our results show that our online algorithm is computationally efficient while making optimal task assignments compared to a value iteration baseline. We evaluate our method on a 24-task representative assembly and a real-world 60-task satellite assembly, and we show that we can find an assignment that results in a near-optimal makespan. keywords: {Uncertainty;Satellites;Automation;Processor scheduling;Computational modeling;Contingency management;Approximation algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160806&isnumber=10160212

K. Elimelech, L. E. Kavraki and M. Y. Vardi, "Extracting generalizable skills from a single plan execution using abstraction-critical state detection," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5772-5778, doi: 10.1109/ICRA48891.2023.10161270.Abstract: Robotic task planning is computationally challenging. To reduce planning cost and support life-long operation, we must leverage prior planning experience. To this end, we address the problem of extracting reusable and generalizable abstract skills from successful plan executions. In previous work, we introduced a supporting framework, allowing us, theoretically, to extract an abstract skill from a single execution and later automatically adapt it and reuse it in new domains. We also proved that, given a library of such skills, we can significantly reduce the planning effort for new problems. Nevertheless, until now, abstract-skill extraction could only be performed manually. In this paper, we finally close the automation loop and explain how abstract skills can be practically and automatically extracted. We start by analyzing the desired qualities of an abstract skill and formulate skill extraction as an optimization problem. We then develop two extraction algorithms, based on the novel concept of abstraction-critical state detection. As we show experimentally, the approach is independent of any planning domain. keywords: {Automation;Costs;Libraries;Planning;Task analysis;Optimization;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161270&isnumber=10160212

S. Paul, W. Li, B. Smyth, Y. Chen, Y. Gel and S. Chowdhury, "Efficient Planning of Multi-Robot Collective Transport using Graph Reinforcement Learning with Higher Order Topological Abstraction," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5779-5785, doi: 10.1109/ICRA48891.2023.10161517.Abstract: Efficient multi-robot task allocation (MRTA) is fundamental to various time-sensitive applications such as disaster response, warehouse operations, and construction. This paper tackles a particular class of these problems that we call MRTA-collective transport or MRTA-CT - here tasks present varying workloads and deadlines, and robots are subject to flight range, communication range, and payload constraints. For large instances of these problems involving 100s-1000's of tasks and 10s-100s of robots, traditional non-learning solvers are often time-inefficient, and emerging learning-based policies do not scale well to larger-sized problems without costly retraining. To address this gap, we use a recently proposed encoder-decoder graph neural network involving Capsule networks and multi-head attention mechanism, and innovatively add topological descriptors (TD) as new features to improve transferability to unseen problems of similar and larger size. Persistent homology is used to derive the TD, and proximal policy optimization is used to train our TD-augmented graph neural network. The resulting policy model compares favorably to state-of-the-art non-learning baselines while being much faster. The benefit of using TD is readily evident when scaling to test problems of size larger than those used in training. keywords: {Training;Scalability;Reinforcement learning;Graph neural networks;Real-time systems;Planning;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161517&isnumber=10160212

K. Gao and J. Yu, "On the Utility of Buffers in Pick-n-Swap Based Lattice Rearrangement," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5786-5792, doi: 10.1109/ICRA48891.2023.10161182.Abstract: We investigate the utility of employing multiple buffers in solving a class of rearrangement problems with pick- n-swap manipulation primitives. In this problem, objects stored randomly in a lattice are to be sorted using a robot arm with k 1 swap spaces or buffers, capable of holding up to $k$ objects on its end-effector simultaneously. On the structural side, we show that the addition of each new buffer brings diminishing returns in saving the end-effector travel distance while holding the total number of pick-n-swap operations at a minimum. This is due to an interesting recursive cycle structure in random m-permutation, where the largest cycle covers over 60% of objects. On the algorithmic side, we propose fast algorithms for 1D and 2D lattice rearrangement problems that can effectively use multiple buffers to boost solution optimality. Numerical experiments demonstrate the efficiency and scalability of our methods, as well as confirm the diminishing return structure as more buffers are employed. Introduction video: https://youtu.be/KtBxoARGaVQ keywords: {Monte Carlo methods;Scalability;Heuristic algorithms;Lattices;Streaming media;Cost function;End effectors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161182&isnumber=10160212

M. Tiger, D. Bergström, S. W. Stranius, E. Holmgren, D. de Leng and F. Heintz, "On-Demand Multi-Agent Basket Picking for Shopping Stores," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5793-5799, doi: 10.1109/ICRA48891.2023.10160398.Abstract: Imagine placing an online order on your way to the grocery store, then being able to pick the collected basket upon arrival or shortly after. Likewise, imagine placing any online retail order, made ready for pickup in minutes instead of days. In order to realize such a low-latency automatic warehouse logistics system, solvers must be made to be basket-aware. That is, it is more important that the full order (the basket) is picked timely and fast, than that any single item in the order is picked quickly. Current state-of-the-art methods are not basket-aware. Nor are they optimized for a positive customer experience, that is; to prioritize customers based on queue place and the difficulty associated with picking their order. An example of the latter is that it is preferable to prioritize a customer ordering a pack of diapers over a customer shopping a larger order, but only as long as the second customer has not already been waiting for too long. In this work we formalize the problem outlined, propose a new method that significantly outperforms the state-of-the-art, and present a new realistic simulated benchmark. The proposed method is demonstrated to work in an on-line and real-time setting, and to solve the on-demand multi-agent basket picking problem for automated shopping stores under realistic conditions. keywords: {Measurement;Automation;Customer satisfaction;Benchmark testing;Real-time systems;Behavioral sciences;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160398&isnumber=10160212

W. Gosrich, S. Mayya, S. Narayan, M. Malencia, S. Agarwal and V. Kumar, "Multi-Robot Coordination and Cooperation with Task Precedence Relationships," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5800-5806, doi: 10.1109/ICRA48891.2023.10160998.Abstract: We propose a new formulation for the multi-robot task planning and allocation problem that incorporates (a) precedence relationships between tasks; (b) coordination for tasks allowing multiple robots to achieve increased efficiency; and (c) cooperation through the formation of robot coalitions for tasks that cannot be performed by individual robots alone. In our formulation, the tasks and the relationships between the tasks are specified by a task graph. We define a set of reward functions over the task graph's nodes and edges. These functions model the effect of robot coalition size on task performance while incorporating the influence of one task's performance on a dependent task. Solving this problem optimally is NP-hard. However, using the task graph formulation allows us to leverage min-cost network flow approaches to obtain approximate solutions efficiently. Additionally, we explore a mixed integer programming approach, which gives optimal solutions for small instances of the problem but is computationally expensive. We also develop a greedy heuristic algorithm as a baseline. Our modeling and solution approaches result in task plans that leverage task precedence relationships and robot coordination and cooperation to achieve high mission performance, even in large missions with many agents. keywords: {Integer programming;Automation;Robot kinematics;Heuristic algorithms;Computational modeling;Approximation algorithms;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160998&isnumber=10160212

M. Iovino, J. Förster, P. Falco, J. J. Chung, R. Siegwart and C. Smith, "On the programming effort required to generate Behavior Trees and Finite State Machines for robotic applications," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5807-5813, doi: 10.1109/ICRA48891.2023.10160972.Abstract: In this paper we provide a practical demonstration of how the modularity in a Behavior Tree (BT) decreases the effort in programming a robot task when compared to a Finite State Machine (FSM). In recent years the way to represent a task plan to control an autonomous agent has been shifting from the standard FSM towards BTs. Many works in the literature have highlighted and proven the benefits of such design compared to standard approaches, especially in terms of modularity, reactivity and human readability. However, these works have often failed in providing a tangible comparison in the implementation of those policies and the programming effort required to modify them. This is a relevant aspect in many robotic applications, where the design choice is dictated both by the robustness of the policy and by the time required to program it. In this work, we compare backward chained BTs with a fault-tolerant design of FSMs by evaluating the cost to modify them. We validate the analysis with a set of experiments in a simulation environment where a mobile manipulator solves an item fetching task. keywords: {Fault tolerance;Costs;Fault tolerant systems;Automata;Programming;Manipulators;Robustness;Behavior Trees;Finite State Machines;Modularity;Mobile Manipulation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160972&isnumber=10160212

G. Sóti, X. Huang, C. Wurll and B. Hein, "Train What You Know – Precise Pick-and-Place with Transporter Networks," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5814-5820, doi: 10.1109/ICRA48891.2023.10161242.Abstract: Precise pick-and-place is essential in robotic applications. To this end, we define an exact training method and an iterative inference method that improve pick-and-place precision with Transporter Networks [1]. We conduct a large scale experiment on 8 simulated tasks. A systematic analysis shows, that the proposed modifications have a significant positive effect on model performance. Considering picking and placing independently, our methods achieve up to 60% lower rotation and translation errors than baselines. For the whole pick-and-place process we observe 50% lower rotation errors for most tasks with slight improvements in terms of translation errors. Furthermore, we propose architectural changes that retain model performance and reduce computational costs and time. We validate our methods with an interactive teaching procedure on real hardware. Supplementary material is available at: https://gergely-soti.github.io/p3 keywords: {Training;Analytical models;Systematics;Shape;Computational modeling;Pipelines;Computational efficiency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161242&isnumber=10160212

C. Gokmen, D. Ho and M. Khansari, "Asking for Help: Failure Prediction in Behavioral Cloning through Value Approximation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5821-5828, doi: 10.1109/ICRA48891.2023.10161004.Abstract: Recent progress in end-to-end Imitation Learning approaches has shown promising results and generalization capabilities on mobile manipulation tasks. Such models are seeing increasing deployment in real-world settings, where scaling up requires robots to be able to operate with high autonomy, i.e. requiring as little human supervision as possible. In order to avoid the need for one-on-one human supervision, robots need to be able to detect and prevent policy failures ahead of time, and ask for help, allowing a remote operator to supervise multiple robots and help when needed. However, the black-box nature of end-to-end Imitation Learning models such as Behavioral Cloning, as well as the lack of an explicit state-value representation, make it difficult to predict failures. To this end, we introduce Behavioral Cloning Value Approximation (BCVA), an approach to learning a state value function based on and trained jointly with a Behavioral Cloning policy that can be used to predict failures. We demonstrate the effectiveness of BCVA by applying it to the challenging mobile manipulation task of latched-door opening, showing that we can identify failure scenarios with with 86% precision and 81 % recall, evaluated on over 2000 real world runs, improving upon the baseline of simple failure classification by 10 percentage-points. keywords: {Representation learning;Costs;Automation;Cloning;Estimation;Closed box;Predictive models;robotics;imitation learning;failure detection;policy evaluation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161004&isnumber=10160212

W. Yang, A. Angleraud, R. S. Pieters, J. Pajarinen and J. -K. Kämäräinen, "Seq2Seq Imitation Learning for Tactile Feedback-based Manipulation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5829-5836, doi: 10.1109/ICRA48891.2023.10161145.Abstract: Robot control for tactile feedback based manip-ulation can be difficult due to modeling of physical contacts, partial observability of the environment, and noise in perception and control. This work focuses on solving partial observability of contact-rich manipulation tasks as a Sequence-to-Sequence (Seq2Seq) Imitation Learning (IL) problem. The proposed Seq2Seq model first produces a robot-environment interaction sequence to estimate the partially observable environment state variables, and then, the observed interaction sequence is transformed to a control sequence for the task itself. The proposed Seq2Seq IL for tactile feedback based manipulation is experimentally validated on a door-open task in a simulated environment and a snap-on insertion task with a real robot. The model is able to learn both tasks from only 50 expert demonstrations while state-of-the-art reinforcement learning and imitation learning methods fail. keywords: {Learning systems;Robot control;Tactile sensors;Reinforcement learning;Transformers;Human in the loop;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161145&isnumber=10160212

K. Shivakumar et al., "SGTM 2.0: Autonomously Untangling Long Cables using Interactive Perception," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5837-5843, doi: 10.1109/ICRA48891.2023.10160574.Abstract: Cables are commonplace in homes, hospitals, and industrial warehouses and are prone to tangling. This paper extends prior work on autonomously untangling long cables by introducing novel uncertainty quantification metrics and actions that interact with the cable to reduce perception uncertainty. We present Sliding and Grasping for Tangle Manipulation 2.0 (SGTM 2.0), a system that autonomously untangles cables approximately 3 meters in length with a bilateral robot using estimates of uncertainty at each step to inform actions. By interactively reducing uncertainty, SGTM 2.0 significantly reduces run-time. Physical experiments with 84 trials suggest that SGTM $2.0$ can achieve 83% untangling success on cables with 1 or 2 overhand and figure-8 knots, and 70% termination detection success across these configurations, outperforming SGTM 1.0 by 43% in untangling accuracy and 200% in completion time. Supplementary material, visualizations, and videos can be found at sites.google.com/view/sgtm2. keywords: {Meters;Measurement;Visualization;Uncertainty;Automation;Hospitals;Grasping},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160574&isnumber=10160212

K. Rohanimanesh, J. Metzger, W. Richards and A. Tamar, "Online Tool Selection with Learned Grasp Prediction Models," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5844-5850, doi: 10.1109/ICRA48891.2023.10160952.Abstract: Deep learning-based grasp prediction models have become an industry standard for robotic bin-picking systems. To maximize pick success, production environments are often equipped with several end-effector tools that can be swapped on-the-fly, based on the target object. Tool-change, however, takes time. Choosing the order of grasps to perform, and corresponding tool-change actions, can improve system throughput; this is the topic of our work. The main challenge in planning tool change is uncertainty - we typically cannot see objects in the bin that are currently occluded. Inspired by queuing and admission control problems, we model the problem as a Markov Decision Process (MDP), where the goal is to maximize expected throughput, and we pursue an approximate solution based on model predictive control, where at each time step we plan based only on the currently visible objects. Special to our method is the idea of void zones, which are geometrical boundaries in which an unknown object will be present, and therefore cannot be accounted for during planning. Our planning problem can be solved using integer linear programming (ILP). However, we find that an approximate solution based on sparse tree search yields near optimal performance at a fraction of the time. Another question that we explore is how to measure the performance of tool-change planning: we find that throughput alone can fail to capture delicate and smooth behavior, and propose a principled alternative. Finally, we demonstrate our algorithms on both synthetic and real world bin picking tasks. keywords: {Uncertainty;Service robots;Production;Predictive models;Throughput;Prediction algorithms;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160952&isnumber=10160212

S. Kang and C. Choi, "FOGL: Federated Object Grasping Learning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5851-5857, doi: 10.1109/ICRA48891.2023.10161191.Abstract: Federated learning is a promising technique for training global models in a data-decentralized environment. In this paper, we propose a federated learning approach for robotic object grasping. The main challenge is that the data collected by multiple robots deployed in different environments tends to form heterogeneous data distributions (i.e., non-IID) and that the existing federated learning methods on such data distributions show serious performance degradation. To tackle this problem, we propose federated object grasping learning (FOGL) that uses cross-evaluation in a general federated learning process to assess the training performance of robots. We cluster robots with similar training patterns and perform independent federated learning on each cluster. Finally, we integrate the global models for each cluster through an ensemble inference. We apply FOGL to various federated learning scenarios in robotic object grasping and show state-of-the-art performance on the Cornell grasping dataset. keywords: {Training;Degradation;Automation;Federated learning;Clustering algorithms;Grasping;Inference algorithms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161191&isnumber=10160212

K. Takahashi and T. Taniguchi, "Goal-Image Conditioned Dynamic Cable Manipulation through Bayesian Inference and Multi-Objective Black-Box Optimization," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5858-5864, doi: 10.1109/ICRA48891.2023.10160884.Abstract: To perform dynamic cable manipulation to realize the configuration specified by a target image, we formulate dynamic cable manipulation as a stochastic forward model. Then, we propose a method to handle uncertainty by maximizing the expectation, which also considers estimation errors of the trained model. To avoid issues like multiple local minima and requirement of differentiability by gradient-based methods, we propose using a black-box optimization (BBO) to optimize joint angles to realize a goal image. Among BBO, we use the Tree-structured Parzen Estimator (TPE), a type of Bayesian optimization. By incorporating constraints into the TPE, the optimized joint angles are constrained within the range of motion. Since TPE is population-based, it is better able to detect multiple feasible configurations using the estimated inverse model. We evaluated image similarity between the target and cable images captured by executing the robot using optimal transport distance. The results show that the proposed method improves accuracy compared to conventional gradient-based approaches and methods that use deterministic models that do not consider uncertainty. keywords: {Estimation error;Uncertainty;Automation;Stochastic processes;Closed box;Bayes methods;Motion control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160884&isnumber=10160212

X. Zhang, S. Jain, B. Huang, M. Tomizuka and D. Romeres, "Learning Generalizable Pivoting Skills," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5865-5871, doi: 10.1109/ICRA48891.2023.10161271.Abstract: The skill of pivoting an object with a robotic system is challenging for the external forces that act on the system, mainly given by contact interaction. The complexity increases when the same skills are required to generalize across different objects. This paper proposes a framework for learning robust and generalizable pivoting skills, which consists of three steps. First, we learn a pivoting policy on an “unitary” object using Reinforcement Learning (RL). Then, we obtain the object's feature space by supervised learning to encode the kinematic properties of arbitrary objects. Finally, to adapt the unitary policy to multiple objects, we learn data-driven projections based on the object features to adjust the state and action space of the new pivoting task. The proposed approach is entirely trained in simulation. It requires only one depth image of the object and can zero-shot transfer to real-world objects. We demonstrate robustness to sim-to-real transfer and generalization to multiple objects. keywords: {Adaptation models;Visualization;Shape;Friction;Supervised learning;Kinematics;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161271&isnumber=10160212

A. Canberk et al., "Cloth Funnels: Canonicalized-Alignment for Multi-Purpose Garment Manipulation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5872-5879, doi: 10.1109/ICRA48891.2023.10161546.Abstract: Automating garment manipulation is challenging due to extremely high variability in object configurations. To reduce this intrinsic variation, we introduce the task of “canonicalized-alignment” that simplifies downstream applications by reducing the possible garment configurations. This task can be considered as “cloth state funnel” that manipulates arbitrarily configured clothing items into a predefined deformable configuration (i.e. canonicalization) at an appropriate rigid pose (i.e. alignment). In the end, the cloth items will result in a compact set of structured and highly visible configurations - which are desirable for downstream manipulation skills. To enable this task, we propose a novel canonicalized-alignment objective that effectively guides learning to avoid adverse local minima during learning. Using this objective, we learn a multi-arm, multi-primitive policy that strategically chooses between dynamic flings and quasi-static pick and place actions to achieve efficient canonicalized-alignment. We evaluate this approach on a real-world ironing and folding system that relies on this learned policy as the common first step. Empirically, we demonstrate that our task-agnostic canonicalized-alignment can enable even simple manually -designed policies to work well where they were pre-viously inadequate, thus bridging the gap between automated non-deformable manufacturing and deformable manipulation. keywords: {Automation;Clothing;Pipelines;Manufacturing;Complexity theory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161546&isnumber=10160212

Y. Geng, B. An, H. Geng, Y. Chen, Y. Yang and H. Dong, "RLAfford: End-to-End Affordance Learning for Robotic Manipulation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5880-5886, doi: 10.1109/ICRA48891.2023.10161571.Abstract: Learning to manipulate 3D objects in an interactive environment has been a challenging problem in Reinforcement Learning (RL). In particular, it is hard to train a policy that can generalize over objects with different semantic categories, diverse shape geometry and versatile functionality. In this study, we focused on the contact information in manipulation processes, and proposed a unified representation for critical interactions to describe different kinds of manipulation tasks. Specifically, we take advantage of the contact information generated during the RL training process and employ it as unified visual representation to predict contact map of interest. Such representation leads to an end-to-end learning framework that combined affordance based and RL based methods for the first time. Our unified framework can generalize over different types of manipulation tasks. Surprisingly, the effectiveness of such framework holds even under the multi-stage and multi-agent scenarios. We tested our method on eight types of manipulation tasks. Results showed that our methods outperform baseline algorithms, including visual affordance methods and RL methods, by a large margin on the success rate. The demonstration can be found at https://sites.google.com/view/rlafford/. keywords: {Training;Geometry;Visualization;Three-dimensional displays;Shape;Affordances;Semantics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161571&isnumber=10160212

L. Zhao et al., "Implementation and Optimization of Grasping Learning with Dual-modal Soft Gripper," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5887-5893, doi: 10.1109/ICRA48891.2023.10161249.Abstract: Robust and efficient grasping of different objects is still an open problem due to the difficulty of integrating multidisciplinary knowledge such as gripper ontology design, perception, control, and learning. In recent years, learning-based methods have achieved excellent results in grasping various novel objects. However, current methods are usually limited to a single grasping mode or rely on different end effectors to grasp objects of different shapes. For human beings, our hands are capable of grasping various objects with changes in grasping methods and form of hands. In light of this, developing a gripper with similar performance could possibly improve the robot's gripping ability. In this paper, we design a dual-modal soft gripper (DSG) and propose a deep reinforcement learning (DRL) framework to implement the operations. Both of our grasping modes, namely enveloping and pinching, are achieved through the tendon drive system and the deformation of the spring steel plate, which enables the gripper to switch between the two grasping modes in real time. We also combined the cutting-edge achievements of deep learning and reinforcement learning to design an autonomous grasping algorithm based on Q-learning and a deep Q network. Moreover, to fully utilize the visual input from the sensor, we added semantic embeddings of target objects to facilitate the learning, which is especially useful in deciding the grasping method for objects previously unseen. We also evaluate our DRL framework in different scenarios, offering a detailed comparison of each grasping mode and the mixed method (with or without semantic information). Our design has proved efficient in reducing the number of failing grasping actions and improving the success rate when facing novel and tricky objects. keywords: {Deep learning;Training;Visualization;Semantics;Grasping;Switches;Steel},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161249&isnumber=10160212

I. Huang, Y. Narang, R. Bajcsy, F. Ramos, T. Hermans and D. Fox, "DefGraspNets: Grasp Planning on 3D Fields with Graph Neural Nets," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5894-5901, doi: 10.1109/ICRA48891.2023.10160986.Abstract: Robotic grasping of 3D deformable objects is critical for real-world applications such as food handling and robotic surgery. Unlike rigid and articulated objects, 3D deformable objects have infinite degrees of freedom. Fully defining their state requires 3D deformation and stress fields, which are exceptionally difficult to analytically compute or experimentally measure. Thus, evaluating grasp candidates for grasp planning typically requires accurate, but slow 3D finite element method (FEM) simulation. Sampling-based grasp planning is often impractical, as it requires evaluation of a large number of grasp candidates. Gradient-based grasp planning can be more efficient, but requires a differentiable model to synthesize optimal grasps from initial candidates. Differentiable FEM simulators may fill this role, but are typically no faster than standard FEM. In this work, we propose learning a predictive graph neural network (GNN), DefGraspNets, to act as our differentiable model. We train DefGraspNets to predict 3D stress and deformation fields based on FEM-based grasp simulations. DefGraspNets not only runs up to 1500x faster than the FEM simulator, but also enables fast gradient-based grasp optimization over 3D stress and deformation metrics. We design DefGraspNets to align with real-world grasp planning practices and demonstrate generalization across multiple test sets, including real-world experiments. keywords: {Solid modeling;Three-dimensional displays;Deformation;Computational modeling;Predictive models;Graph neural networks;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160986&isnumber=10160212

J. Chen, T. Lan and V. Aggarwal, "Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5902-5908, doi: 10.1109/ICRA48891.2023.10160374.Abstract: Hierarchical Imitation Learning (HIL) has been proposed to recover highly-complex behaviors in long-horizon tasks from expert demonstrations by modeling the task hierarchy with the option framework. Existing methods either overlook the causal relationship between the subtask and its corresponding policy or cannot learn the policy in an end-to-end fashion, which leads to suboptimality. In this work, we develop a novel HIL algorithm based on Adversarial Inverse Reinforcement Learning and adapt it with the Expectation-Maximization algorithm in order to directly recover a hierarchical policy from the unannotated demonstrations. Further, we introduce a directed information term to the objective function to enhance the causality and propose a Variational Autoencoder framework for learning with our objectives in an end-to-end fashion. Theoretical justifications and evaluations on challenging robotic control tasks are provided to show the superiority of our algorithm. The codes are available at https://github.com/LucasCJYSDL/HierAIRL. keywords: {Codes;Automation;Reinforcement learning;Solids;Multitasking;Linear programming;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160374&isnumber=10160212

S. Hegde and G. S. Sukhatme, "Efficiently Learning Small Policies for Locomotion and Manipulation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5909-5915, doi: 10.1109/ICRA48891.2023.10160791.Abstract: Neural control of memory-constrained, agile robots requires small, yet highly performant models. We leverage graph hyper networks to learn graph hyper policies trained with off-policy reinforcement learning resulting in networks that are two orders of magnitude smaller than commonly used networks yet encode policies comparable to those encoded by much larger networks trained on the same task. We show that our method can be appended to any off-policy reinforcement learning algorithm, without any change in hyperparameters, by showing results across locomotion and manipulation tasks. Further, we obtain an array of working policies, with differing numbers of parameters, allowing us to pick an optimal network for the memory constraints of a system. Training multiple policies with our method is as sample efficient as training a single policy. Finally, we provide a method to select the best architecture, given a constraint on the number of parameters. Project website: https://sites.google.com/usc.edu/graphhyperpolicy keywords: {Training;Automation;Memory management;Reinforcement learning;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160791&isnumber=10160212

G. Schiavi, P. Wulkop, G. Rizzi, L. Ott, R. Siegwart and J. J. Chung, "Learning Agent-Aware Affordances for Closed-Loop Interaction with Articulated Objects," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5916-5922, doi: 10.1109/ICRA48891.2023.10160747.Abstract: Interactions with articulated objects are a challenging but important task for mobile robots. To tackle this challenge, we propose a novel closed-loop control pipeline, which integrates manipulation priors from affordance estimation with sampling-based whole-body control. We introduce the concept of agent-aware affordances which fully reflect the agent's capabilities and embodiment and we show that they outperform their state-of-the-art counterparts which are only conditioned on the end-effector geometry. Additionally, closed-loop affordance inference is found to allow the agent to divide a task into multiple non-continuous motions and recover from failure and unexpected states. Finally, the pipeline is able to perform long-horizon mobile manipulation tasks, i.e. opening and closing an oven, in the real world with high success rates (opening: 71%, closing: 72%). keywords: {Geometry;Visualization;Automation;Ovens;Affordances;Pipelines;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160747&isnumber=10160212

J. Urain, N. Funk, J. Peters and G. Chalvatzaki, "SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5923-5930, doi: 10.1109/ICRA48891.2023.10161569.Abstract: Multi-objective optimization problems are ubiquitous in robotics, e.g., the optimization of a robot manipulation task requires a joint consideration of grasp pose configurations, collisions and joint limits. While some demands can be easily hand-designed, e.g., the smoothness of a trajectory, several task-specific objectives need to be learned from data. This work introduces a method for learning data-driven SE(3) cost functions as diffusion models. Diffusion models can represent highly-expressive multimodal distributions and exhibit proper gradients over the entire space due to their score-matching training objective. Learning costs as diffusion models allows their seamless integration with other costs into a single differentiable objective function, enabling joint gradient-based motion optimization. In this work, we focus on learning SE(3) diffusion models for 6DoF grasping, giving rise to a novel framework for joint grasp and motion optimization without needing to decouple grasp selection from trajectory generation. We evaluate the representation power of our SE(3) diffusion models w.r.t. classical generative models, and we showcase the superior performance of our proposed optimization framework in a series of simulated and real-world robotic manipulation tasks against representative baselines. Videos, code and additional details are available at: https://sites.google.com/view/se3dif keywords: {Training;Robot motion;Adaptation models;Costs;Cost function;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161569&isnumber=10160212

P. Mitrano, A. LaGrassa, O. Kroemer and D. Berenson, "Focused Adaptation of Dynamics Models for Deformable Object Manipulation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5931-5937, doi: 10.1109/ICRA48891.2023.10161366.Abstract: In order to efficiently learn a dynamics model for a task in a new environment, one can adapt a model learned in a similar source environment. However, existing adaptation methods can fail when the target dataset contains transitions where the dynamics are very different from the source environment. For example, the source environment dynamics could be of a rope manipulated in free space, whereas the target dynamics could involve collisions and deformation on obstacles. Our key insight is to improve data efficiency by focusing model adaptation on only the regions where the source and target dynamics are similar. In the rope example, adapting the free-space dynamics requires significantly less data than adapting the free-space dynamics while also learning collision dynamics. We propose a new method for adaptation that is effective in adapting to regions of similar dynamics. Additionally, we combine this adaptation method with prior work on planning with unreliable dynamics to make a method for data-efficient online adaptation, called FOCUS. We first demonstrate that the proposed adaptation method achieves statistically significantly lower prediction error in regions of similar dynamics on simulated rope manipulation and plant watering tasks. We then show on a bimanual rope manipulation task that FOCUS achieves data-efficient online learning, in simulation and in the real world. keywords: {Training;Deformable models;Adaptation models;Deformation;Training data;Focusing;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161366&isnumber=10160212

K. Xu et al., "Dexterous Manipulation from Images: Autonomous Real-World RL via Substep Guidance," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5938-5945, doi: 10.1109/ICRA48891.2023.10161493.Abstract: Complex and contact-rich robotic manipulation tasks, particularly those that involve multi-fingered hands and underactuated object manipulation, present a significant challenge to any control method. Methods based on reinforcement learning offer an appealing choice for such settings, as they can enable robots to learn to delicately balance contact forces and dexterously reposition objects without strong modeling assumptions. However, running reinforcement learning on real-world dexterous manipulation systems often requires significant manual engineering. This negates the benefits of autonomous data collection and ease of use that reinforcement learning should in principle provide. In this paper, we describe a system for vision-based dexterous manipulation that provides a “programming-free” approach for users to define new tasks and enable robots with complex multi-fingered hands to learn to perform them through interaction. The core principle under-lying our system is that, in a vision-based setting, users should be able to provide high-level intermediate supervision that circumvents challenges in teleoperation or kinesthetic teaching which allows a robot to not only learn a task efficiently but also to autonomously practice. Our system includes a framework for users to define a final task and intermediate sub-tasks with image examples, a reinforcement learning procedure that learns the task autonomously without interventions, and experimental results with a four-finger robotic hand learning multi-stage object manipulation tasks directly in the real world, without simulation, manual modeling, or reward engineering. keywords: {Automation;Education;Reinforcement learning;Manuals;Data collection;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161493&isnumber=10160212

A. Gupta, M. E. Shepherd and S. Gupta, "Predicting Motion Plans for Articulating Everyday Objects," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5946-5953, doi: 10.1109/ICRA48891.2023.10160752.Abstract: Mobile manipulation tasks such as opening a door, pulling open a drawer, or lifting a toilet seat require constrained motion of the end-effector under environmental and task constraints. This, coupled with partial information in novel environments, makes it challenging to employ classical motion planning approaches at test time. Our key insight is to cast it as a learning problem to leverage past experience of solving similar planning problems to directly predict motion plans for mobile manipulation tasks in novel situations at test time. To enable this, we develop a simulator, ArtObjSim, that simulates articulated objects placed in real scenes. We then introduce $\mathbf{SeqIK}+\theta_{0}$, a fast and flexible representation for motion plans. Finally, we learn models that use $\mathbf{SeqIK}+\theta_{0}$ to quickly predict motion plans for articulating novel objects at test time. Experimental evaluation shows improved speed and accuracy at generating motion plans than pure search-based methods and pure learning methods. keywords: {Learning systems;Automation;Predictive models;End effectors;Planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160752&isnumber=10160212

S. P. Arunachalam, S. Silwal, B. Evans and L. Pinto, "Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5954-5961, doi: 10.1109/ICRA48891.2023.10160275.Abstract: Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement learning having been previously explored in literature. Such prior work often require extensive trial-and-error training along with task-specific tuning of reward functions, which makes applying dexterous manipulation for general purpose problems quite impractical. A sample-efficient and practical alternate to trial-and-error learning is imitation learning. However, collecting and learning from demonstrations in dexterous manipulation is quite challenging due to the high-dimensional action-space involved with multi-finger control. In this work, we propose ‘Dexterous Imitation Made Easy’ (DIME) a new imitation learning framework for dexterous manipulation. DIME only requires a single RGB camera that observes a human operator to teleoperate a robotic hand. Once demonstrations are collected, DIME employs state-of-the-art imitation learning methods to train dexterous manipulation policies. On real robot benchmarks we demonstrate that DIME can be used to solve complex, in-hand manipulation tasks such as ‘flipping’, ‘spinning’, and ‘rotating’ objects with just 30 demonstrations and no additional robot training. Our code, pre-collected demonstrations, and robot videos are publicly available at: https://nyu-robot-learning.github.io/dime. keywords: {Training;Learning systems;Codes;Robot vision systems;Reinforcement learning;Cameras;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160275&isnumber=10160212

S. P. Arunachalam, I. Güzey, S. Chintala and L. Pinto, "Holo-Dex: Teaching Dexterity with Immersive Mixed Reality," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5962-5969, doi: 10.1109/ICRA48891.2023.10160547.Abstract: A fundamental challenge in teaching robots is to provide an effective interface for human teachers to demonstrate useful skills to a robot. This challenge is exacerbated in dexterous manipulation, where teaching high-dimensional, contact-rich behaviors often require esoteric teleoperation tools. In this work, we present Holo − Dex, a framework for dexter-ous manipulation that places a teacher in an immersive mixed reality through commodity VR headsets. The high-fidelity hand pose estimator onboard the headset is used to teleoperate the robot and collect demonstrations for a variety of general-purpose dexterous tasks. Given these demonstrations, we use powerful feature learning combined with non-parametric imi-tation to train dexterous skills. Our experiments on six common dexterous tasks, including in-hand rotation, spinning, and bottle opening, indicate that HOLO-DEX can both collect high-quality demonstration data and train skills in a matter of hours. Finally, we find that our trained skills can exhibit generalization on objects not seen in training. Videos of HOLO − DEX are available on {https://holo-dex.github.io/.} keywords: {Headphones;Training;Representation learning;Automation;Mixed reality;Behavioral sciences;Spinning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160547&isnumber=10160212

E. K. Gordon and R. S. Zarrin, "Online augmentation of learned grasp sequence policies for more adaptable and data-efficient in-hand manipulation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5970-5976, doi: 10.1109/ICRA48891.2023.10161003.Abstract: When using a tool, the grasps used for picking it up, reposing, and holding it in a suitable pose for the desired task could be distinct. Therefore, a key challenge for autonomous in-hand tool manipulation is finding a sequence of grasps that facilitates every step of the tool use process while continuously maintaining force closure and stability. Due to the complexity of modeling the contact dynamics, reinforcement learning (RL) techniques can provide a solution in this continuous space subject to highly parameterized physical models. However, these techniques impose a trade-off in adaptability and data efficiency. At test time the tool properties, desired trajectory, and desired application forces could differ substantially from training scenarios. Adapting to this necessitates more data or computationally expensive online policy updates. In this work, we apply the principles of discrete dynamic programming (DP) to augment RL performance with domain knowledge. Specifically, we first design a computationally simple approximation of our environment. We then demonstrate in physical simulation that performing tree searches (i.e., lookaheads) and policy rollouts with this approximation can improve an RL-derived grasp sequence policy with minimal additional online computation. Additionally, we show that pretraining a deep RL network with the DP-derived solution to the discretized problem can speed up policy training. keywords: {Training;Adaptation models;Uncertainty;Computational modeling;Reinforcement learning;Stability analysis;Hardware},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161003&isnumber=10160212

A. Handa et al., "DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5977-5984, doi: 10.1109/ICRA48891.2023.10160216.Abstract: Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in diverse kinds of hardware and simulator setups, and in our case, with the Allegro Hand and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for researchers to achieve such results with commonly-available, affordable robot hands and cameras. Videos of the resulting policy and supplementary information, including experiments and demos, can be found on the website. keywords: {Industries;Heart;Service robots;Robot vision systems;Reinforcement learning;Real-time systems;Motion capture},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160216&isnumber=10160212

Z. Bing, A. Koch, X. Yao, K. Huang and A. Knoll, "Meta-Reinforcement Learning via Language Instructions," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5985-5991, doi: 10.1109/ICRA48891.2023.10160626.Abstract: Although deep reinforcement learning has recently been very successful at learning complex behaviors, it requires a tremendous amount of data to learn a task. One of the fundamental reasons causing this limitation lies in the nature of the trial-and-error learning paradigm of reinforcement learning, where the agent communicates with the environment and pro-gresses in the learning only relying on the reward signal. This is implicit and rather insufficient to learn a task well. On the con-trary, humans are usually taught new skills via natural language instructions. Utilizing language instructions for robotic motion control to improve the adaptability is a recently emerged topic and challenging. In this paper, we present a meta-RL algorithm that addresses the challenge of learning skills with language instructions in multiple manipulation tasks. On the one hand, our algorithm utilizes the language instructions to shape its in-terpretation of the task, on the other hand, it still learns to solve task in a trial-and-error process. We evaluate our algorithm on the robotic manipulation benchmark (Meta-World) and it significantly outperforms state-of-the-art methods in terms of training and testing task success rates. Codes are available at https://tumi6robot.wixsite.com/million. keywords: {Training;Robot motion;Deep learning;Shape;Natural languages;Reinforcement learning;Encoding},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160626&isnumber=10160212

G. Meng, Y. Wu and Q. Chen, "Improving Video Super-Resolution with Long-Term Self-Exemplars," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5992-5998, doi: 10.1109/ICRA48891.2023.10160844.Abstract: Existing video super-resolution methods often utilize a few neighboring frames to generate a higher-resolution image for each frame. However, the abundant information in distant frames has not been fully exploited in these methods: corresponding patches of the same instance appear across distant frames at different scales. Based on this observation, we propose to improve the video super-resolution quality with long-term cross-scale aggregation that leverages similar patches (self-exemplars) across distant frames. Our method can be implemented as post-processing for any super-resolution methods to improve performance. Our model consists of a multi-reference alignment module to fuse the features derived from similar patches: we fuse the features of distant references to perform high-quality super-resolution. We also propose a novel and practical training strategy for reference-based super-resolution. To evaluate the performance of our proposed method, we conduct extensive experiments on our collected CarCam dataset, the Waymo Open dataset, and the REDS dataset, and the results demonstrate our method outperforms state-of-the-art methods. keywords: {Training;Automation;Fuses;Surveillance;Superresolution;Robot vision systems;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160844&isnumber=10160212

C. Elich, I. Armeni, M. R. Oswald, M. Pollefeys and J. Stueckler, "Learning-based Relational Object Matching Across Views," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 5999-6005, doi: 10.1109/ICRA48891.2023.10161393.Abstract: Intelligent robots require object-level scene understanding to reason about possible tasks and interactions with the environment. Moreover, many perception tasks such as scene reconstruction, image retrieval, or place recognition can benefit from reasoning on the level of objects. While keypoint-based matching can yield strong results for finding correspondences for images with small to medium view point changes, for large view point changes, matching semantically on the object-level becomes advantageous. In this paper, we propose a learning-based approach which combines local keypoints with novel object-level features for matching object detections between RGB images. We train our object-level matching features based on appearance and inter-frame and cross-frame spatial relations between objects in an associative graph neural network. We demonstrate our approach in a large variety of views on realistically rendered synthetic images. Our approach compares favorably to previous state-of-the-art object-level matching approaches and achieves improved performance over a pure keypoint-based approach for large view-point changes. keywords: {Location awareness;Three-dimensional displays;Image recognition;Image retrieval;Object detection;Feature extraction;Graph neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161393&isnumber=10160212

T. Sangam, I. R. Dave, W. Sultani and M. Shah, "TransVisDrone: Spatio-Temporal Transformer for Vision-based Drone-to-Drone Detection in Aerial Videos," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6006-6013, doi: 10.1109/ICRA48891.2023.10161433.Abstract: Drone-to-drone detection using visual feed has crucial applications, such as detecting drone collisions, detecting drone attacks, or coordinating flight with other drones. However, existing methods are computationally costly, follow non-end-to-end optimization, and have complex multi-stage pipelines, making them less suitable for real-time deployment on edge devices. In this work, we propose a simple yet effective framework, TransVisDrone, that provides an end-to-end solution with higher computational efficiency. We utilize CSPDarkNet-53 network to learn object-related spatial features and VideoSwin model to improve drone detection in challenging scenarios by learning spatio-temporal dependencies of drone motion. Our method achieves state-of-the-art performance on three challenging real-world datasets (Average Precision@0.5IOU): NPS 0.95, FLDrones 0.75, and AOT 0.80, and a higher throughput than previous methods. We also demonstrate its deployment capability on edge devices and its usefulness in detecting drone-collision (encounter). Project: https://tusharsangam.github.io/TransVisDrone-project-page/ keywords: {Performance evaluation;Visualization;Image edge detection;Robot vision systems;Transformers;Throughput;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161433&isnumber=10160212

L. Gan, C. Lee and S. -J. Chung, "Unsupervised RGB-to-Thermal Domain Adaptation via Multi-Domain Attention Network," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6014-6020, doi: 10.1109/ICRA48891.2023.10160872.Abstract: This work presents a new method for unsupervised thermal image classification and semantic segmentation by transferring knowledge from the RGB domain using a multi-domain attention network. Our method does not require any thermal annotations or co-registered RGB-thermal pairs, enabling robots to perform visual tasks at night and in adverse weather conditions without incurring additional costs of data labeling and registration. Current unsupervised domain adaptation methods look to align global images or features across domains. However, when the domain shift is significantly larger for cross-modal data, not all features can be transferred. We solve this problem by using a shared backbone network that promotes generalization, and domain-specific attention that reduces negative transfer by attending to domain-invariant and easily-transferable features. Our approach outperforms the state-of-the-art RGB-to-thermal adaptation method in classification benchmarks, and is successfully applied to thermal river scene segmentation using only synthetic RGB images. Our code is made publicly available at https://github.com/ganlumomo/thermal-uda-attention. keywords: {Adaptation models;Visualization;Costs;Semantic segmentation;Performance gain;Rivers;Labeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160872&isnumber=10160212

A. K. Kosta and K. Roy, "Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking Neural Networks with Learnable Neuronal Dynamics," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6021-6027, doi: 10.1109/ICRA48891.2023.10160551.Abstract: Event-based cameras have recently shown great potential for high-speed motion estimation owing to their ability to capture temporally rich information asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired event-driven processing can efficiently handle such asynchronous data, while neuron models such as the leaky-integrate and fire (LIF) can keep track of the quintessential timing information contained in the inputs. SNNs achieve this by maintaining a dynamic state in the neuron memory, retaining important information while forgetting redundant data over time. Thus, we posit that SNNs would allow for better performance on sequential regression tasks compared to similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult to train due to vanishing spikes at later layers. To that effect, we propose an adaptive fully-spiking framework with learnable neuronal dynamics to alleviate the spike vanishing problem. We utilize surrogate gradient-based backpropagation through time (BPTT) to train our deep SNNs from scratch. We validate our approach for the task of optical flow estimation on the Multi-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset. Our experiments on these datasets show an average reduction of ∼ 13% in average endpoint error (AEE) compared to state-of-the-art ANNs. We also explore several down-scaled models and observe that our SNN models consistently outperform similarly sized ANNs offering ∼10%-16% lower AEE. These results demonstrate the importance of SNNs for smaller models and their suitability at the edge. In terms of efficiency, our SNNs offer substantial savings in network parameters (∼ 48.3 ×) and computational energy (∼ 10.2 ×) while attaining ∼ 10% lower EPE compared to the state-of-the-art ANN implementations. keywords: {Training;Adaptation models;Tracking;Computational modeling;Neurons;Estimation;Timing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160551&isnumber=10160212

H. Mohaghegh, H. Rahmani, H. Laga, F. Boussaid and M. Bennamoun, "Reinforced Learning for Label-Efficient 3D Face Reconstruction," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6028-6034, doi: 10.1109/ICRA48891.2023.10161362.Abstract: 3D face reconstruction plays a major role in many human-robot interaction systems, from automatic face authentication to human-computer interface-based entertainment. To improve robustness against occlusions and noise, 3D face reconstruction networks are often trained on a set of in-the-wild face images preferably captured along different viewpoints of the subject. However, collecting the required large amounts of 3D annotated face data is expensive and time-consuming. To address the high annotation cost and due to the importance of training on a useful set, we propose an Active Learning (AL) framework that actively selects the most informative and representative samples to be labeled. To the best of our knowledge, this paper is the first work on tackling active learning for 3D face reconstruction to enable a label-efficient training strategy. In particular, we propose a Reinforcement Active Learning approach in conjunction with a clustering-based pooling strategy to select informative view-points of the subjects. Experimental results on 300W-LP and AFLW2000 datasets demonstrate that our proposed method is able to 1) efficiently select the most influencing view-points for labeling and outperforms several baseline AL techniques and 2) further improve the performance of a 3D Face Reconstruction network trained on the full dataset. keywords: {Training;Solid modeling;Three-dimensional displays;Human-robot interaction;Reinforcement learning;Robustness;Labeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161362&isnumber=10160212

R. Xu, J. Li, X. Dong, H. Yu and J. Ma, "Bridging the Domain Gap for Multi-Agent Perception," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6035-6042, doi: 10.1109/ICRA48891.2023.10160871.Abstract: Existing multi-agent perception algorithms usually select to share deep neural features extracted from raw sensing data between agents, achieving a trade-off between accuracy and communication bandwidth limit. However, these methods assume all agents have identical neural networks, which might not be practical in the real world. The transmitted features can have a large domain gap when the models differ, leading to a dramatic performance drop in multi-agent perception. In this paper, we propose the first lightweight framework to bridge such domain gaps for multi-agent perception, which can be a plug-in module for most of the existing systems while maintaining confidentiality. Our framework consists of a learnable feature resizer to align features in multiple dimensions and a sparse cross-domain transformer for domain adaption. Extensive experiments on the public multi-agent perception dataset V2XSet have demonstrated that our method can effectively bridge the gap for features from different domains and outperform other baseline methods significantly by at least 8% for point-cloud-based 3D object detection. keywords: {Bridges;Representation learning;Visualization;Three-dimensional displays;Neural networks;Object detection;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160871&isnumber=10160212

Y. -C. Tseng, T. -Y. Ke and F. Wu, "UPLIFT: Unsupervised Person Labeling and Identification via Cooperative Learning with Mobile Robots," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6043-6049, doi: 10.1109/ICRA48891.2023.10161103.Abstract: As robots are widely used in assisting manual tasks, an interesting challenge is: Can mobile robots help create a labeled knowledge dataset that can be used for efficiently creating deep learning models for other sensors? This paper proposes an Unsupervised Person Labeling and Identification (UPLIFT) framework to automatically enlarge the labeled knowledge dataset. Typically, manual data labeling is very costly, especially when the user population is large and dynamic. To reduce the cost, we use a mobile robot to serve as a knowledge seed and to provide the pseudo-ground-truth for the system so that unlabeled images from other fixed surveillance cameras can be paired with the pseudo-ground-truth. Ultimately, the knowledge dataset can be generated via a system-to-system knowledge transfer process from the former to the latter and gradually expanded as the system operates longer. Experimental results in two environments indicate that UPLIFT achieves an accuracy of 94.1% on average to detect pedestrians' IDs every 10 seconds. keywords: {Pedestrians;Surveillance;Robot vision systems;Manuals;Cameras;Trajectory;Labeling},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161103&isnumber=10160212

Y. Jing and T. Kong, "Learning to Explore Informative Trajectories and Samples for Embodied Perception," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6050-6056, doi: 10.1109/ICRA48891.2023.10160951.Abstract: We are witnessing significant progress on perception models, specifically those trained on large-scale internet images. However, efficiently generalizing these perception models to unseen embodied tasks is insufficiently studied, which will help various relevant applications (e.g., home robots). Unlike static perception methods trained on pre-collected images, the embodied agent can move around in the environment and obtain images of objects from any viewpoints. Therefore, efficiently learning the exploration policy and collection method to gather informative training samples is the key to this task. To do this, we first build a 3D semantic distribution map to train the exploration policy self-supervised by introducing the semantic distribution disagreement and the semantic distribution uncertainty rewards. Note that the map is generated from multi-view observations and can weaken the impact of misidentification from an unfamiliar viewpoint. Our agent is then encouraged to explore the objects with different semantic distributions across viewpoints, or uncertain semantic distributions. With the explored informative trajectories, we propose to select hard samples on trajectories based on the semantic distribution uncertainty to reduce unnecessary observations that can be correctly identified. Experiments show that the perception model fine-tuned with our method outperforms the baselines trained with other exploration policies. Further, we demonstrate the robustness of our method in real-robot experiments. keywords: {Training;Uncertainty;Three-dimensional displays;Automation;Semantics;Robustness;Trajectory;Embodied Perception;Trajectory Exploration;Hard Sample Selection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160951&isnumber=10160212

R. Bigazzi, M. Cornia, S. Cascianelli, L. Baraldi and R. Cucchiara, "Embodied Agents for Efficient Exploration and Smart Scene Description," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6057-6064, doi: 10.1109/ICRA48891.2023.10160668.Abstract: The development of embodied agents that can communicate with humans in natural language has gained increasing interest over the last years, as it facilitates the diffusion of robotic platforms in human-populated environments. As a step towards this objective, in this work, we tackle a setting for visual navigation in which an autonomous agent needs to explore and map an unseen indoor environment while portraying interesting scenes with natural language descriptions. To this end, we propose and evaluate an approach that combines recent advances in visual robotic exploration and image captioning on images generated through agent-environment interaction. Our approach can generate smart scene descriptions that maximize semantic knowledge of the environment and avoid repetitions. Further, such descriptions offer user-understandable insights into the robot's representation of the environment by high-lighting the prominent objects and the correlation between them as encountered during the exploration. To quantitatively assess the performance of the proposed approach, we also devise a specific score that takes into account both exploration and description skills. The experiments carried out on both photorealistic simulated environments and real-world ones demonstrate that our approach can effectively describe the robot's point of view during exploration, improving the human-friendly interpretability of its observations. keywords: {Visualization;Correlation;Automation;Navigation;Natural languages;Semantics;Autonomous agents},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160668&isnumber=10160212

E. Cereda et al., "Deep Neural Network Architecture Search for Accurate Visual Pose Estimation aboard Nano-UAVs," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6065-6071, doi: 10.1109/ICRA48891.2023.10160369.Abstract: Miniaturized autonomous unmanned aerial vehicles (UAVs) are an emerging and trending topic. With their form factor as big as the palm of one hand, they can reach spots otherwise inaccessible to bigger robots and safely operate in human surroundings. The simple electronics aboard such robots (sub-100 mW) make them particularly cheap and attractive but pose significant challenges in enabling onboard sophisticated intelligence. In this work, we leverage a novel neural architecture search (NAS) technique to automatically identify several Pareto-optimal convolutional neural networks (CNNs) for a visual pose estimation task. Our work demonstrates how reallife and field-tested robotics applications can concretely leverage NAS technologies to automatically and efficiently optimize CNNs for the specific hardware constraints of small UAVs. We deploy several NAS-optimized CNNs and run them in closed-loop aboard a 27-g Crazyflie nano-UAV equipped with a parallel ultra-low power System-on-Chip. Our results improve the State-of-the-Art by reducing the in-field control error of 32% while achieving a real-time onboard inference-rate of ~10Hz@10mW and ~50Hz@90mW. keywords: {Performance evaluation;Visualization;Power demand;Pose estimation;Throughput;Real-time systems;System-on-chip},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160369&isnumber=10160212

J. Morlana and J. M. M. Montiel, "Reuse your features: unifying retrieval and feature-metric alignment," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6072-6079, doi: 10.1109/ICRA48891.2023.10160501.Abstract: We propose a compact pipeline to unify all the steps of Visual Localization: image retrieval, candidate re-ranking and initial pose estimation, and camera pose refinement. Our key assumption is that the deep features used for these individual tasks share common characteristics, so we should reuse them in all the procedures of the pipeline. Our DRAN (Deep Retrieval and image Alignment Network) is able to extract global descriptors for efficient image retrieval, use intermediate hierarchical features to re-rank the retrieval list and produce an initial pose guess, which is finally refined by means of a feature-metric optimization based on learned deep multi-scale dense features. DRAN is the first single network able to produce the features for the three steps of visual localization. DRAN achieves competitive performance in terms of robustness and accuracy under challenging conditions in public benchmarks, outperforming other unified approaches and consuming lower computational and memory cost than its counterparts using multiple networks. Code and models will be publicly available at github.com/jmorlana/DRAN. keywords: {Location awareness;Training;Visualization;Simultaneous localization and mapping;Pipelines;Image retrieval;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160501&isnumber=10160212

B. Berenguel-Baeta, J. Bermudez-Cameo and J. J. Guerrero, "FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast Fourier Convolutions from Single Panoramas," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6080-6086, doi: 10.1109/ICRA48891.2023.10161142.Abstract: In this work we present FreDSNet, a deep learning solution which obtains semantic 3D understanding of indoor environments from single panoramas. Omnidirectional images reveal task-specific advantages when addressing scene understanding problems due to the 360-degree contextual information about the entire environment they provide. However, the inherent characteristics of the omnidirectional images add additional problems to obtain an accurate detection and segmentation of objects or a good depth estimation. To overcome these problems, we exploit convolutions in the frequential domain obtaining a wider receptive field in each convolutional layer. These convolutions allow to leverage the whole context information from omnidirectional images. FreDSNet is the first network that jointly provides monocular depth estimation and semantic segmentation from a single panoramic image exploiting fast Fourier convolutions. Our experiments show that FreDSNet has slight better performance than the sole state-of-the-art method that obtains both semantic segmentation and depth estimation from panoramas. FreDSNet code is publicly available in https://github.com/Sbrunoberenguel/FreDSNet keywords: {Convolutional codes;Training;Visualization;Three-dimensional displays;Semantic segmentation;Semantics;Estimation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161142&isnumber=10160212

G. Peng et al., "CAHIR: Co-Attentive Hierarchical Image Representations for Visual Place Recognition," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6087-6094, doi: 10.1109/ICRA48891.2023.10160512.Abstract: Robust visual place recognition (VPR) against significant appearance changes is crucial for the life-long operation of mobile robots. Focusing on this task, we propose a Co-Attentive Hierarchical Image Representations (CAHIR) framework for VPR, which unifies attention-sharing global and local descriptor generation into one encoding pipeline. The hierarchical descriptors are applied to a coarse-to-fine VPR system with global retrieval and local geometric verification. To explore high-quality local matches between task-relevant visual elements, a cross-attention mutual enhancement layer is introduced to strengthen the information interaction between the local descriptors. Through the proposed selective matching distillation, the mutual enhancement layer can learn from state-of-the-art local matchers in a distillation manner. After weighted cross-matching of the enhanced local descriptors, geometric verification is applied to evaluate the spatial consistency of the compared image pair. Experiments show CAHIR outperforms the existing global and local representations for VPR in terms of performance and efficiency. Quantitatively, it achieves state-of-the-art results on three city-scale benchmark datasets. Qualitatively, CAHIR proves to attach great importance to task-relevant visual elements and excels at finding local correspondences that are discriminative to the VPR task. keywords: {Visualization;Image recognition;Image coding;Pipelines;Focusing;Image representation;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160512&isnumber=10160212

D. Wofk, R. Ranftl, M. Müller and V. Koltun, "Monocular Visual-Inertial Depth Estimation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6095-6101, doi: 10.1109/ICRA48891.2023.10161013.Abstract: We present a visual-inertial depth estimation pipeline that integrates monocular depth estimation and visual- inertial odometry to produce dense depth estimates with metric scale. Our approach performs global scale and shift alignment against sparse metric depth, followed by learning-based dense alignment. We evaluate on the TartanAir and VOID datasets, observing up to 30% reduction in inverse RMSE with dense scale alignment relative to performing just global alignment alone. Our approach is especially competitive at low density; with just 150 sparse metric depth points, our dense- to-dense depth alignment method achieves over 50 % lower iRMSE over sparse-to-dense depth completion by KBNet, currently the state of the art on VOID. We demonstrate successful zero-shot transfer from synthetic TartanAir to real-world VOID data and perform generalization tests on NYUv2 and VCU-RVI. Our approach is modular and is compatible with a variety of monocular depth estimation models. keywords: {Measurement;Visualization;Automation;Pipelines;Estimation;Training data;Odometry},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161013&isnumber=10160212

Q. Meng et al., "KGNet: Knowledge-Guided Networks for Category-Level 6D Object Pose and Size Estimation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6102-6108, doi: 10.1109/ICRA48891.2023.10160349.Abstract: Despite the giant leap made in object 6D pose estimation and robotic grasping under structured scenarios, most approaches depend heavily on the exact CAD models of target objects beforehand, thereby limiting their wide applications. To address this, we propose a novel knowledge-guided network - KGNet to estimate the pose and size of category-level unseen objects. This network includes three primary innovations: knowledge-guided categorical model generation, pointwise deformation probability matrix and synergetic RGBD feature fusion, with the former two leveraging categorical object knowledge for unseen object reconstruction and the latter one facilitating pose-sensitive feature extraction. Exten-sive experiments on CAMERA25 and REAL275 verify their effectiveness, and KGNet achieves the SOTA performance on these two acknowledged benchmarks. Additionally, a real-world robotic grasping experiment is conducted, and its results further qualitatively prove the practicability and robustness of KGNet. keywords: {Deformable models;Knowledge engineering;Solid modeling;Deformation;Pose estimation;Grasping;Benchmark testing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160349&isnumber=10160212

C. Liu, B. Eckart and J. Kautz, "Online Consistent Video Depth with Gaussian Mixture Representation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6109-6116, doi: 10.1109/ICRA48891.2023.10160785.Abstract: We demonstrate how off-the-shelf single-image depth estimation methods can be augmented with guidance from optical flow to achieve consistent and accurate online depth estimation using video sequences of static scenes. While previous work has successfully leveraged the complementary nature of optical flow and depth estimation, these techniques use computationally expensive test time optimization strategies that do not generalize beyond a single video sequence and also require knowledge of the future. In contrast, we present a computationally efficient feed-forward design that runs in an online fashion by utilizing learned data priors from previously seen video sequences. To accomplish this, we propose a continuous geometric scene representation that parametrically and compositionally represents the scene as a Gaussian Mixture Model (GMM). Based on this representation, our pipeline learns to estimate consistent depths and associated camera poses from video sequences of static scenes without direct supervision. Our online method achieves state-of-the-art results compared against offline methods that require all sequence frames. keywords: {Automation;Video sequences;Pipelines;Estimation;Cameras;Computational efficiency;Optical flow},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160785&isnumber=10160212

P. Gao, Q. Zhu, H. Lu, C. Gan and H. Zhang, "Deep Masked Graph Matching for Correspondence Identification in Collaborative Perception," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6117-6123, doi: 10.1109/ICRA48891.2023.10161231.Abstract: Correspondence identification (CoID) is an essential component for collaborative perception in multi-robot systems, such as connected autonomous vehicles. The goal of CoID is to identify the correspondence of objects observed by multiple robots in their own field of view in order for robots to consistently refer to the same objects. CoID is challenging due to perceptual aliasing, object non-covisibility, and noisy sensing. In this paper, we introduce a novel deep masked graph matching approach to enable CoID and address the challenges. Our approach formulates CoID as a graph matching problem and we design a masked neural network to integrate the multimodal visual, spatial, and GPS information to perform CoID. In addition, we design a new technique to explicitly address object non-covisibility caused by occlusion and the vehicle's limited field of view. We evaluate our approach in a variety of street environments using a high-fidelity simulation that integrates the CARLA and SUMO simulators. The experimental results show that our approach outperforms the previous approaches and achieves state-of-the- art CoID performance in connected autonomous driving applications. Our work is available at: https://github.com/gaopeng5/DMGM.git. keywords: {Training;Visualization;Neural networks;Collaboration;Robot sensing systems;Sensors;Object recognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161231&isnumber=10160212

T. Nakamura, S. Kawano, A. Yuguchi, Y. Kawanishi and K. Yoshino, "Operative Action Captioning for Estimating System Actions," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6124-6130, doi: 10.1109/ICRA48891.2023.10161545.Abstract: Human-assistive systems, such as robots, need to correctly understand the surrounding situation based on obser-vations and output the required support actions for humans. Language is one of the important channels to communicate with humans, and robots are required to have the ability to express their understanding and action-planning results. In this study, we propose a new task of operative action captioning that estimates and verbalizes the actions to be taken by the system in a human-assisting domain. We constructed a system that outputs a verbal description of a possible operative action that changes the current state to the given target state. We collected a dataset consisting of two images as observations, which express the current state and the state changed by actions and a caption that describes the actions that change the current state to the target state, by crowdsourcing in daily life situations. Then we constructed a system that estimates an operative action by a caption. Since the operative action's caption is expected to contain some state-changing actions, we use scene graph prediction as an auxiliary task because the events written in the scene graphs correspond to the state changes. Experimental results showed that our system successfully described the operative actions that should be conducted between the current and target states. The auxiliary tasks that predict the scene graphs improved the quality of the estimation results. keywords: {Crowdsourcing;Automation;Focusing;Estimation;Predictive models;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161545&isnumber=10160212

G. Lu, "Deep Unsupervised Visual Odometry Via Bundle Adjusted Pose Graph Optimization," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6131-6137, doi: 10.1109/ICRA48891.2023.10160703.Abstract: Unsupervised visual odometry as an active topic has attracted extensive attention, benefiting from its label-free practical value and robustness in real-world scenarios. However, the performance of camera pose estimation and tracking through deep neural network is still not as ideal as most other tasks, such as detection, segmentation and depth estimation, due to the lack of drift correction in the estimated trajectory and map optimization in the recovered 3D scenes. In this work, we introduce pose graph and bundle adjustment optimization to our network training process, which iteratively updates both the motion and depth estimations from the deep learning network, and enforces the refined outputs to further meet the unsupervised photometric and geometric constraints. The integration of pose graph and bundle adjustment is easy to implement and significantly enhances the training effectiveness. Experiments on KITTI dataset demonstrate that the introduced method achieves a significant improvement in motion estimation compared with other recent unsupervised monocular visual odometry algorithms. keywords: {Bundle adjustment;Training;Deep learning;Neural networks;Cameras;Robustness;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160703&isnumber=10160212

H. -g. Chi, S. Chi, S. Chan and K. Ramani, "Pose Relation Transformer Refine Occlusions for Human Pose Estimation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6138-6145, doi: 10.1109/ICRA48891.2023.10161259.Abstract: Accurately estimating the human pose is an essential task for many applications in robotics. However, existing pose estimation methods suffer from poor performance when occlusion occurs. Recent advances in NLP have been very successful in predicting the missing words conditioned on visible words. We draw upon the sentence completion analogy in NLP to guide our model to address occlusions in the pose estimation problem. We propose a novel approach that can mitigate the effect of occlusions motivated by the sentence completion task of NLP. In an analogous manner, we designed our model to reconstruct occluded joints given the visible joints utilizing joint correlations by capturing the implicit joint connectivity through the attention mechanism. In this work, we propose a POse Relation Transformer (PORT) that captures the global context of the pose using self-attention and a local context by aggregating adjacent joint features. To supervise PORT in learning joint correlations, we guide PORT to reconstruct randomly masked joints, which we call Masked Joint Modeling (MJM). PORT trained with MJM adds to existing keypoint detection methods and successfully refines occlusions. Notably, PORT is a model-agnostic plug-and-play module for pose refinement under occlusion that can be plugged into any keypoint detector with substantially low computational costs. We conducted extensive experiments to demonstrate the advantage of PORT mitigating the occlusion on the hand and body pose PORT improves the pose estimation accuracy of existing human pose estimation methods by up to 16% with only 5% of additional parameters. The code is publicly available at https://github.com/stnoah1/PORT. keywords: {Correlation;Codes;Computational modeling;Pose estimation;Detectors;Transformers;Computational efficiency},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161259&isnumber=10160212

F. Matsuzawa, Y. Qiu, K. Iwata, H. Kataoka and Y. Satoh, "Question Generation for Uncertainty Elimination in Referring Expressions in 3D Environments," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6146-6152, doi: 10.1109/ICRA48891.2023.10160386.Abstract: We introduce a new task of question generation to eliminate the uncertainty of referring expressions in 3D indoor environments (3D-REQ). Referring to an object using natural language is one of the most common occurrences in daily human conversations; therefore, instructing robots to identify a certain object using natural language could be an essential task in var-ious robotic applications, such as room arrangement. However, human instructions are sometimes uncertain. Existing research on visual grounding using natural language in a 3D environment assumes that the referring expression can uniquely identify the object and does not consider that humans unconsciously give uncertain expressions. When faced with uncertainties, humans ask questions to gain further information. Inspired by the above observation, we propose a method that reduces uncertainty by asking questions when being given an obscure referring expression. The purpose of this method is to predict the positions of all candidate objects that satisfy the referring expressions in a 3D indoor environment and then to ask the appropriate questions to narrow down the target objects from them. To achieve this, we constructed a new 3D-REQ dataset, the input of which is a referring expression with uncertainties in the 3D environment and point clouds, and the output of which is the bounding boxes of all candidate objects satisfying the referring expression and a question to eliminate the uncertainty. To the best of our knowledge, 3D-REQ is the first effort to eliminate the uncertainty of referring expressions for object grounding in 3D environments. keywords: {Point cloud compression;Visualization;Uncertainty;Three-dimensional displays;Grounding;Natural languages;Indoor environment},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160386&isnumber=10160212

C. Bandi and U. Thomas, "A New Efficient Eye Gaze Tracker for Robotic Applications," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6153-6159, doi: 10.1109/ICRA48891.2023.10161347.Abstract: Gaze estimation provides insight into a person's intent and engagement level, which is helpful in collaborative human-robot applications. With significant advancements in deep learning architectures, appearance-based gaze estimation has gained much attention. Appearance-based methods have shown significant improvement in gaze accuracy and, unlike traditional approaches, they function well in environments where there are no constraints. We present another convolution-based gaze estimation approach to further reduce the angular error. For estimating gaze under extreme conditions such as head variations and distances, full-face images have been shown to be efficient, so we rely on full-face and pay more attention to necessary features. With the proposed architecture, we achieve an accuracy of 3.75° on the MPIIFaceGaze dataset and 3.96° on the ETH-XGaze open-source dataset. In addition, we test eye gaze tracking in real-time robotic applications, such as attention detection, and pick-and-place. keywords: {Head;Automation;Estimation;Collaboration;Human-robot interaction;Deep architecture;Gaze tracking},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161347&isnumber=10160212

F. Robinson and G. Nejat, "A Deep Learning Human Activity Recognition Framework for Socially Assistive Robots to Support Reablement of Older Adults," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6160-6167, doi: 10.1109/ICRA48891.2023.10161404.Abstract: Many older adults prefer to stay in their own homes and age-in-place. However, physical and cognitive limitations in independently completing activities of daily living (ADLs) requires older adults to receive assistive support, often necessitating transitioning to care centers. In this paper, we present the development of a novel deep learning human activity recognition and classification architecture capable of autonomously identifying ADLs in home environments to enable long-term deployment of socially assistive robots to aid older adults. Our deep learning architecture is the first to use multimodal inputs to create an embedding vector approach for classifying and monitoring multiple ADLs. It uses spatial mid-fusion to combine geometric, motion and semantic features of users, environments, and objects to classify and track ADLs. We leverage transfer learning to extract generic features using the early layers of deep networks trained on large datasets to apply our architecture to various ADLs. The embedding vector enables identification of unseen ADLs and determines intra-class variance for monitoring user ADL performance. Our proposed unique architecture can be used by socially assistive robots to promote reablement in the home via autonomously supporting the assistance of varying ADLs. Extensive experiments show improved classification accuracy compared to unimodal/dual-modal models and the ADL embedding space also incorporates the ability to distinctly identify and track seen and unseen ADLs. keywords: {Deep learning;Training;Tracking;Transfer learning;Semantics;Feature extraction;Assistive robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161404&isnumber=10160212

D. Feng, Z. He, J. Hou, S. Schwertfeger and L. Zhang, "FloorplanNet: Learning Topometric Floorplan Matching for Robot Localization," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6168-6174, doi: 10.1109/ICRA48891.2023.10160977.Abstract: Given a building floorplan, humans can localize themselves by matching the observation of the environment with the floorplan using geometric, semantic, and topological clues. Inspired by this insight, this paper proposes a learning- based topometric robot localization method FloorplanNet, which implements a match between a metric robot map and the potentially inaccurate building floorplan in nonuniform scales and different shapes by semantic information. The method uses a novel Graph Neural Network to learn descriptors of nodes from topometric graphs generated from the input maps. We demonstrate that our method can match the 3D point cloud sub-map generated by the robot during the SLAM process with the 2D map. Furthermore, we apply our map-matching algorithm for real-world robot localization. We evaluate our method on several publicly available real-world datasets. Even though our network is solely trained using simulation data, our method demonstrates high robustness and effectiveness in real- world indoor environments and outperforms the existing SOTA map-matching algorithms. We further develop a simulator that automatically creates and annotates the required training data to train our neural networks. The method and simulator are released at: https://github.com/fengdelin/FloorplanNet.git keywords: {Three-dimensional displays;Semantics;Buildings;Training data;Robot localization;Robustness;Graph neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160977&isnumber=10160212

K. Koledić, I. Cvišić, I. Marković and I. Petrović, "MOFT: Monocular odometry based on deep depth and careful feature selection and tracking," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6175-6181, doi: 10.1109/ICRA48891.2023.10160588.Abstract: Autonomous localization in unknown environments is a fundamental problem in many emerging fields and the monocular visual approach offers many advantages, due to being a rich source of information and avoiding comparatively more complicated setups and multisensor calibration. Deep learning opened new venues for monocular odometry yielding not only end-to-end approaches but also hybrid methods combining the well studied geometry with specific deep components. In this paper we propose a monocular odometry that leverages deep depth within a feature based geometrical framework yielding a lightweight frame-to-frame approach with metrically scaled trajectories and state-of-the-art accuracy. The front-end is based on a multihypothesis matcher with perspective correction coupled with deep depth predictions that enables careful feature selection and tracking; especially of ground plane features that are suitable for translation estimation. The back-end is based on point-to-epipolar line minimization for rotation and unit translation estimation, followed by deep depth aided reprojection error minimization for metrically correct translation estimation. Furthermore, we also present a domain shift adaptation approach that allows for generalization over different camera intrinsic and extrinsic setups. The proposed approach is evaluated on the KITTI and KITTI-360 datasets, showing competitive results and in most cases outperforming other state-of-the-art stereo and monocular methods. keywords: {Location awareness;Geometry;Visualization;Robot vision systems;Estimation;Minimization;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160588&isnumber=10160212

T. -H. Wu and K. -W. Chen, "LGCNet: Feature Enhancement and Consistency Learning Based on Local and Global Coherence Network for Correspondence Selection," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6182-6188, doi: 10.1109/ICRA48891.2023.10160290.Abstract: Correspondence selection, a crucial step in many computer vision tasks, aims to distinguish between inliers and outliers from putative correspondences. The coherence of correspondences is often used for predicting inlier probability, but it is difficult for neural networks to extract coherence contexts based only on quadruple coordinates. To overcome this difficulty, we propose enhancing the preliminary features using local and global handcrafted coherent characteristics before model learning, which strengthens the discrimination of each correspondence and guides the model to prune obvious outliers. Furthermore, to fully utilize local information, neighbors are searched in coordinate space as well as feature space. These two kinds of neighbors provide complementary and plentiful contexts for inlier probability prediction. Finally, a novel neighbor representation and a fusion architecture are proposed to retain detailed features. Experiments demonstrate that our method achieves state-of-the-art performance on relative camera pose estimation and correspondence selection metrics on the outdoor YFCC100M [1] and the indoor SUN3D [2] datasets. keywords: {Measurement;Computer vision;Pose estimation;Robot vision systems;Neural networks;Coherence;Computer architecture},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160290&isnumber=10160212

H. Dong, X. Chen, M. Dusmanu, V. Larsson, M. Pollefeys and C. Stachniss, "Learning-Based Dimensionality Reduction for Computing Compact and Effective Local Feature Descriptors," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6189-6195, doi: 10.1109/ICRA48891.2023.10161381.Abstract: A distinctive representation of image patches in form of features is a key component of many computer vision and robotics tasks, such as image matching, image retrieval, and visual localization. State-of-the-art descriptors, from hand-crafted descriptors such as SIFT to learned ones such as HardNet, are usually high-dimensional; 128 dimensions or even more. The higher the dimensionality, the larger the memory consumption and computational time for approaches using such descriptors. In this paper, we investigate multi-layer perceptrons (MLPs) to extract low-dimensional but high-quality descriptors. We thoroughly analyze our method in unsuper-vised, self-supervised, and supervised settings, and evaluate the dimensionality reduction results on four representative descriptors. We consider different applications, including visual localization, patch verification, image matching and retrieval. The experiments show that our lightweight MLPs trained using supervised method achieve better dimensionality reduction than PCA. The lower-dimensional descriptors generated by our approach outperform the original higher-dimensional descriptors in downstream tasks, especially for the hand-crafted ones. The code is available at https://github.com/PRBonn/descriptor-dr. keywords: {Dimensionality reduction;Location awareness;Visualization;Runtime;Image matching;Memory management;Supervised learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161381&isnumber=10160212

S. Xu, H. Xiong, Q. Wu, T. Yao, Z. Wang and Z. Wang, "Online Visual SLAM Adaptation against Catastrophic Forgetting with Cycle-Consistent Contrastive Learning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6196-6202, doi: 10.1109/ICRA48891.2023.10161464.Abstract: Visual SLAM (Simultaneous Localisation and Mapping) aims to simultaneously estimate camera poses and depth maps from navigation videos captured. While recent deep learning based methods have achieved great success on this task, they tend to work well on source domain data and suffer from performance degradation on the unseen data of target domain. Hence, we propose an online adaptation approach to continuously adapt a pre-trained visual SLAM model to changing environments in a self-supervised manner. To preserve pre-learned knowledge against catastrophic forgetting, we perform updating on a novel adapter proposed rather than fine-tuning the whole model for adaptation. The adapter includes a cross-domain feature translation module that translates pre-learned features into translated features suitable for adaptation. Ideally, the translated new features should not only contain pre-learned knowledge but also substantially distinct from pre-learned features since these two features represent different domains. We thus introduce cycle-consistent contrastive learning to maximize the dissimilarity between these two features by enlarging the distance between them in the feature space. Besides, our contrastive learning method exploiting cycle-consistency contraint enables the translated features to be transferred back to the pre-learned ones, which helps the translated features better preserve pre-learned knowledge. Comprehensive experiments on both synthetic and real-world datasets demonstrate superior adaptation performance of our proposed method over several state-of-the-art baselines. keywords: {Learning systems;Degradation;Deep learning;Visualization;Adaptation models;Simultaneous localization and mapping;Navigation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161464&isnumber=10160212

N. Akai, "SLAMER: Simultaneous Localization and Map-Assisted Environment Recognition," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6203-6209, doi: 10.1109/ICRA48891.2023.10160639.Abstract: This paper presents a simultaneous localization and map-assisted environment recognition (SLAMER) method. Mobile robots usually have an environment map and environment information can be assigned to the map. Important information such as no entry zone can be predicted from the map if localization has succeeded. However, this prediction is failed when localization does not work. Uncertainty of pose estimate must be considered for robust-map-based environ-mental object prediction. Robots also have external sensors and can recognize environmental object; however, sensor-based recognition of course contain uncertainty. SLAMER fuses map-based prediction and sensor-based recognition while coping with these uncertainties and achieves accurate localization and environment recognition. In this paper, we demonstrate LiDAR-based implementation of SLAMER in two cases. In the first case, we use the SemanticKITTI dataset and show that SLAMER achieves accurate estimate more than traditional methods. In the second case, we use an indoor mobile robot and show that unmeasurable environmental objects such as open doors and no entry lines can be recognized. keywords: {Location awareness;Uncertainty;Automation;Fuses;Predictive models;Robot sensing systems;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160639&isnumber=10160212

X. Guo, J. Hu, H. Bao and G. Zhang, "Descriptor Distillation for Efficient Multi-Robot SLAM," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6210-6216, doi: 10.1109/ICRA48891.2023.10160541.Abstract: Performing accurate localization while maintaining the low-level communication bandwidth is an essential challenge of multi-robot simultaneous localization and mapping (MR-SLAM). In this paper, we tackle this problem by generating a compact yet discriminative feature descriptor with minimum inference time. We propose descriptor distillation that formulates the descriptor generation into a learning problem under the teacher-student framework. To achieve real-time descriptor generation, we design a compact student network and learn it by transferring the knowledge from a pre-trained large teacher model. To reduce the descriptor dimensions from the teacher to the student, we propose a novel loss function that enables the knowledge transfer between two different dimensional descriptors. The experimental results demonstrate that our model is 30% lighter than the state-of-the-art model and produces better descriptors in patch matching. Moreover, we build a MR-SLAM system based on the proposed method and show that our descriptor distillation can achieve higher localization performance for MR-SLAM with lower bandwidth. keywords: {Location awareness;Knowledge engineering;Simultaneous localization and mapping;Automation;Bandwidth;Real-time systems;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160541&isnumber=10160212

J. Han, Y. Min, H. -J. Chae, B. -M. Jeong and H. -L. Choi, "DS-K3DOM: 3-D Dynamic Occupancy Mapping with Kernel Inference and Dempster-Shafer Evidential Theory," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6217-6223, doi: 10.1109/ICRA48891.2023.10160364.Abstract: Occupancy mapping has been widely utilized to represent the surroundings for autonomous robots to perform tasks such as navigation and manipulation. While occupancy mapping in 2-D environments has been well-studied, there have been few approaches suitable for 3-D dynamic occupancy mapping which is essential for aerial robots. This paper presents a novel 3-D dynamic occupancy mapping algorithm called DS-K3DOM. We first establish a Bayesian method to sequentially update occupancy maps for a stream of measurements based on the random finite set theory. Then, we approximate it with particles in the Dempster-Shafer domain to enable real-time computation. Moreover, the algorithm applies kernel-based inference with Dirichlet basic belief assignment to enable dense mapping from sparse measurements. The efficacy of the proposed algorithm is demonstrated through simulations and real experimentsiiThe code is available at: https://github.com/JuyeopHan/dsk3dom_public. keywords: {Atmospheric measurements;Heuristic algorithms;Sensor fusion;Approximation algorithms;Particle measurements;Set theory;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160364&isnumber=10160212

C. Chen, P. Geneva, Y. Peng, W. Lee and G. Huang, "Monocular Visual-Inertial Odometry with Planar Regularities," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6224-6231, doi: 10.1109/ICRA48891.2023.10160620.Abstract: State-of-the-art monocular visual-inertial odometry (VIO) approaches rely on sparse point features in part due to their efficiency, robustness, and prevalence, while ignoring high-level structural regularities such as planes that are common to man-made environments and can be exploited to further constrain motion. Generally, planes can be observed by a camera for significant periods of time due to their large spatial presence and thus, are amenable for long-term navigation. Therefore, in this paper, we design a novel real-time monocular VIO system that is fully regularized by planar features within a lightweight multi-state constraint Kalman filter (MSCKF). At the core of our method is an efficient robust monocular-based plane detection algorithm, which does not require additional sensing modalities such as a stereo or depth camera as commonly seen in the literature, while enabling real-time regularization of point features to environmental planes. Specifically, in the proposed MSCKF, long-lived planes are maintained in the state vector, while shorter ones are marginalized after use for efficiency. Planar regularities are applied to both in-state SLAM features and out-of-state MSCKF features, thus fully exploiting the environmental plane information to improve VIO performance. The proposed approach is evaluated with extensive Monte-Carlo simulations and different real-world experiments including an author-collected AR scenario, and shown to outperform the point-based VIO in structured environments. Video Demonstration https://youtu.be/bec7LbYaOS8AR Table Dataset https://github.com/rpng/ar_table_dataset keywords: {Simultaneous localization and mapping;Monte Carlo methods;Navigation;Cameras;Feature extraction;Real-time systems;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160620&isnumber=10160212

W. Zhang, S. Wang, X. Dong, R. Guo and N. Haala, "BAMF-SLAM: Bundle Adjusted Multi-Fisheye Visual-Inertial SLAM Using Recurrent Field Transforms," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6232-6238, doi: 10.1109/ICRA48891.2023.10160905.Abstract: In this paper, we present BAMF-SLAM, a novel multi-fisheye visual-inertial SLAM system that utilizes Bundle Adjustment (BA) and recurrent field transforms (RFT) to achieve accurate and robust state estimation in challenging scenarios. First, our system directly operates on raw fisheye images, enabling us to fully exploit the wide Field-of-View (FoV) of fisheye cameras. Second, to overcome the low-texture challenge, we explore the tightly-coupled integration of multi-camera inputs and complementary inertial measurements via a unified factor graph and jointly optimize the poses and dense depth maps. Third, for global consistency, the wide FoV of the fisheye camera allows the system to find more potential loop closures, and powered by the broad convergence basin of RFT, our system can perform very wide baseline loop closing with little overlap. Furthermore, we introduce a semi-pose-graph BA method to avoid the expensive full global BA. By combining relative pose factors with loop closure factors, the global states can be adjusted efficiently with modest memory footprint while maintaining high accuracy. Evaluations on TUM-VI, Hilti-Oxford and Newer College datasets show the superior performance of the proposed system over prior works. In the Hilti SLAM Challenge 2022, our VIO version achieves second place. In a subsequent submission, our complete system, including the global BA backend, outperforms the winning approach. keywords: {Bundle adjustment;Visualization;Simultaneous localization and mapping;Transforms;Cameras;Robustness;Odometry},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160905&isnumber=10160212

S. Gopinath, K. Dantu and S. Y. Ko, "Improving the Performance of Local Bundle Adjustment for Visual-Inertial SLAM with Efficient Use of GPU Resources," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6239-6245, doi: 10.1109/ICRA48891.2023.10160499.Abstract: In this paper, we present our approach to efficiently leveraging GPU resources to improve the performance of local bundle adjustment for visual-inertial SLAM. We observe that for local bundle adjustment (i) the Schur complement method, a technique often used to speed up bundle adjustment, has the largest overhead when solving for the parameter update, and (ii) the workload consists of operations on small- to medium-sized matrices. Based on these observations, we develop and combine several techniques that efficiently handle small- to medium-sized matrices. We then implement these techniques as a drop-in replacement block solver for g2o, a library frequently used for bundle adjustment, and integrate it with ORB-SLAM3, a well-known open-source visual-inertial SLAM system. Our evaluation done with two popular datasets, EuRoC and TUM-VI, shows that we can reduce the time taken by local bundle adjustment by 13.81%-33.79% with our techniques across an embedded device and a desktop machine. keywords: {Bundle adjustment;Performance evaluation;Simultaneous localization and mapping;Automation;Graphics processing units;Libraries},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160499&isnumber=10160212

S. Jia, R. Xiong and Y. Wang, "Distributed Initialization for Visual-Inertial-Ranging Odometry with Position-Unknown UWB Network," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6246-6252, doi: 10.1109/ICRA48891.2023.10161382.Abstract: In recent years, the visual-inertial-ranging (VIR) state estimator with a position-unknown UWB network has become popular. However, most existing VIR methods leverage centralized algorithms to initialize the UWB anchors, which are challenging to be applied to massive UWB networks. In this paper, we propose a distributed initialization method for consistent visual-inertial-ranging odometry with a position-unknown UWB network (DC-VIRO). For the position-unknown UWB anchors, we solve a Robot-aided Distributed Localization (RaDL) to initialize their positions. For robot state estimation, we fuse the ranging measurements of initialized anchors and visual-inertial measurements in a consistent filter. The RaDL is formulated as a consensus-based optimization problem and solved by the Distributed Alternating Direction Method of Multipliers (D-ADMM) algorithm. To identify the unobservable conditions, we propose a self-contained Fisher Information Matrix (FIM) based criterion which can be evaluated by each anchor directly with locally-preserved ranging measurements. We use Covariance Intersection (CI) to estimate the covariance of initialized anchors' positions for consistent data fusion. The proposed DC-VIRO is validated in both simulation and real-world experiments. keywords: {Location awareness;Fuses;Robot vision systems;Information filters;Distance measurement;Odometry;Multi-robot systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161382&isnumber=10160212

K. Dharmarajan et al., "Automating Vascular Shunt Insertion with the dVRK Surgical Robot," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6781-6788, doi: 10.1109/ICRA48891.2023.10160966.Abstract: Vascular shunt insertion is a fundamental surgical procedure used to temporarily restore blood flow to tissues. It is often performed in the field after major trauma. We formulate a problem of automated vascular shunt insertion and propose a pipeline to perform Automated Vascular Shunt Insertion (AVSI) using a da Vinci Research Kit. The pipeline uses a learned visual model to estimate the locus of the vessel rim, plans a grasp on the rim, and moves to grasp at that point. The first robot gripper then pulls the rim to stretch open the vessel with a dilation motion. The second robot gripper then proceeds to insert a shunt into the vessel phantom (a model of the blood vessel) with a chamfer tilt followed by a screw motion. Results suggest that AVSI achieves a high success rate even with tight tolerances and varying vessel orientations up to 30°. Supplementary material, dataset, videos, and visualizations can be found at https://sites.google.com/berkeley.edu/autolab-avsi. keywords: {Visualization;Medical robotics;Automation;Pipelines;Phantoms;Fasteners;Blood vessels},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160966&isnumber=10160212

S. Kumar et al., "CogniDaVinci: Towards Estimating Mental Workload Modulated by Visual Delays During Telerobotic Surgery - An EEG-based Analysis," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6789-6794, doi: 10.1109/ICRA48891.2023.10161007.Abstract: Communication latency in any delicate telerobotic operation (such as remote surgery over distance) would impose a significant challenge due to the temporal degradation of visual perception and can substantially affect the outcomes. Less is known, however, about the neurophysiological basis of how operators adapt/react to delayed visual feedback. Identification of such neural markers might provide novel ways for future applications to monitor the mental workload (MW). In this study, we recorded electroencephalography (EEG) data from nine users while performing a peg transfer task using the da Vinci Research Kit with three levels of induced visual delay in the video feedback. Our results suggest that spectral EEG-based features can provide markers of the operator's MW modulated by arbitrary visual delay. We also show that the exposure to different visual delays could be successfully classified/detected solely from EEG data, using a Riemannian geometry-based classifier, which highlights the utility of EEG signals for detecting the effect of visual delay on brain activity. keywords: {Visualization;Medical robotics;Surgery;Electroencephalography;Real-time systems;Delays;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161007&isnumber=10160212

E. Z. Ahronovich, N. Shihora, J. -H. Shen, K. Joos and N. Simaan, "Exploring An External Approach to Subretinal Drug Delivery via Robot Assistance and B-Mode OCT," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6795-6801, doi: 10.1109/ICRA48891.2023.10161441.Abstract: Injections into specific retinal layers of the eye present a serious challenge to surgeons in terms of accuracy and perception. The emergence of new gene therapies further emphasizes the need for effective tools for localized drug delivery. Unlike the dominant approach of delivering drugs via a transvitreal intraocular pathway, this paper demonstrates the feasibility of delivering injections into the space between the choroid and the retina using an external approach. The design of a cooperative robotic system for enabling robot-assisted extraocular subretinal injections is presented. The system uses a distal micromanipulator that can serve as a hand-held tool for OCT-aided injection or attach to a six degree of freedom (DOF) serial robot arm for cooperative manipulation. The kinematics and control of the robot for constrained cooperative control motions to enable safe needle injection is presented and experimentally evaluated. These results suggest that the proposed external drug delivery approach is feasible, thereby enabling the advantages of preserving the integrity of the retina and omitting the necessity for vitrectomy. keywords: {Micromanipulators;Surgery;Manuals;Retina;Needles;Drug delivery;Gene therapy;Serial robots;kinematics;OCT;cooperative control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161441&isnumber=10160212

K. Hutchinson, Z. Li, I. Reyes and H. Alemzadeh, "Towards Surgical Context Inference and Translation to Gestures," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6802-6809, doi: 10.1109/ICRA48891.2023.10160383.Abstract: Manual labeling of gestures in robot-assisted surgery is labor intensive, prone to errors, and requires expertise or training. We propose a method for automated and explainable generation of gesture transcripts that leverages the abundance of data for image segmentation. Surgical context is detected using segmentation masks by examining the distances and intersections between the tools and objects. Next, context labels are translated into gesture transcripts using knowledge-based Finite State Machine (FSM) and data-driven Long Short Term Memory (LSTM) models. We evaluate the performance of each stage of our method by comparing the results with the ground truth segmentation masks, the consensus context labels, and the gesture labels in the JIGSAWS dataset. Our results show that our segmentation models achieve state-of-the-art performance in recognizing needle and thread in Suturing and we can automatically detect important surgical states with high agreement with crowd-sourced labels (e.g., contact between graspers and objects in Suturing). We also find that the FSM models are more robust to poor segmentation and labeling performance than LSTMs. Our proposed method can significantly shorten the gesture labeling process (~2.8 times). keywords: {Context;Training;Image segmentation;Instruction sets;Knowledge based systems;Surgery;Manuals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160383&isnumber=10160212

H. Zhou, S. Yang, L. Halamek and T. Nanayakkara, "A Method to Use Haptic Feedback of Laryngoscope Force Vector for Endotracheal Intubation Training," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6810-6816, doi: 10.1109/ICRA48891.2023.10160755.Abstract: Endotracheal intubation is a mandatory competency for most medical staff. This procedure involves opening the entrance of the patient's upper windpipe using a laryngoscope and then inserting a tube into the windpipe to supply Oxygen to the patient. This time critical intervention requires careful control of the force vector on the tongue to lift it parallel to the jaw than to push the jaw to open the mouth. However, traditional intubation training methods in which novices practice intubation on prostheses lack haptic feedback to improve force control. We designed a sensorised intubation training phantom that can provide trainees with vibrotactile feedback reflecting the laryngoscope's force on the tongue. The critical component of this phantom is a silicon rubber tongue embedded with magnets and hall effect sensors. We calibrated the hall effect sensor readings to predict the force vector exerted on the tongue with errors less than 0.5 N in the lifting and pushing directions. We conducted a controlled experiment, mainly comparing the training results between participants with and without haptic feedback. Results show a statistically significant drop in the undesired forces due to haptic feedback, and the skill is retained when tested after 24 hours without haptic feedback. keywords: {Training;Tongue;Magnetic sensors;Force;Phantoms;Mouth;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160755&isnumber=10160212

S. Onorati, F. Semproni, L. Paternò, G. Casagrande, V. Iacovacci and A. Menciassi, "A hydraulic soft robotic detrusor based on an origami design," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6817-6822, doi: 10.1109/ICRA48891.2023.10160652.Abstract: As a permanent solution for patients who cannot contract their urinary bladder, an artificial detrusor muscle appears a higher outcome approach compared to current sacral neurostimulators featured by severe long-term side effects. In this paper, a novel soft robotic detrusor is presented to overcome the limitations of the state-of-the-art solutions. It is based on two identical origami-based hydraulic actuators, which completely surround the bladder and contract upon water aspiration. Design, manufacturing, and experimental characterization both in terms of contraction capabilities and voiding efficiency on ex vivo swine bladders are reported for two different origami geometries, as well as a proof-of-concept implementation of an autonomous driving circuit as control unit. Results from assisted urination tests outlined very good performances proving an active voiding efficiency of the hydraulic soft robotic detrusor equal to 84.8% $\pm 7.4\%$ in simulated environment. keywords: {Geometry;Automation;Hydraulic actuators;Hydraulic systems;Soft robotics;Bladder;Muscles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160652&isnumber=10160212

D. Bautista-Salinas, C. Kirby, M. E. M. K. Abdelaziz, B. Temelkuran, C. T. Huins and F. Rodriguez y Baena, "Semi-autonomous robotic control of a self-shaping cochlear implant," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6823-6829, doi: 10.1109/ICRA48891.2023.10161565.Abstract: Cochlear implants (CIs) can improve hearing in patients suffering from sensorineural hearing loss via an electrode array (EA) carefully inserted in the scala tympani. Current EAs can cause trauma during insertion, threatening hearing preservation; hence we proposed a pre-curved thermally drawn EA that curls into the cochlea under the influence of body temperature. However, the additional surgical skill required to insert pre-curved EAs usually produces worse surgical outcomes. Medical robots can offer an effective solution to assist surgeons in improving surgical outcomes and reducing outliers. This work proposes a collaborative approach to insert our EA where manageable tasks are automated using a vision-based system. The insertion strategy presented allowed us to insert our EA successfully. The feasibility study showed that we can insert EAs following the defined control strategy while keeping the exerted contact forces within safe levels. The teleoperated robotic system and robotic vision approach to control a self-shaping CI has thus shown potential to provide the tools for a more delicate and atraumatic approach. keywords: {Temperature sensors;Cochlear implants;Medical robotics;Machine vision;Surgery;Phantoms;Auditory system},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161565&isnumber=10160212

C. He, R. H. Nguyen, C. Forbrigger, J. Drake, T. Looi and E. Diller, "A Hybrid Steerable Robot with Magnetic Wrist for Minimally Invasive Epilepsy Surgery," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6830-6836, doi: 10.1109/ICRA48891.2023.10160446.Abstract: Dexterity is demanded for an endoscopic tool to handle complicated procedures in neurosurgery, e.g., removing diseased tissue from inside the deep brain along a tortuous path. Current robotic tools are either rigid or lack wristed motion ability at the tip, leading to limited usage in minimally invasive procedures. In this paper, a hybrid steerable robot with a magnetic wristed forceps is proposed to provide enhanced dexterity for endoscopic epilepsy surgery. A set of three precurved Nitinol tubes with concentric deployment, called a concentric tube robot (CTR), serves as a 6 degrees-of-freedom (DoF) robotic positioner. The magnetic wristed forceps is composed of a rotational wrist joint, and forceps at the tip, both of which are actuated remotely by magnetic fields. The magnetic wrist and forceps provide an extra rotational DoF and a gripping DoF on top of the CTR, respectively. The magnetic wrist and gripper are designed to have a hollow channel along their common axis, inside which a soft tube is deployed as a second functional tool for irrigation or suction. An electromagnetic navigation system (eMNS) with 8 coils is used to create the quasi-static magnetic fields. Experimental characterization of the robot kinematics is performed and the results show the mean motion error of CTR is 2.8 mm. The workspace is also analyzed and results indicate that the proposed hybrid robot has a significantly larger reachable area compared to the one of the CTR alone. Mock epilepsy procedures are performed on a brain phantom to validate the feasibility of the hybrid robot for neurosurgery applications. keywords: {Wrist;Minimally invasive surgery;Navigation;Robot kinematics;Epilepsy;Phantoms;Electron tubes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160446&isnumber=10160212

N. Shihora and N. Simaan, "Induced Vertex Motion As a Performance Measure for Surgery in Confined Spaces," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6837-6843, doi: 10.1109/ICRA48891.2023.10161512.Abstract: While in the design phase of a robotic system for the procedures performed in surgical confined spaces or hard-to-reach-deep surgical fields, designers can leverage a systematic method to compare the design alternatives for tele-surgical manipulators quantitatively. Unlike most of the work in the literature, we propose an approach for comparing design alternatives by considering the spurious motions along the length of the manipulator in lieu of existing approaches looking at only the end-effector dexterity measures. We propose a performance measure quantifying these spurious motions while the end-effector executes the application-critical tasks such as suturing and tying a knot. A good manipulator design should yield minimal swept volume along its length portions within the confined space. If informed about these spurious motions, that design would lead to reduced force on the internal organs, reducing the pain and discomfort as well as occurrences of extracorporeal inter-manipulator collisions. To validate the proposed approach, we present two illustrative simulation case studies: (1) two planar rigid link serial robots performing the task of following a desired trajectory and (2) two different architectures of tele-surgical manipulators performing the task of passing a circular suture needle under the fulcrum constraints. The results show the applicability of the proposed performance measure in determining the suitability of a particular design alternative for a given task. Although results are promising, using this measure alone for design optimization may compromise overall device dexterity. Therefore, this measure needs to be incorporated into a weighted optimization framework for robot design. keywords: {Performance evaluation;Wrist;Weight measurement;Systematics;Surgery;Extraterrestrial measurements;End effectors;spurious motion;performance measure;confined space;deep surgical fields},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161512&isnumber=10160212

Y. Cheng, Y. Huang, Z. Wang and E. Burdet, "Foot gestures to control the grasping of a surgical robot," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6844-6850, doi: 10.1109/ICRA48891.2023.10160368.Abstract: Many surgical tasks require three or more tools working together, where a hands-free interface could extend a surgeon's actions to control a third surgical tool. However, most current interfaces do not allow skilled control of grasping critical to robotic manipulation. Here we first present a systematic study to identify efficient and intuitive interaction strategies to control grasping of a surgical tool. A series of experiments were conducted to evaluate six foot pressure-based gestures. Based on the results, three modular novel foot-machine interfaces were developed, which can be integrated with other motion control interfaces. The identified interaction strategies were implemented to control a laparoscopic tool in a surgical simulator, and evaluated in a user study. The results illustrate how naive participants can operate grasping yielding smooth and pick & place operation. keywords: {Laparoscopes;Systematics;Medical robotics;Automation;Grasping;Task analysis;Motion control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160368&isnumber=10160212

A. M. Zeidan et al., "Design and Development of a Novel Force-Sensing Robotic System for the Transseptal Puncture in Left Atrial Catheter Ablation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6851-6858, doi: 10.1109/ICRA48891.2023.10160254.Abstract: Transseptal puncture (TSP) is a prerequisite for left atrial catheter ablation for atrial fibrillation, requiring access from the right side of the heart. It is a demanding procedural step associated with complications, including inadvertent puncturing and application of large forces on the tissue wall. Robotic systems have shown great potential to overcome such challenges by introducing force-sensing capabilities and increased precision and localization accuracy. Therefore, this work introduces the design and development of a novel robotic system developed to perform TSP. We integrated optoelectronic sensors into the tools' fixtures, measuring tissue contact and puncture forces along one axis. The novelty of this design is in the system's ability to manipulate a Brockenbrough (BRK) needle and dilator-sheath simultaneously and measure tissue contact and puncture forces. In performing puncture experiments on anthropomorphic tissue models, an average puncture force of 3.97 ± 0.45 N (1SD) was established - similar to the force reported in literature on the manual procedure. This research highlights the potential for improving patient safety by enforcing force constraints, paving the way to more automated and safer TSP. keywords: {Location awareness;Force measurement;Integrated optoelectronics;Force;Manuals;Robot sensing systems;Needles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160254&isnumber=10160212

L. Bai, M. Islam, L. Seenivasan and H. Ren, "Surgical-VQLA:Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6859-6865, doi: 10.1109/ICRA48891.2023.10160403.Abstract: Despite the availability of computer-aided simulators and recorded videos of surgical procedures, junior residents still heavily rely on experts to answer their queries. However, expert surgeons are often overloaded with clinical and academic workloads and limit their time in answering. For this purpose, we develop a surgical question-answering system to facilitate robot-assisted surgical scene and activity understanding from recorded videos. Most of the existing visual question answering (VQA) methods require an object detector and regions based feature extractor to extract visual features and fuse them with the embedded text of the question for answer generation. However, (i) surgical object detection model is scarce due to smaller datasets and lack of bounding box annotation; (ii) current fusion strategy of heterogeneous modalities like text and image is naive; (iii) the localized answering is missing, which is crucial in complex surgical scenarios. In this paper, we propose Visual Question Localized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific surgical area during the answer prediction. To deal with the fusion of the heterogeneous modalities, we design gated vision-language embedding (GVLE) to build input patches for the Language Vision Transformer (LViT) to predict the answer. To get localization, we add the detection head in parallel with the prediction head of the LViT. We also integrate generalized intersection over union (GIoU) loss to boost localization performance by preserving the accuracy of the question-answering model. We annotate two datasets of VQLA by utilizing publicly available surgical videos from EndoVis-17 and 18 of the MICCAI challenges. Our validation results suggest that Surgical-VQLA can better understand the surgical scene and localized the specific area related to the question-answering. GVLE presents an efficient language-vision embedding technique by showing superior performance over the existing benchmarks. keywords: {Location awareness;Visualization;Head;Surgery;Predictive models;Logic gates;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160403&isnumber=10160212

H. Zhang, L. Zhu, J. Shen and A. Song, "Implicit Neural Field Guidance for Teleoperated Robot-assisted Surgery," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6866-6872, doi: 10.1109/ICRA48891.2023.10160475.Abstract: Teleoperated techniques enable remote human-robot interaction and have been widely accepted in robot-assisted surgeries. However, it is still hard to guarantee the safety of teleoperated surgery due to the imperfect input commands limited by remote perception, preventing teleoperated surgery from being widely used. We propose a new framework to avoid the collision of surgery robots and human tissue caused by inaccurate inputs. We directly take the medical volume data and propose to use the implicit neural field to guide teleoperated robot-assisted surgery. With guidance, the trajectory of the robot manipulator is optimized to safely work inside a narrow workspace. We evaluated our method in several aspects and conducted a real-world experiment on a head phantom. Experimental results show that our proposed method can effectively avoid the collision between the surgical tool and the human tissue during teleoperation. keywords: {Training;Medical robotics;Neural networks;Surgery;Phantoms;Mouth;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160475&isnumber=10160212

A. Zhang, Z. Min, L. Liu and M. Q. . -H. Meng, "Bidirectional Generalised Rigid Point Set Registration," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6873-6879, doi: 10.1109/ICRA48891.2023.10160361.Abstract: In medical robotics and image-guided surgery (IGS), registration is needed in order to align together the coordinate frames of robots, medical imaging modalities, surgical tools, and patients. Existing registration algorithms often assume one point set to be a noise-free model while the other to contain noise and outliers. However, in real scenarios, noise and outliers can exist in both point sets to be registered. To eliminate the above-mentioned challenge, in this paper, we formally formulate the Bi-directional Generalised Rigid Point Set Registration (Bi-GRPSR) problem where normal vectors are adopted, bi-directional probability density function (PDFs) and Hybrid Mixture Models (HMMs) are constructed to derive the objective function. Bi-GRPSR considering anisotropic positional noise is thus cast as a maximum likelihood estimation (MLE) problem, which is solved by the proposed Bi-directional Generalised Anisotropic Coherent Point Drift (Bi-AGCPD) where spatially nearby points are considered to move coherently and iterative expectation maximization (EM) steps are involved. Experimental results on two human bone point sets, under different settings of noise, outliers, and overlapping ratios, validate the effectiveness and improvements of Bi-AGCPD over existing probabilistic and learning-based methods. keywords: {Learning systems;Maximum likelihood estimation;Uncertainty;Robot kinematics;Surgery;Hidden Markov models;Bidirectional control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160361&isnumber=10160212

K. Almpanidis, T. Kastritsi and Z. Doulgeri, "Finding the Optimal Incision Point in Robotic Assisted Surgery," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6880-6885, doi: 10.1109/ICRA48891.2023.10160936.Abstract: In robotic assisted surgeries, surgical tools are inserted into the human body via an incision point in the abdominal wall, which is imposed as a remote center of motion (RCM). The selection of the incision's point location in the human body is critical for the success of the surgical procedure. In this paper, we propose a simulation tool for finding the optimal incision point location, which can be utilized by the surgeon during the preoperative stage. The surgeon can plan the path/region of intervention as well as sensitive regions which should be protected from unintentional damage by the surgical tool on the preoperative images of internal organs. A target admittance model that enforces a candidate incision as a RCM is utilized in the simulation enhanced by a term for following the planned path. We propose a cost evaluation function taking into account metrics involving the distance of the tool from sensitive areas, the tool links maximum pressure on tumors and the robot's dexterity measure. The example of a tumor resection task is used with the simulation tool to demonstrate its use in finding the incision points that ensures minimal intraoperative risks and accurate task execution. keywords: {Measurement;Target tracking;Costs;Minimally invasive surgery;Surgery;Robot sensing systems;Pressure measurement;Robot assisted minimal invasive surgery;Simulation tool;Remote Center of Motion;Evaluation function},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160936&isnumber=10160212

A. Martsopoulos, T. L. Hill, R. Persad, S. Bolomytis and A. Tzemanaki, "Development and Experimental Verification of a 3D Dynamic Absolute Nodal Coordinate Formulation Model of Flexible Prostate Biopsy/Brachytherapy Needles," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6886-6892, doi: 10.1109/ICRA48891.2023.10161254.Abstract: Robot-assisted percutaneous needle insertion is expected to significantly increase targeting accuracy in minimally invasive operations. For this, it is necessary to provide mathematical models that can accurately capture the underlying dynamics of medical needles. Here, we present a novel nonlinear mathematical model of flexible medical needles based on the Absolute Nodal Coordinate Formulation. The model allows the description of large needle deflections and arbitrarily large rigid body motions. Tailored to the requirements of transperineal prostate biopsy and brachytherapy, it can correlate both the translational and rotational coordinates of the needle's base with its deflection, provide force feedback and accept arbitrary loading conditions. The model is optimised in terms of computational efficiency in order to allow real-time simulation and control. Experiments show that the proposed model allows for submillimeter precision in both static and dynamic needle deflection settings. Due to its accuracy and computational efficiency, it is expected to constitute a valuable tool for both real-time visual/haptic simulation and control of percutaneous needle insertion. keywords: {Solid modeling;Three-dimensional displays;Biological system modeling;Computational modeling;Dynamics;Needles;Mathematical models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161254&isnumber=10160212

R. Mieling et al., "Collaborative Robotic Biopsy with Trajectory Guidance and Needle Tip Force Feedback," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6893-6900, doi: 10.1109/ICRA48891.2023.10161377.Abstract: The diagnostic value of biopsies is highly dependent on the placement of needles. Robotic trajectory guidance has been shown to improve needle positioning, but feedback for real-time navigation is limited. Haptic display of needle tip forces can provide rich feedback for needle navigation by enabling localization of tissue structures along the insertion path. We present a collaborative robotic biopsy system that combines trajectory guidance with kinesthetic feedback to assist the physician in needle placement. The robot aligns the needle while the insertion is performed in collaboration with a medical expert who controls the needle position on site. We present a needle design that senses forces at the needle tip based on optical coherence tomography and machine learning for real-time data processing. Our robotic setup allows operators to sense deep tissue interfaces independent of frictional forces to improve needle placement relative to a desired target structure. We first evaluate needle tip force sensing in ex-vivo tissue in a phantom study. We characterize the tip forces during insertions with constant velocity and demonstrate the ability to detect tissue interfaces in a collaborative user study. Participants are able to detect 91 percent of ex-vivo tissue interfaces based on needle tip force feedback alone. Finally, we demonstrate that even smaller, deep target structures can be accurately sampled by performing post-mortem in situ biopsies of the pancreas. keywords: {Navigation;Force feedback;Collaboration;Phantoms;Needles;Robot sensing systems;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161377&isnumber=10160212

