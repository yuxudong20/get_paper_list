J. Haworth et al., "Development and Evaluation of a Robotic Vessel Positioning System for Semi-Automatic Microvascular Anastomosis," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6901-6908, doi: 10.1109/ICRA48891.2023.10161296.Abstract: This paper describes a novel tissue positioning system with an integrated suturing robot and demonstrates its ability to perform semi-automatic anastomoses of synthetic blood vessels. We began with a finite element analysis-based design consideration for achieving adequate grasping of blood vessels to demonstrate robust performance under expected clinical forces. We then conducted standardized positioning tests to measure the repeatability of the system and incorporated a high-resolution optical coherence tomography (OCT) fiber imaging sensor within the tip of the suturing tool to provide position feedback of the robot during a suturing task. Using the microvascular positioner and OCT sensor, the system performed semi-automatic suturing of synthetic 5 mm diameter blood vessels ($\mathrm{N}=4$), and the suture quality was evaluated for consistency in spacing, bite depth, percent lumen reduction, and maximum suture strength. The system completed the task in an average time of 31.75 minutes. The samples had zero missed stitches, average spacing of 1.64 mm, an average bite depth of 2.14 mm, an average lumen reduction of 57.98%, and an average suture strength of 3.13 N. keywords: {Optical fiber sensors;Optical coherence tomography;Blood vessels;Lumen;Position measurement;Optical variables measurement;Robot sensing systems;Semi-Automatic;robotic suturing;microvascular anastomosis;OCT imaging;microsurgery},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161296&isnumber=10160212

D. Raina, S. Chandrashekhara, R. Voyles, J. Wachs and S. K. Saha, "Robotic Sonographer: Autonomous Robotic Ultrasound using Domain Expertise in Bayesian Optimization," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6909-6915, doi: 10.1109/ICRA48891.2023.10161542.Abstract: Ultrasound is a vital imaging modality utilized for a variety of diagnostic and interventional procedures. However, an expert sonographer is required to make accurate maneuvers of the probe over the human body while making sense of the ultrasound images for diagnostic purposes. This procedure requires a substantial amount of training and up to a few years of experience. In this paper, we propose an autonomous robotic ultrasound system that uses Bayesian Optimization (BO) in combination with the domain expertise to predict and effectively scan the regions where diagnostic quality ultrasound images can be acquired. The quality map, which is a distribution of image quality in a scanning region, is estimated using Gaussian process in BO. This relies on a prior quality map modeled using expert's demonstration of the high-quality probing maneuvers. The ultrasound image quality feedback is provided to BO, which is estimated using a deep convolution neural network model. This model was previously trained on database of images labelled for diagnostic quality by expert radiologists. Experiments on three different urinary bladder phantoms validated that the proposed autonomous ultrasound system can acquire ultrasound images for diagnostic purposes with a probing position and force accuracy of 98.7% and 97.8%, respectively. keywords: {Image quality;Training;Ultrasonic imaging;Protocols;System performance;Imaging phantoms;Robot sensing systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161542&isnumber=10160212

Y. Lu et al., "Autonomous Intelligent Navigation for Flexible Endoscopy Using Monocular Depth Guidance and 3-D Shape Planning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 1-7, doi: 10.1109/ICRA48891.2023.10161505.Abstract: Recent advancements toward perception and decision-making of flexible endoscopes have shown great potential in computer-aided surgical interventions. However, owing to modeling uncertainty and inter-patient anatomical variation in flexible endoscopy, the challenge remains for efficient and safe navigation in patient-specific scenarios. This paper presents a novel data-driven framework with self-contained visual-shape fusion for autonomous intelligent navigation of flexible endoscopes requiring no priori knowledge of system models and global environments. A learning-based adaptive visual servoing controller is proposed to online update the eye-in-hand vision-motor configuration and steer the endoscope, which is guided by monocular depth estimation via a vision transformer (ViT). To prevent unnecessary and excessive interactions with surrounding anatomy, an energy-motivated shape planning algorithm is introduced through entire endoscope 3-D proprioception from embedded fiber Bragg grating (FBG) sensors. Furthermore, a model predictive control (MPC) strategy is developed to minimize the elastic potential energy flow and simultaneously optimize the steering policy. Dedicated navigation experiments on a robotic-assisted flexible endoscope with an FBG fiber in several phantom environments demonstrate the effectiveness and adaptability of the proposed framework. keywords: {Potential energy;Adaptation models;Endoscopes;Navigation;Shape;Computational modeling;Phantoms},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161505&isnumber=10160212

H. Sato, T. Ikeda and K. Nishiwaki, "A Probabilistic Rotation Representation for Symmetric Shapes With an Efficiently Computable Bingham Loss Function," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6923-6929, doi: 10.1109/ICRA48891.2023.10160682.Abstract: In recent years, a deep learning framework has been widely used for object pose estimation. While quaternion is a common choice for rotation representation, it cannot represent the ambiguity of the observation. In order to handle the ambiguity, the Bingham distribution is one promising solution. However, it requires complicated calculation when yielding the negative log-likelihood (NLL) loss. An alternative easy-to-implement loss function has been proposed to avoid complex computations but has difficulty expressing symmetric distribution. In this paper, we introduce a fast-computable and easy-to-implement NLL loss function for Bingham distribution. We also create the inference network and show that our loss function can capture the symmetric property of target objects from their point clouds. keywords: {Point cloud compression;Deep learning;Shape;Planets;Quaternions;Pose estimation;Neural networks},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160682&isnumber=10160212

J. Wakulicz, K. M. Brian Lee, T. Vidal-Calleja and R. Fitch, "Topological Trajectory Prediction with Homotopy Classes," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6930-6936, doi: 10.1109/ICRA48891.2023.10160250.Abstract: Trajectory prediction in a cluttered environment is key to many important robotics tasks such as autonomous navigation. However, there are an infinite number of possible trajectories to consider. To simplify the space of trajectories under consideration, we utilise homotopy classes to partition the space into countably many mathematically equivalent classes. All members within a class demonstrate identical high-level motion with respect to the environment, i.e., travelling above or below an obstacle. This allows high-level prediction of a trajectory in terms of a sparse label identifying its homotopy class. We therefore present a light-weight learning framework based on variable-order Markov processes to learn and predict homotopy classes and thus high-level agent motion. By informing a Gaussian mixture model (GMM) with our homotopy class predictions, we see great improvements in low-level trajectory prediction compared to a naive GMM on a real dataset. keywords: {Instruments;Markov processes;Search problems;Prediction algorithms;Trajectory;Space exploration;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160250&isnumber=10160212

D. T. Larsson, A. Asgharivaskasi, J. Lim, N. Atanasov and P. Tsiotras, "Information-theoretic Abstraction of Semantic Octree Models for Integrated Perception and Planning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6937-6943, doi: 10.1109/ICRA48891.2023.10160407.Abstract: In this paper, we develop an approach that enables autonomous robots to build and compress semantic environment representations from point-cloud data. Our approach builds a three-dimensional, semantic tree representation of the environment from raw sensor data which is then compressed by a novel information-theoretic tree-pruning approach. The proposed approach is probabilistic and incorporates the uncertainty in semantic classification inherent in real-world environments. Moreover, our approach allows robots to prioritize individual semantic classes when generating the compressed trees, so as to design multi-resolution representations that retain the relevant semantic information while simultaneously discarding unwanted semantic categories. We demonstrate the approach by compressing semantic octree models of a large outdoor, semantically rich, real-world environment. In addition, we show how the octree abstractions can be used to create semantically-informed graphs for motion planning, and provide a comparison of our approach with uninformed graph construction methods such as Halton sequences. keywords: {Solid modeling;Uncertainty;Semantics;Octrees;Robot sensing systems;Probabilistic logic;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160407&isnumber=10160212

H. Biggie, A. Beathard and C. Heckman, "BO-ICP: Initialization of Iterative Closest Point Based on Bayesian Optimization," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6944-6950, doi: 10.1109/ICRA48891.2023.10160570.Abstract: Typical algorithms for point cloud registration such as Iterative Closest Point (ICP) require a favorable initial transform estimate between two point clouds in order to perform a successful registration. State-of-the-art methods for choosing this starting condition rely on stochastic sampling or global optimization techniques such as branch and bound. In this work, we present a new method based on Bayesian optimization for finding the critical initial ICP transform. We provide three different configurations for our method which highlights the versatility of the algorithm to both find rapid results and refine them in situations where more runtime is available such as offline map building. Experiments are run on popular data sets and we show that our approach outperforms state-of-the-art methods when given similar computation time. Furthermore, it is compatible with other improvements to ICP, as it focuses solely on the selection of an initial transform, a starting point for all ICP-based methods. keywords: {Point cloud compression;Runtime;Automation;Iterative closest point algorithm;Buildings;Transforms;Bayes methods},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160570&isnumber=10160212

X. Wang, J. Lei, H. Lan, A. Al-Jawari and X. Wei, "DuEqNet: Dual-Equivariance Network in Outdoor 3D Object Detection for Autonomous Driving," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6951-6957, doi: 10.1109/ICRA48891.2023.10161353.Abstract: Outdoor 3D object detection has played an essential role in the environment perception of autonomous driving. In complicated traffic situations, precise object recognition provides indispensable information for prediction and planning in the dynamic system, improving self-driving safety and reliability. However, with the vehicle's veering, the constant rotation of the surrounding scenario makes a challenge for the perception systems. Yet most existing methods have not focused on alleviating the detection accuracy impairment brought by the vehicle's rotation, especially in outdoor 3D detection. In this paper, we propose DuEqNet, which first introduces the concept of equivariance into 3D object detection network by leveraging a hierarchical embedded framework. The dual-equivariance of our model can extract the equivariant features at both local and global levels, respectively. For the local feature, we utilize the graph-based strategy to guarantee the equivariance of the feature in point cloud pillars. In terms of the global feature, the group equivariant convolution layers are adopted to aggregate the local feature to achieve the global equivariance. In the experiment part, we evaluate our approach with different baselines in 3D object detection tasks and obtain State-Of-The-Art performance. According to the results, our model presents higher accuracy on orientation and better prediction efficiency. Moreover, our dual-equivariance strategy exhibits the satisfied plug-and-play ability on various popular object detection frameworks to improve their performance. keywords: {Point cloud compression;Three-dimensional displays;Object detection;Predictive models;Feature extraction;Safety;Planning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161353&isnumber=10160212

A. Popov, P. Gebhardt, K. Chen and R. Oldja, "NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6958-6964, doi: 10.1109/ICRA48891.2023.10160592.Abstract: Detecting obstacles is crucial for safe and efficient autonomous driving. To this end, we present NVRadarNet, a deep neural network (DNN) that detects dynamic obstacles and drivable free space using automotive RADAR sensors. The network utilizes temporally accumulated data from multiple RADAR sensors to detect dynamic obstacles and compute their orientation in a top-down bird's-eye view (BEV). The network also regresses drivable free space to detect unclassified obstacles. Our DNN is the first of its kind to utilize sparse RADAR signals in order to perform obstacle and free space detection in real time from RADAR data only. The network has been successfully used for perception on our autonomous vehicles in real self-driving scenarios. The network runs faster than real time on an embedded GPU and shows good generalization across geographic regions.11Video at https://youtu.be/WlwJJMltoJY. keywords: {Deep learning;Computers;Spaceborne radar;Neural networks;Radar detection;Graphics processing units;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160592&isnumber=10160212

H. Zou, Z. Xie, J. Ou and Y. Gao, "TransRSS: Transformer-based Radar Semantic Segmentation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6965-6972, doi: 10.1109/ICRA48891.2023.10161200.Abstract: Radar semantic segmentation is a challenging task in environmental understanding, due as the radar data is noisy and suffers measurement ambiguities, which could lead to poor feature learning. To better tackle such difficulties, we present a novel and high-performance Transformer-based Radar Semantic Segmentation method, named TransRSS, to effectively and efficiently feature extraction for radar segmentation. Our approach first introduces the transformer into radar semantic segmentation and deeply integrates the merits of the Convolutional Neural Network (CNN) and transformer to extract more discriminative and global-level semantic features. On the one hand, it takes advantage of the CNN with flexible receptive fields to process images thanks to the shift convolution scheme. On the other hand, it takes advantage of the transformer to model long-range dependency with the self-attention mechanism. Meanwhile, we propose a Dual Position Attention module to aggregate rich context interdependencies between the multi-view features, which achieves an implicit mechanism for adaptively feature aggregation. Extensive experiments on the CARRADA dataset and RADIal dataset demonstrate that our TransRSS surpasses the state-of-the-art (SOTA) radar segmentation methods with remarkable margins. keywords: {Adaptation models;Radar measurements;Semantic segmentation;Semantics;Radar;Radar imaging;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161200&isnumber=10160212

D. Hegde, V. Kilic, V. Sindagi, A. B. Cooper, M. Foster and V. M. Patel, "Source-free Unsupervised Domain Adaptation for 3D Object Detection in Adverse Weather," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6973-6980, doi: 10.1109/ICRA48891.2023.10161341.Abstract: A domain shift exists between the distributions of large scale, outdoor lidar datasets due to being captured using different types of lidar sensors, in different locations, and under varying weather conditions. Inclement weather in particular affects the quality of lidar data, adding artifacts such as scattered and missed points, leading to a drop in performance of 3D object detection networks trained on standard lidar datasets. Domain adaptation methods seek to adapt source-trained neural networks to a target domain. Pseudo-label based self training approaches are popular methods for source-free unsupervised domain adaptation. However, their efficacy depends on the quality of the labels generated by the source trained model. These labels may be incorrect with high confidence, rendering thresholding methods ineffective. In order to avoid reinforcing errors caused by label noise, we propose an uncertainty-aware mean teacher framework which implicitly filters incorrect pseudo-labels during training. Leveraging model uncertainty allows the mean teacher network to perform implicit filtering by down-weighing losses corresponding to uncertain pseudo-labels. Effectively, we perform automatic soft-sampling of pseudo-labeled data while aligning predictions from the student and teacher networks. We demonstrate our domain adaptation method on an adverse weather dataset created by augmenting lidar scenes from KITTI with rain, snow, and fog and show that it out-performs current domain adaptation frameworks. We make our code publicly available 11https://github.com/deeptibhegde/UncertaintyAwareMeanTeacher. keywords: {Training;Adaptation models;Solid modeling;Laser radar;Three-dimensional displays;Uncertainty;Snow},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161341&isnumber=10160212

L. Mur-Labadia, R. Martinez-Cantin and J. J. Guerrero, "Bayesian deep learning for affordance segmentation in images," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6981-6987, doi: 10.1109/ICRA48891.2023.10160606.Abstract: Affordances are a fundamental concept in robotics since they relate available actions for an agent depending on its sensory-motor capabilities and the environment. We present a novel Bayesian deep network to detect affordances in images, at the same time that we quantify the distribution of the aleatoric and epistemic variance at the spatial level. We adapt the Mask-RCNN architecture to learn a probabilistic representation using Monte Carlo dropout. Our results outperform the state-of-the-art of deterministic networks. We attribute this improvement to a better probabilistic feature space representation on the encoder and the Bayesian variability induced at the mask generation, which adapts better to the object contours. We also introduce the new Probability-based Mask Quality measure that reveals the semantic and spatial differences on a probabilistic instance segmentation model. We modify the existing Probabilistic Detection Quality metric by comparing the binary masks rather than the predicted bounding boxes, achieving a finer-grained evaluation of the probabilistic segmentation. We find aleatoric variance in the contours of the objects due to the camera noise, while epistemic variance appears in visual challenging pixels. keywords: {Measurement;Image segmentation;Visualization;Uncertainty;Affordances;Semantics;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160606&isnumber=10160212

A. Li and A. P. Schoellig, "Multi-View Keypoints for Reliable 6D Object Pose Estimation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6988-6994, doi: 10.1109/ICRA48891.2023.10160354.Abstract: 6D Object pose estimation is a fundamental component in robotics enabling efficient interaction with the environment. 6D pose estimation is particularly challenging in bin- picking applications, where many objects are low-feature and reflective, and self-occlusion between objects of the same type is common. We propose a novel multi-view approach leveraging known camera transformations from an eye-in-hand setup to combine heatmap and keypoint estimates into a probability density map over 3D space. The result is a robust approach that is scalable in the number of views. It relies on a confidence score composed of keypoint probabilities and point-cloud alignment error, which allows reliable rejection of false positives. We demonstrate an average pose estimation error of approximately 0.5 mm and 2 degrees across a variety of difficult low-feature and reflective objects in the ROBI dataset, while also surpassing the state-of-art correct detection rate, measured using the 10% object diameter threshold on ADD error. keywords: {Three-dimensional displays;Automation;Pose estimation;Robot vision systems;Measurement uncertainty;Cameras;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160354&isnumber=10160212

D. Sacoransky, J. A. Marshall and K. Hashtrudi-Zaad, "Towards Unsupervised Filtering of Millimetre-Wave Radar Returns for Autonomous Vehicle Road Following," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 6995-7001, doi: 10.1109/ICRA48891.2023.10161274.Abstract: Path planning and localization in low-light and inclement weather conditions are critical problems facing autonomous vehicle systems. Our proposed method applies a single modality, millimetre-wave radar perception system for the detection of roadside retro-reflectors. Radar-based perception tasks can be challenging to perform due to the sparse and noisy nature of radar data. We propose the use of an unsupervised learning approach for filtering radar point clouds through Density-Based Spatial Clustering of Applications with Noise (DBSCAN). The DBSCAN algorithm segments retro-reflector points from noise points, thus providing the autonomous vehicle with a predicted path for the road ahead. We tested the approach via indoor experiments that make use of Continental's ARS 408 radar, a mobile Husky A2000 robot, and a Vicon motion capture system for ground truth validation. The experimental results of the proposed system demonstrated a classification accuracy of 84.13 % and F1 score of 83.71 %. keywords: {Point cloud compression;Filtering;Roads;Radar detection;Prediction algorithms;Path planning;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161274&isnumber=10160212

K. Seemakurthy, P. Bosilj, E. Aptoula and C. Fox, "Domain Generalised Fully Convolutional One Stage Detection," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7002-7009, doi: 10.1109/ICRA48891.2023.10160937.Abstract: Real-time vision in robotics plays an important role in localising and recognising objects. Recently, deep learning approaches have been widely used in robotic vision. However, most of these approaches have assumed that training and test sets come from similar data distributions, which is not valid in many real world applications. This study proposes an approach to address domain generalisation (i.e. out-of-distribution generalisation, OODG) where the goal is to train a model via one or more source domains, that will generalise well to unknown target domains using single stage detectors. All existing approaches which deal with OODG either use slow two stage detectors or operate under the covariate shift assumption which may not be useful for real-time robotics. This is the first paper to address domain generalisation in the context of single stage anchor free object detector FCOS without the covariate shift assumption. We focus on improving the generalisation ability of object detection by proposing new regularisation terms to address the domain shift that arises due to both classification and bounding box regression. Also, we include an additional consistency regularisation term to align the local and global level predictions. The proposed approach is implemented as a Domain Generalised Fully Convolutional One Stage (DGFCOS) detection and evaluated using four object detection datasets which provide domain metadata (GWHD, Cityscapes, BDD100K, Sim10K) where it exhibits a consistent performance improvement over the baselines and is able to run in real-time for robotics. keywords: {Training;Deep learning;Automation;Detectors;Object detection;Metadata;Real-time systems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160937&isnumber=10160212

W. -H. Liao, C. -C. Wang and W. -C. Lin, "GNN-Based Point Cloud Maps Feature Extraction and Residual Feature Fusion for 3D Object Detection," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7010-7016, doi: 10.1109/ICRA48891.2023.10160932.Abstract: LiDAR detection of long-range vehicles is challenging because very few and sparse points are measured in long distances and vehicles with similar shapes of targets could lead to false positives easily. To tackle these challenges, taking the environment information (HD maps) into account could be beneficial to predetermine where targets are more or less likely to appear. Compared with semantic maps, HD maps formed by point clouds provide much richer information from surrounding static objects and scenes. In this work, we construct a GNN-based feature extraction of point cloud maps to increase the receptive fields of learning map features. Our work is based on PVRCNN, the state-of-the-art LiDAR object detection method. With point-wise and voxel-wise features obtained from PVRCNN, residual feature fusion is proposed to fuse the features from PVRCNN and the map features from GNN. Our approach is evaluated on NuScenes dataset. It achieves a 24.78% average precision improvement for long-range objects at 40–50 meters, the farthest areas with ground truth annotation. Our approach also has a 4.22% reduction of false positives in the entire sensing areas. keywords: {Point cloud compression;Solid modeling;Laser radar;Three-dimensional displays;Fuses;Shape;Shape measurement;LiDAR;Object Detection;Self-Driving Cars;Point Cloud Maps},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160932&isnumber=10160212

S. Lu, Y. Deng, A. Boularias and K. Bekris, "Self-Supervised Learning of Object Segmentation from Unlabeled RGB-D Videos," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7017-7023, doi: 10.1109/ICRA48891.2023.10160786.Abstract: This work proposes a self-supervised learning system for segmenting rigid objects in RGB images. The proposed pipeline is trained on unlabeled RGB-D videos of static objects, which can be captured with a camera carried by a mobile robot. A key feature of the self-supervised training process is a graph-matching algorithm that operates on the over-segmentation output of the point cloud that is reconstructed from each video. The graph matching, along with point cloud registration, is able to find reoccurring object patterns across videos and combine them into 3D object pseudo labels, even under occlusions or different viewing angles. Projected 2D object masks from 3D pseudo labels are used to train a pixel-wise feature extractor through contrastive learning. During online inference, a clustering method uses the learned features to cluster foreground pixels into object segments. Experiments highlight the method's effectiveness on both real and synthetic video datasets, which include cluttered scenes of tabletop objects. The proposed method outperforms existing unsupervised methods for object segmentation by a large margin. keywords: {Point cloud compression;Training;Three-dimensional displays;Robot vision systems;Pipelines;Self-supervised learning;Object segmentation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160786&isnumber=10160212

D. Park, J. Li, D. Chen, V. Guizilini and A. Gaidon, "Depth Is All You Need for Monocular 3D Detection," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7024-7031, doi: 10.1109/ICRA48891.2023.10160483.Abstract: A key contributor to recent progress in 3D detection from single images is monocular depth estimation. Existing methods focus on how to leverage depth explicitly, by generating pseudo-pointclouds or providing attention cues for image features. More recent works leverage depth prediction as a pretraining task and fine-tune the depth representation while training it for 3D detection. However, the adaptation is limited in scale by manual labels. In this work, we propose further aligning the depth representation with the target domain in an unsupervised fashion. Our methods leverage commonly available LiDAR or RGB videos during training time to fine-tune the depth representation, which leads to improved 3D detectors. Especially when using RGB videos, we show that our two-stage training by first generating depth pseudo-labels is critical, because of the inconsistency in loss distribution between the two tasks. With either type of reference data, our multi-task learning approach improves over the state of the art on both KITTI and NuScenes, while matching the test-time complexity of its single-task sub-network. Source code and pretrained models are available on https://github.com/TRI-ML/DD3D. keywords: {Training;Representation learning;Three-dimensional displays;Laser radar;Source coding;Estimation;Manuals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160483&isnumber=10160212

V. Kozák, J. Mikula, L. Bertl, K. Košnar and L. Přeučil, "Towards Visual Classification Under Class Ambiguity," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7032-7038, doi: 10.1109/ICRA48891.2023.10161568.Abstract: Visual classification under uncertainty is a complex computer vision problem. We present a thorough comparison of several variants of convolutional neural network (CNN) classification techniques in the context of ambiguous image data interpretation. We explore possible improvements in classification accuracy achieved by insertion of prior ambiguity information during the annotation process. This enables us to harness known similarities between individual classes and use them as probability distributions for soft ground-truth labels. We also present an approach based on Bayesian CNNs, offering the possibility of further interpretation of classification results in a problem where the neural network model is often considered as a black box. The presented techniques are verified on a practical spot weld inspection problem. keywords: {Visualization;Uncertainty;Neural networks;Machine learning;Spot welding;Inspection;Probability distribution},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161568&isnumber=10160212

Z. Leng et al., "Lidar Augment: Searching for Scalable 3D LiDAR Data Augmentations," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7039-7045, doi: 10.1109/ICRA48891.2023.10161037.Abstract: Data augmentations are important for training high-performance 3D object detectors that use point clouds. Despite recent efforts on designing new data augmentations, perhaps surprisingly, most current state-of-the-art 3D detectors only rely on a few simple data augmentations. In particular, different from 2D image data augmentations, 3D data augmentations need to account for different representations of input data and require being customized for different models, which introduces significant overhead. In this paper, we propose LidarAugment, a practical and effective data augmentation strategy for 3D object detection. Unlike previous methods, which require tuning all augmentation policies in an exponentially large search space, we propose to factorize and align the search space of each data augmentation, which cuts down the 20+ hyperparameters to 2, and significantly reduces the search complexity. We show LidarAugment can be easily adapted to different model architectures with different input representations by a simple 2D grid search, and consistently improve a range of detectors including both convolution-based UPillars/StarNet/RSN and transformer-based SWFormer. Furthermore, Lidar Augment mitigates overfitting and enables 3D detectors to scale up to larger capacities. When combined with the latest 3D detectors, Lidar Augment achieves a new state-of-the-art 74.8 mAPH L2 on the Waymo Open Dataset. keywords: {Solid modeling;Three-dimensional displays;Laser radar;Detectors;Object detection;Data augmentation;Search problems},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161037&isnumber=10160212

J. Zou, Z. Zhu, J. Huang, T. Yang, G. Huang and X. Wang, "HFT: Lifting Perspective Representations via Hybrid Feature Transformation for BEV Perception," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7046-7053, doi: 10.1109/ICRA48891.2023.10161214.Abstract: Restoring an accurate Bird's Eye View (BEV) map plays a crucial role in the perception of autonomous driving. The existing works of lifting representations from frontal view to BEV can be classified into two categories, i.e., Camera model-Based Feature Transformation (CBFT) and Camera model-Free Feature Transformation (CFFT). We empirically analyze the significant differences between CBFT and CFFT. The former method lift perspective features based on the flat- world assumption, which often causes distortion of regions lying above the ground plane. The latter method is limited in the perception performance due to the absence of geometric priors and time-consuming computing. In this paper, we propose a novel framework with a Hybrid Feature Transformation module (HFT) to lift perspective representations. Furthermore, we design a mutual learning scheme to augment hybrid transformation. The deformable attention mechanism enables the model to pay more attention to relevant regions and capture features with more semantics. We illustrate the effectiveness of HFT in BEV perception tasks, such as segmentation and object detection. Notably, in the task of semantic segmentation, extensive experiments demonstrate that HFT outperforms the previous state-of-the-art method by relatively 17.9% on the Argoverse and 22.0% on the KITTI 3D Object dataset. With negligible computing budget, HFT outperforms existing image- based methods on 3D object detection. The code will be released soon. keywords: {Geometry;Deformable models;Three-dimensional displays;Semantic segmentation;Semantics;Object detection;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161214&isnumber=10160212

M. Zeller, V. S. Sandhu, B. Mersch, J. Behley, M. Heidingsfeld and C. Stachniss, "Radar Velocity Transformer: Single-scan Moving Object Segmentation in Noisy Radar Point Clouds," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7054-7061, doi: 10.1109/ICRA48891.2023.10161152.Abstract: The awareness about moving objects in the surroundings of a self-driving vehicle is essential for safe and reliable autonomous navigation. The interpretation of LiDAR and camera data achieves exceptional results but typically requires to accumulate and process temporal sequences of data in order to extract motion information. In contrast, radar sensors, which are already installed in most recent vehicles, can overcome this limitation as they directly provide the Doppler velocity of the detections and, hence incorporate instantaneous motion information within a single measurement. In this paper, we tackle the problem of moving object segmentation in noisy radar point clouds. We also consider differentiating parked from moving cars, to enhance scene understanding. Instead of exploiting temporal dependencies to identify moving objects, we develop a novel transformer-based approach to perform single-scan moving object segmentation in sparse radar scans accurately. The key to our Radar Velocity Transformer is to incorporate the valuable velocity information throughout each module of the network, thereby enabling the precise segmentation of moving and non-moving objects. Additionally, we propose a transformer-based upsampling, which enhances the performance by adaptively combining information and over-coming the limitation of interpolation of sparse point clouds. Finally, we create a new radar moving object segmentation benchmark based on the RadarScenes dataset and compare our approach to other state-of-the-art methods. Our network runs faster than the frame rate of the sensor and shows superior segmentation results using only single-scan radar data. keywords: {Point cloud compression;Radar;Object segmentation;Benchmark testing;Transformers;Doppler radar;Sensors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161152&isnumber=10160212

Y. Bai, Z. Chen, Z. Fu, L. Peng, P. Liang and E. Cheng, "CurveFormer: 3D Lane Detection by Curve Propagation with Curve Queries and Attention," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7062-7068, doi: 10.1109/ICRA48891.2023.10161160.Abstract: 3D lane detection is an integral part of au-tonomous driving systems. Previous CNN and Transformer-based methods usually first generate a bird's-eye-view (BEV) feature map from the front view image, and then use a sub-network with BEV feature map as input to predict 3D lanes. Such approaches require an explicit view transformation between BEV and front view, which itself is still a challenging problem. In this paper, we propose CurveFormer, a single-stage Transformer-based method that directly calculates 3D lane pa-rameters and can circumvent the difficult view transformation step. Specifically, we formulate 3D lane detection as a curve propagation problem by using curve queries. A 3D lane query is represented by a dynamic and ordered anchor point set. In this way, queries with curve representation in Transformer decoder iteratively refine the 3D lane detection results. Moreover, a curve cross-attention module is introduced to compute the similarities between curve queries and image features. Additionally, a context sampling module that can capture more relative image features of a curve query is provided to further boost the 3D lane detection performance. We evaluate our method for 3D lane detection on both synthetic and real-world datasets, and the experimental results show that our method achieves promising performance compared with the state-of-the-art approaches. The effectiveness of each component is validated via ablation studies as well. keywords: {Three-dimensional displays;Automation;Lane detection;Transformers;Feature extraction;Decoding;Autonomous vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161160&isnumber=10160212

Y. Liu, N. Mishra, P. Abbeel and X. Chen, "Distributional Instance Segmentation: Modeling Uncertainty and High Confidence Predictions with Latent-MaskRCNN," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7069-7075, doi: 10.1109/ICRA48891.2023.10160812.Abstract: Object recognition and instance segmentation are fundamental skills in any robotic or autonomous system. Existing state-of-the-art methods are often unable to capture meaningful uncertainty in challenging or ambiguous scenes, and as such can cause critical errors in high-performance applications. In this paper, we explore a class of distributional instance segmentation models using latent codes that can model uncertainty over plausible hypotheses of object masks. For robotic picking applications, we propose a confidence mask method to achieve the high precision necessary in industrial use cases. We show that our method can significantly reduce critical errors in robotic systems, including our newly released dataset of ambiguous scenes in a robotic application. On a real-world apparel-picking robot, our method significantly reduces double pick errors while maintaining high performance. keywords: {Uncertainty;Codes;Automation;Service robots;Autonomous systems;Predictive models;Object recognition},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160812&isnumber=10160212

K. Montalban, C. Reymann, D. Atchuthan, P. -E. Dupouy, N. Rivière and S. Lacroix, "Bayesian inference of fog visibility from LiDAR point clouds and correlation with probabilities of detection," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7076-7082, doi: 10.1109/ICRA48891.2023.10161535.Abstract: Degraded visual environments have strong impacts on the quality of LiDAR data. Experiments in artificial fog conditions show that noise points caused by water particles present various distance distributions which depend on visibility. This article introduces a mathematical framework based on Bayesian inference and Markov Chain Monte-Carlo sampling to infer optical visibility from point clouds. The visibility estimation is cast as a classification problem based on the identification of the distance distributions. Contrary to deep learning methods, our approach is model-based and focuses on the design of a full probabilistic framework, more comprehensible, which is critical for autonomous driving. Ultimately, the impact of the optical visibility on the probability of detection of standard targets is assessed, which can yield improvements on autonomous vehicles performances in adverse weather conditions. keywords: {Point cloud compression;Laser radar;Correlation;Probabilistic logic;Adaptive optics;Data models;Bayes methods},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161535&isnumber=10160212

S. Kalwar, D. Patel, A. Aanegola, K. R. Konda, S. Garg and K. M. Krishna, "GDIP: Gated Differentiable Image Processing for Object Detection in Adverse Conditions," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7083-7089, doi: 10.1109/ICRA48891.2023.10160356.Abstract: Detecting objects under adverse weather and lighting conditions is crucial for the safe and continuous operation of an autonomous vehicle, and remains an unsolved problem. We present a Gated Differentiable Image Processing (GDIP) block, a domain-agnostic network architecture, which can be plugged into existing object detection networks (e.g., Yolo) and trained end-to-end with adverse condition images such as those captured under fog and low lighting. Our pro-posed GDIP block learns to enhance images directly through the downstream object detection loss. This is achieved by learning parameters of multiple image pre-processing (IP) techniques that operate concurrently, with their outputs combined using weights learned through a novel gating mechanism. We further improve GDIP through a multi-stage guidance procedure for progressive image enhancement. Finally, trading off accuracy for speed, we propose a variant of GDIP that can be used as a regularizer for training Yolo, which eliminates the need for GDIP-based image enhancement during inference, resulting in higher throughput and plausible real-world deployment. We demonstrate significant improvement in detection performance over several state-of-the-art methods through quantitative and qualitative studies on synthetic datasets such as PascalVOC, and real-world foggy (RTTS) and low-lighting (ExDark) datasets. keywords: {Training;Lighting;Object detection;Network architecture;Logic gates;Throughput;Autonomous vehicles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160356&isnumber=10160212

S. Shin, S. Golodetz, M. Vankadari, K. Zhou, A. Markham and N. Trigoni, "Sample, Crop, Track: Self-Supervised Mobile 3D Object Detection for Urban Driving LiDAR," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7090-7096, doi: 10.1109/ICRA48891.2023.10160980.Abstract: Deep learning has led to great progress in the detection of mobile (i.e. movement-capable) objects in urban driving scenes in recent years. Supervised approaches typically require the annotation of large training sets; there has thus been great interest in leveraging weakly, semi- or self- supervised methods to avoid this, with much success. Whilst weakly and semi-supervised methods require some annotation, self-supervised methods have used cues such as motion to relieve the need for annotation altogether. However, a complete absence of annotation typically degrades their performance, and ambiguities that arise during motion grouping can inhibit their ability to find accurate object boundaries. In this paper, we propose a new self-supervised mobile object detection approach called SCT. This uses both motion cues and expected object sizes to improve detection performance, and predicts a dense grid of 3$D$ oriented bounding boxes to improve object discovery. We significantly outperform the state-of-the-art self-supervised mobile object detection method TCR on the KITTI tracking benchmark, and achieve performance that is within 30 % of the fully supervised PV-RCNN++ method for IoUs $\leq$ 0.5. Our source code will be made available online. keywords: {Training;Three-dimensional displays;Target tracking;Pedestrians;Annotations;Source coding;Object detection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160980&isnumber=10160212

M. Zürn, M. Wnuk, A. Lechler and A. Verl, "Topology Matching of Branched Deformable Linear Objects," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7097-7103, doi: 10.1109/ICRA48891.2023.10161483.Abstract: This paper presents a new method for correspondence estimation between a previously known topology of a branched deformable linear object and an image representation from a 3D stereo camera. Although frequently encountered in production, robotic deformable linear object manipulation still lacks reliable sensor feedback. Especially for branched deformable linear objects, such as wire harnesses, correspondence estimation is very challenging. Due to their flexible nature, they have an infinite-dimensional configuration space, such that visual appearances of the same object can vary strongly. Knowing the correspondence is vital for various applications, e.g., estimating valid grasping positions for robotic wire routing or augmented reality support for workers. Therefore, this paper presents a method for matching the topology of a branched deformable linear object to camera sensor data. Asymmetries in the wire harness design reduce the solution space by comparing the known topology of a model to the topology extracted from sensor data. The problem of finding the most likely solution to the matching problem requires features extracted from camera images. These features are used to construct a graph-based topology representation, which can then be matched to a graph-based topology representation of the known branched deformable linear object. The presented method is evaluated using multiple different non-overlapping configurations of a wire harness, showing the effectiveness of a graph-based segment matching approach. keywords: {Image segmentation;Visualization;Wires;Robot vision systems;Estimation;Cameras;Feature extraction;branched deformable linear object;wire harness localization;graph-based topology matching},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161483&isnumber=10160212

P. Kicki, A. Szymko and K. Walas, "DLOFTBs – Fast Tracking of Deformable Linear Objects with B-splines," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7104-7110, doi: 10.1109/ICRA48891.2023.10160437.Abstract: While manipulating rigid objects is an extensively explored research topic, deformable linear object (DLO) manipulation seems significantly underdeveloped. A potential reason for this is the inherent difficulty in describing and observing the state of the DLO as its geometry changes during manipulation. This paper proposes an algorithm for fast-tracking the shape of a DLO based on the masked image. Having no prior knowledge about the tracked object, the proposed method finds a reliable representation of the shape of the tracked object within tens of milliseconds. This algorithm's main idea is to first skeletonize the DLO mask image, walk through the parts of the DLO skeleton, arrange the segments into an ordered path, and finally fit a B-spline into it. Experiments show that our solution outperforms the State-of-the-Art approaches in DLO's shape reconstruction accuracy and algorithm running time and can handle challenging scenarios such as severe occlusions, self-intersections, and multiple DLOs in a single image. keywords: {Training;Image segmentation;Three-dimensional displays;Shape;Power cables;Training data;Skeleton},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160437&isnumber=10160212

Z. Huang, X. Lin and D. Held, "Self-supervised Cloth Reconstruction via Action-conditioned Cloth Tracking," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7111-7118, doi: 10.1109/ICRA48891.2023.10160653.Abstract: State estimation is one of the greatest challenges for cloth manipulation due to cloth's high dimensionality and self-occlusion. Prior works propose to identify the full state of crumpled clothes by training a mesh reconstruction model in simulation. However, such models are prone to suffer from a sim-to-real gap due to differences between cloth simulation and the real world. In this work, we propose a self-supervised method to finetune a mesh reconstruction model in the real world. Since the full mesh of crumpled cloth is difficult to obtain in the real world, we design a special data collection scheme and an action-conditioned model-based cloth tracking method to generate pseudo-labels for self-supervised learning. By finetuning the pretrained mesh reconstruction model on this pseudo-labeled dataset, we show that we can improve the quality of the reconstructed mesh without requiring human annotations, and improve the performance of downstream manipulation task. More visualizations and results can be found on our project website. keywords: {Training;Automation;Annotations;Computational modeling;Self-supervised learning;Reconstruction algorithms;Data collection},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160653&isnumber=10160212

K. Lv, M. Yu, Y. Pu, X. Jiang, G. Huang and X. Li, "Learning to Estimate 3-D States of Deformable Linear Objects from Single-Frame Occluded Point Clouds," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7119-7125, doi: 10.1109/ICRA48891.2023.10160784.Abstract: Accurately and robustly estimating the state of deformable linear objects (DLOs), such as ropes and wires, is crucial for DLO manipulation and other applications. However, it remains a challenging open issue due to the high dimensionality of the state space, frequent occlusions, and noises. This paper focuses on learning to robustly estimate the states of DLOs from single-frame point clouds in the presence of occlusions using a data-driven method. We propose a novel two-branch network architecture to exploit global and local information of input point cloud respectively and design a fusion module to effectively leverage the advantages of both methods. Simulation and real-world experimental results demonstrate that our method can generate globally smooth and locally precise DLO state estimation results even with heavily occluded point clouds, which can be directly applied to real-world robotic manipulation of DLOs in 3-D space. keywords: {Point cloud compression;Learning systems;Geometry;Solid modeling;Automation;Shape;Wires},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160784&isnumber=10160212

P. Böhm, P. Pounds and A. C. Chapman, "Feature Extraction for Effective and Efficient Deep Reinforcement Learning on Real Robotic Platforms," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7126-7132, doi: 10.1109/ICRA48891.2023.10160862.Abstract: Deep reinforcement learning (DRL) methods can solve complex continuous control tasks in simulated environments by taking actions based solely on state observations at each decision point. Because of the dynamics involved, individual snapshots of real-world sensor measurements afford only partial state observability, so it is typical to use a history of observations to improve training and policy performance. Such intertemporal information can be further exploited using a recurrent neural network (RNN) to reduce the dimensionality of the dynamic state representation. However, using RNNs as an internal part of a DRL network presents challenges of its own; and even then, the improvements in resulting policies are usually limited. To address these shortcomings, we propose using gated feature extraction to improve DRL training of real-world robots. Specifically, we use an untrained gated recurrent unit (GRU) to encode a low-dimension representation of the state observation sequence before passing it to the DRL training procedure. In addition to dimensionality reduction, this allows us to unroll the RNN by encoding the observations cumulatively as they are collected, thereby avoiding same-length input requirements, and train the RL network on the raw observations at the current step combined with the GRU-encoding of the preceding steps. Our simulation experiments employ gated feature extraction with the TD3 algorithm. Our results show that the GRU-encoded state observations improve the training speed and execution performance of the TD3 algorithm, improving the learned policies in all 19 test cases, exceeding the maximum achieved reward by over 38% in 8 and doubling the maximum achieved reward in three, while also outperforming a baseline implementation of SAC in 17 out of 19 environments. Moreover, the greatest improvement is seen in real-world experiments, where our approach successfully learns to balance a pendulum as well as a complex quadrupedal locomotion task. In contrast, the standard TD3 algorithm not only does not show any learning progress at all, but also repeatedly damages the hardware. keywords: {Training;Deep learning;Recurrent neural networks;Reinforcement learning;Computer architecture;Logic gates;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160862&isnumber=10160212

L. Marzari, E. Marchesini and A. Farinelli, "Online Safety Property Collection and Refinement for Safe Deep Reinforcement Learning in Mapless Navigation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7133-7139, doi: 10.1109/ICRA48891.2023.10161312.Abstract: Safety is essential for deploying Deep Reinforcement Learning (DRL) algorithms in real-world scenarios. Recently, verification approaches have been proposed to allow quantifying the number of violations of a DRL policy over input-output relationships, called properties. However, such properties are hard-coded and require task-level knowledge, making their application intractable in challenging safety-critical tasks. To this end, we introduce the Collection and Refinement of Online Properties (CROP) framework to design properties at training time. CROP employs a cost signal to identify unsafe interactions and use them to shape safety properties. Hence, we propose a refinement strategy to combine properties that model similar unsafe interactions. Our evaluation compares the benefits of computing the number of violations using standard hard-coded properties and the ones generated with CROP. We evaluate our approach in several robotic mapless navigation tasks and demonstrate that the violation metric computed with CROP allows higher returns and lower violations over previous Safe DRL approaches. keywords: {Deep learning;Training;Navigation;Shape;Crops;Reinforcement learning;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161312&isnumber=10160212

W. Ding et al., "Learning to View: Decision Transformers for Active Object Detection," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7140-7146, doi: 10.1109/ICRA48891.2023.10160946.Abstract: Active perception describes a broad class of techniques that couple planning and perception systems to move the robot in a way to give the robot more information about the environment. In most robotic systems, perception is typically independent of motion planning. For example, traditional object detection is passive: it operates only on the images it receives. However, we have a chance to improve the results if we allow planning to consume detection signals and move the robot to collect views that maximize the quality of the results. In this paper, we use reinforcement learning (RL) methods to control the robot in order to obtain images that maximize the detection quality. Specifically, we propose using a Decision Transformer with online fine-tuning, which first optimizes the policy with a pre-collected expert dataset and then improves the learned policy by exploring better solutions in the environment. We evaluate the performance of proposed method on an interactive dataset collected from an indoor scenario simulator. Experimental results demonstrate that our method outperforms all baselines, including expert policy and pure offline RL methods. We also provide exhaustive analyses of the reward distribution and observation space. keywords: {Training;Uncertainty;Automation;Measurement uncertainty;Object detection;Reinforcement learning;Aerospace electronics},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160946&isnumber=10160212

M. Schier, C. Reinders and B. Rosenhahn, "Deep Reinforcement Learning for Autonomous Driving using High-Level Heterogeneous Graph Representations," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7147-7153, doi: 10.1109/ICRA48891.2023.10160762.Abstract: Graph networks have recently been used for decision making in automated driving tasks for their ability to capture a variable number of traffic participants. Current high-level graph-based approaches, however, do not model the entire road network and thus must rely on handcrafted features for vehicle-to-vehicle edges encompassing the road topology indirectly. We propose an entity-relation framework that intuitively models the road network and the traffic participants in a heterogeneous graph, representing all relevant information. Our novel architecture transforms the heterogeneous road-vehicle graph into a simpler graph of homogeneous node and edge types to allow effective training for deep reinforcement learning while introducing minimal prior knowledge. Unlike previous approaches, the vehicle-to-vehicle edges of this reduced graph are fully learnable and can therefore encode traffic rules without explicit feature design, an important step towards a holistic reinforcement learning model for automated driving. We show that our proposed method outperforms precomputed handcrafted features on intersection scenarios while also learning the semantics of right-of-way rules. keywords: {Training;Deep learning;Network topology;Roads;Semantics;Vehicular ad hoc networks;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160762&isnumber=10160212

A. Nair, B. Zhu, G. Narayanan, E. Solowjow and S. Levine, "Learning on the Job: Self-Rewarding Offline-to-Online Finetuning for Industrial Insertion of Novel Connectors from Vision," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7154-7161, doi: 10.1109/ICRA48891.2023.10161491.Abstract: Learning-based methods in robotics hold the promise of generalization, but what can be done if a learned policy does not generalize to a new situation? In principle, if an agent can at least evaluate its own success (i.e., with a reward classifier that generalizes well even when the policy does not), it could actively practice the task and finetune the policy in this situation. We study this problem in the setting of industrial insertion tasks, such as inserting connectors in sockets and setting screws. Existing algorithms rely on precise localization of the connector or socket and carefully managed physical setups, such as assembly lines, to succeed at the task. But in unstructured environments such as homes or even some industrial settings, robots cannot rely on precise localization and may be tasked with previously unseen connectors. Offline reinforcement learning on a variety of connector insertion tasks is a potential solution, but what if the robot is tasked with inserting previously unseen connector? In such a scenario, we will still need methods that can robustly solve such tasks with online practice. One of the main observations we make in this work is that, with a suitable representation learning and domain generalization approach, it can be significantly easier for the reward function to generalize to a new but structurally similar task (e.g., inserting a new type of connector) than for the policy. This means that a learned reward function can be used to facilitate the finetuning of the robot's policy in situations where the policy fails to generalize in zero shot, but the reward function generalizes successfully. We show that such an approach can be instantiated in the real world, pretrained on 50 different connectors, and successfully finetuned to new connectors via the learned reward function. Videos and visualizations can be viewed at sites.google.com/view/learningonthejob keywords: {Location awareness;Representation learning;Learning systems;Visualization;Service robots;Sockets;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161491&isnumber=10160212

C. Igoe, S. Pande, S. Venkatraman and J. Schneider, "Multi-Alpha Soft Actor-Critic: Overcoming Stochastic Biases in Maximum Entropy Reinforcement Learning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7162-7168, doi: 10.1109/ICRA48891.2023.10161395.Abstract: The successful application of robotic control requires intelligent decision-making to handle the long tail of complex scenarios that arise in real-world environments. Recently, Deep Reinforcement Learning (DRL) has provided a data-driven framework to automatically learn effective policies in such complex settings. Since its introduction in 2018, Soft Actor-Critic (SAC) remains as one of the most popular off-policy DRL algorithms and has been used extensively to learn performant robotic control policies. However, in this paper we argue that by relying on the maximum entropy formalism to define learning objectives, previous work introduces a significant bias away from optimal decision making, which often requires near-deterministic behaviour for high-precision tasks. Moreover, we show that when training with the original variants of SAC, overcoming this bias by reducing entropy budgets or entropy coefficients introduces separate issues that lead to slow or unstable learning. We address these shortcomings by treating the entropy coefficient $\alpha$ as a random variable and introduce Multi-Alpha Soft Actor-Critic (MAS). We show how MAS overcomes the stochastic bias of SAC in a variety of robotic control tasks including the CARLA urban-driving simulator, while maintaining the stability and sample efficiency of the original algorithms. keywords: {Training;Deep learning;Decision making;Reinforcement learning;Tail;Entropy;Stability analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161395&isnumber=10160212

Z. Wu et al., "Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7169-7175, doi: 10.1109/ICRA48891.2023.10160764.Abstract: Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner. keywords: {Automation;Reinforcement learning;Encoding;Task analysis;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160764&isnumber=10160212

G. Zhou, L. Ke, S. Srinivasa, A. Gupta, A. Rajeswaran and V. Kumar, "Real World Offline Reinforcement Learning with Realistic Data Source," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7176-7183, doi: 10.1109/ICRA48891.2023.10161474.Abstract: Offline reinforcement learning (ORL) holds great promise for robot learning due to its ability to learn from arbitrary pre-generated experience. However, current ORL benchmarks are almost entirely in simulation and utilize contrived datasets like replay buffers of online RL agents or sub-optimal trajectories, and thus hold limited relevance for real-world robotics. In this work (Real-ORL), we posit that data collected from safe operations of closely related tasks are more practical data sources for real-world robot learning. Under these settings, we perform an extensive (6500+ trajectories collected over 800+ robot hours and 270+ human labor hour) empirical study evaluating generalization and transfer capabilities of representative ORL methods on four real-world tabletop manipulation tasks. Our study finds that ORL and imitation learning prefer different action spaces, and that ORL algorithms can generalize from leveraging offline heterogeneous data sources and outperform imitation learning. We release our dataset and implementations at URL: https://sites.google.com/view/real-orl. keywords: {Uniform resource locators;Soft sensors;Heuristic algorithms;Transfer learning;Reinforcement learning;Multitasking;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161474&isnumber=10160212

T. Lew et al., "Robotic Table Wiping via Reinforcement Learning and Whole-body Trajectory Optimization," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7184-7190, doi: 10.1109/ICRA48891.2023.10161283.Abstract: We propose a framework to enable multipurpose assistive mobile robots to autonomously wipe tables to clean spills and crumbs. This problem is challenging, as it requires planning wiping actions while reasoning over uncertain latent dynamics of crumbs and spills captured via high-dimensional visual observations. Simultaneously, we must guarantee constraints satisfaction to enable safe deployment in unstructured cluttered environments. To tackle this problem, we first propose a stochastic differential equation to model crumbs and spill dynamics and absorption with a robot wiper. Using this model, we train a vision-based policy for planning wiping actions in simulation using reinforcement learning (RL). To enable zero-shot sim-to-real deployment, we dovetail the RL policy with a whole-body trajectory optimization framework to compute base and arm joint trajectories that execute the desired wiping motions while guaranteeing constraints satisfaction. We extensively validate our approach in simulation and on hardware. Video of experiments: https://youtu.be/inORKP4F3EI keywords: {Analytical models;Visualization;Computational modeling;Reinforcement learning;Mathematical models;Hardware;Data models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161283&isnumber=10160212

S. Karten, M. Tucker, S. Kailas and K. Sycara, "Towards True Lossless Sparse Communication in Multi-Agent Systems," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7191-7197, doi: 10.1109/ICRA48891.2023.10161322.Abstract: Communication enables agents to cooperate to achieve their goals. Learning when to communicate, i.e., sparse (in time) communication, and whom to message is particularly important when bandwidth is limited. However, recent work in learning sparse individualized communication suffers from high variance during training, where decreasing communication comes at the cost of decreased reward, particularly in cooperative tasks. We use the information bottleneck to reframe sparsity as a representation learning problem, which we show naturally enables lossless sparse communication at lower budgets than prior art. In this paper, we propose a method for true lossless sparsity in communication via Information Maximizing Gated Sparse Multi-Agent Communication (IMGS-MAC). Our model uses two individualized regularization objectives, an information maximization autoencoder and sparse communication loss, to create informative and sparse communication. We evaluate the learned communication ‘language’ through direct causal analysis of messages in non-sparse runs to determine the range of lossless sparse budgets, which allow zero-shot sparsity, and the range of sparse budgets that will inquire a reward loss, which is minimized by our learned gating function with few-shot sparsity. To demonstrate the efficacy of our results, we experiment in cooperative multi-agent tasks where communication is essential for success. We evaluate our model with both continuous and discrete messages. We focus our analysis on a variety of ablations to show the effect of message representations, including their properties, and lossless performance of our model. keywords: {Training;Representation learning;Analytical models;Costs;Automation;Art;Bandwidth},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161322&isnumber=10160212

C. Liu, E. -J. van Kampen and G. C. H. E. de Croon, "Adaptive Risk-Tendency: Nano Drone Navigation in Cluttered Environments with Distributional Reinforcement Learning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7198-7204, doi: 10.1109/ICRA48891.2023.10160324.Abstract: Enabling the capability of assessing risk and making risk-aware decisions is essential to applying reinforcement learning to safety-critical robots like drones. In this paper, we investigate a specific case where a nano quadcopter robot learns to navigate an apriori-unknown cluttered environment under partial observability. We present a distributional reinforcement learning framework to generate adaptive risk-tendency policies. Specifically, we propose to use lower tail conditional variance of the learnt return distribution as intrinsic uncertainty estimation, and use exponentially weighted average forecasting (EWAF) to adapt the risk-tendency in accordance with the estimated uncertainty. In simulation and real-world empirical results, we show that (1) the most effective risk-tendency varies across states, (2) the agent with adaptive risk-tendency achieves superior performance compared to risk-neutral policy or risk-averse policy baselines. Code and video can be found in this repository: https://github.com/tudelft/risk-sensitive-rl.git keywords: {Adaptation models;Uncertainty;Navigation;Estimation;Reinforcement learning;Tail;Observability},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160324&isnumber=10160212

T. Li, H. Lei and Q. Zhu, "Self-Adaptive Driving in Nonstationary Environments through Conjectural Online Lookahead Adaptation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7205-7211, doi: 10.1109/ICRA48891.2023.10161368.Abstract: Powered by deep representation learning, re-inforcement learning (RL) provides an end-to-end learning framework capable of solving self-driving (SD) tasks without manual designs. However, time-varying nonstationary environments cause proficient but specialized RL policies to fail at execution time. For example, an RL-based SD policy trained under sunny days does not generalize well to rainy weather. Even though meta learning enables the RL agent to adapt to new tasks/environments, its offline operation fails to equip the agent with online adaptation ability when facing nonstationary environments. This work proposes an online meta reinforcement learning algorithm based on the conjectural online lookahead adaptation (COLA). COLA determines the online adaptation at every step by maximizing the agent's conjecture of the future performance in a lookahead horizon. Experimental results demonstrate that under dynamically changing weather and lighting conditions, the COLA-based self-adaptive driving outperforms the baseline policies regarding online adaptability. A demo video, source code, and appendixes are available at https://github.com/Panshark/COLA keywords: {Representation learning;Metalearning;Heuristic algorithms;Source coding;Lighting;Reinforcement learning;Manuals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161368&isnumber=10160212

R. Juan, H. Ju, J. Huang, R. Gomez, K. Nakamura and G. Li, "Sim-to-Real Policy and Reward Transfer with Adaptive Forward Dynamics Model," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7212-7218, doi: 10.1109/ICRA48891.2023.10161298.Abstract: Deep reinforcement learning has shown promise in learning robust skills for robot control, but typically requires a large amount of samples to achieve good performance. Sim-to-real transfer learning has been developed to solve this problem, but the policy trained in simulation usually has unsatisfactory performance in the real world because simulators inevitably model the dynamics of reality imperfectly. To enable sample-efficient learning in the real world, we proposed progressive policy transfer with adaptive dynamics model (PPTADM). PPTADM assumes the dynamics of simulation and real world do not match but the state space is the same, transfers policy from simulation via progressive neural network (PNN) and further improves the policy with a learned forward dynamics model in reality. In addition, for real-world tasks in which reward functions are difficult or even impossible to define and verify the effectiveness, PPTADM can learn in real world solely from a transferred reward function that is estimated from simulation even though their dynamics do not match. Our results in five simulated tasks and on a real robot arm show that with PPTADM, the robot's learning efficiency and performance in the real world can be significantly improved. keywords: {Deep learning;Adaptation models;Heuristic algorithms;Transfer learning;Robot control;Neural networks;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161298&isnumber=10160212

Z. Feng, B. Zhang, J. Bi and H. Soh, "Safety-Constrained Policy Transfer with Successor Features," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7219-7225, doi: 10.1109/ICRA48891.2023.10161256.Abstract: In this work, we focus on the problem of safe policy transfer in reinforcement learning: we seek to leverage existing policies when learning a new task with specified constraints. This problem is important for safety-critical applications where interactions are costly and unconstrained exploration can lead to undesirable or dangerous outcomes, e.g., with physical robots that interact with humans. We propose a Constrained Markov Decision Process (CMDP) formulation that simultaneously enables the transfer of policies and adherence to safety constraints. Our formulation cleanly separates task goals from safety considerations and permits the specification of a wide variety of constraints. Our approach relies on a novel extension of generalized policy improvement to constrained settings via a Lagrangian formulation. We devise a dual optimization algorithm that estimates the optimal dual variable of a target task, thus enabling safe transfer of policies derived from successor features learned on source tasks. Our experiments in simulated domains show that our approach is effective; it visits unsafe states less frequently and outperforms alternative state-of-the-art methods when taking safety constraints into account. keywords: {Automation;Reinforcement learning;Markov processes;Safety;Task analysis;Robots;Optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161256&isnumber=10160212

D. Shah, A. Sridhar, A. Bhorkar, N. Hirose and S. Levine, "GNM: A General Navigation Model to Drive Any Robot," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7226-7233, doi: 10.1109/ICRA48891.2023.10161227.Abstract: Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diverse data leads to robustness against degradation in sensing and actuation. Using a pre-trained navigation model with broad generalization capabilities can bootstrap applications on novel robots going forward, and we hope that the GNM represents a step in that direction. For more information on the datasets, code, and videos, please check out our project page11sites.google.com/view/drive-any-robot. keywords: {Training;Navigation;Training data;Robot sensing systems;Data models;Robustness;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161227&isnumber=10160212

H. Sun, Y. Wang, X. Cai, X. Bai and D. Li, "ViPFormer: Efficient Vision-and-Pointcloud Transformer for Unsupervised Pointcloud Understanding," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7234-7242, doi: 10.1109/ICRA48891.2023.10160658.Abstract: Recently, a growing number of work design unsupervised paradigms for point cloud processing to alleviate the limitation of expensive manual annotation and poor transferability of supervised methods. Among them, CrossPoint follows the contrastive learning framework and exploits image and point cloud data for unsupervised point cloud understanding. Although the promising performance is presented, the unbalanced architecture makes it unnecessarily complex and inefficient. For example, the image branch in CrossPoint is ~8.3x heavier than the point cloud branch leading to higher complexity and latency. To address this problem, in this paper, we propose a lightweight Vision-and-Pointcloud Transformer (ViPFormer) to unify image and point cloud processing in a single architecture. ViPFormer learns in an unsupervised manner by optimizing intra-modal and cross-modal contrastive objectives. Then the pretrained model is transferred to various downstream tasks, including 3D shape classification and semantic segmentation. Experiments on different datasets show ViPFormer surpasses previous state-of-the-art unsupervised methods with higher accuracy, lower model complexity and runtime latency. Finally, the effectiveness of each component in ViPFormer is validated by extensive ablation studies. The implementation of the proposed method is available at https://github.com/auniquesun/ViPFormer. keywords: {Point cloud compression;Solid modeling;Runtime;Three-dimensional displays;Shape;Semantic segmentation;Manuals},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160658&isnumber=10160212

A. Allaire and C. G. Atkeson, "Learning Exploration Strategies to Solve Real-World Marble Runs," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7243-7249, doi: 10.1109/ICRA48891.2023.10160759.Abstract: Tasks involving locally unstable or discontinuous dynamics (such as bifurcations and collisions) remain challenging in robotics, because small variations in the environment can have a significant impact on task outcomes. For such tasks, learning a robust deterministic policy is difficult. We focus on structuring exploration with multiple stochastic policies based on a mixture of experts (MoE) policy representation that can be efficiently adapted. The MoE policy is composed of stochastic sub-policies that allow exploration of multiple distinct regions of the action space (or strategies) and a high-level selection policy to guide exploration towards the most promising regions. We develop a robot system to evaluate our approach in a real-world physical problem solving domain. After training the MoE policy in simulation, online learning in the real world demonstrates efficient adaptation within just a few dozen attempts, with a minimal sim2real gap. Our results confirm that representing multiple strategies promotes efficient adaptation in new environments and strategies learned under different dynamics can still provide useful information about where to look for good strategies. keywords: {Training;Adaptation models;Automation;Bifurcation;Problem-solving;Task analysis;Collision avoidance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160759&isnumber=10160212

C. Yu, W. Zhang, H. Lai, Z. Tian, L. Kneip and J. Wang, "Multi-embodiment Legged Robot Control as a Sequence Modeling Problem," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7250-7257, doi: 10.1109/ICRA48891.2023.10161034.Abstract: Robots are traditionally bounded by a fixed embodiment during their operational lifetime, which limits their ability to adapt to their surroundings. Co-optimizing control and morphology of a robot, however, is often inefficient due to the complex interplay between the controller and morphology. In this paper, we propose a learning-based control method that can inherently take morphology into consideration such that once the control policy is trained in the simulator, it can be easily deployed to real robots with different embodiments. In particular, we present the Embodiment-aware Transformer (EAT), an architecture that casts this control problem as conditional sequence modeling. EAT outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired robot embodiment, past states, and actions, our EAT model can generate future actions that best fit the current robot embodiment. Experimental results show that EAT can outperform all other alternatives in embodiment-varying tasks, and succeed in an example of real-world evolution tasks: stepping down a stair through updating the morphology alone. We hope that EAT will inspire a new push toward real-world evolution across many domains, where algorithms like EAT can blaze a trail by bridging the field of evolutionary robotics and big data sequence modeling. keywords: {Evolutionary robotics;Robot control;Morphology;Stairs;Transformers;Data models;Trajectory},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161034&isnumber=10160212

S. Vats, M. Likhachev and O. Kroemer, "Efficient Recovery Learning using Model Predictive Meta-Reasoning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7258-7264, doi: 10.1109/ICRA48891.2023.10160382.Abstract: Operating under real world conditions is challenging due to the possibility of a wide range of failures induced by execution errors and state uncertainty. In relatively benign settings, such failures can be overcome by retrying or executing one of a small number of hand-engineered recovery strategies. By contrast, contact-rich sequential manipulation tasks, like opening doors and assembling furniture, are not amenable to exhaustive hand-engineering. To address this issue, we present a general approach for robustifying manipulation strategies in a sample-efficient manner. Our approach incrementally improves robustness by first discovering the failure modes of the current strategy via exploration in simulation and then learning additional recovery skills to handle these failures. To ensure efficient learning, we propose an online algorithm called Meta-Reasoning for Skill Learning (MetaReSkill) that monitors the progress of all recovery policies during training and allocates training resources to recoveries that are likely to improve the task performance the most. We use our approach to learn recovery skills for door-opening and evaluate them both in simulation and on a real robot with little fine-tuning. Compared to open-loop execution, our experiments show that even a limited amount of recovery learning improves task success substantially from 71% to 92.4% in simulation and from 75% to 90% on a real robot. keywords: {Training;Adaptation models;Uncertainty;Computational modeling;Predictive models;Prediction algorithms;Robustness},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160382&isnumber=10160212

J. Bi et al., "Multi-swarm Genetic Gray Wolf Optimizer with Embedded Autoencoders for High-dimensional Expensive Problems," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7265-7271, doi: 10.1109/ICRA48891.2023.10161299.Abstract: High-dimensional expensive problems are often encountered in the design and optimization of complex robotic and automated systems and distributed computing systems, and they suffer from a time-consuming fitness evaluation process. It is extremely challenging and difficult to produce promising solutions in a high-dimensional search space. This work proposes an evolutionary optimization framework with embedded autoencoders that effectively solve optimization problems with high-dimensional search space. Autoencoders provide strong dimension reduction and feature extraction abilities that compress a high-dimensional space to an informative low-dimensional one. Search operations are performed in a low-dimensional space, thereby guiding whole population to converge to the optimal solution more efficiently. Multiple subpopulations coevolve iteratively in a distributed manner. One subpopulation is embedded by an autoencoder, and the other one is guided by a newly proposed Multi-swarm Gray-wolf-optimizer based on Genetic-learning (MGG). Thus, the proposed multi-swarm framework is named Autoencoder-based MGG (AMGG). AMGG consists of three proposed strategies that balance exploration and exploitation abilities, i.e., a dynamic subgroup number strategy for reducing the number of subpopulations, a subpopulation reorganization strategy for sharing useful information about each subpopulation, and a purposeful detection strategy for escaping from local optima and improving exploration ability. AMGG is compared with several widely used algorithms by solving benchmark problems and a real-life optimization one. The results well verify that AMGG outperforms its peers in terms of search accuracy and convergence efficiency. keywords: {Dimensionality reduction;Sociology;Benchmark testing;Genetics;Search problems;Robustness;Particle swarm optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161299&isnumber=10160212

K. Ota et al., "H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7272-7278, doi: 10.1109/ICRA48891.2023.10160575.Abstract: The world is filled with articulated objects that are difficult to determine how to use from vision alone, e.g., a door might open inwards or outwards. Humans handle these objects with strategic trial-and-error: first pushing a door then pulling if that doesn't work. We enable these capabilities in autonomous agents by proposing “Hypothesize, Simulate, Act, Update, and Repeat” (H-SAUR), a probabilistic generative framework that simultaneously generates a distribution of hypotheses about how objects articulate given input observations, captures certainty over hypotheses over time, and infer plausible actions for exploration and goal-conditioned manipulation. We compare our model with existing work in manipulating objects after a handful of exploration actions, on the PartNet-Mobility dataset. We further propose a novel PuzzleBoxes benchmark that contains locked boxes that require multiple steps to solve. We show that the proposed model significantly outperforms the current state-of-the-art articulated object manipulation framework, despite using zero training data. We further improve the test-time efficiency of H-SAUR by integrating a learned prior from learning-based vision models. keywords: {Geometry;Adaptation models;Visualization;Motion segmentation;Training data;Kinematics;Probabilistic logic},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160575&isnumber=10160212

L. Wang, N. Dvornik, R. Dubeau, M. Mittal and A. Garg, "Self-Supervised Learning of Action Affordances as Interaction Modes," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7279-7286, doi: 10.1109/ICRA48891.2023.10161371.Abstract: When humans perform a task with an articulated object, they interact with the object only in a handful of ways, while the space of all possible interactions is nearly endless. This is because humans have prior knowledge about what interactions are likely to be successful, i.e., to open a new door we first try the handle. While learning such priors without supervision is easy for humans, it is notoriously hard for machines. In this work, we tackle unsupervised learning of priors of useful interactions with articulated objects, which we call interaction modes. In contrast to the prior art, we use no supervision or privileged information; we only assume access to the depth sensor in the simulator to learn the interaction modes. More precisely, we define a successful interaction as the one changing the visual environment substantially and learn a generative model of such interactions, that can be conditioned on the desired goal state of the object. In our experiments, we show that our model covers most of the human interaction modes, outperforms existing state-of-the-art methods for affordance learning, and can generalize to objects never seen during training. Additionally, we show promising results in the goal-conditional setup, where our model can be quickly fine-tuned to perform a given task. We show in the experiments that such affordance learning predicts interaction which covers most modes of interaction for the querying articulated object and can be fine-tuned to a goal-conditional model. For supplementary: https://actaim.github.io/. keywords: {Training;Adaptation models;Visualization;Affordances;Self-supervised learning;Predictive models;Diversity methods},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161371&isnumber=10160212

A. Bucker et al., "LATTE: LAnguage Trajectory TransformEr," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7287-7294, doi: 10.1109/ICRA48891.2023.10161068.Abstract: Natural language is one of the most intuitive ways to express human intent. However, translating instructions and commands towards robotic motion generation and deployment in the real world is far from being an easy task. The challenge of combining a robot's inherent low-level geometric and kinodynamic constraints with a human's high-level semantic instructions traditionally is solved using task-specific solutions with little generalizability between hardware platforms, often with the use of static sets of target actions and commands. This work instead proposes a flexible language-based framework that allows a user to modify generic robotic trajectories. Our method leverages pre-trained language models (BERT and CLIP) to encode the user's intent and target objects directly from a free-form text input and scene images, fuses geometrical features generated by a transformer encoder network, and finally outputs trajectories using a transformer decoder, without the need of priors related to the task or robot information. We significantly extend our own previous work presented in [1] by expanding the trajectory parametrization space to 3D and velocity as opposed to just XY movements. In addition, we now train the model to use actual images of the objects in the scene for context (as opposed to textual descriptions), and we evaluate the system in a diverse set of scenarios beyond manipulation, such as aerial and legged robots. Our simulated and real-life experiments demonstrate that our transformer model can successfully follow human intent, modifying the shape and speed of trajectories within multiple environments. Codebase avail-able at: https://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr.git. keywords: {Robot motion;Legged locomotion;Three-dimensional displays;Shape;Semantics;Natural languages;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161068&isnumber=10160212

A. Loquercio, A. Kumar and J. Malik, "Learning Visual Locomotion with Cross-Modal Supervision," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7295-7302, doi: 10.1109/ICRA48891.2023.10160760.Abstract: In this work, we show how to learn a visual walking policy that only uses a monocular RGB camera and proprioception. Since simulating RGB is hard, we necessarily have to learn vision in the real world. We start with a blind walking policy trained in simulation. This policy can traverse some terrains in the real world but often struggles since it lacks knowledge of the upcoming geometry. This can be resolved with the use of vision. We train a visual module in the real world to predict the upcoming terrain with our proposed algorithm Cross-Modal Supervision (CMS). CMS uses time-shifted proprioception to supervise vision and allows the policy to continually improve with more real-world experience. We evaluate our vision-based walking policy over a diverse set of terrains including stairs (up to 19cm high), slippery slopes (inclination of 35°), curbs and tall steps (up to 20cm), and complex discrete terrains. We achieve this performance with less than 30 minutes of real-world data. Finally, we show that our policy can adapt to shifts in the visual field with a limited amount of real-world experience. keywords: {Legged locomotion;Geometry;Visualization;Automation;Stairs;Prediction algorithms;Cameras},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160760&isnumber=10160212

O. Formoso, G. Trinh, D. Catanoso, I. -W. Park, C. Gregg and K. Cheung, "MMIC-I: A Robotic Platform for Assembly Integration and Internal Locomotion through Mechanical Meta-Material Structures," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7303-7309, doi: 10.1109/ICRA48891.2023.10161263.Abstract: In-space assembly is crucial to creating large-scale space structures and enabling long term space missions. Natural limitations in the size of transportation vehicles and ISRU production facilities necessitate an additive strategy with the size of the typical structural unit being essentially fixed and inversely proportional to the final assembly size. In prior robotic and space assembly examples, reversible mechanical integration of structural modules is typically achieved with actuated alignment and fastening mechanisms onboard every structural module. Additive assembly or manufacturing planning approaches often feature a “build front” that receives new materials or parts and progresses gradually across the target geometry. The system we describe here places much of the alignment and fastener actuation systems onboard a mobile robot that can operate at a build front while companion robots (Scaling Omni-directional Lattice Locomoting Explorer, SOLL-E) provide part or material transportation. The design and evaluation of this Mobile Meta-Material Interior Co-Integrator (MMIC-I), an inchworm-style locomoting robotic assembler, is described here with an emphasis on ease of assembly and a low number of unique parts for a simple design. It is designed to assist in alignment of cuboctahedron structural unit cells with captive fasteners, defining the build front in operation. Adjacent structural unit cells are locked together with specified axial and rotational actuation of the fasteners. Hardware prototypes show that the robot is able to successfully locomote to any indexed location within a lattice structure and bolt together each set of fasteners on any interface. keywords: {Additives;Space missions;Lattices;Transportation;Fasteners;Robot sensing systems;Metamaterials},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161263&isnumber=10160212

G. Knizhnik, P. Li, M. Yim and M. A. Hsieh, "Flow-Based Rendezvous and Docking for Marine Modular Robots in Gyre-Like Environments," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7310-7316, doi: 10.1109/ICRA48891.2023.10161430.Abstract: Modular self-assembling systems typically assume that modules are present to assemble. But in sparsely observed ocean environments modules of an aquatic modular robotic system may be separated by distances they do not have the energy to cross, and the information needed for optimal path planning is often unavailable. In this work we present a flow-based rendezvous and docking controller that allows aquatic robots in gyre-like environments to rendezvous with and dock to a target by leveraging environmental forces. This approach does not require complete knowledge of the flow, but suffices with imperfect knowledge of the flow's center and shape. We validate the performance of this control approach in both simulations and experiments relative to naive rendezvous and docking strategies and show that energy efficiency improves as the scale of the gyre increases. keywords: {Costs;Shape;Oceans;Shape measurement;Lattices;Sea measurements;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161430&isnumber=10160212

J. Lim et al., "Mobility Analysis of Screw-Based Locomotion and Propulsion in Various Media," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7317-7323, doi: 10.1109/ICRA48891.2023.10160777.Abstract: Robots “in-the-wild” encounter and must traverse widely varying terrain, ranging from solid ground to granular materials like sand to full liquids. Numerous approaches exist, including wheeled and legged robots, each excelling in specific domains. Screw-based locomotion is a promising approach for multi-domain mobility, leveraged in exploratory robotic designs, including amphibious vehicles and snake robotics. However, unlike other forms of locomotion, there is limited exploration of the models, parameter effects, and efficiency for multi-terrain Archimedes screw locomotion. In this work, we present work towards this missing component in understanding screw-based locomotion: comprehensive experimental results and performance analysis across different media. We designed a mobile test bed for indoor and outdoor experimentation to collect this data. Beyond quantitatively showing the multi-domain mobility of screw-based locomotion, we envision future researchers and engineers using the presented results to design effective screw-based locomotion systems. keywords: {Legged locomotion;Liquids;Fasteners;Media;Propulsion;Solids;Ice},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160777&isnumber=10160212

X. Liu et al., "TJ-FlyingFish: Design and Implementation of an Aerial-Aquatic Quadrotor with Tiltable Propulsion Units," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7324-7330, doi: 10.1109/ICRA48891.2023.10160899.Abstract: Aerial-aquatic vehicles are capable to move in the two most dominant fluids, making them more promising for a wide range of applications. We propose a prototype with special designs for propulsion and thruster configuration to cope with the vast differences in the fluid properties of water and air. For propulsion, the operating range is switched for the different mediums by the dual-speed propulsion unit, providing sufficient thrust and also ensuring output efficiency. For thruster configuration, thrust vectoring is realized by the rotation of the propulsion unit around the mount arm, thus enhancing the underwater maneuverability. This paper presents a quadrotor prototype of this concept and the design details and realization in practice. keywords: {Water;Fluids;Automation;Attitude control;Prototypes;Switches;Propulsion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160899&isnumber=10160212

Y. Kim, S. Choi, J. Song and D. Yun, "Modular Multi-axis Elastic Actuator with Torque Sensing Capable p-CFH for Highly Impact Resistive Robot Leg," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7331-7337, doi: 10.1109/ICRA48891.2023.10161131.Abstract: This study proposes a modular Multi-axis Elastic Actuator (MAEA) for legged robots that can effectively cope with impacts that may occur during dynamic maneuvering. MAEA has multi-axis compliance and can measure the torque without additional encoders. Therefore, effective impact resistance is possible with less volume and weight than conventional Series Elastic Actuators (SEA). The 6-axis stiffness analysis of paired-Crossed Flexural Hinge (p-CFH) is extended from small deformation to large deformation, and the accuracy is verified through Finite Element Analysis (FEA) and experiments. Based on the analysis, the torque of p-CFH is measured, and feedback torque control is also performed. Finally, the robot leg was constructed with MAEA, and the multi-axis impact resistance performance of MAEA was demonstrated by analyzing the applied impact during landing experiments at various angles. keywords: {Legged locomotion;Resistance;Actuators;Torque;Deformation;Gears;Torque control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161131&isnumber=10160212

H. M. Lam, W. J. Walker, L. Jonasch, D. Schreiber and M. C. Yip, "Design and Mechanics of Cable-Driven Rolling Diaphragm Transmission for High-Transparency Robotic Motion," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7338-7344, doi: 10.1109/ICRA48891.2023.10160832.Abstract: Applications of rolling diaphragm transmissions for medical and teleoperated robotics are of great interest, due to the low friction of rolling diaphragms combined with the power density and stiffness of hydraulic transmissions. However, the stiffness-enabling pressure preloads can form a tradeoff against bearing loading in some rolling diaphragm layouts, and transmission setup can be difficult. Utilization of cable drives compliment the rolling diaphragm transmission's advantages, but maintaining cable tension is crucial for optimal and consistent performance. In this paper, a coaxial opposed rolling diaphragm layout with cable drive and an electronic transmission control system are investigated, with a focus on system reliability and scalability. Mechanical features are proposed which enable force balancing, decoupling of transmission pressure from bearing loads, and maintenance of cable tension. Key considerations and procedures for automation of transmission setup, phasing, and operation are also presented. We also present an analysis of system stiffness to identify key compliance contributors, and conduct experiments to validate prototype design performance. keywords: {Robot motion;Coaxial cables;Automation;Torque;Friction;Scalability;Layout},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160832&isnumber=10160212

K. Tanaka and M. Hamaya, "Twist Snake: Plastic table-top cable-driven robotic arm with all motors located at the base link," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7345-7351, doi: 10.1109/ICRA48891.2023.10160995.Abstract: Table-top robotic arms for education and research must be low-cost for availability and lightweight and soft for safety. Therefore, as such a robot, this study focuses on designing a plastic table-top cable-driven robotic arm with all motors located at the base link. However, locating all motors at the base link results in a significant distance between a driving motor and driven joint, increases the number of parts for the force transmission, and increases the risk of a cable loosening and coming off of a pulley. To overcome these issues, this study proposed a novel cable-driven robotic arm named Twist Snake. We designed a joint composition of Twist Snake to minimize the number of parts for the force transmission. In addition, it has a compact cable-pretension/termination-mechanism and covering parts to prevent the cable from loosening and coming off of the pulley. The arm comprised 475 mm long moving links with an 802 g. The feasibility of the arm was experimentally demonstrated by contact rich tasks, the insertion of a toy peg into a hole and swiping a whiteboard with a cleaner. The optimization of the proposed design and the development of a learning method for the arm that leverages contact will be investigated in future work. keywords: {Learning systems;Pulleys;Force;Toy manufacturing industry;Mechanical cables;Manipulators;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160995&isnumber=10160212

C. J. Kimmer, M. S. Han and C. K. Harnett, "Strained Elastic Surfaces with Adjustable-Modulus Edges (SESAMEs) for Soft Robotic Actuation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7352-7358, doi: 10.1109/ICRA48891.2023.10160299.Abstract: For robots to interact safely with humans and travel with minimal weight, low-density packable actuators are sought. Electronically-driven active materials like shape memory wire and other artificial muscle fibers offer solutions, but these materials need a restoring force. Moreover, if joint bending is required, the actuators must exert a bending moment around the joint. In this paper, we model the three-dimensional shapes of strained elastic surfaces with adjustable-modulus edges (SESAMEs), then implement SESAMEs by machine embroidering shape memory alloy wire onto stretched elastic fabric, showing a path to lightweight actuators that exert bending forces and have built-in restoring forces. SESAMEs start out planar, and upon release from the plane take on three-dimensional shapes thanks to the balance between bending energy in the boundary and strain energy in the elastic surface. The elastic creates both a restoring force to bring the boundary back to its original shape after actuation, and an out-of-plane structure for applying a bending moment. We demonstrate SESAMEs' properties as soft robotic actuators individually and in arrays, and coupled to flexible plastic frames during the planar fabrication process as bending actuators to switch bistable mechanical structures. keywords: {Actuators;Solid modeling;Shape;Wires;Force;Switches;Bending},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160299&isnumber=10160212

S. Y. Kim and D. J. Braun, "Controllable Mechanical-domain Energy Accumulators," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7359-7364, doi: 10.1109/ICRA48891.2023.10161146.Abstract: Springs are efficient in storing and returning elastic potential energy but are unable to hold the energy they store in the absence of an external load. Lockable springs use clutches to hold elastic potential energy in the absence of an external load, but have not yet been widely adopted in applications, partly because clutches introduce design complexity, reduce energy efficiency, and typically do not afford high fidelity control over the energy stored by the spring. Here, we present the design of a novel lockable compression spring that uses a small capstan clutch to passively lock a mechanical spring. The capstan clutch can lock over 1000 N force at any arbitrary deflection, unlock the spring in less than 10 ms with a control force less than 1% of the maximal spring force, and provide an 80% energy storage and return efficiency (comparable to a highly efficient electric motor operated at constant nominal speed). By retaining the form factor of a regular spring while providing high-fidelity locking capability even under large spring forces, the proposed design could facilitate the development of energy-efficient spring-based actuators and robots. keywords: {Potential energy;Friction;Force;Bidirectional control;Energy efficiency;Timing;Springs},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161146&isnumber=10160212

Z. Lyu and Q. Xu, "Concept Design of a New XY Compliant Parallel Manipulator With Spatial Configuration," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7365-7370, doi: 10.1109/ICRA48891.2023.10161526.Abstract: This paper proposes the concept design of a novel XY compliant parallel manipulator (CPM) with spatial configuration, which is beneficial to promote the performance of the XY CPM. Evolved from a planar configuration, a spatial compliant parallelogram flexure is devised as the basic module structure. Then, a mirror-symmetric XY CPM adopting spatial layout is proposed based on four-prismatic-prismatic (4-PP) parallel mechanism. The prototypes are fabricated by 3D printing for testing. The performance analysis and verification is conducted through theoretical modeling, finite element simulation, and experimental study. For comparison study, a planar XY CPM with similar mechanism is also developed. Results show that the proposed XY CPM with spatial configuration provides the benefits of smaller plane footprint, large working stroke, and enhanced load-bearing capacity as compared to the planar one. It is appropriate for precise positioning scenarios, like soft-contact lithography, which require high loading capacity and great compactness. keywords: {Analytical models;Loading;Lithography;Prototypes;Three-dimensional printing;Manipulators;Finite element analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161526&isnumber=10160212

F. Velasquez, B. Thomaszewski and S. Coros, "Computational Design of 3D-Printable Compliant Mechanisms with Bio-Inspired Sliding Joints," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7371-7377, doi: 10.1109/ICRA48891.2023.10160584.Abstract: We propose a computational approach for designing fully-integrated compliant mechanisms with bio-inspired joints that are stabilized and actuated by elastic elements. Similar to human knees or finger phalanges, our mechanisms leverage sliding between pairs of contacting surfaces to generate complex motions. Due to the vast design space, however, finding surface shapes that lead to ideal approximations of given target motions is a challenging and time-consuming task. To assist users in this process, our computational design tool combines forward and inverse simulation strategies that allow for guided and automated exploration of the parameter space. We demonstrate the potential of our method on a set of compliant mechanism with different joint geometries and validate our simulation results on 3D-printed prototypes. keywords: {Geometry;Manufacturing processes;Shape;Inverse problems;Simulation;Fingers;Prototypes},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160584&isnumber=10160212

M. Dragusanu, D. Troisi, D. Prattichizzo and M. Malvezzi, "Compliant Finger Joint with Controlled Variable Stiffness based on Twisted Strings Actuation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7378-7384, doi: 10.1109/ICRA48891.2023.10160353.Abstract: Underactuated tendon-driven fingers are a simple, yet effective solution, for realizing robotic grippers and hands. The lack of controllable degrees of actuation and precise sensing is compensated by the deformable structure of the finger, which is able to adapt to the objects to be grasped and manipulated, and also to implement grasping strategies based on environmental constraint exploitation. One of the main drawbacks of these robotic fingers is that, due to the limited number of actuators, they can only realize a limited number of movements. Finger closure motion realized by activating the tendon depends on finger mechanical properties, and in particular on elastic joint stiffness. In this paper, we introduce a passive elastic joint to be implemented in monolithic fingers in which the stiffness can be actively regulated by applying a pre-compression to the structure, controlled by a twisted-string actuator (TSA). The paper describes the working principle of the joint, investigates the relationship between pre-compression and flexural stiffness, and finally shows its application to a robotic finger composed of three phalanges. keywords: {Actuators;Automation;Grasping;Robot sensing systems;Mechanical factors;Sensors;Grippers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160353&isnumber=10160212

C. W. Mathews and D. J. Braun, "Design of a Variable Stiffness Spring with Human-Selectable Stiffness," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7385-7390, doi: 10.1109/ICRA48891.2023.10161305.Abstract: Springs are commonly used in wearable robotic devices to provide assistive joint torque without the need for motors and batteries. However, different tasks (such as walking or running) and different users (such as athletes with strong legs or the elderly with weak legs) necessitate different assistive joint torques, and therefore, springs with different stiffness. Variable stiffness springs are a special class of springs which can exert more or less torque upon the same deflection, provided that the user is able to change the stiffness of the spring. In this paper, we present a novel variable stiffness spring design in which the user can select a preferred spring stiffness similar to switching gears on a bicycle. Using a leg-swing experiment, we demonstrate that the user can increment and decrement spring stiffness in a large range to effectively assist the hip joint during leg oscillations. Variable stiffness springs with human-selectable stiffness could be key components of wearable devices which augment locomotion tasks, such as walking, running, and swimming. keywords: {Legged locomotion;Torque;Gears;Wearable computers;Exoskeletons;Bicycles;Switches},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161305&isnumber=10160212

C. A. Dempsey and D. J. Braun, "Novel Spring Mechanism Enables Iterative Energy Accumulation under Force and Deformation Constraints," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7391-7397, doi: 10.1109/ICRA48891.2023.10161577.Abstract: Springs can provide force at zero net energy cost by recycling negative mechanical work to benefit motor-driven robots or spring-augmented humans. However, humans have limited force and range of motion, and motors have a limited ability to produce force. These limits constrain how much energy a conventional spring can store and, consequently, how much assistance a spring can provide. In this paper, we introduce an approach to accumulating negative work in assistive springs over several motion cycles. We show that, by utilizing a novel floating spring mechanism, the weight of a human or robot can be used to iteratively increase spring compression, irrespective of the potential energy stored by the spring. Decoupling the force required to compress a spring from the energy stored by a spring advances prior works, and could enable spring-driven robots and humans to perform physically demanding tasks without the use of large actuators. keywords: {Deformation;Force;Exoskeletons;Prototypes;Iterative methods;Springs;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161577&isnumber=10160212

A. L. Bernhard and J. M. Schimmels, "Fast, Reliable Constrained Manipulation Using a VSA Driven Planar Robot," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7398-7404, doi: 10.1109/ICRA48891.2023.10160318.Abstract: This paper presents the design and performance of a planar 3R robot capable of dexterous constrained manipulation when interacting with a stiff environment. A novel variable stiffness actuator (VSA) having a stiffness ratio of approximately 500 is also described. Variable stiffness actuation, together with a combined position/compliance manipulation path, is used to: 1) allow the robot to passively comply with its environment along kinematically constrained directions despite model error in constraint locations, and 2) generate high stiffness for accurate motion control along kinematically unconstrained directions despite resisting forces. This manipulation strategy provides dexterity for cases in which mechanical work must be performed while complying with constraints. The manipulation strategy and robot performance were evaluated with the task of turning a steel crank to lift a weight. Results show that, when using passive compliance control, the robot completed the task 29 times faster with constraint forces 80% lower than when using traditional active compliance control (with VSAs at their highest stiffness). keywords: {Actuators;Automation;Turning;Time factors;Steel;Reliability;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160318&isnumber=10160212

Z. Hu, A. Ahmed, W. Wan, T. Watanabe and K. Harada, "A Stiffness-Changeable Soft Finger Based on Chain Mail Jamming," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7405-7411, doi: 10.1109/ICRA48891.2023.10161061.Abstract: This paper presents a stiffness-changeable soft finger using chain mail jamming. This finger can achieve adaptive grasping and in-hand manipulation by reshaping and exerting changeable gripping force. The jamming phenomenon happens when particles in a chamber get interlocked where confining pressure is exerted at their boundaries, which is widely used to construct mechanisms with changeable stiffness. Compared with the traditional granular media, chain mail has a lower packing fraction and provides a stronger tensile force. In this paper, we proposed to apply chain mail jamming to the field of robotic finger design. Especially, we propose the design of the finger, the fabrication process, the method of predicting gripping force, and the grasping strategies. The experiments quantitatively verify the model of gripping force prediction. The demonstrations validate the advantages of adaptive grasp by picking a variety of items including foods, goods, and industrial components, and show the application of in-hand manipulation. keywords: {Fabrication;Automation;Fingers;Force;Grasping;Predictive models;Media;Soft finger;Chain mail jamming;Stiffness-changeable mechanism;Grasp;Manipulation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161061&isnumber=10160212

S. Sadachika, M. Kanekiyo, H. Nabae and G. Endo, "Repetitive Twisting Durability of Synthetic Fiber Ropes," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7412-7418, doi: 10.1109/ICRA48891.2023.10160745.Abstract: Synthetic fiber ropes are widely used for robots because of their advantages such as lightweight, high tensile strength and flexibility. However, there is limited information on the physical properties of synthetic fiber ropes when used for robots. This study focuses on repetitive twisting of synthetic fiber ropes and provides information for selecting them for robots based on durability. To this end, we conducted repetitive twisting experiments on five types of ropes made from different fibers; we revealed that Dyneema has higher durability against repetitive twisting than the other ropes when a single rope is twisted. In addition, we conducted experiments on Dyneema by applying torsion to two ropes in parallel like a twisted string actuator. The result indicated that two Dyneema ropes in parallel have higher durability than a single rope; however, we revealed that the the tensile strength decreases sharply with an increase in the angle of twist. keywords: {Heating systems;Actuators;Automation;Synthetic fibers;Loss measurement;Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160745&isnumber=10160212

K. V. Nasonov, D. V. Ivolga, I. I. Borisov and S. A. Kolyubin, "Computational Design of Closed-Chain Linkages: Hopping Robot Driven by Morphological Computation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7419-7425, doi: 10.1109/ICRA48891.2023.10161209.Abstract: The main advantages of legged robots over wheeled ones are their abilities to traverse on uneven terrain due to the use of intermittent contacts and an ability to shift the center of mass relative to the contact location. A robot's leg design can be implemented by using an open-chain mechanism actuated with high-density torque actuators though this solution needs a vast energy budget. An alternative way to design a leg mechanism is the application of morphological computation principle. According to the principle, most of the desired robot's behavior can be delegated to the mechanics with minimum control effort needed to excite, stabilize or augment it. Within this paper, we have proposed a method to synthesize a leg for hopping robots. Due to optimization of mechanical structure, geometric parameters, mass distribution, and elasticity allocation, our method allows getting an energy-efficient robot with minimal control system complexity, which is accomplished via series elastic allocation and active variable length link. Based on this approach, we have designed a hopping robot with two low performance actuators that can achieve hopping, running, and, in the case of a biped or quadruped robot, walking motion. The paper describes a synthesized leg linkage and overviews prototype design, control strategy, and test results of a physical prototype. keywords: {Legged locomotion;Couplings;Actuators;Torque;Prototypes;Energy efficiency;Resource management},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161209&isnumber=10160212

D. H. Salunkhe, D. Chablat and P. Wenger, "Trajectory planning issues in cuspidal commercial robots," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7426-7432, doi: 10.1109/ICRA48891.2023.10161444.Abstract: A cuspidal serial robot can travel from one inverse kinematic solution to another without crossing a singularity. Cuspidal robots ask for extra care and caution in trajectory planning, as identifying an aspect related to one unique inverse kinematic solution is not possible. The issues related to motion planning with cuspidal robots are related to the inherent property arising from the geometric design of the robot. The cuspidality property has not been considered in recent industrial 6R robots with a non-spherical wrist. In this work, cuspidality is illustrated with the JACO robot (gen 2, non-spherical wrist), a serial arm by Kinova Robotics which is deployed in various applications and is cuspidal in nature. A nonsingular change of solutions for the robot is provided to highlight the effect of cuspidal robots on the interference with the environment. The pose with multiple inverse kinematic solutions in an aspect is presented. Problems in choosing the initial solution of the path in cuspidal robots, and its consequence, is illustrated with an example path in the workspace of the JACO robot. The paper presents the importance of cuspidality analysis of 6R robots and the implications of neglecting it. keywords: {Wrist;Automation;Service robots;Trajectory planning;Kinematics;Interference;Manipulators},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161444&isnumber=10160212

N. H. Yim, J. Ryu and Y. Y. Kim, "Big data approach for synthesizing a spatial linkage mechanism," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7433-7439, doi: 10.1109/ICRA48891.2023.10161300.Abstract: This paper presents a novel two-step method for synthesizing spatial linkage mechanisms. Compared with planar mechanisms, the main challenge in synthesizing spatial mechanisms is that the generating motion varies depending on its mechanism topologies. Therefore, we propose a big data approach to determine the topology of spatial mechanisms. We adopt a three-dimensional (3D) spring-connected rigid block model to represent the topology of the spatial mechanism and project 3D motion onto three orthogonal planes to determine the mechanism topology with big data. In addition, a gradient-based dimension synthesis procedure was carried out to determine a detailed dimension using already determined mechanism topology by mechanism big data. Also, several successful case studies by the proposed approach are presented to support the effectiveness of the proposed synthesis method. keywords: {Couplings;Solid modeling;Three-dimensional displays;Costs;Big Data;Data models;Topology},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161300&isnumber=10160212

G. Perry, J. L. G. del Castillo y López and N. Melenbrink, "Croche-Matic: a robot for crocheting 3D cylindrical geometry," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7440-7446, doi: 10.1109/ICRA48891.2023.10160345.Abstract: Crochet is a textile craft that has resisted mech-anization and industrialization except for a select number of one-off crochet machines. These machines are only capable of producing a limited subset of common crochet stitches. Crochet machines are not used in the textile industry, yet mass-produced crochet objects and clothes sold in stores like Target and Zara are almost certainly the products of crochet sweatshops. The popularity of crochet and the existence of crochet products in major chain stores shows that there is both a clear demand for this craft as well as a need for it to be produced in a more ethical way. In this paper, we present Croche-Matic, a radial crochet machine for generating three-dimensional cylindrical geometry. The Croche-Matic is designed based on Magic Ring technique, a method for hand crocheting 3D cylindrical objects. The machine consists of nine mechanical axes that work in sequence to complete different types of crochet stitches, and includes a sensor component for measuring and regulating yarn tension within the mechanical system. Croche-Matic can complete the four main stitches used in Magic Ring technique. It has a success rate of 50.7% with single crochet stitches, and has demonstrated an ability to create three-dimensional objects. keywords: {Geometry;Ethics;Three-dimensional displays;Programming;Robot sensing systems;Mechanical variables measurement;Mechanical systems;Mechanism Design;Product Design;Develop-ment and Prototyping;Domestic Robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160345&isnumber=10160212

V. -N. Tran, Q. -D. Pham, T. -S. Ha, Y. H. Wong and S. -K. Yeung, "A Novel Platform to Control Biofouling in Pearl Oysters Cultivation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7447-7453, doi: 10.1109/ICRA48891.2023.10160471.Abstract: This paper presents a simple yet effective design of a platform to automate the task of shellfish aquaculture, specifically pearl oysters. Compared to traditional methods, our platform can eliminate the tedious task of cleaning the pearl oysters due to fouling. Inspired by the low and high tide characteristics of the intertidal zone, our platform employs an air-water displacement mechanism to periodically float pearl oysters above the water's surface, exposing fouling organisms to air and sunlight. While pearl oysters have developed the ability to stay alive during low tide, these fouling organisms cannot survive after prolonged exposure, thus preventing them from developing. Additionally, the platform provides an alternative approach to grow not only pearl oysters but also various types of shellfish, consequently benefiting the aquaculture industry. We introduce the design of the platform and provide a comprehensive analysis. We also demonstrate the practical deployment of the platform for cultivating pearl oysters. keywords: {Water;Industries;Automation;Organisms;Cleaning;Tides;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160471&isnumber=10160212

J. Berre, L. Rubbert, F. Geiskopf and P. Renaud, "Embedded Active Stiffening Mechanisms to Modulate Kresling Tower Kinetostatic Properties," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7454-7460, doi: 10.1109/ICRA48891.2023.10160882.Abstract: Non-rigidly foldable origamis are of great interest to build robotic components, as they are light, offer large deployability and can also be multistable. In this paper, we consider the Kresling tower, and propose an original way to actively modulate its kinetostatic properties. Actuated stiffening mechanisms are embedded on some folds of the origami. By adjusting the axial stiffness of the folds, modulation of the axial stiffness and the force required to switch between stable configurations are demonstrated. This adjustment can in addition be performed independently from the height of the stable configurations, which makes it simple to use. The interest of fold stiffening is outlined experimentally. Three actuation strategies are considered and implemented. Impact on Kresling tower properties are shown, with complementary performances of pneumatic, SMA-based and DC motor actuation. keywords: {Poles and towers;Force;Modulation;Switches;Kinematics;Bending;DC motors},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160882&isnumber=10160212

Z. Bons, G. C. Thomas, L. M. Mooney and E. J. Rouse, "A Compact, Two-Part Torsion Spring Architecture," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7461-7467, doi: 10.1109/ICRA48891.2023.10161174.Abstract: Springs are essential mechanical elements that are used across a wide variety of industries and mechanisms. Common across many spring types and applications is the importance of compactness, low mass and customizability. In this paper, we present a novel rotary spring design that is lightweight, compact and customizable. In addition, we empirically validate the design by experimentally quantifying the performance of two test springs on a custom dynamometry testbed. Our two-part spring geometry is comprised of a central rotating gear-like cam shaft, and a disk that includes a circular array of radially-spaced tapered cantilevered beams. The two springs that we designed and tested matched desired performance specifications within 3–6%, confirming the efficacy of this unique design approach. keywords: {Industries;Geometry;Automation;Energy efficiency;Structural beams;Springs;Camshafts},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161174&isnumber=10160212

M. Fulton, A. Prabhu and J. Sattar, "HREyes: Design, Development, and Evaluation of a Novel Method for AUVs to Communicate Information and Gaze Direction*," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7468-7475, doi: 10.1109/ICRA48891.2023.10161179.Abstract: We present the design, development, and evaluation of HREyes: biomimetic communication devices which use light to communicate information and, for the first time, gaze direction from AUVs to humans. First, we introduce two types of information displays using the HREye devices: active lucemes and ocular lucemes. Active lucemes communicate information explicitly through animations, while ocular lucemes communicate gaze direction implicitly by mimicking human eyes. We present a human study in which our system is compared to the use of an embedded digital display that explicitly communicates information to a diver by displaying text. Our results demonstrate accurate recognition of active lucemes for trained interactants, limited intuitive understanding of these lucemes for untrained interactants, and relatively accurate perception of gaze direction for all interactants. The results on active luceme recognition demonstrate more accurate recognition than previous light-based communication systems for AUVs (albeit with different phrase sets). Additionally, the ocular lucemes we introduce in this work represent the first method for communicating gaze direction from an AUV, a critical aspect of nonverbal communication used in collabo-rative work. With readily available hardware as well as open-source and easily re-configurable programming, HREyes can be easily integrated into any AUV with the physical space for the devices and used to communicate effectively with divers in any underwater environment with appropriate visibility. keywords: {Automation;Communication systems;Biomimetics;Symbols;Collaboration;Programming;Animation},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161179&isnumber=10160212

R. Liu, Z. Liu, H. Zhang, G. Zhang, Z. Zuo and W. Sheng, "Dense Depth Completion Based on Multi-Scale Confidence and Self-Attention Mechanism for Intestinal Endoscopy," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7476-7482, doi: 10.1109/ICRA48891.2023.10161549.Abstract: Doctors perform limited one-way intestine endoscopy, in which advanced surgical robots with depth sensors, such as stereo and ToF endoscopes, can only provide sparse and incomplete depth information. However, dense, accurate and instant depth estimation during endoscopy is vital for doctors to judge the 3D location and shape of intestinal tissues, which affects the human-robot interaction between doctors and surgical robots, such as the operation on the subsequent moving of the probe. In this paper, we present a deep learning-based dense depth completion method for intestine endoscopy. We utilize the scattered depth information from depth sensors to make up for the deficiency of features in the intestine and design a multi-scale confidence prediction network to extract dense geometric depth features. Then, we introduce the structure awareness module based on the self-attention mechanism in the depth completion network to enhance the geometry and texture features of the intestine. We also present a virtual multi-modal RGBD intestine dataset and conduct comprehensive experiments on a total of three intestine datasets. The experimental results clearly demonstrate that our method achieves better results in all metrics in all intestinal environments compared to state-of-the-art methods. keywords: {Geometry;Intestines;Three-dimensional displays;Medical robotics;Simultaneous localization and mapping;Endoscopes;Shape;Endoscopy;depth completion;self-attention mechanism;human-robot interaction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161549&isnumber=10160212

S. Hjorth, E. Lamon, D. Chrysostomou and A. Ajoudani, "Design of an Energy-Aware Cartesian Impedance Controller for Collaborative Disassembly," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7483-7489, doi: 10.1109/ICRA48891.2023.10160993.Abstract: Human-robot collaborative disassembly is an emerging trend in the sustainable recycling process of electronic and mechanical products. It requires the use of advanced technologies to assist workers in repetitive physical tasks and deal with creaky and potentially damaged components. Nevertheless, when disassembling worn-out or damaged components, unexpected robot behaviors may emerge, so harmless and symbiotic physical interaction with humans and the environment becomes paramount. This work addresses this challenge at the control level by ensuring safe and passive behaviors in unplanned interactions and contact losses. The proposed algorithm capitalizes on an energy-aware Cartesian impedance controller, which features energy scaling and damping injection, and an augmented energy tank, which limits the power flow from the controller to the robot. The controller is evaluated in a real-world flawed unscrewing task with a Franka Emika Panda and is compared to a standard impedance controller and a hybrid force-impedance controller. The results demonstrate the high potential of the algorithm in human-robot collaborative disassembly tasks. keywords: {Symbiosis;Force;Collaboration;Market research;Behavioral sciences;Recycling;Impedance},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160993&isnumber=10160212

S. Sagheb et al., "Towards Robots that Influence Humans over Long-Term Interaction," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7490-7496, doi: 10.1109/ICRA48891.2023.10160321.Abstract: When humans interact with robots influence is inevitable. Consider an autonomous car driving near a human: the speed and steering of the autonomous car will affect how the human drives. Prior works have developed frameworks that enable robots to influence humans towards desired behaviors. But while these approaches are effective in the short-term (i.e., the first few human-robot interactions), here we explore long-term influence (i.e., repeated interactions between the same human and robot). Our central insight is that humans are dynamic: people adapt to robots, and behaviors which are influential now may fall short once the human learns to anticipate the robot's actions. With this insight, we experimentally demonstrate that a prevalent game-theoretic formalism for generating influential robot behaviors becomes less effective over repeated interactions. Next, we propose three modifications to Stackelberg games that make the robot's policy both influential and unpredictable. We finally test these modifications across simulations and user studies: our results suggest that robots which purposely make their actions harder to anticipate are better able to maintain influence over long-term interaction. See videos here: https://youtu.be/ydO83cgjZ2Q keywords: {Adaptation models;Automation;Human-robot interaction;Games;Predictive models;Behavioral sciences;Automobiles},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160321&isnumber=10160212

D. Sirintuna, I. Ozdamar and A. Ajoudani, "Carrying the uncarriable: a deformation-agnostic and human-cooperative framework for unwieldy objects using multiple robots," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7497-7503, doi: 10.1109/ICRA48891.2023.10160677.Abstract: This manuscript introduces an object deformability-agnostic framework for co-carrying tasks that are shared between a person and multiple robots. Our approach allows the full control of the co-carrying trajectories by the person while sharing the load with multiple robots depending on the size and the weight of the object. This is achieved by merging the haptic information transferred through the object and the human motion information obtained from a motion capture system. One important advantage of the framework is that no strict internal communication is required between the robots, regardless of the object size and deformation characteristics. We validate the framework with two challenging real-world scenarios: co-transportation of a wooden rigid closet and a bulky box on top of forklift moving straps, with the latter characterizing deformable objects. In order to evaluate the generalizability of the proposed framework, a heterogenous team of two mobile manipulators that consist of an Omni-directional mobile base and a collaborative robotic arm with different DoFs is chosen for the experiments. The qualitative comparison between our controller and the baseline controller (i.e., an admittance controller) during these experiments demonstrated the effectiveness of the proposed framework especially when co-carrying deformable objects. Furthermore, we believe that the performance of our framework during the experiment with the lifting straps offers a promising solution for the co-transportation of bulky and ungraspable objects. keywords: {Deformation;Collaboration;Transportation;Manipulators;Motion capture;Trajectory;Mobile robots},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160677&isnumber=10160212

L. Rapetti et al., "A Control Approach for Human-Robot Ergonomic Payload Lifting," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7504-7510, doi: 10.1109/ICRA48891.2023.10161454.Abstract: Collaborative robots can relief human operators from excessive efforts during payload lifting activities. Modelling the human partner allows the design of safe and efficient collaborative strategies. In this paper, we present a control approach for human-robot collaboration based on human monitoring through whole-body wearable sensors, and interaction modelling through coupled rigid-body dynamics. Moreover, a trajectory advancement strategy is proposed, allowing for online adaptation of the robot trajectory depending on the human motion. The resulting framework allows us to perform payload lifting tasks, taking into account the ergonomic requirements of the agents. Validation has been performed in an experimental scenario using the iCub3 humanoid robot and a human subject sensorized with the iFeel wearable system. keywords: {Adaptation models;Ergonomics;Collaboration;Humanoid robots;Robot sensing systems;Trajectory;Task analysis},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161454&isnumber=10160212

V. Myers, E. Bıyık and D. Sadigh, "Active Reward Learning from Online Preferences," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7511-7518, doi: 10.1109/ICRA48891.2023.10160439.Abstract: Robot policies need to adapt to human preferences and/or new environments. Human experts may have the domain knowledge required to help robots achieve this adaptation. However, existing works often require costly offline re-training on human feedback, and those feedback usually need to be frequent and too complex for the humans to reliably provide. To avoid placing undue burden on human experts and allow quick adaptation in critical real-world situations, we propose designing and sparingly presenting easy-to-answer pairwise action preference queries in an online fashion. Our approach designs queries and determines when to present them to maximize the expected value derived from the queries' information. We demonstrate our approach with experiments in simulation, human user studies, and real robot experiments. In these settings, our approach outperforms baseline techniques while presenting fewer queries to human experts. Experiment videos, code and appendices are found on our website: http://tinyurl.com/online-active keywords: {Adaptation models;Codes;Automation;Reliability;Robots;Videos},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160439&isnumber=10160212

E. Ballesteros, B. Man and H. H. Asada, "Supernumerary Robotic Limbs for Next Generation Space Suit Technology," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7519-7525, doi: 10.1109/ICRA48891.2023.10161579.Abstract: This paper discusses the incorporation of a pair of Supernumerary Robotic Limbs (SuperLimbs) onto the next generation of NASA space suits. The wearable robots attached to the space suit assist an astronaut in performing Extra-Vehicular Activities (EVAs). The SuperLimbs grab handrails fixed to the outside of a space vehicle to securely hold the astronaut body. The astronaut can use both hands for performing an EVA task, rather than using one hand for securing the body or operating a tether. The SuperLimbs can also assist an astronaut in repositioning the body and stabilizing it during an EVA mission. A control algorithm based on Admittance Control is developed for a) virtually reducing the inertial load of the entire body so that an astronaut can reposition his/her body with reduced effort, and b) bracing the body stably despite reaction forces and disturbances acting on the astronaut during an EVA operation. A full-scale prototype of Space Suit SuperLimbs was constructed and tested. Results from the experimentation indicated that with the aid of SuperLimbs, energy consumption during EVAs is reduced significantly. keywords: {Space vehicles;Space technology;NASA;Wearable robots;Aerospace electronics;Safety;Risk management;Supernumerary Robotic Limbs;Wearable Robotics;Human-Assistive Robotics;Human-Robot Interaction;Spacesuits;Astronaut Ergonomics;Static Bracing;Admittance Control},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161579&isnumber=10160212

E. Ng, Z. Liu and M. Kennedy, "It Takes Two: Learning to Plan for Human-Robot Cooperative Carrying," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7526-7532, doi: 10.1109/ICRA48891.2023.10161386.Abstract: Cooperative table-carrying is a complex task due to the continuous nature of the action and state-spaces, multimodality of strategies, and the need for instantaneous adaptation to other agents. In this work, we present a method for predicting realistic motion plans for cooperative human-robot teams on the task. Using a Variational Recurrent Neural Network (VRNN) to model the variation in the trajectory of a human-robot team across time, we are able to capture the distribution over the team's future states while leveraging information from interaction history. The key to our approach is leveraging human demonstration data to generate trajectories that synergize well with humans during test time in a receding horizon fashion. Comparison between a baseline, sampling-based planner RRT (Rapidly-exploring Random Trees) and the VRNN planner in centralized planning shows that the VRNN generates motion more similar to the distribution of human-human demonstrations than the RRT. Results in a human-in-the-loop user study show that the VRNN planner outperforms decentralized RRT on task-related metrics, and is significantly more likely to be perceived as human than the RRT planner. Finally, we demonstrate the VRNN planner on a real robot paired with a human teleoperating another robot. keywords: {Recurrent neural networks;Navigation;Predictive models;Human in the loop;Trajectory;Planning;Noise measurement},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161386&isnumber=10160212

D. Zurlo, T. Heitmann, M. Morlock and A. De Luca, "Collision Detection and Contact Point Estimation Using Virtual Joint Torque Sensing Applied to a Cobot," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7533-7539, doi: 10.1109/ICRA48891.2023.10160661.Abstract: In physical human-robot interaction (pHRI) it is essential to reliably estimate and localize contact forces between the robot and the environment. In this paper, a complete contact detection, isolation, and reaction scheme is presented and tested on a new 6-dof industrial collaborative robot. We combine two popular methods, based on monitoring energy and generalized momentum, to detect and isolate collisions on the whole robot body in a more robust way. The experimental results show the effectiveness of our implementation on the LARA 5 cobot, that only relies on motor current and joint encoder measurements. For validation purposes, contact forces are also measured using an external GTE CoboSafe sensor. After a successful collision detection, the contact point location is isolated using a combination of the residual method based on the generalized momentum with a contact particle filter (CPF) scheme. We show for the first time a successful implementation of such combination on a real robot, without relying on joint torque sensor measurements. keywords: {Torque;Atmospheric measurements;Service robots;Current measurement;Estimation;Robot sensing systems;Particle measurements},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160661&isnumber=10160212

Q. Zhang, Z. Hu, Y. Song, J. Pei and J. Liu, "The Human Gaze Helps Robots Run Bravely and Efficiently in Crowds," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7540-7546, doi: 10.1109/ICRA48891.2023.10161222.Abstract: In human-aware navigation, the robot tacitly games with humans, balancing safety and efficiency according to human intentions. Poor balance or bad intent recognition causes the robot to stop conservatively or advance rashly, resulting in a deadlock or even a collision respectively. To address the issue, this paper proposes an improved limit cycle for collaboratively parameterizing human intentions and planning robot motions. The human-robot interaction is modeled as a dynamic chicken game with incomplete information, where the human gaze is introduced to depict the unique characteristics of each person, allowing the robot to approach with different safety margins. Our method is tested in challenging indoor scenarios and outperforms traditional methods in both safety and efficiency. We enable robots to utilize human wisdom to solve problems that cannot be solved on their own. The robot bravely goes through oncoming crowds by getting closer to people with higher attention on it and has the foresight to stably cross in front or behind people. keywords: {Robot motion;Navigation;Limit-cycles;Human-robot interaction;Games;System recovery;Safety},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161222&isnumber=10160212

J. D. P. Prada, M. H. Lee and C. Song, "A Gaze-Speech System in Mixed Reality for Human-Robot Interaction," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7547-7553, doi: 10.1109/ICRA48891.2023.10161010.Abstract: Human-robot interaction (HRI) demands efficient time performance along the tasks. However, some interaction approaches may extend the time to complete such tasks. Thus, the time performance in HRI must be enhanced. This work presents an effective way to enhance the time performance in HRI tasks with a mixed reality (MR) method based on a gaze-speech system. In this paper, we design an MR world for pick-and-place tasks. The hardware system includes an MR headset, the Baxter robot, a table, and six cubes. In addition, the holographic MR scenario offers two modes of interaction: gesture mode (GM) and gaze-speech mode (GSM). The input actions during the GM and GSM methods are based on the pinch gesture and gaze with speech commands, respectively. The proposed GSM approach can improve the time performance in pick-and-place scenarios. The GSM system is 21.33 % faster than the traditional system, GM. Also, we evaluated the target- to-target time performance against a reference based on Fitts' law. Our findings show a promising method for time reduction in HRI tasks through MR environments. keywords: {GSM;Surveys;System performance;Human-robot interaction;Mixed reality;Virtual reality;Sensor fusion},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161010&isnumber=10160212

B. Jin et al., "ADAPT: Action-aware Driving Caption Transformer," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7554-7561, doi: 10.1109/ICRA48891.2023.10160326.Abstract: End-to-end autonomous driving has great potential in the transportation industry. However, the lack of transparency and interpretability of the automatic decision-making process hinders its industrial adoption in practice. There have been some early attempts to use attention maps or cost volume for better model explainability which is difficult for ordinary passengers to understand. To bridge the gap, we propose an end-to-end transformer-based architecture, ADAPT (Action-aware Driving cAPtion Transformer), which provides user-friendly natural language narrations and reasoning for each decision making step of autonomous vehicular control and action. ADAPT jointly trains both the driving caption task and the vehicular control prediction task, through a shared video representation. Experiments on BDD-X (Berkeley DeepDrive eXplanation) dataset demonstrate state-of-the-art performance of the ADAPT framework on both automatic metrics and human evaluation. To illustrate the feasibility of the proposed framework in real-world applications, we build a novel deployable system that takes raw car videos as input and outputs the action narrations and reasoning in real time. The code, models and data are available at https://github.com/jxbbb/ADAPT. keywords: {Measurement;Training;Adaptation models;Transportation industry;Decision making;Streaming media;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160326&isnumber=10160212

D. Marta, S. Holk, C. Pek, J. Tumova and I. Leite, "Aligning Human Preferences with Baseline Objectives in Reinforcement Learning," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7562-7568, doi: 10.1109/ICRA48891.2023.10161261.Abstract: Practical implementations of deep reinforcement learning (deep RL) have been challenging due to an amplitude of factors, such as designing reward functions that cover every possible interaction. To address the heavy burden of robot reward engineering, we aim to leverage subjective human preferences gathered in the context of human-robot interaction, while taking advantage of a baseline reward function when available. By considering baseline objectives to be designed beforehand, we are able to narrow down the policy space, solely requesting human attention when their input matters the most. To allow for control over the optimization of different objectives, our approach contemplates a multi-objective setting. We achieve human-compliant policies by sequentially training an optimal policy from a baseline specification and collecting queries on pairs of trajectories. These policies are obtained by training a reward estimator to generate Pareto optimal policies that include human preferred behaviours. Our approach ensures sample efficiency and we conducted a user study to collect real human preferences, which we utilized to obtain a policy on a social navigation environment. keywords: {Training;Measurement;Navigation;Human-robot interaction;Reinforcement learning;Predictive models;Pareto optimization},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161261&isnumber=10160212

V. Narayanan, B. M. Manoghar, R. P. RV and A. Bera, "EWareNet: Emotion-Aware Pedestrian Intent Prediction and Adaptive Spatial Profile Fusion for Social Robot Navigation," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7569-7575, doi: 10.1109/ICRA48891.2023.10161504.Abstract: We present EWareNet, a novel intent and affect-aware social robot navigation algorithm among pedestrians. Our approach predicts the trajectory-based pedestrian intent from gait sequence, which is then used for intent-guided navigation taking into account social and proxemic constraints. We propose a transformer-based model that works on commodity RGB-D cameras mounted onto a moving robot. Our intent prediction routine is integrated into a mapless navigation scheme and makes no assumptions about the environment of pedestrian motion. Our navigation scheme consists of a novel obstacle profile representation methodology that is dynamically adjusted based on the pedestrian pose, intent, and affect. The navigation scheme is based on a reinforcement learning algorithm that takes pedestrian intent and robot's impact on pedestrian intent into consideration, in addition to the environmental configuration. We outperform current state-of-art algorithms for intent prediction from 3D gaits. keywords: {Pedestrians;Three-dimensional displays;Navigation;Heuristic algorithms;Social robots;Robot vision systems;Reinforcement learning},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161504&isnumber=10160212

J. Oh et al., "SCAN: Socially-Aware Navigation Using Monte Carlo Tree Search," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7576-7582, doi: 10.1109/ICRA48891.2023.10160270.Abstract: Designing a socially-aware navigation method for crowded environments has become a critical issue in robotics. In order to perform navigation in a crowded environment without causing discomfort to nearby pedestrians, it is necessary to design a global planner that is able to consider both human-robot interaction (HRI) and prediction of future states. In this paper, we propose a socially-aware global planner called SCAN, which is a global planner that generates appropriate local goals considering HRI and prediction of future states. Our method simulates future states considering the effects of the robot's actions on the future intentions of pedestrians using Monte Carlo tree search (MCTS), which estimates the quality of local goals. For fast simulation, we execute pedestrian motion prediction using Y-net and future state simulation using MCTS in parallel. Neural networks are only used in Y-net and not in MCTS, which enables fast simulation and prediction of a long horizon of future states. We evaluate the proposed method based on the proposed socially-aware navigation metric using realistic pedestrian simulation and real-world experiments. The results show that the proposed method outperforms existing methods significantly, indicating the importance of considering human-robot interaction for socially-aware navigation. keywords: {Measurement;Pedestrians;Monte Carlo methods;Navigation;Neural networks;Human-robot interaction;Predictive models},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160270&isnumber=10160212

S. Chan, W. Wang, Z. Shao and C. Bai, "SGPT: The Secondary Path Guides the Primary Path in Transformers for HOI Detection," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7583-7590, doi: 10.1109/ICRA48891.2023.10160329.Abstract: HOI detection is essential for human-computer interaction, especially in behavior detection and robot manipulation. Existing mainstream transformer methods of HOI detection are focused on single-stream detection only, e.g., $image \rightarrow HOI(\mathcal{P}_{1})$, or $image \rightarrow HO\rightarrow I(\mathcal{P}_{2})$. Both paths have their own characteristics of concern, so we propose a novel method, using the Secondary path $(\mathcal{P}_{2})$ Guides the Primary path $(\mathcal{P}_{1})$ in Transformers (SGPT). SGPT contains two core modules: the Dual-Path Consistency (DPC) module and the Instance Interaction Attention (IIA) module. DPC keeps human, object and interaction consistent on the dual-path and lets $\mathcal{P}_{2}$ guide $\mathcal{P}_{1}$ to learn more meaningful features. IIA fuses human and object to enhance interaction in $\mathcal{P}_{2}$, which allows instance to constrain interaction. Our proposed dual-path are employed during training, and only the $\mathcal{P}_{1}$ path is used for inference. Hence, SGPT improves generalization without increasing model capacity in HICO-DET and V-COCO datasets compared to the state-of-the-arts. The code of this work is available at https://github.com/visualVk/sgpt.git. keywords: {Training;Human computer interaction;Codes;Automation;Fuses;Transformer cores;Transformers},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160329&isnumber=10160212

H. Ye, J. Zhao, Y. Pan, W. Cherr, L. He and H. Zhang, "Robot Person Following Under Partial Occlusion," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7591-7597, doi: 10.1109/ICRA48891.2023.10160738.Abstract: Robot person following (RPF) is a capability that supports many useful human-robot-interaction (HRI) applications. However, existing solutions to person following often as-sume full observation of the tracked person. As a consequence, they cannot track the person reliably under partial occlusion where the assumption of full observation is not satisfied. In this paper, we focus on the problem of robot person following under partial occlusion caused by a limited field of view of a monocular camera. Based on the key insight that it is possible to locate the target person when one or more of hislher joints are visible, we propose a method in which each visible joint contributes a location estimate of the followed person. Experiments on a public person-following dataset show that, even under partial occlusion, the proposed method can still locate the person more reliably than the existing SOTA methods. As well, the application of our method is demonstrated in real experiments on a mobile robot. keywords: {Measurement;Target tracking;Automation;Robot vision systems;Human-robot interaction;Cameras;Neck},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160738&isnumber=10160212

M. Eisenbach, J. Lübberstedt, D. Aganian and H. -M. Gross, "A Little Bit Attention Is All You Need for Person Re-Identification," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7598-7605, doi: 10.1109/ICRA48891.2023.10160304.Abstract: Person re-identification plays a key role in applications where a mobile robot needs to track its users over a long period of time, even if they are partially unobserved for some time, in order to follow them or be available on demand. In this context, deep-learning-based real-time feature extraction on a mobile robot is often performed on special-purpose devices whose computational resources are shared for multiple tasks. Therefore, the inference speed has to be taken into account. In contrast, person re-identification is often improved by architectural changes that come at the cost of significantly slowing down inference. Attention blocks are one such example. We will show that some well-performing attention blocks used in the state of the art are subject to inference costs that are far too high to justify their use for mobile robotic applications. As a consequence, we propose an attention block that only slightly affects the inference speed while keeping up with much deeper networks or more complex attention blocks in terms of re-identification accuracy. We perform extensive neural architecture search to derive rules at which locations this attention block should be integrated into the architecture in order to achieve the best trade-off between speed and accuracy. Finally, we confirm that the best performing configuration on a re-identification benchmark also performs well on an indoor robotic dataset. keywords: {Performance evaluation;Costs;Transfer learning;Computer architecture;Benchmark testing;Performance gain;Feature extraction},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160304&isnumber=10160212

B. Tang, R. Cao, R. Chen, X. Chen, B. Hua and F. Wu, "Automatic Generation of Robot Facial Expressions with Preferences," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7606-7613, doi: 10.1109/ICRA48891.2023.10160409.Abstract: The capability of humanoid robots to generate facial expressions is crucial for enhancing interactivity and emotional resonance in human-robot interaction. However, humanoid robots vary in mechanics, manufacturing, and ap-pearance. The lack of consistent processing techniques and the complexity of generating facial expressions pose significant challenges in the field. To acquire solutions with high confidence, it is necessary to enable robots to explore the solution space automatically based on performance feedback. To this end, we designed a physical robot with a human-like appearance and developed a general framework for automatic expression generation using the MAP-Elites algorithm. The main advan-tage of our framework is that it does not only generate facial expressions automatically but can also be customized according to user preferences. The experimental results demonstrate that our framework can efficiently generate realistic facial expressions without hard coding or prior knowledge of the robot kinematics. Moreover, it can guide the solution-generation process in accordance with user preferences, which is desirable in many real-world applications. keywords: {Automation;Robot kinematics;Humanoid robots;Human-robot interaction;Encoding;Space exploration;Manufacturing},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160409&isnumber=10160212

M. Lippi, P. Di Lillo and A. Marino, "A Task Allocation Framework for Human Multi-Robot Collaborative Settings," 2023 IEEE International Conference on Robotics and Automation (ICRA), London, United Kingdom, 2023, pp. 7614-7620, doi: 10.1109/ICRA48891.2023.10161458.Abstract: The requirements of modern production systems together with more advanced robotic technologies have fostered the integration of teams comprising humans and autonomous robots. While this integration has the potential to provide various benefits, it also raises questions about how to effectively manage these teams, taking into account the different characteristics of the agents involved. This paper presents a framework for task allocation in a human multi-robot collaborative scenario. The proposed solution combines an optimal offline allocation with an online reallocation strategy which accounts for inaccuracies of the offline plan and/or unforeseen events, human subjective preferences and cost of task switching. Experiments with two manipulators cooperating with a human operator in a box filling task are presented. keywords: {Production systems;Costs;Collaboration;Switches;Manipulators;Filling;Behavioral sciences},URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161458&isnumber=10160212

